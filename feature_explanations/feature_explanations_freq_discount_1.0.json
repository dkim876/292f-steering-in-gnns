{
  "2": {
    "explanation": "distributed consensus and control in multi-agent systems under noise and failures",
    "topk": [
      {
        "node_idx": 45381,
        "score_0_10": 10,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 111246,
        "score_0_10": 10,
        "title": "consensus of multi agent systems with general linear and lipschitz nonlinear dynamics using distributed adaptive protocols",
        "abstract": "This technical brief considers the distributed consensus problems for multi-agent systems with general linear and Lipschitz nonlinear dynamics. Distributed relative-state consensus protocols with an adaptive law for adjusting the coupling weights between neighboring agents are designed for both the linear and nonlinear cases, under which consensus is reached for all undirected connected communication graphs. Extensions to the case with a leader-follower communication graph are further studied. In contrast to the existing results in the literature, the adaptive consensus protocols here can be implemented by each agent in a fully distributed fashion without using any global information."
      },
      {
        "node_idx": 43544,
        "score_0_10": 9,
        "title": "decentralized abstractions for feedback interconnected multi agent systems",
        "abstract": "The purpose of this report is to define abstractions for multi-agent systems under coupled constraints. In the proposed decentralized framework, we specify a finite or countable transition system for each agent which only takes into account the discrete positions of its neighbors. The dynamics of the considered systems consist of two components. An appropriate feedback law which guarantees that certain performance requirements (eg. connectivity) are preserved and induces the coupled constraints and additional free inputs which we exploit in order to accomplish high level tasks. In this work we provide sufficient conditions on the space and time discretization of the system which ensure that we can extract a well posed and hence meaningful finite transition system."
      },
      {
        "node_idx": 27774,
        "score_0_10": 9,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 106315,
        "score_0_10": 9,
        "title": "soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor",
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
      },
      {
        "node_idx": 155507,
        "score_0_10": 8,
        "title": "cooperative event based rigid formation control",
        "abstract": "This paper discusses cooperative stabilization control of rigid formations via an event-based approach. We first design a centralized event-based formation control system, in which a central event controller determines the next triggering time and broadcasts the event signal to all the agents for control input update. We then build on this approach to propose a distributed event control strategy, in which each agent can use its local event trigger and local information to update the control input at its own event time. For both cases, the triggering condition, event function and triggering behavior are discussed in detail, and the exponential convergence of the event-based formation system is guaranteed."
      },
      {
        "node_idx": 84316,
        "score_0_10": 8,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 41251,
        "score_0_10": 8,
        "title": "structural controllability of multi agent networks robustness against simultaneous failures",
        "abstract": "In this paper, structural controllability of a leader-follower multi-agent system with multiple leaders is studied from a graph-theoretic point of view. The problem of preservation of structural controllability under simultaneous failures in both the communication links and the agents is investigated. The effects of the loss of agents and communication links on the controllability of an information flow graph are previously studied. In this work, the corresponding results are exploited to introduce some useful indices and importance measures that help characterize and quantify the role of individual links and agents in the controllability of the overall network. Existing results are then extended by considering the effects of losses in both links and agents at the same time. To this end, the concepts of joint (r,s)-controllability and joint t-controllability are introduced as quantitative measures of reliability for a multi-agent system, and their important properties are investigated. Lastly, the class of jointly critical digraphs is introduced, and it is stated that if a digraph is jointly critical, then joint t-controllability is a necessary and sufficient condition for remaining controllable following the failure of any set of links and agents, with cardinality less than t. Various examples are exploited throughout the paper to elaborate on the analytical findings."
      },
      {
        "node_idx": 131346,
        "score_0_10": 8,
        "title": "robust consensus for multi agent systems communicating over stochastic uncertain networks",
        "abstract": "In this paper, we study the robust consensus problem for a set of discrete-time linear agents to coordinate over an uncertain communication network, which is to achieve consensus against the transmission errors and noises resulted from the information exchange between the agents. We model the network by means of communication links subject to multiplicative stochastic uncertainties, which are susceptible to describing packet dropout, random delay, and fading phenomena. Different communication topologies, such as undirected graphs and leader-follower graphs, are considered. We derive sufficient conditions for robust consensus in the mean square sense. This results unveil intrinsic constraints on consensus attainment imposed by the network synchronizability, the unstable agent dynamics, and the channel uncertainty variances. Consensus protocols are designed based on the state information transmitted over the uncertain channels, by solving a modified algebraic Riccati equation."
      },
      {
        "node_idx": 117327,
        "score_0_10": 8,
        "title": "multi agent consensus with relative state dependent measurement noises",
        "abstract": "In this note, the distributed consensus corrupted by relative-state-dependent measurement noises is considered. Each agent can measure or receive its neighbors' state information with random noises, whose intensity is a vector function of agents' relative states. By investigating the structure of this interaction and the tools of stochastic differential equations, we develop several small consensus gain theorems to give sufficient conditions in terms of the control gain, the number of agents and the noise intensity function to ensure mean square (m. s.) and almost sure (a. s.) consensus and quantify the convergence rate and the steady-state error. Especially, for the case with homogeneous communication and control channels, a necessary and sufficient condition to ensure m. s. consensus on the control gain is given and it is shown that the control gain is independent of the specific network topology, but only depends on the number of nodes and the noise coefficient constant. For symmetric measurement models, the almost sure convergence rate is estimated by the Iterated Logarithm Law of Brownian motions."
      }
    ]
  },
  "3": {
    "explanation": "coding theory and maximal codes in monoids",
    "topk": [
      {
        "node_idx": 80899,
        "score_0_10": 10,
        "title": "monoids and maximal codes",
        "abstract": "In recent years codes that are not Uniquely Decipherable (UD) are been studied partitioning them in classes that localize the ambiguities of the code. A natural question is how we can extend the notion of maximality to codes that are not UD. In this paper we give an answer to this question. To do this we introduce a partial order in the set of submonoids of a monoid showing the existence, in this poset, of maximal elements that we call full monoids. Then a set of generators of a full monoid is, by definition, a maximal code. We show how this definition extends, in a natural way, the existing definition concerning UD codes and we find a characteristic property of a monoid generated by a maximal UD code."
      },
      {
        "node_idx": 56746,
        "score_0_10": 9,
        "title": "non prefix free codes for constrained sequences",
        "abstract": "In this paper we consider the use of variable length non prefix-free codes for coding constrained sequences of symbols. We suppose to have a Markov source where some state transitions are impossible, i.e. the stochastic matrix associated with the Markov chain has some null entries. We show that classic Kraft inequality is not a necessary condition, in general, for unique decodability under the above hypothesis and we propose a relaxed necessary inequality condition. This allows, in some cases, the use of non prefix-free codes that can give very good performance, both in terms of compression and computational efficiency. Some considerations are made on the relation between the proposed approach and other existing coding paradigms."
      },
      {
        "node_idx": 76222,
        "score_0_10": 9,
        "title": "elsa efficient long term secure storage of large datasets",
        "abstract": "An increasing amount of information today is generated, exchanged, and stored digitally. This also includes long-lived and highly sensitive information (e.g., electronic health records, governmental documents) whose integrity and confidentiality must be protected over decades or even centuries. While there is a vast amount of cryptography-based data protection schemes, only few are designed for long-term protection. Recently, Braun et al. (AsiaCCS'17) proposed the first long-term protection scheme that provides renewable integrity protection and information-theoretic confidentiality protection. However, computation and storage costs of their scheme increase significantly with the number of stored data items. As a result, their scheme appears suitable only for protecting databases with a small number of relatively large data items, but unsuitable for databases that hold a large number of relatively small data items (e.g., medical record databases). In this work, we present a solution for efficient long-term integrity and confidentiality protection of large datasets consisting of relatively small data items. First, we construct a renewable vector commitment scheme that is information-theoretically hiding under selective decommitment. We then combine this scheme with renewable timestamps and information-theoretically secure secret sharing. The resulting solution requires only a single timestamp for protecting a dataset while the state of the art requires a number of timestamps linear in the number of data items. We implemented our solution and measured its performance in a scenario where 12 000 data items are aggregated, stored, protected, and verified over a time span of 100 years. Our measurements show that our new solution completes this evaluation scenario an order of magnitude faster than the state of the art."
      },
      {
        "node_idx": 152568,
        "score_0_10": 9,
        "title": "church rosser systems codes with bounded synchronization delay and local rees extensions",
        "abstract": "What is the common link, if there is any, between Church-Rosser systems, prefix codes with bounded synchronization delay, and local Rees extensions? The first obvious answer is that each of these notions relates to topics of interest for WORDS: Church-Rosser systems are certain rewriting systems over words, codes are given by sets of words which form a basis of a free submonoid in the free monoid of all words (over a given alphabet) and local Rees extensions provide structural insight into regular languages over words. So, it seems to be a legitimate title for an extended abstract presented at the conference WORDS 2017. However, this work is more ambitious, it outlines some less obvious but much more interesting link between these topics. This link is based on a structure theory of finite monoids with varieties of groups and the concept of local divisors playing a prominent role. Parts of this work appeared in a similar form in conference proceedings where proofs and further material can be found."
      },
      {
        "node_idx": 105448,
        "score_0_10": 8,
        "title": "network coding for distributed storage systems",
        "abstract": "Distributed storage systems provide reliable access to data through redundancy spread over individually unreliable nodes. Application scenarios include data centers, peer-to-peer storage systems, and storage in wireless networks. Storing data using an erasure code, in fragments spread across nodes, requires less redundancy than simple replication for the same level of reliability. However, since fragments must be periodically replaced as nodes fail, a key question is how to generate encoded fragments in a distributed way while transferring as little data as possible across the network. #R##N#For an erasure coded system, a common practice to repair from a node failure is for a new node to download subsets of data stored at a number of surviving nodes, reconstruct a lost coded block using the downloaded data, and store it at the new node. We show that this procedure is sub-optimal. We introduce the notion of regenerating codes, which allow a new node to download \\emph{functions} of the stored data from the surviving nodes. We show that regenerating codes can significantly reduce the repair bandwidth. Further, we show that there is a fundamental tradeoff between storage and repair bandwidth which we theoretically characterize using flow arguments on an appropriately constructed graph. By invoking constructive results in network coding, we introduce regenerating codes that can achieve any point in this optimal tradeoff."
      },
      {
        "node_idx": 57558,
        "score_0_10": 8,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 52884,
        "score_0_10": 8,
        "title": "regular languages are church rosser congruential",
        "abstract": "This paper proves a long standing conjecture in formal language theory. It shows that all regular languages are Church-Rosser congruential. The class of Church-Rosser congruential languages was introduced by McNaughton, Narendran, and Otto in 1988. A language L is Church-Rosser congruential, if there exists a finite confluent, and length-reducing semi-Thue system S such that L is a finite union of congruence classes modulo S. It was known that there are deterministic linear context-free languages which are not Church-Rosser congruential, but on the other hand it was strongly believed that all regular language are of this form. Actually, this paper proves a more general result."
      },
      {
        "node_idx": 41252,
        "score_0_10": 8,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 26928,
        "score_0_10": 8,
        "title": "propyla privacy preserving long term secure storage",
        "abstract": "The amount of electronically stored information increases rapidly. Sensitive information requires integrity and confidentiality protection, sometimes for decades or even centuries (e.g., health records or governmental documents). Commonly used cryptographic schemes, however, are not designed to provide protection over such long time periods. Their security usually relies on the hardness of a specific computational problem and security cannot be maintained against unforeseeable developments in computational technology (e.g., quantum computers breaking RSA-based systems). Recently, Braun et al.\\ (\\mboxAsiaCCS'17 ) proposed the first storage architecture that supports integrity protection renewal while guaranteeing information theoretic confidentiality. However, their solution only considers the storage of unstructured data and does not allow for reading or writing subparts of the data.    Our contribution is the first long-term secure storage architecture that supports storage of structured databases and provides long-term integrity, confidentiality, and access pattern hiding security. To achieve this, we combine several cryptographic components (i.e., secret sharing, renewable timestamps, and renewable commitments) with an information-theoretically secure \\mboxORAM such that the described security properties are achieved. We also prove our construction secure and show that it only introduces a small overhead compared to standard secret sharing and ORAM based storage solutions."
      },
      {
        "node_idx": 160260,
        "score_0_10": 8,
        "title": "new classes of permutation binomials and permutation trinomials over finite fields",
        "abstract": "Permutation polynomials over finite fields play important roles in finite fields theory. They also have wide applications in many areas of science and engineering such as coding theory, cryptography, combinatorial design, communication theory and so on. Permutation binomials and trinomials attract people's interest due to their simple algebraic form and additional extraordinary properties. In this paper, several new classes of permutation binomials and permutation trinomials are constructed. Some of these permutation polynomials are generalizations of known ones."
      }
    ]
  },
  "4": {
    "explanation": "high-performance parallel solvers and optimization for finite element PDEs",
    "topk": [
      {
        "node_idx": 133245,
        "score_0_10": 10,
        "title": "matrix free gpu implementation of a preconditioned conjugate gradient solver for anisotropic elliptic pdes",
        "abstract": "Many problems in geophysical and atmospheric modelling require the fast solution of elliptic partial differential equations (PDEs) in \"flat\" three dimensional geometries. In particular, an anisotropic elliptic PDE for the pressure correction has to be solved at every time step in the dynamical core of many numerical weather prediction models, and equations of a very similar structure arise in global ocean models, subsurface flow simulations and gas and oil reservoir modelling. The elliptic solve is often the bottleneck of the forecast, and an algorithmically optimal method has to be used and implemented efficiently. Graphics Processing Units have been shown to be highly efficient for a wide range of applications in scientific computing, and recently iterative solvers have been parallelised on these architectures. We describe the GPU implementation and optimisation of a Preconditioned Conjugate Gradient (PCG) algorithm for the solution of a three dimensional anisotropic elliptic PDE for the pressure correction in NWP. Our implementation exploits the strong vertical anisotropy of the elliptic operator in the construction of a suitable preconditioner. As the algorithm is memory bound, performance can be improved significantly by reducing the amount of global memory access. We achieve this by using a matrix-free implementation which does not require explicit storage of the matrix and instead recalculates the local stencil. Global memory access can also be reduced by rewriting the algorithm using loop fusion and we show that this further reduces the runtime on the GPU. We demonstrate the performance of our matrix-free GPU code by comparing it to a sequential CPU implementation and to a matrix-explicit GPU code which uses existing libraries. The absolute performance of the algorithm for different problem sizes is quantified in terms of floating point throughput and global memory bandwidth."
      },
      {
        "node_idx": 37357,
        "score_0_10": 10,
        "title": "robust multigrid for high order discontinuous galerkin methods",
        "abstract": "We present a polynomial multigrid method for nodal interior penalty and local discontinuous Galerkin formulations of the Poisson equation on Cartesian grids. For smoothing we propose two classes of overlapping Schwarz methods. The first class comprises element-centered and the second face-centered methods. Within both classes we identify methods that achieve superior convergence rates, prove robust with respect to the mesh spacing and the polynomial order, at least up to P=32. Consequent structure exploitation yields a computational complexity of O(PN), where N is the number of unknowns. Further we demonstrate the suitability of the face-centered method for element aspect ratios up to 32. P-multigrid for discontinuous formulations of Poisson problems on Cartesian grids.Robust with respect to the mesh size and polynomial order, at least up to P=32.Convergence rates between 0.02 and 0.003, improving with increasing P.Computational complexity of PN for N unknowns, linear complexity in runtime.Excellent performance up to aspect ratios of 1:16."
      },
      {
        "node_idx": 160422,
        "score_0_10": 10,
        "title": "high level implementation of geometric multigrid solvers for finite element problems applications in atmospheric modelling",
        "abstract": "The implementation of efficient multigrid preconditioners for elliptic partial differential equations (PDEs) is a challenge due to the complexity of the resulting algorithms and corresponding computer code. For sophisticated (mixed) finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in-depth knowledge of the mathematical theory, parallel computing and optimisation techniques on manycore CPUs. In this paper we show how the development of bespoke multigrid preconditioners can be simplified significantly by using a framework which allows the expression of the each component of the algorithm at the correct abstraction level. Our approach (1) allows the expression of the finite element problem in a language which is close to the mathematical formulation of the problem, (2) guarantees the automatic generation and efficient execution of parallel optimised low-level computer code and (3) is flexible enough to support different abstraction levels and give the programmer control over details of the preconditioner. We use the composable abstractions of the Firedrake/PyOP2 package to demonstrate the efficiency of this approach for the solution of strongly anisotropic PDEs in atmospheric modelling. The weak formulation of the PDE is expressed in Unified Form Language (UFL) and the lower PyOP2 abstraction layer allows the manual design of computational kernels for a bespoke geometric multigrid preconditioner. We compare the performance of this preconditioner to a single-level method and hypre's BoomerAMG algorithm. The Firedrake/PyOP2 code is inherently parallel and we present a detailed performance analysis for a single node (24 cores) on the ARCHER supercomputer. Our implementation utilises a significant fraction of the available memory bandwidth and shows very good weak scaling on up to 6,144 compute cores."
      },
      {
        "node_idx": 80698,
        "score_0_10": 9,
        "title": "accurate iteration free mixed stabilised formulation for laminar incompressible navier stokes applications to fluid structure interaction",
        "abstract": "Stabilised mixed velocity-pressure formulations are one of the widely-used finite element schemes for computing the numerical solutions of laminar incompressible Navier-Stokes. In these formulations, the Newton-Raphson scheme is employed to solve the nonlinearity in the convection term. One fundamental issue with this approach is the computational cost incurred in the Newton-Raphson iterations at every load/time step. In this paper, we present an iteration-free mixed-stabilised finite element formulation for incompressible Navier-Stokes that preserves second-order temporal accuracy for both velocity and pressure fields. We prove the second-order temporal accuracy using an example with a manufactured solution. We then illustrate the accuracy and the computational benefits of the proposed scheme by studying the benchmark example of flow past a fixed circular cylinder and then using two benchmark examples in fluid-flexible structure interaction."
      },
      {
        "node_idx": 16792,
        "score_0_10": 9,
        "title": "extreme scale multigrid components within petsc",
        "abstract": "Elliptic partial differential equations (PDEs) frequently arise in continuum descriptions of physical processes relevant to science and engineering. Multilevel preconditioners represent a family of scalable techniques for solving discrete PDEs of this type and thus are the method of choice for high-resolution simulations. The scalability and time-to-solution of massively parallel multilevel preconditioners can be adversely effected by using a coarse-level solver with sub-optimal algorithmic complexity. To maintain scalability, agglomeration techniques applied to the coarse level have been shown to be necessary. #R##N#In this work, we present a new software component introduced within the Portable Extensible Toolkit for Scientific computation (PETSc) which permits agglomeration. We provide an overview of the design and implementation of this functionality, together with several use cases highlighting the benefits of agglomeration. Lastly, we demonstrate via numerical experiments employing geometric multigrid with structured meshes, the flexibility and performance gains possible using our MPI-rank agglomeration implementation."
      },
      {
        "node_idx": 77067,
        "score_0_10": 9,
        "title": "a stencil based implementation of parareal in the c domain specific embedded language stella",
        "abstract": "In view of the rapid rise of the number of cores in modern supercomputers, time-parallel methods that introduce concurrency along the temporal axis are becoming increasingly popular. For the solution of time-dependent partial differential equations, these methods can add another direction for concurrency on top of spatial parallelization. The paper presents an implementation of the time-parallel Parareal method in a C++ domain specific language for stencil computations (STELLA). STELLA provides both an OpenMP and a CUDA backend for a shared memory parallelization, using the CPU or GPU inside a node for the spatial stencils. Here, we intertwine this node-wise spatial parallelism with the time-parallel Parareal. This is done by adding an MPI-based implementation of Parareal, which allows us to parallelize in time across nodes. The performance of Parareal with both backends is analyzed in terms of speedup, parallel efficiency and energy-to-solution for an advection-diffusion problem with a time-dependent diffusion coefficient."
      },
      {
        "node_idx": 152299,
        "score_0_10": 9,
        "title": "explicit parallel in time integration of a linear acoustic advection system",
        "abstract": "The applicability of the Parareal parallel-in-time integration scheme for the solution of a linear, two-dimensional hyperbolic acoustic-advection system, which is often used as a test case for integration schemes for numerical weather prediction (NWP), is addressed. Parallel-in-time schemes are a possible way to increase, on the algorithmic level, the amount of parallelism, a requirement arising from the rapidly growing number of CPUs in high performance computer systems. A recently introduced modification of the \u201cparallel implicit time-integration algorithm\u201d could successfully solve hyperbolic problems arising in structural dynamics. It has later been cast into the framework of Parareal. The present paper adapts this modified Parareal and employs it for the solution of a hyperbolic flow problem, where the initial value problem solved in parallel arises from the spatial discretization of a partial differential equation by a finite difference method. It is demonstrated that the modified Parareal is stable and can produce reasonably accurate solutions while allowing for a noticeable reduction of the time-to-solution. The implementation relies on integration schemes already widely used in NWP (RK-3, partially split forward Euler, forward\u2013backward). It is demonstrated that using an explicit partially split scheme for the coarse integrator allows to avoid the use of an implicit scheme while still achieving speedup."
      },
      {
        "node_idx": 50337,
        "score_0_10": 9,
        "title": "a scalable parallel finite element framework for growing geometries application to metal additive manufacturing",
        "abstract": "This work introduces an innovative parallel, fully-distributed finite element framework for growing geometries and its application to metal additive manufacturing. It is well-known that virtual part design and qualification in additive manufacturing requires highly-accurate multiscale and multiphysics analyses. Only high performance computing tools are able to handle such complexity in time frames compatible with time-to-market. However, efficiency, without loss of accuracy, has rarely been the focus in the numerical community. Here, in contrast, the framework is designed to adequately exploit the resources of high-end distributed-memory machines. It is grounded on three building blocks: (1) Hierarchical adaptive mesh refinement with octree-based meshes; (2) a parallel strategy to model the geometry growth; (3) the customization of a parallel iterative linear solver, which leverages the so-called balancing domain decomposition by constraints preconditioning approach for fast convergence and high parallel scalability. Computational experiments consider the part-scale thermal analysis of the printing process by powder-bed technologies. After verification against a 3D benchmark, a strong scaling analysis is carried out for a simulation of 48 layers printed in a cuboid. The cuboid is adaptively meshed to model a layer-by-layer metal deposition process and the average global problem size amounts to 10.3 million unknowns. An unprecedented scalability for problems with growing domains is achieved, with the capability of simulating the printing and recoat of a single layer in 8 seconds average on 3,072 processors. Hence, this framework contributes to take on higher complexity and/or accuracy, not only of part-scale simulations of metal or polymer additive manufacturing, but also in welding, sedimentation, atherosclerosis, or any other physical problem with growing-in-time geometries."
      },
      {
        "node_idx": 123651,
        "score_0_10": 9,
        "title": "distributed memory parallelization of the aggregated unfitted finite element method",
        "abstract": "The aggregated unfitted finite element method (AgFEM) is a methodology recently introduced in order to address conditioning and stability problems associated with embedded, unfitted, or extended finite element methods. The method is based on removal of basis functions associated with badly cut cells by introducing carefully designed constraints, which results in well-posed systems of linear algebraic equations, while preserving the optimal approximation order of the underlying finite element spaces. The specific goal of this work is to present the implementation and performance of the method on distributed-memory platforms aiming at the efficient solution of large-scale problems. In particular, we show that, by considering AgFEM, the resulting systems of linear algebraic equations can be effectively solved using standard algebraic multigrid preconditioners. This is in contrast with previous works that consider highly customized preconditioners in order to allow one the usage of iterative solvers in combination with unfitted techniques. Another novelty with respect to the methods available in the literature is the problem sizes that can be handled with the proposed approach. While most of previous references discussing linear solvers for unfitted methods are based on serial non-scalable algorithms, we propose a parallel distributed-memory method able to efficiently solve problems at large scales. This is demonstrated by means of a weak scaling test defined on complex 3D domains up to 300M degrees of freedom and one billion cells on 16K CPU cores in the Marenostrum-IV platform. The parallel implementation of the AgFEM method is available in the large-scale finite element package FEMPAR."
      },
      {
        "node_idx": 53187,
        "score_0_10": 9,
        "title": "cross loop optimization of arithmetic intensity for finite element local assembly",
        "abstract": "We study and systematically evaluate a class of composable code transformations that improve arithmetic intensity in local assembly operations, which represent a significant fraction of the execution time in finite element methods. Their performance optimization is indeed a challenging issue. Even though affine loop nests are generally present, the short trip counts and the complexity of mathematical expressions, which vary among different problems, make it hard to determine an optimal sequence of successful transformations. Our investigation has resulted in the implementation of a compiler (called COFFEE) for local assembly kernels, fully integrated with a framework for developing finite element methods. The compiler manipulates abstract syntax trees generated from a domain-specific language by introducing domain-aware optimizations for instruction-level parallelism and register locality. Eventually, it produces C code including vector SIMD intrinsics. Experiments using a range of real-world finite element problems of increasing complexity show that significant performance improvement is achieved. The generality of the approach and the applicability of the proposed code transformations to other domains is also discussed."
      }
    ]
  },
  "5": {
    "explanation": "fair division and allocation algorithms for goods and chores",
    "topk": [
      {
        "node_idx": 164991,
        "score_0_10": 10,
        "title": "fair division of a graph",
        "abstract": "We consider fair allocation of indivisible items under an additional constraint: there is an undirected graph describing the relationship between the items, and each agent's share must form a connected subgraph of this graph. This framework captures, e.g., fair allocation of land plots, where the graph describes the accessibility relation among the plots. We focus on agents that have additive utilities for the items, and consider several common fair division solution concepts, such as proportionality, envy-freeness and maximin share guarantee. While finding good allocations according to these solution concepts is computationally hard in general, we design efficient algorithms for special cases where the underlying graph has simple structure, and/or the number of agents -or, less restrictively, the number of agent types- is small. In particular, despite non-existence results in the general case, we prove that for acyclic graphs a maximin share allocation always exists and can be found efficiently."
      },
      {
        "node_idx": 111906,
        "score_0_10": 10,
        "title": "online circle packing",
        "abstract": "We consider the online problem of packing circles into a square container. A sequence of circles has to be packed one at a time, without knowledge of the following incoming circles and without moving previously packed circles. We present an algorithm that packs any online sequence of circles with a combined area not larger than 0.350389 0.350389 of the square's area, improving the previous best value of {\\pi}/10 \\approx 0.31416; even in an offline setting, there is an upper bound of {\\pi}/(3 + 2 \\sqrt{2}) \\approx 0.5390. If only circles with radii of at least 0.026622 are considered, our algorithm achieves the higher value 0.375898. As a byproduct, we give an online algorithm for packing circles into a 1\\times b rectangle with b \\geq 1. This algorithm is worst case-optimal for b \\geq 2.36."
      },
      {
        "node_idx": 44846,
        "score_0_10": 10,
        "title": "online square into square packing",
        "abstract": "In 1967, Moon and Moser proved a tight bound on the critical density of squares in squares: any set of squares with a total area of at most 1/2 can be packed into a unit square, which is tight. The proof requires full knowledge of the set, as the algorithmic solution consists in sorting the objects by decreasing size, and packing them greedily into shelves. Since then, the online version of the problem has remained open; the best upper bound is still 1/2, while the currently best lower bound is 1/3, due to Han et al. (2008). In this paper, we present a new lower bound of 11/32, based on a dynamic shelf allocation scheme, which may be interesting in itself. We also give results for the closely related problem in which the size of the square container is not fixed, but must be dynamically increased in order to ac- commodate online sequences of objects. For this variant, we establish an upper bound of 3/7 for the critical density, and a lower bound of 1/8. When aiming for accommodating an online sequence of squares, this corresponds to a 2.82...- competitive method for minimizing the required container size, and a lower bound of 1.33 . . . for the achievable factor."
      },
      {
        "node_idx": 113725,
        "score_0_10": 10,
        "title": "maximin fairness with mixed divisible and indivisible goods",
        "abstract": "We study fair resource allocation when the resources contain a mixture of divisible and indivisible goods, focusing on the well-studied fairness notion of maximin share fairness (MMS). With only indivisible goods, a full MMS allocation may not exist, but a constant multiplicative approximate allocation always does. We analyze how the MMS approximation guarantee would be affected when the resources to be allocated also contain divisible goods. In particular, we show that the worst-case MMS approximation guarantee with mixed goods is no worse than that with only indivisible goods. However, there exist problem instances to which adding some divisible resources would strictly decrease the MMS approximation ratio of the instance. On the algorithmic front, we propose a constructive algorithm that will always produce an $\\alpha$-MMS allocation for any number of agents, where $\\alpha$ takes values between $1/2$ and $1$ and is a monotone increasing function determined by how agents value the divisible goods relative to their MMS values."
      },
      {
        "node_idx": 27058,
        "score_0_10": 10,
        "title": "fair allocation of combinations of indivisible goods and chores",
        "abstract": "We consider the problem of fairly dividing a set of items. Much of the fair division literature assumes that the items are `goods' i.e., they yield positive utility for the agents. There is also some work where the items are `chores' that yield negative utility for the agents. In this paper, we consider a more general scenario where an agent may have negative or positive utility for each item. This framework captures, e.g., fair task assignment, where agents can have both positive and negative utilities for each task. We show that whereas some of the positive axiomatic and computational results extend to this more general setting, others do not. We present several new and efficient algorithms for finding fair allocations in this general setting. We also point out several gaps in the literature regarding the existence of allocations satisfying certain fairness and efficiency properties and further study the complexity of computing such allocations."
      },
      {
        "node_idx": 71378,
        "score_0_10": 10,
        "title": "almost envy free allocations with connected bundles",
        "abstract": "We study the existence of allocations of indivisible goods that are envy-free up to one good (EF1), under the additional constraint that each bundle needs to be connected in an underlying item graph G. When the items are arranged in a path, we show that EF1 allocations are guaranteed to exist for arbitrary monotonic utility functions over bundles, provided that either there are at most four agents, or there are any number of agents but they all have identical utility functions. Our existence proofs are based on classical arguments from the divisible cake-cutting setting, and involve discrete analogues of cut-and-choose, of Stromquist's moving-knife protocol, and of the Su-Simmons argument based on Sperner's lemma. Sperner's lemma can also be used to show that on a path, an EF2 allocation exists for any number of agents. Except for the results using Sperner's lemma, all of our procedures can be implemented by efficient algorithms. Our positive results for paths imply the existence of connected EF1 or EF2 allocations whenever G is traceable, i.e., contains a Hamiltonian path. For the case of two agents, we completely characterize the class of graphs $G$ that guarantee the existence of EF1 allocations as the class of graphs whose biconnected components are arranged in a path. This class is strictly larger than the class of traceable graphs; one can be check in linear time whether a graph belongs to this class, and if so return an EF1 allocation."
      },
      {
        "node_idx": 155818,
        "score_0_10": 10,
        "title": "democratic fair allocation of indivisible goods",
        "abstract": "We study the problem of fairly allocating indivisible goods to groups of agents. Agents in the same group share the same set of goods even though they may have different preferences. Previous work has focused on unanimous fairness, in which all agents in each group must agree that their group's share is fair. Under this strict requirement, fair allocations exist only for small groups. We introduce the concept of democratic fairness, which aims to satisfy a certain fraction of the agents in each group. This concept is better suited to large groups such as cities or countries. We present protocols for democratic fair allocation among two or more arbitrarily large groups of agents with monotonic, additive, or binary valuations. For two groups with arbitrary monotonic valuations, we give an efficient protocol that guarantees envy-freeness up to one good for at least $1/2$ of the agents in each group, and prove that the $1/2$ fraction is optimal. We also present other protocols that make weaker fairness guarantees to more agents in each group, or to more groups. Our protocols combine techniques from different fields, including combinatorial game theory, cake cutting, and voting."
      },
      {
        "node_idx": 76010,
        "score_0_10": 10,
        "title": "approximate maximin shares for groups of agents",
        "abstract": "Abstract   We investigate the problem of fairly allocating indivisible goods among interested agents using the concept of maximin share. Procaccia and Wang showed that while an allocation that gives every agent at least her maximin share does not necessarily exist, one that gives every agent at least    2  \u2215  3    of her share always does. In this paper, we consider the more general setting where we allocate the goods to  groups  of agents. The agents in each group share the same set of goods even though they may have conflicting preferences. For two groups, we characterize the cardinality of the groups for which a positive approximation of the maximin share is possible regardless of the number of goods. We also show settings where an approximation is possible or impossible when there are several groups."
      },
      {
        "node_idx": 43576,
        "score_0_10": 10,
        "title": "algorithms for competitive division of chores",
        "abstract": "We study the problem of allocating divisible bads (chores) among multiple agents with additive utilities, when money transfers are not allowed. The competitive rule is known to be the best mechanism for goods with additive utilities and was recently extended to chores by Bogomolnaia et al (2017). For both goods and chores, the rule produces Pareto optimal and envy-free allocations. In the case of goods, the outcome of the competitive rule can be easily computed. Competitive allocations solve the Eisenberg-Gale convex program; hence the outcome is unique and can be approximately found by standard gradient methods. An exact algorithm that runs in polynomial time in the number of agents and goods was given by Orlin. In the case of chores, the competitive rule does not solve any convex optimization problem; instead, competitive allocations correspond to local minima, local maxima, and saddle points of the Nash Social Welfare on the Pareto frontier of the set of feasible utilities. The rule becomes multivalued and none of the standard methods can be applied to compute its outcome. In this paper, we show that all the outcomes of the competitive rule for chores can be computed in strongly polynomial time if either the number of agents or the number of chores is fixed. The approach is based on a combination of three ideas: all consumption graphs of Pareto optimal allocations can be listed in polynomial time; for a given consumption graph, a candidate for a competitive allocation can be constructed via explicit formula; and a given allocation can be checked for being competitive using a maximum flow computation as in Devanur et al (2002). Our algorithm immediately gives an approximately-fair allocation of indivisible chores by the rounding technique of Barman and Krishnamurthy (2018)."
      },
      {
        "node_idx": 166719,
        "score_0_10": 10,
        "title": "transformations of normal form games by preplay offers for payments among players",
        "abstract": "We consider transformations of normal form games by binding preplay offers of players for payments of utility to other players conditional on them playing designated in the offers strategies. The game-theoretic effect of such preplay offers is transformation of the payoff matrix of the game by transferring payoffs between players. Here we analyze and completely characterize the possible transformations of the payoff matrix of a normal form game by sets of preplay offers."
      }
    ]
  },
  "9": {
    "explanation": "complexity and algorithms in computation and communication systems",
    "topk": [
      {
        "node_idx": 109380,
        "score_0_10": 10,
        "title": "survey of inter satellite communication for small satellite systems physical layer to network layer view",
        "abstract": "Small satellite systems enable whole new class of missions for navigation, communications, remote sensing and scientific research for both civilian and military purposes. As individual spacecraft are limited by the size, mass and power constraints, mass-produced small satellites in large constellations or clusters could be useful in many science missions such as gravity mapping, tracking of forest fires, finding water resources, etc. Constellation of satellites provide improved spatial and temporal resolution of the target. Small satellite constellations contribute innovative applications by replacing a single asset with several very capable spacecraft which opens the door to new applications. With increasing levels of autonomy, there will be a need for remote communication networks to enable communication between spacecraft. These space based networks will need to configure and maintain dynamic routes, manage intermediate nodes, and reconfigure themselves to achieve mission objectives. Hence, inter-satellite communication is a key aspect when satellites fly in formation. In this paper, we present the various researches being conducted in the small satellite community for implementing inter-satellite communications based on the Open System Interconnection (OSI) model. This paper also reviews the various design parameters applicable to the first three layers of the OSI model, i.e., physical, data link and network layer. Based on the survey, we also present a comprehensive list of design parameters useful for achieving inter-satellite communications for multiple small satellite missions. Specific topics include proposed solutions for some of the challenges faced by small satellite systems, enabling operations using a network of small satellites, and some examples of small satellite missions involving formation flying aspects."
      },
      {
        "node_idx": 76418,
        "score_0_10": 10,
        "title": "zeno machines and hypercomputation",
        "abstract": "This paper reviews the Church-Turing Thesis (or rather, theses) with reference to their origin and application and considers some models of \"hypercomputation\", concentrating on perhaps the most straight-forward option: Zeno machines (Turing machines with accelerating clock). The halting problem is briefly discussed in a general context and the suggestion that it is an inevitable companion of any reasonable computational model is emphasised. It is hinted that claims to have \"broken the Turing barrier\" could be toned down and that the important and well-founded role of Turing computability in the mathematical sciences stands unchallenged."
      },
      {
        "node_idx": 141909,
        "score_0_10": 9,
        "title": "pancake flipping is hard",
        "abstract": "Pancake Flipping is the problem of sorting a stack of pancakes of different sizes (that is, a permutation), when the only allowed operation is to insert a spatula anywhere in the stack and to flip the pancakes above it (that is, to perform a prefix reversal). In the burnt variant, one side of each pancake is marked as burnt, and it is required to finish with all pancakes having the burnt side down. Computing the optimal scenario for any stack of pancakes and determining the worst-case stack for any stack size have been challenges over more than three decades. Beyond being an intriguing combinatorial problem in itself, it also yields applications, e.g. in parallel computing and computational biology.#R##N##R##N#In this paper, we show that the Pancake Flipping problem, in its original (unburnt) variant, is NP-hard, thus answering the long-standing question of its computational complexity."
      },
      {
        "node_idx": 1074,
        "score_0_10": 9,
        "title": "pspace completeness of sliding block puzzles and other problems through the nondeterministic constraint logic model of computation",
        "abstract": "We present a nondeterministic model of computation based on reversing edge directions in weighted directed graphs with minimum in-flow constraints on vertices. Deciding whether this simple graph model can be manipulated in order to reverse the direction of a particular edge is shown to be PSPACE-complete by a reduction from Quantified Boolean Formulas. We prove this result in a variety of special cases including planar graphs and highly restricted vertex configurations, some of which correspond to a kind of passive constraint logic. Our framework is inspired by (and indeed a generalization of) the ``Generalized Rush Hour Logic'' developed by Flake and Baum. #R##N#We illustrate the importance of our model of computation by giving simple reductions to show that several motion-planning problems are PSPACE-hard. Our main result along these lines is that classic unrestricted sliding-block puzzles are PSPACE-hard, even if the pieces are restricted to be all dominoes (1x2 blocks) and the goal is simply to move a particular piece. No prior complexity results were known about these puzzles. This result can be seen as a strengthening of the existing result that the restricted Rush Hour puzzles are PSPACE-complete, of which we also give a simpler proof. Finally, we strengthen the existing result that the pushing-blocks puzzle Sokoban is PSPACE-complete, by showing that it is PSPACE-complete even if no barriers are allowed."
      },
      {
        "node_idx": 98022,
        "score_0_10": 8,
        "title": "tree spanners of bounded degree graphs",
        "abstract": "A tree   t     t       -spanner of a graph   G     G        is a spanning tree of   G     G        such that the distance between pairs of vertices in the tree is at most   t     t        times their distance in   G     G       . Deciding tree   t     t       -spanner admissible graphs has been proved to be tractable for   t     t    3        and NP-complete for   t>3     t  >  3       , while the complexity status of this problem is unresolved when   t=3     t  =  3       . For every   t>2     t  >  2        and   b>0     b  >  0       , an efficient dynamic programming algorithm to decide tree   t     t       -spanner admissibility of graphs with vertex degrees less than   b     b        is presented. Only for   t=3     t  =  3       , the algorithm remains efficient, when graphs   G     G        with degrees less than   blog|V(G)|     b  log   |  V   (  G  )   |         are examined."
      },
      {
        "node_idx": 111433,
        "score_0_10": 8,
        "title": "zigzag decodable fountain codes",
        "abstract": "This paper proposes a fountain coding system which has lower space decoding complexity and lower decoding erasure rate than the Raptor coding systems. The main idea of the proposed fountain code is employing shift and exclusive OR to generate the output packets. This technique is known as the zigzag decodable code, which is efficiently decoded by the zigzag decoder. In other words, we propose a fountain code based on the zigzag decodable code in this paper. Moreover, we analyze the overhead for the received packets, decoding erasure rate, decoding complexity, and asymptotic overhead of the proposed fountain code. As the result, we show that the proposed fountain code outperforms the Raptor codes in terms of the overhead and decoding erasure rate. Simulation results show that the proposed fountain coding system outperforms Raptor coding system in terms of the overhead and the space decoding complexity."
      },
      {
        "node_idx": 124031,
        "score_0_10": 8,
        "title": "borel hierarchy and omega context free languages",
        "abstract": "We give in this paper additional answers to questions of Lescow and Thomas [Logical Specifications of Infinite Computations, In:\"A Decade of Concurrency\", Springer LNCS 803 (1994), 583-621], proving new topological properties of omega context free languages : there exist some omega-CFL which are non Borel sets. And one cannot decide whether an omega-CFL is a Borel set. We give also an answer to questions of Niwinski and Simonnet about omega powers of finitary languages, giving an example of a finitary context free language L such that L^omega is not a Borel set. Then we prove some recursive analogues to preceding properties: in particular one cannot decide whether an omega-CFL is an arithmetical set."
      },
      {
        "node_idx": 1470,
        "score_0_10": 8,
        "title": "improvements for free",
        "abstract": "Theorems for Free! (Wadler, FPCA 1989) is a slogan for a technique that allows to derive statements about functions just from their types. So far, the statements considered have always had a purely extensional flavor: statements relating the value semantics of program expressions, but not statements relating their runtime (or other) cost. Here we study an extension of the technique that allows precisely statements of the latter flavor, by deriving quantitative theorems for free. After developing the theory, we walk through a number of example derivations. Probably none of the statements derived in those simple examples will be particularly surprising to most readers, but what is maybe surprising, and at the very least novel, is that there is a general technique for obtaining such results on a quantitative level in a principled way. Moreover, there is good potential to bring that technique to bear on more complex examples as well. We turn our attention to short-cut fusion (Gill et al., FPCA 1993) in particular."
      },
      {
        "node_idx": 46578,
        "score_0_10": 8,
        "title": "scalable time responsive digital energy efficient molecular circuits using dna strand displacement",
        "abstract": "We propose a novel theoretical biomolecular design to implement any Boolean circuit using the mechanism of DNA strand displacement. The design is scalable: all species of DNA strands can in principle be mixed and prepared in a single test tube, rather than requiring separate purification of each species, which is a barrier to large-scale synthesis. The design is time-responsive: the concentration of output species changes in response to the concentration of input species, so that time-varying inputs may be continuously processed. The design is digital: Boolean values of wires in the circuit are represented as high or low concentrations of certain species, and we show how to construct a single-input, single-output signal restoration gate that amplifies the difference between high and low, which can be distributed to each wire in the circuit to overcome signal degradation. This means we can achieve a digital abstraction of the analog values of concentrations. Finally, the design is energy-efficient: if input species are specified ideally (meaning absolutely 0 concentration of unwanted species), then output species converge to their ideal concentrations at steady-state, and the system at steady-state is in (dynamic) equilibrium, meaning that no energy is consumed by irreversible reactions until the input again changes."
      },
      {
        "node_idx": 92213,
        "score_0_10": 8,
        "title": "staged self assembly nanomanufacture of arbitrary shapes with o 1 glues",
        "abstract": "We introduce staged self-assembly of Wang tiles, where tiles can be added dynamically in sequence and where intermediate constructions can be stored for later mixing. This model and its various constraints and performance measures are motivated by a practical nanofabrication scenario through protein-based bioengineering. Staging allows us to break through the traditional lower bounds in tile self-assembly by encoding the shape in the staging algorithm instead of the tiles. All of our results are based on the practical assumption that only a constant number of glues, and thus only a constant number of tiles, can be engineered, as each new glue type requires significant biochemical research and experiments. Under this assumption, traditional tile self-assembly cannot even manufacture an n*n square; in contrast, we show how staged assembly enables manufacture of arbitrary orthogonal shapes in a variety of precise formulations of the model."
      }
    ]
  },
  "20": {
    "explanation": "named entity recognition in Persian text",
    "topk": [
      {
        "node_idx": 128801,
        "score_0_10": 10,
        "title": "peyma a tagged corpus for persian named entities",
        "abstract": "The goal in the NER task is to classify proper nouns of a text into classes such as person, location, and organization. This is an important preprocessing step in many NLP tasks such as question-answering and summarization. Although many research studies have been conducted in this area in English and the state-of-the-art NER systems have reached performances of higher than 90 percent in terms of F1 measure, there are very few research studies for this task in Persian. One of the main important causes of this may be the lack of a standard Persian NER dataset to train and test NER systems. In this research we create a standard, big-enough tagged Persian NER dataset which will be distributed for free for research purposes. In order to construct such a standard dataset, we studied standard NER datasets which are constructed for English researches and found out that almost all of these datasets are constructed using news texts. So we collected documents from ten news websites. Later, in order to provide annotators with some guidelines to tag these documents, after studying guidelines used for constructing CoNLL and MUC standard English datasets, we set our own guidelines considering the Persian linguistic rules."
      },
      {
        "node_idx": 94119,
        "score_0_10": 10,
        "title": "hybrid approaches for automatic vowelization of arabic texts",
        "abstract": "Hybrid approaches for automatic vowelization of Arabic texts are presented in this article. The process is made up of two modules. In the first one, a morphological analysis of the text words is performed using the open source morphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out of context, are its different possible vowelizations. The integration of this Analyzer in our vowelization system required the addition of a lexical database containing the most frequent words in Arabic language. Using a statistical approach based on two hidden Markov models (HMM), the second module aims to eliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic words are the observed states and the vowelized words are the hidden states. The observed states of the second HMM are identical to those of the first, but the hidden states are the lists of possible diacritics of the word without its Arabic letters. Our system uses Viterbi algorithm to select the optimal path among the solutions proposed by Al Khalil Morpho Sys. Our approach opens an important way to improve the performance of automatic vowelization of Arabic texts for other uses in automatic natural language processing."
      },
      {
        "node_idx": 40530,
        "score_0_10": 10,
        "title": "arabic text diacritization using deep neural networks",
        "abstract": "Diacritization of Arabic text is both an interesting and a challenging problem at the same time with various applications ranging from speech synthesis to helping students learning the Arabic language. Like many other tasks or problems in Arabic language processing, the weak efforts invested into this problem and the lack of available (open-source) resources hinder the progress towards solving this problem. This work provides a critical review for the currently existing systems, measures and resources for Arabic text diacritization. Moreover, it introduces a much-needed free-for-all cleaned dataset that can be easily used to benchmark any work on Arabic diacritization. Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words. After constructing the dataset, existing tools and systems are tested on it. The results of the experiments show that the neural Shakkala system significantly outperforms traditional rule-based approaches and other closed-source tools with a Diacritic Error Rate (DER) of 2.88% compared with 13.78%, which the best DER for the non-neural approach (obtained by the Mishkal tool)."
      },
      {
        "node_idx": 136313,
        "score_0_10": 10,
        "title": "an improved feature descriptor for recognition of handwritten bangla alphabet",
        "abstract": "Appropriate feature set for representation of pattern classes is one of the most important aspects of handwritten character recognition. The effectiveness of features depends on the discriminating power of the features chosen to represent patterns of different classes. However, discriminatory features are not easily measurable. Investigative experimentation is necessary for identifying discriminatory features. In the present work we have identified a new variation of feature set which significantly outperforms on handwritten Bangla alphabet from the previously used feature set. 132 number of features in all viz. modified shadow features, octant and centroid features, distance based features, quad tree based longest run features are used here. Using this feature set the recognition performance increases sharply from the 75.05% observed in our previous work (7), to 85.40% on 50 character classes with MLP based classifier on the same dataset."
      },
      {
        "node_idx": 13959,
        "score_0_10": 9,
        "title": "classifier based text simplification for improved machine translation",
        "abstract": "Machine Translation is one of the research fields of Computational Linguistics. The objective of many MT Researchers is to develop an MT System that produce good quality and high accuracy output translations and which also covers maximum language pairs. As internet and Globalization is increasing day by day, we need a way that improves the quality of translation. For this reason, we have developed a Classifier based Text Simplification Model for English-Hindi Machine Translation Systems. We have used support vector machines and Na\\\"ive Bayes Classifier to develop this model. We have also evaluated the performance of these classifiers."
      },
      {
        "node_idx": 76949,
        "score_0_10": 9,
        "title": "an enhanced harmony search method for bangla handwritten character recognition using region sampling",
        "abstract": "Identification of minimum number of local regions of a handwritten character image, containing well-defined discriminating features which are sufficient for a minimal but complete description of the character is a challenging task. A new region selection technique based on the idea of an enhanced Harmony Search methodology has been proposed here. The powerful framework of Harmony Search has been utilized to search the region space and detect only the most informative regions for correctly recognizing the handwritten character. The proposed method has been tested on handwritten samples of Bangla Basic, Compound and mixed (Basic and Compound characters)characters separately with SVM based classifier using a longest run based feature-set obtained from the image sub-regions formed by a CG based quad-tree partitioning approach. Applying this methodology on the above mentioned three types of datasets, respectively 43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction and 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition accuracy. The results show a sizeable reduction in the minimal number of descriptive regions as well a significant increase in recognition accuracy for all the datasets using the proposed technique. Thus the time and cost related to feature extraction is decreased without dampening the corresponding recognition accuracy."
      },
      {
        "node_idx": 29514,
        "score_0_10": 9,
        "title": "mtil17 english to indian langauge statistical machine translation",
        "abstract": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation."
      },
      {
        "node_idx": 116566,
        "score_0_10": 9,
        "title": "time efficient approach to offline hand written character recognition using associative memory net",
        "abstract": "In this paper, an efficient Offline Hand Written Character Recognition algorithm is proposed based on Associative Memory Net (AMN). The AMN used in this work is basically auto associative. The implementation is carried out completely in 'C' language. To make the system perform to its best with minimal computation time, a Parallel algorithm is also developed using an API package OpenMP. Characters are mainly English alphabets (Small (26), Capital (26)) collected from system (52) and from different persons (52). The characters collected from system are used to train the AMN and characters collected from different persons are used for testing the recognition ability of the net. The detailed analysis showed that the network recognizes the hand written characters with recognition rate of 72.20% in average case. However, in best case, it recognizes the collected hand written characters with 88.5%. The developed network consumes 3.57 sec (average) in Serial implementation and 1.16 sec (average) in Parallel implementation using OpenMP."
      },
      {
        "node_idx": 83401,
        "score_0_10": 9,
        "title": "handwritten bangla basic and compound character recognition using mlp and svm classifier",
        "abstract": "A novel approach for recognition of handwritten compound Bangla characters, along with the Basic characters of Bangla alphabet, is presented here. Compared to English like Roman script, one of the major stumbling blocks in Optical Character Recognition (OCR) of handwritten Bangla script is the large number of complex shaped character classes of Bangla alphabet. In addition to 50 basic character classes, there are nearly 160 complex shaped compound character classes in Bangla alphabet. Dealing with such a large varieties of handwritten characters with a suitably designed feature set is a challenging problem. Uncertainty and imprecision are inherent in handwritten script. Moreover, such a large varieties of complex shaped characters, some of which have close resemblance, makes the problem of OCR of handwritten Bangla characters more difficult. Considering the complexity of the problem, the present approach makes an attempt to identify compound character classes from most frequently to less frequently occurred ones, i.e., in order of importance. This is to develop a frame work for incrementally increasing the number of learned classes of compound characters from more frequently occurred ones to less frequently occurred ones along with Basic characters. On experimentation, the technique is observed produce an average recognition rate of 79.25 after three fold cross validation of data with future scope of improvement and extension."
      },
      {
        "node_idx": 116621,
        "score_0_10": 9,
        "title": "identifying bengali multiword expressions using semantic clustering",
        "abstract": "One of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical, syntactical and semantic properties. The semantics of a MWE cannot be expressed after combining the semantics of its constituents. Therefore, the formalism of semantic clustering is often viewed as an instrument for extracting MWEs especially for resource constraint languages like Bengali. The present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a MWE. In this experiment, we apply the semantic clustering approach for noun-noun bigram MWEs, though it can be extended to any types of MWEs. In parallel, the well known statistical models, namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR), Significance function are also employed to extract MWEs from the Bengali corpus. The comparative evaluation shows that the semantic clustering approach outperforms all other competing statistical models. As a by-product of this experiment, we have started developing a standard lexicon in Bengali that serves as a productive Bengali linguistic thesaurus."
      }
    ]
  },
  "22": {
    "explanation": "spatio-temporal visual sequence modeling and recognition",
    "topk": [
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 168237,
        "score_0_10": 8,
        "title": "the symbol grounding problem",
        "abstract": "There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the \"symbol grounding problem\": How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) \"iconic representations\" , which are analogs of the proximal sensory projections of distal objects and events, and (2) \"categorical representations\" , which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) \"symbolic representations\" , grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., \"An X is a Y that is Z\"). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic \"module,\" however; the symbolic functions would emerge as an intrinsically \"dedicated\" symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded."
      },
      {
        "node_idx": 39369,
        "score_0_10": 8,
        "title": "person re identification by local maximal occurrence representation and metric learning",
        "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively."
      },
      {
        "node_idx": 26460,
        "score_0_10": 7,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 57930,
        "score_0_10": 7,
        "title": "show attend and tell neural image caption generation with visual attention",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO."
      },
      {
        "node_idx": 21564,
        "score_0_10": 7,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 15365,
        "score_0_10": 7,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 51364,
        "score_0_10": 7,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 151734,
        "score_0_10": 7,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      }
    ]
  },
  "24": {
    "explanation": "Privacy-preserving secure multiparty computation methods and applications",
    "topk": [
      {
        "node_idx": 114380,
        "score_0_10": 10,
        "title": "leveraging secure multiparty computation in the internet of things",
        "abstract": "Centralized systems in the Internet of Things---be it local middleware or cloud-based services---fail to fundamentally address privacy of the collected data. We propose an architecture featuring secure multiparty computation at its core in order to realize data processing systems which already incorporate support for privacy protection in the architecture."
      },
      {
        "node_idx": 106392,
        "score_0_10": 10,
        "title": "a performance and resource consumption assessment of secret sharing based secure multiparty computation",
        "abstract": "In recent years, Secure Multiparty Computation (SMC) advanced from a theoretical technique to a practically applicable cryptographic technology. Several frameworks were proposed of which some are still actively developed."
      },
      {
        "node_idx": 92005,
        "score_0_10": 10,
        "title": "data querying and access control for secure multiparty computation",
        "abstract": "In the Internet of Things and smart environments data, collected from distributed sensors, is typically stored and processed by a central middleware. This allows applications to query the data they need for providing further services. However, centralization of data causes several privacy threats: The middleware becomes a third party which has to be trusted, linkage and correlation of data from different context becomes possible and data subject lose control over their data. #R##N#Hence, other approaches than centralized processing should be considered. Here, Secure Multiparty Computation is a promising candidate for secure and privacy-preserving computation happening close to the sources of the data. #R##N#In order to make SMC fit for application in these contexts, we extend SMC to act as a service: We provide elements which allow third parties to query computed data from a group of peers performing SMC. Furthermore, we establish fine-granular access control on the level of individual data queries, yielding data protection of the computed results. By adding measures to inform data sources about requests and the usage of their data, we show how a fully privacy-preserving service can be built on the foundation of SMC."
      },
      {
        "node_idx": 162400,
        "score_0_10": 10,
        "title": "ambiguity and communication",
        "abstract": "The ambiguity of a nondeterministic finite automaton (NFA) N for input size n is the maximal number of accepting computations of N for an input of size n. For all k, r 2 N we construct languages Lr,k which can be recognized by NFA's with size k poly(r) and ambiguity O(nk), but Lr,k has only NFA's with exponential size, if ambiguity o(nk) is required. In particular, a hierarchy for polynomial ambiguity is obtained, solving a long standing open problem (Ravikumar and Ibarra, 1989, Leung, 1998)."
      },
      {
        "node_idx": 149372,
        "score_0_10": 10,
        "title": "distributed transient frequency control in power networks",
        "abstract": "Modern power networks face increasing challenges in controlling their transient frequency behavior at acceptable levels due to low inertia and highly-dynamic units. This paper presents a distributed control strategy regulated on a subset of buses in a power network to maintain their transient frequencies in safe regions while preserving asymptotic stability of the overall system. Building on Lyapunov stability and set invariance theory, we formulate the transient frequency requirement and the asymptotic stability requirement as two separate constraints for the control input. Hereby, for each bus of interest, we synthesize a controller satisfying both constraints simultaneously. The controller is distributed and Lipschitz, guaranteeing the existence and uniqueness of the trajectories of the closed-loop system. Simulations on the IEEE 39-bus power network illustrate the results."
      },
      {
        "node_idx": 15976,
        "score_0_10": 10,
        "title": "remembering chandra kintala",
        "abstract": "With this contribution we would like to remember Chandra M. R. Kintala who passed away in November 2009. We will give short overviews of his CV and his contributions to the field of theoretical and applied computer science and, given the opportunity, will attempt to present the current state of limited nondeterminism and limited resources for machines. Finally, we will briefly touch on some research topics which hopefully will be addressed in the not so distant future."
      },
      {
        "node_idx": 35379,
        "score_0_10": 10,
        "title": "multiparty testing preorders",
        "abstract": "Variants of the must testing approach have been successfully applied in Service Oriented Computing for analysing the compliance between (contracts exposed by) clients and servers or, more generally, between two peers. It has however been argued that multiparty scenarios call for more permissive notions of compliance because partners usually do not have full coordination capabilities. We propose two new testing preorders, which are obtained by restricting the set of potential observers. For the first preorder, called uncoordinated, we allow only sets of parallel observers that use different parts of the interface of a given service and have no possibility of intercommunication. For the second preorder, that we call individualistic, we instead rely on parallel observers that perceive as silent all the actions that are not in the interface of interest. We have that the uncoordinated preorder is coarser than the classical must testing preorder and finer than the individualistic one. We also provide a characterisation in terms of decorated traces for both preorders: the uncoordinated preorder is defined in terms of must-sets and Mazurkiewicz traces while the individualistic one is described in terms of classes of filtered traces that only contain designated visible actions and must-sets."
      },
      {
        "node_idx": 54610,
        "score_0_10": 10,
        "title": "digital watermarking in the singular vector domain",
        "abstract": "Many current watermarking algorithms insert data in the spatial or transform domains like the discrete cosine, the discrete Fourier, and the discrete wavelet transforms. In this paper, we present a data-hiding algorithm that exploits the singular value decomposition (SVD) representation of the data. We compute the SVD of the host image and the watermark and embed the watermark in the singular vectors of the host image. The proposed method leads to an imperceptible scheme for digital images, both in grey scale and color and is quite robust against attacks like noise and JPEG compression."
      },
      {
        "node_idx": 38981,
        "score_0_10": 10,
        "title": "a management framework for secure multiparty computation in dynamic environments",
        "abstract": "Secure multiparty computation (SMC) is a promising technology for privacy-preserving collaborative computation. In the last years several feasibility studies have shown its practical applicability in different fields. However, it is recognized that administration, and management overhead of SMC solutions are still a problem. A vital next step is the incorporation of SMC in the emerging fields of the Internet of Things and (smart) dynamic environments. In these settings, the properties of these contexts make utilization of SMC even more challenging since some vital premises for its application regarding environmental stability and preliminary configuration are not initially fulfilled. We bridge this gap by providing FlexSMC, a management and orchestration framework for SMC which supports the discovery of nodes, supports a trust establishment between them and realizes robustness of SMC session by handling nodes failures and communication interruptions. The practical evaluation of FlexSMC shows that it enables the application of SMC in dynamic environments with reasonable performance penalties and computation durations allowing soft real-time and interactive use cases."
      },
      {
        "node_idx": 150452,
        "score_0_10": 10,
        "title": "opportunistic adaptation knowledge discovery",
        "abstract": "Adaptation has long been considered as the Achilles' heel of case-based reasoning since it requires some domain-specific knowledge that is difficult to acquire. In this paper, two strategies are combined in order to reduce the knowledge engineering cost induced by the adaptation knowledge (CA) acquisition task: CA is learned from the case base by the means of knowledge discovery techniques, and the CA acquisition sessions are opportunistically triggered, i.e., at problem-solving time."
      }
    ]
  },
  "28": {
    "explanation": "Android malware detection using machine learning and ensemble methods",
    "topk": [
      {
        "node_idx": 111842,
        "score_0_10": 10,
        "title": "android malware detection using autoencoder",
        "abstract": "Smartphones have become an intrinsic part of human's life. The smartphone unifies diverse advanced characteristics. It enables users to store various data such as photos, health data, credential bank data, and personal information. The Android operating system is the prevalent mobile operating system and, in the meantime, the most targeted operating system by malware developers. Recently the unparalleled development of Android malware put pressure on researchers to propose effective methods to suppress the spread of the malware. In this paper, we propose a deep learning approach for Android malware detection. The proposed approach investigates five different feature sets and applies Autoencoder to identify malware. The experimental results show that the proposed approach can identify malware with high accuracy."
      },
      {
        "node_idx": 104477,
        "score_0_10": 10,
        "title": "nosebreak attacking honeynets",
        "abstract": "It is usually assumed that honeynets are hard to detect and that attempts to detect or disable them can be unconditionally monitored. We scrutinize this assumption and demonstrate a method how a host in a honeynet can be completely controlled by an attacker without any substantial logging taking place."
      },
      {
        "node_idx": 35373,
        "score_0_10": 10,
        "title": "powermonads and tensors of unranked effects",
        "abstract": "In semantics and in programming practice, algebraic concepts such as monads or, essentially equivalently, (large) Lawvere theories are a well-established tool for modelling generic side-effects. An important issue in this context are combination mechanisms for such algebraic effects, which allow for the modular design of programming languages and verification logics. The most basic combination operators are sum and tensor: while the sum of effects is just their non-interacting union, the tensor imposes commutation of effects. However, for effects with unbounded arity, such as continuations or unbounded nondeterminism, it is not a priori clear whether these combinations actually exist in all cases. Here, we introduce the class of uniform effects, which includes unbounded nondeterminism and continuations, and prove that the tensor does always exist if one of the component effects is uniform, thus in particular improving on previous results on tensoring with continuations. We then treat the case of nondeterminism in more detail, and give an order-theoretic characterization of effects for which tensoring with nondeterminism is conservative, thus enabling nondeterministic arguments such as a generic version of the Fischer-Ladner encoding of control operators."
      },
      {
        "node_idx": 95061,
        "score_0_10": 10,
        "title": "decentralised ltl monitoring",
        "abstract": "Users wanting to monitor distributed or component-based systems often perceive them as monolithic systems which, seen from the outside, exhibit a uniform behaviour as opposed to many components displaying many local behaviours that together constitute the system's global behaviour. This level of abstraction is often reasonable, hiding implementation details from users who may want to specify the system's global behaviour in terms of an LTL formula. However, the problem that arises then is how such a specification can actually be monitored in a distributed system that has no central data collection point, where all the components' local behaviours are observable. In this case, the LTL specification needs to be decomposed into sub-formulae which, in turn, need to be distributed amongst the components' locally attached monitors, each of which sees only a distinct part of the global behaviour. The main contribution of this paper is an algorithm for distributing and monitoring LTL formulae, such that satisfac- tion or violation of specifications can be detected by local monitors alone. We present an implementation and show that our algorithm introduces only a minimum delay in detecting satisfaction/violation of a specification. Moreover, our practical results show that the communication overhead introduced by the local monitors is considerably lower than the number of messages that would need to be sent to a central data collection point."
      },
      {
        "node_idx": 109394,
        "score_0_10": 9,
        "title": "attribute dependencies for data with grades",
        "abstract": "This paper examines attribute dependencies in data that involve grades, such as a grade to which an object is red or a grade to which two objects are similar. We thus extend the classical agenda by allowing graded, or fuzzy, attributes instead of Boolean attributes in case of attribute implications, and allowing approximate match based on degrees of similarity instead of exact match in case of functional dependencies. In a sense, we move from bivalence, inherently present in the now-available theories of dependencies, to a more flexible setting that involves grades. Such a shift has far-reaching consequences. We argue that a reasonable theory of dependencies may be developed by making use of mathematical fuzzy logic. Namely, the theory of dependencies is then based on a solid logic calculus the same way the classical dependencies are based on classical logic. For instance, rather than handling degrees of similarity in an ad hoc manner, we consistently treat them as truth values, the same way as true (match) and false (mismatch) are treated in classical theories. In addition, several notions intuitively embraced in the presence of grades, such as a degree of validity of a particular dependence or a degree of entailment, naturally emerge and receive a conceptually clean treatment in the presented approach. In the paper, we discuss motivations, provide basic notions of syntax and semantics, and develop basic results which include entailment of dependencies, associated closure structures, a logic of dependencies with two versions of completeness theorem, results and algorithms regarding complete non-redundant sets of dependencies, relationship to and a possible reductionist interface to classical dependencies, and relationship to functional dependencies over domains with similarity."
      },
      {
        "node_idx": 90869,
        "score_0_10": 9,
        "title": "minimal change integrity maintenance using tuple deletions",
        "abstract": "We address the problem of minimal-change integrity maintenance in the context of integrity constraints in relational databases. We assume that integrity-restoration actions are limited to tuple deletions. We identify two basic computational issues: repair checking (is a database instance a repair of a given database?) and consistent query answers (is a tuple an answer to a given query in every repair of a given database?). We study the computational complexity of both problems, delineating the boundary between the tractable and the intractable. We consider denial constraints, general functional and inclusion dependencies, as well as key and foreign key constraints. Our results shed light on the computational feasibility of minimal-change integrity maintenance. The tractable cases should lead to practical implementations. The intractability results highlight the inherent limitations of any integrity enforcement mechanism, e.g., triggers or referential constraint actions, as a way of performing minimal-change integrity maintenance."
      },
      {
        "node_idx": 127161,
        "score_0_10": 9,
        "title": "neutrosophic information in the framework of multi valued representation",
        "abstract": "The paper presents some steps for multi-valued representation of neutrosophic information. These steps are provided in the framework of multi- valued logics using the following logical value: true, false, neutral, unknown and saturated. Also, this approach provides some calculus formulae for the fol- lowing neutrosophic features: truth, falsity, neutrality, ignorance, under- definedness, over-definedness, saturation and entropy. In addition, it was de- fined net truth, definedness and neutrosophic score."
      },
      {
        "node_idx": 156288,
        "score_0_10": 9,
        "title": "high accuracy android malware detection using ensemble learning",
        "abstract": "With over 50 billion downloads and more than 1.3 million apps in Google's official market, Android has continued to gain popularity among smartphone users worldwide. At the same time there has been a rise in malware targeting the platform, with more recent strains employing highly sophisticated detection avoidance techniques. As traditional signature-based methods become less potent in detecting unknown malware, alternatives are needed for timely zero-day discovery. Thus, this study proposes an approach that utilises ensemble learning for Android malware detection. It combines advantages of static analysis with the efficiency and performance of ensemble machine learning to improve Android malware detection accuracy. The machine learning models are built using a large repository of malware samples and benign apps from a leading antivirus vendor. Experimental results and analysis presented shows that the proposed method which uses a large feature space to leverage the power of ensemble learning is capable of 97.3-99% detection accuracy with very low false positive rates."
      },
      {
        "node_idx": 147065,
        "score_0_10": 9,
        "title": "the neutrosophic entropy and its five components",
        "abstract": "This paper presents two variants of penta-valued representation for neutrosophic entropy. The first is an extension of Kaufmann's formula and the second is an extension of Kosko's formula. #R##N#Based on the primary three-valued information represented by the degree of truth, degree of falsity and degree of neutrality there are built some penta-valued representations that better highlights some specific features of neutrosophic entropy. Thus, we highlight five features of neutrosophic uncertainty such as ambiguity, ignorance, contradiction, neutrality and saturation. These five features are supplemented until a seven partition of unity by adding two features of neutrosophic certainty such as truth and falsity. #R##N#The paper also presents the particular forms of neutrosophic entropy obtained in the case of bifuzzy representations, intuitionistic fuzzy representations, paraconsistent fuzzy representations and finally the case of fuzzy representations."
      },
      {
        "node_idx": 215,
        "score_0_10": 9,
        "title": "android malware detection using parallel machine learning classifiers",
        "abstract": "Mobile malware has continued to grow at an alarming rate despite on-going mitigation efforts. This has been much more prevalent on Android due to being an open platform that is rapidly overtaking other competing platforms in the mobile smart devices market. Recently, a new generation of Android malware families has emerged with advanced evasion capabilities which make them much more difficult to detect using conventional methods. This paper proposes and investigates a parallel machine learning based classification approach for early detection of Android malware. Using real malware samples and benign applications, a composite classification model is developed from parallel combination of heterogeneous classifiers. The empirical evaluation of the model under different combination schemes demonstrates its efficacy and potential to improve detection accuracy. More importantly, by utilizing several classifiers with diverse characteristics, their strengths can be harnessed not only for enhanced Android malware detection but also quicker white box analysis by means of the more interpretable constituent classifiers."
      }
    ]
  },
  "29": {
    "explanation": "deep and wide neural network architectures with residual connections",
    "topk": [
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 29312,
        "score_0_10": 9,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 49351,
        "score_0_10": 8,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 111390,
        "score_0_10": 8,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 119322,
        "score_0_10": 8,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      }
    ]
  },
  "31": {
    "explanation": "spatial transformation and invariance in convolutional networks",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 112726,
        "score_0_10": 8,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 26460,
        "score_0_10": 8,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 121343,
        "score_0_10": 8,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 69942,
        "score_0_10": 7,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 151734,
        "score_0_10": 7,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 25895,
        "score_0_10": 7,
        "title": "prolific compositions",
        "abstract": "Under what circumstances might every extension of a combinatorial structure contain more copies of another one than the original did? This property, which we call prolificity, holds universally in some cases (e.g., finite linear orders) and only trivially in others (e.g., permutations). Integer compositions, or equivalently layered permutations, provide a middle ground. In that setting, there are prolific compositions for a given pattern if and only if that pattern begins and ends with 1. For each pattern, there is an easily constructed automaton that recognises prolific compositions for that pattern. Some instances where there is a unique minimal prolific composition for a pattern are classified."
      },
      {
        "node_idx": 12113,
        "score_0_10": 7,
        "title": "gans trained by a two time scale update rule converge to a nash equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Frechet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark."
      }
    ]
  },
  "32": {
    "explanation": "progressive holographic image and data representation methods",
    "topk": [
      {
        "node_idx": 141503,
        "score_0_10": 10,
        "title": "holographic image sensing",
        "abstract": "Holographic representations of data enable distributed storage with progressive refinement when the stored packets of data are made available in any arbitrary order. In this paper, we propose and test patch-based transform coding holographic sensing of image data. Our proposal is optimized for progressive recovery under random order of retrieval of the stored data. The coding of the image patches relies on the design of distributed projections ensuring best image recovery, in terms of the $\\ell_2$ norm, at each retrieval stage. The performance depends only on the number of data packets that has been retrieved thus far. #R##N#Several possible options to enhance the quality of the recovery while changing the size and number of data packets are discussed and tested. This leads us to examine several interesting bit-allocation and rate-distortion trade offs, highlighted for a set of natural images with ensemble estimated statistical properties."
      },
      {
        "node_idx": 146266,
        "score_0_10": 10,
        "title": "holographic sensing",
        "abstract": "Holographic representations of data encode information in packets of equal importance that enable progressive recovery. The quality of recovered data improves as more and more packets become available. This progressive recovery of the information is independent of the order in which packets become available. Such representations are ideally suited for distributed storage and for the transmission of data packets over networks with unpredictable delays and or erasures.  Several methods for holographic representations of signals and images have been proposed over the years and multiple description information theory also deals with such representations. Surprisingly, however, these methods had not been considered in the classical framework of optimal least-squares estimation theory, until very recently. We develop a least-squares approach to the design of holographic representation for stochastic data vectors, relying on the framework widely used in modeling signals and images."
      },
      {
        "node_idx": 124619,
        "score_0_10": 10,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 109276,
        "score_0_10": 10,
        "title": "the author topic model for authors and documents",
        "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output."
      },
      {
        "node_idx": 68684,
        "score_0_10": 9,
        "title": "cross lingual keyword assignment",
        "abstract": "This paper presents a language-independent approach to controlled vocabulary keyword assignment using the EUROVOC thesaurus. Due to the multilingual nature of EUROVOC, the keywords for a document written in one language can be displayed in all eleven official European Union languages. The mapping of documents written in different languages to the same multilingual thesaurus furthermore allows cross-language document comparison. The assignment of the controlled vocabulary thesaurus descriptors is achieved by applying a statistical method that uses a collection of manually indexed documents to identify, for each thesaurus descriptor, a large number of lemmas that are statistically associated to the descriptor. These associated words are then used during the assignment procedure to identify a ranked list of those EUROVOC terms that are most likely to be good keywords for a given document. The paper also describes the challenges of this task and discusses the achieved results of the fully functional prototype."
      },
      {
        "node_idx": 122540,
        "score_0_10": 9,
        "title": "bibliometric enhanced information retrieval",
        "abstract": "Bibliometric techniques are not yet widely used to enhance retrieval processes in digital libraries, although they offer value-added effects for users. In this workshop we will explore how statistical modelling of scholarship, such as Bradfordizing or network analysis of coauthorship network, can improve retrieval services for specific communities, as well as for large, cross-domain collections. This workshop aims to raise awareness of the missing link between information retrieval (IR) and bibliometrics/scientometrics and to create a common ground for the incorporation of bibliometric-enhanced services into retrieval at the digital library interface."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 43919,
        "score_0_10": 8,
        "title": "automatic identification of document translations in large multilingual document collections",
        "abstract": "Abstract Texts and their translations are a rich lin-guistic resource that can be used to train and test statistics-based Machine Translation systems and many other applications. In this paper, we present a working system that can identify translations and other very similar documents among a large number of candi-dates, by representing the document con-tents with a vector of thesaurus terms from a multilingual thesaurus, and by then measur-ing the semantic similarity between the vec-tors. Tests on different text types have shown that the system can detect transla-tions with over 96% precision in a large search space of 820 documents or more. The system was tuned to ignore language-specific similarities and to give similar documents in a second language the same similarity score as equivalent documents in the same language. The application can also be used to detect cross-lingual document plagiarism. 1 Introduction The task of mining for translational equivalences at document level presented in this paper is based on the automatic mapping of documents onto an existing multilingual knowledge structure, the"
      },
      {
        "node_idx": 40714,
        "score_0_10": 8,
        "title": "tweeting biomedicine an analysis of tweets and citations in the biomedical literature",
        "abstract": "Data collected by social media platforms have recently been introduced as a new source for indicators to help measure the impact of scholarly research in ways that are complementary to traditional citation-based indicators. Data generated from social media activities related to scholarly content can be used to reflect broad types of impact. This paper aims to provide systematic evidence regarding how often Twitter is used to diffuse journal articles in the biomedical and life sciences. The analysis is based on a set of 1.4 million documents covered by both PubMed and Web of Science (WoS) and published between 2010 and 2012. The number of tweets containing links to these documents was analyzed to evaluate the degree to which certain journals, disciplines, and specialties were represented on Twitter. It is shown that, with less than 10% of PubMed articles mentioned on Twitter, its uptake is low in general. The relationship between tweets and WoS citations was examined for each document at the level of journals and specialties. The results show that tweeting behavior varies between journals and specialties and correlations between tweets and citations are low, implying that impact metrics based on tweets are different from those based on citations. A framework utilizing the coverage of articles and the correlation between Twitter mentions and citations is proposed to facilitate the evaluation of novel social-media based metrics and to shed light on the question in how far the number of tweets is a valid metric to measure research impact."
      }
    ]
  },
  "36": {
    "explanation": "communication-efficient federated learning optimization and architecture",
    "topk": [
      {
        "node_idx": 100147,
        "score_0_10": 10,
        "title": "communication efficient learning of deep networks from decentralized data",
        "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. #R##N#We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
      },
      {
        "node_idx": 45381,
        "score_0_10": 9,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 84316,
        "score_0_10": 9,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 34735,
        "score_0_10": 8,
        "title": "federated learning strategies for improving communication efficiency",
        "abstract": "Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude."
      },
      {
        "node_idx": 111390,
        "score_0_10": 8,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 85721,
        "score_0_10": 8,
        "title": "reinforcement learning a survey",
        "abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."
      },
      {
        "node_idx": 137724,
        "score_0_10": 8,
        "title": "market oriented cloud computing vision hype and reality for delivering it services as computing utilities",
        "abstract": "This keynote paper: presents a 21st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21st century vision."
      },
      {
        "node_idx": 27774,
        "score_0_10": 8,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 100857,
        "score_0_10": 8,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      }
    ]
  },
  "41": {
    "explanation": "automated theorem proving and educational proof verification methods",
    "topk": [
      {
        "node_idx": 58946,
        "score_0_10": 10,
        "title": "learning how to prove from the coq proof assistant to textbook style",
        "abstract": "We have developed an alternative approach to teaching computer science students how to prove. First, students are taught how to prove theorems with the Coq proof assistant. In a second, more difficult, step students will transfer their acquired skills to the area of textbook proofs. In this article we present a realisation of the second step. #R##N#Proofs in Coq have a high degree of formality while textbook proofs have only a medium one. Therefore our key idea is to reduce the degree of formality from the level of Coq to textbook proofs in several small steps. For that purpose we introduce three proof styles between Coq and textbook proofs, called line by line comments, weakened line by line comments, and structure faithful proofs. #R##N#While this article is mostly conceptional we also report on experiences with putting our approach into practise."
      },
      {
        "node_idx": 148390,
        "score_0_10": 10,
        "title": "abstract computability algebraic specification and initiality",
        "abstract": "computable functions are defined by abstract finite deterministic algorithms on many-sorted algebras. We show that there exist finite universal algebraic specifications that specify uniquely (up to isomorphism) (i) all abstract computable functions on any many-sorted algebra; and (ii) all functions effectively approximable by abstract computable functions on any metric algebra. #R##N#We show that there exist universal algebraic specifications for all the classically computable functions on the set R of real numbers. The algebraic specifications used are mainly bounded universal equations and conditional equations. We investigate the initial algebra semantics of these specifications, and derive situations where algebraic specifications define precisely the computable functions."
      },
      {
        "node_idx": 94126,
        "score_0_10": 10,
        "title": "an adaptive and intelligent tutor by expert systems for mobile devices",
        "abstract": "Mobile Learning (M-Learning) is an emerging discipline in the area of education and educational technology. So researchers are trying to optimize and expanding its application in the field of education. The aim of this paper is to investigate the role of mobile devices and expert systems in disseminating and supporting the knowledge gained by intelligent tutors and to propose a system based on integration of intelligent M-Learning with expert systems. It acts as an intelligent tutor which can perform three processes - pre-test, learning concept and post-test - according to characteristic of the learner. The proposed system can improves the education efficiency highly as well as decreases costs. As a result, every time and everywhere (ETEW) simple and cheap learning would be provided via SMS, MMS and so on in this system. The global intention of M-Learning is to make learning \"a way of being\"."
      },
      {
        "node_idx": 52155,
        "score_0_10": 10,
        "title": "chestx ray8 hospital scale chest x ray database and benchmarks on weakly supervised classification and localization of common thorax diseases",
        "abstract": "The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems."
      },
      {
        "node_idx": 54545,
        "score_0_10": 10,
        "title": "a web based multilingual intelligent tutor system based on jackson s learning styles profiler and expert systems",
        "abstract": "Nowadays, Intelligent Tutoring Systems (ITSs) are so regarded in order to improve education quality via new technologies in this area. One of the problems is that the language of ITSs is different from the learner's. It forces the learners to learn the system language. This paper tries to remove this necessity by using an Automatic Translator Component in system structure like Google Translate API. This system carry out a pre-test and post-test by using Expert System and Jackson Model before and after of training a concept. It constantly updates learner model to save all changes in learning process. So this paper offers an E-Learning system which is web-based, intelligent, adaptive, multilingual and remotely accessible where tutors and learners can have non-identical language. It is also applicable Every Time and Every Where (ETEW). Furthermore, it trains the concepts in the best method with any language and low cost."
      },
      {
        "node_idx": 82955,
        "score_0_10": 10,
        "title": "web based cross language plagiarism detection",
        "abstract": "As the Internet help us cross language and cultural border by providing different types of translation tools, cross language plagiarism, also known as translation plagiarism are bound to arise. Especially among the academic works, such issue will definitely affect the student's works including the quality of their assignments and paper works. In this paper, we propose a new approach in detecting cross language plagiarism. Our web based cross language plagiarism detection system is specially tuned to detect translation plagiarism by implementing different techniques and tools to assist the detection process. Google Translate API is used as our translation tool and Google Search API, which is used in our information retrieval process. Our system is also integrated with the fingerprint matching technique, which is a widely used plagiarism detection technique. In general, our proposed system is started by translating the input documents from Malay to English, followed by removal of stop words and stemming words, identification of similar documents in corpus, comparison of similar pattern and finally summary of the result. Three least-frequent 4-grams fingerprint matching is used to implement the core comparison phase during the plagiarism detection process. In K-gram fingerprint matching technique, although any value of K can be considered, yet K = 4 was stated as an ideal choice. This is because smaller values of K (i.e., K = 1, 2, or 3), do not provide good discrimination between sentences. On the other hand, the larger the values of K (i.e., K = 5, 6, 7...etc), the better discrimination of words in one sentence from words in another."
      },
      {
        "node_idx": 81844,
        "score_0_10": 10,
        "title": "the elusive model of technology media social development and financial sustainability",
        "abstract": "We recount in this essay the decade-long story of Gram Vaani, a social enterprise with a vision to build appropriate ICTs (Information and Communication Technologies) for participatory media in rural and low-income settings, to bring about social development and community empowerment. Other social enterprises will relate to the learning gained and the strategic pivots that Gram Vaani had to undertake to survive and deliver on its mission, while searching for a robust financial sustainability model. While we believe the ideal model still remains elusive, we conclude this essay with an open question about the reason to differentiate between different kinds of enterprises - commercial or social, for-profit or not-for-profit - and argue that all enterprises should have an ethical underpinning to their work."
      },
      {
        "node_idx": 1821,
        "score_0_10": 10,
        "title": "the elfe system verifying mathematical proofs of undergraduate students",
        "abstract": "Elfe is an interactive system for teaching basic proof methods in discrete mathematics. The user inputs a mathematical text written in fair English which is converted to a special data-structure of first-order formulas. Certain proof obligations implied by this intermediate representation are checked by automated theorem provers which try to either prove the obligations or find countermodels if an obligation is wrong. The result of the verification process is then returned to the user. Elfe is implemented in Haskell and can be accessed via a reactive web interface or from the command line. Background libraries for sets, relations and functions have been developed. It has been tested by students in the beginning of their mathematical studies."
      },
      {
        "node_idx": 20042,
        "score_0_10": 10,
        "title": "features based text similarity detection",
        "abstract": "As the Internet help us cross cultural border by providing different information, plagiarism issue is bound to arise. As a result, plagiarism detection becomes more demanding in overcoming this issue. Different plagiarism detection tools have been developed based on various detection techniques. Nowadays, fingerprint matching technique plays an important role in those detection tools. However, in handling some large content articles, there are some weaknesses in fingerprint matching technique especially in space and time consumption issue. In this paper, we propose a new approach to detect plagiarism which integrates the use of fingerprint matching technique with four key features to assist in the detection process. These proposed features are capable to choose the main point or key sentence in the articles to be compared. Those selected sentence will be undergo the fingerprint matching process in order to detect the similarity between the sentences. Hence, time and space usage for the comparison process is reduced without affecting the effectiveness of the plagiarism detection."
      },
      {
        "node_idx": 118331,
        "score_0_10": 10,
        "title": "improving the correct eventual consistency tool",
        "abstract": "Preserving invariants while designing distributed applications under weak consistency models is difficult. The CEC (Correct Eventual Consistency Tool) is meant to aid the application designer in this task. It provides information about the errors during concurrent operations and suggestions on how and where to synchronize operations. This report presents two features of the tool: providing a counterexample for debugging and concurrency control suggestions."
      }
    ]
  },
  "42": {
    "explanation": "network information theory and secure MIMO communication principles",
    "topk": [
      {
        "node_idx": 46136,
        "score_0_10": 10,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      },
      {
        "node_idx": 32700,
        "score_0_10": 9,
        "title": "the secrecy capacity region of the gaussian mimo multi receiver wiretap channel",
        "abstract": "In this paper, we consider the Gaussian multiple-input multiple-output (MIMO) multi-receiver wiretap channel in which a transmitter wants to have confidential communication with an arbitrary number of users in the presence of an external eavesdropper. We derive the secrecy capacity region of this channel for the most general case. We first show that even for the single-input single-output (SISO) case, existing converse techniques for the Gaussian scalar broadcast channel cannot be extended to this secrecy context, to emphasize the need for a new proof technique. Our new proof technique makes use of the relationships between the minimum-mean-square-error and the mutual information, and equivalently, the relationships between the Fisher information and the differential entropy. Using the intuition gained from the converse proof of the SISO channel, we first prove the secrecy capacity region of the degraded MIMO channel, in which all receivers have the same number of antennas, and the noise covariance matrices can be arranged according to a positive semi-definite order. We then generalize this result to the aligned case, in which all receivers have the same number of antennas, however there is no order among the noise covariance matrices. We accomplish this task by using the channel enhancement technique. Finally, we find the secrecy capacity region of the general MIMO channel by using some limiting arguments on the secrecy capacity region of the aligned MIMO channel. We show that the capacity achieving coding scheme is a variant of dirty-paper coding with Gaussian signals."
      },
      {
        "node_idx": 145848,
        "score_0_10": 9,
        "title": "lecture notes on network information theory",
        "abstract": "These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at this http URL The previous (and obsolete) version of the lecture notes can be found at this http URL"
      },
      {
        "node_idx": 30590,
        "score_0_10": 9,
        "title": "exponential decreasing rate of leaked information in universal random privacy amplification",
        "abstract": "We derive a new upper bound for Eve's information in secret key generation from a common random number without communication. This bound improves on Bennett 's bound based on the Renyi entropy of order 2 because the bound obtained here uses the Renyi entropy of order 1+s for s \u2208 [0,1]. This bound is applied to a wire-tap channel. Then, we derive an exponential upper bound for Eve's information. Our exponent is compared with Hayashi 's exponent. For the additive case, the bound obtained here is better. The result is applied to secret key agreement by public discussion."
      },
      {
        "node_idx": 33272,
        "score_0_10": 9,
        "title": "the approximate capacity of the many to one and one to many gaussian interference channels",
        "abstract": "Recently, Etkin, Tse, and Wang found the capacity region of the two-user Gaussian interference channel to within 1 bit/s/Hz. A natural goal is to apply this approach to the Gaussian interference channel with an arbitrary number of users. We make progress towards this goal by finding the capacity region of the many-to-one and one-to-many Gaussian interference channels to within a constant number of bits. The result makes use of a deterministic model to provide insight into the Gaussian channel. The deterministic model makes explicit the dimension of signal level. A central theme emerges: the use of lattice codes for alignment of interfering signals on the signal level."
      },
      {
        "node_idx": 109425,
        "score_0_10": 9,
        "title": "ergodic interference alignment",
        "abstract": "This paper develops a new communication strategy, ergodic interference alignment, for the K-user interference channel with time-varying fading. At any particular time, each receiver will see a superposition of the transmitted signals plus noise. The standard approach to such a scenario results in each transmitter-receiver pair achieving a rate proportional to 1/K its interference-free ergodic capacity. However, given two well-chosen time indices, the channel coefficients from interfering users can be made to exactly cancel. By adding up these two observations, each receiver can obtain its desired signal without any interference. If the channel gains have independent, uniform phases, this technique allows each user to achieve at least 1/2 its interference-free ergodic capacity at any signal-to-noise ratio. Prior interference alignment techniques were only able to attain this performance as the signal-to-noise ratio tended to infinity. Extensions are given for the case where each receiver wants a message from more than one transmitter as well as the \"X channel\" case (with two receivers) where each transmitter has an independent message for each receiver. Finally, it is shown how to generalize this strategy beyond Gaussian channel models. For a class of finite field interference channels, this approach yields the ergodic capacity region."
      },
      {
        "node_idx": 48784,
        "score_0_10": 9,
        "title": "secrecy in cooperative relay broadcast channels",
        "abstract": "We investigate the effects of user cooperation on the secrecy of broadcast channels by considering a cooperative relay broadcast channel. We show that user cooperation can increase the achievable secrecy region. We propose an achievable scheme that combines Marton's coding scheme for broadcast channels and Cover and El Gamal's compress-and-forward scheme for relay channels. We derive outer bounds for the rate-equivocation region using auxiliary random variables for single-letterization. Finally, we consider a Gaussian channel and show that both users can have positive secrecy rates, which is not possible for scalar Gaussian broadcast channels without cooperation."
      },
      {
        "node_idx": 71257,
        "score_0_10": 8,
        "title": "the two user gaussian interference channel a deterministic view",
        "abstract": "This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel."
      },
      {
        "node_idx": 46067,
        "score_0_10": 8,
        "title": "dimension in complexity classes",
        "abstract": "A theory of resource-bounded dimension is developed using gales, which are natural generalizations of martingales. When the resource bound \\Delta (a parameter of the theory) is unrestricted, the resulting dimension is precisely the classical Hausdorff dimension (sometimes called fractal dimension). Other choices of the parameter \\Delta yield internal dimension theories in E, E2, ESPACE, and other complexity classes, and in the class of all decidable problems. In general, if C is such a class, then every set X of languages has a dimension in C, which is a real number dim(X|C) in [0,1]. Along with the elements of this theory, two preliminary applications are presented: #R##N#1. For every real number \\alpha in (0,1/2), the set FREQ(<=\\alpha), consisting of all languages that asymptotically contain at most \\alpha of all strings, has dimension H(\\alpha) -- the binary entropy of \\alpha -- in E and in E2. #R##N#2. For every real number \\alpha in (0,1), the set SIZE(\\alpha* (2^n)/n), consisting of all languages decidable by Boolean circuits of at most \\alpha*(2^n)/n gates, has dimension \\alpha in ESPACE."
      },
      {
        "node_idx": 53332,
        "score_0_10": 8,
        "title": "towards the secrecy capacity of the gaussian mimo wire tap channel the 2 2 1 channel",
        "abstract": "We find the secrecy capacity of the 2-2-1 Gaussian MIMO wiretap channel, which consists of a transmitter and a receiver with two antennas each, and an eavesdropper with a single antenna. We determine the secrecy capacity of this channel by proposing an achievable scheme and then developing a tight upper bound that meets the proposed achievable secrecy rate. We show that, for this channel, Gaussian signalling in the form of beam-forming is optimal, and no pre-processing of information is necessary."
      }
    ]
  },
  "45": {
    "explanation": "spatial transformation and localization in deep convolutional networks",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 22502,
        "score_0_10": 10,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 53950,
        "score_0_10": 10,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 73053,
        "score_0_10": 9,
        "title": "semantic image segmentation with deep convolutional nets and fully connected crfs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      }
    ]
  },
  "46": {
    "explanation": "iterated bounded hairpin completion closure properties",
    "topk": [
      {
        "node_idx": 133164,
        "score_0_10": 10,
        "title": "on the iterated hairpin completion",
        "abstract": "The (bounded) hairpin completion and its iterated versions are operations on formal lan- guages which have been inspired by the hairpin formation in DNA-biochemistry. The paper answers two questions asked in the literature about the iterated hairpin completion. #R##N#The first question is whether the class of regular languages is closed under iterated bounded hairpin completion. Here we show that this is true by providing a more general result which applies to all the classes of languages which are closed under finite union, intersection with regular sets, and concatenation with regular sets. In particular, all Chomsky classes and all standard complexity classes are closed under iterated bounded hairpin completion. #R##N#In the second part of the paper we address the question whether the iterated hairpin completion of a singleton is always regular. In contrast to the first question, this one has a negative answer. We exhibit an example of a singleton language whose iterated hairpin completion is not regular, actually it is not context-free, but context-sensitive."
      },
      {
        "node_idx": 100601,
        "score_0_10": 9,
        "title": "on the hairpin incompletion",
        "abstract": "Hairpin completion and its variant called bounded hairpin completion are operations on formal languages, inspired by a hairpin formation in molecular biology. Another variant called hairpin lengthening has been recently introduced, and the related closure properties and algorithmic problems concerning several families of languages have been studied. In this paper, we introduce a new operation of this kind, called hairpin incompletion which is not only an extension of bounded hairpin completion, but also a restricted (bounded) variant of hairpin lengthening. Further, the hairpin incompletion operation provides a formal language theoretic framework that models a bio-molecular technique nowadays known as Whiplash PCR. We study the closure properties of language families under both the operation and its iterated version. We show that a family of languages closed under intersection with regular sets, concatenation with regular sets, and finite union is closed under one-sided iterated hairpin incompletion, and that a family of languages containing all linear languages and closed under circular permutation, left derivative and substitution is also closed under iterated hairpin incompletion."
      },
      {
        "node_idx": 79738,
        "score_0_10": 9,
        "title": "llr based successive cancellation list decoding of polar codes",
        "abstract": "We show that successive cancellation list decoding can be formulated exclusively using log-likelihood ratios. In addition to numerical stability, the log-likelihood ratio based formulation has useful properties that simplify the sorting step involved in successive cancellation list decoding. We propose a hardware architecture of the successive cancellation list decoder in the log-likelihood ratio domain which, compared with a log-likelihood domain implementation, requires less irregular and smaller memories. This simplification, together with the gains in the metric sorter, lead to    $ 56\\%$   to   $137\\%$   higher throughput per unit area than other recently proposed architectures. We then evaluate the empirical performance of the CRC-aided successive cancellation list decoder at different list sizes using different CRCs and conclude that it is important to adapt the CRC length to the list size in order to achieve the best error-rate performance of concatenated polar codes. Finally, we synthesize conventional successive cancellation decoders at large block-lengths with the same block-error probability as our proposed CRC-aided successive cancellation list decoders to demonstrate that, while our decoders have slightly lower throughput and larger area, they have a significantly smaller decoding latency."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 88803,
        "score_0_10": 8,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 85171,
        "score_0_10": 8,
        "title": "spectral normalization for generative adversarial networks",
        "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques."
      },
      {
        "node_idx": 88168,
        "score_0_10": 8,
        "title": "semi supervised learning with deep generative models",
        "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning."
      },
      {
        "node_idx": 12113,
        "score_0_10": 8,
        "title": "gans trained by a two time scale update rule converge to a nash equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Frechet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark."
      },
      {
        "node_idx": 77197,
        "score_0_10": 8,
        "title": "adversarial feature learning",
        "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning."
      },
      {
        "node_idx": 124702,
        "score_0_10": 8,
        "title": "convolutional sequence to sequence learning",
        "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU."
      }
    ]
  },
  "47": {
    "explanation": "deep reinforcement learning for control and energy-efficient robotics",
    "topk": [
      {
        "node_idx": 84316,
        "score_0_10": 10,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 27774,
        "score_0_10": 10,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 134817,
        "score_0_10": 9,
        "title": "model predictive control with inverse statics optimization for tensegrity spine robots",
        "abstract": "Robots with flexible spines based on tensegrity structures have potential advantages over traditional designs with rigid torsos. However, these robots can be difficult to control due to their high-dimensional nonlinear dynamics and actuator constraints. This work presents two controllers for tensegrity spine robots, using model-predictive control (MPC) and inverse statics optimization. The controllers introduce two different approaches to making the control problem computationally tractable. The first utilizes smoothing terms in the MPC problem. The second uses a new inverse statics optimization algorithm, which gives the first feasible solutions to the problem for certain tensegrity robots, to generate reference input trajectories in combination with MPC. Tracking the inverse statics reference input trajectory significantly reduces the number of tuning parameters. The controllers are validated against simulations of two-dimensional and three-dimensional tensegrity spines. Both approaches show noise insensitivity and low tracking error, and can be used for different control goals. The results here demonstrate the first closed-loop control of such structures."
      },
      {
        "node_idx": 106315,
        "score_0_10": 9,
        "title": "soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor",
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
      },
      {
        "node_idx": 85721,
        "score_0_10": 9,
        "title": "reinforcement learning a survey",
        "abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."
      },
      {
        "node_idx": 86277,
        "score_0_10": 9,
        "title": "hierarchical deep reinforcement learning integrating temporal abstraction and intrinsic motivation",
        "abstract": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'."
      },
      {
        "node_idx": 104643,
        "score_0_10": 8,
        "title": "deep recurrent q learning for partially observable mdps",
        "abstract": "Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \\textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes."
      },
      {
        "node_idx": 10047,
        "score_0_10": 8,
        "title": "bio inspired tensegrity soft modular robots",
        "abstract": "In this paper, we introduce a design principle to develop novel soft modular robots based on tensegrity structures and inspired by the cytoskeleton of living cells. We describe a novel strategy to realize tensegrity structures using planar manufacturing techniques, such as 3D printing. We use this strategy to develop icosahedron tensegrity structures with programmable variable stiffness that can deform in a three-dimensional space. We also describe a tendon-driven contraction mechanism to actively control the deformation of the tensegrity mod-ules. Finally, we validate the approach in a modular locomotory worm as a proof of concept."
      },
      {
        "node_idx": 60577,
        "score_0_10": 8,
        "title": "a hybrid dynamic regenerative damping scheme for energy regeneration in variable impedance actuators",
        "abstract": "Increasing research efforts have been made to improve the energy efficiency of variable impedance actuators (VIAs) through reduction of energy consumption. However, the harvesting of dissipated energy in such systems remains under-explored. This study proposes a novel variable damping module design enabling energy regeneration in VIAs by exploiting the regenerative braking effect of DC motors. The proposed damping module uses four switches to combine regenerative and dynamic braking, in a hybrid approach that enables energy regeneration without reduction in the range of damping achievable. Numerical simulations and a physical experiment are presented in which the proposed module shows an optimal trade-off between task-performance and energy efficiency."
      },
      {
        "node_idx": 162098,
        "score_0_10": 8,
        "title": "energy regenerative damping in variable impedance actuators for long term robotic deployment",
        "abstract": "Energy efficiency is a crucial issue towards longterm deployment of compliant robots in the real world. In the context of variable impedance actuators (VIAs), one of the main focuses has been on improving energy efficiency through reduction of energy consumption. However, the harvesting of dissipated energy in such systems remains under-explored. This study proposes a novel variable damping module design enabling energy regeneration in VIAs by exploiting the regenerative braking effect of DC motors. The proposed damping module uses four switches to combine regenerative and dynamic braking, in a hybrid approach that enables energy regeneration without a reduction in the range of damping achievable. A physical implementation on a simple VIA mechanism is presented in which the regenerative properties of the proposed module are characterised and compared against theoretical predictions. To investigate the role of variable regenerative damping in terms of energy efficiency of longterm operation, experiments are reported in which the VIA equipped with the proposed damping module performs sequential reaching to a series of stochastic targets. The results indicate that the combination of variable stiffness and variable regenerative damping is preferable to achieve the optimal trade-off between task performance and energy efficiency. Use of the latter results in a 25% performance improvement on overall performance metrics (incorporating reaching accuracy, settling time, energy consumption and regeneration), over comparable schemes where either stiffness or damping are fixed."
      }
    ]
  },
  "51": {
    "explanation": "Reinforcement learning algorithms and neural network architectures for control tasks",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 27774,
        "score_0_10": 9,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 106315,
        "score_0_10": 9,
        "title": "soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor",
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 112674,
        "score_0_10": 9,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 84316,
        "score_0_10": 9,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 63929,
        "score_0_10": 9,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 12803,
        "score_0_10": 9,
        "title": "controlled sensing for multihypothesis testing",
        "abstract": "The problem of multiple hypothesis testing with observation control is considered in both fixed sample size and sequential settings. In the fixed sample size setting, for binary hypothesis testing, the optimal exponent for the maximal error probability corresponds to the maximum Chernoff information over the choice of controls, and a pure stationary open-loop control policy is asymptotically optimal within the larger class of all causal control policies. For multihypothesis testing in the fixed sample size setting, lower and upper bounds on the optimal error exponent are derived. It is also shown through an example with three hypotheses that the optimal causal control policy can be strictly better than the optimal open-loop control policy. In the sequential setting, a test based on earlier work by Chernoff for binary hypothesis testing, is shown to be first-order asymptotically optimal for multihypothesis testing in a strong sense, using the notion of decision making risk in place of the overall probability of error. Another test is also designed to meet hard risk constrains while retaining asymptotic optimality. The role of past information and randomization in designing optimal control policies is discussed."
      },
      {
        "node_idx": 155778,
        "score_0_10": 8,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      }
    ]
  },
  "55": {
    "explanation": "end-to-end CNN driving control from raw images",
    "topk": [
      {
        "node_idx": 45381,
        "score_0_10": 10,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 84316,
        "score_0_10": 8,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 119052,
        "score_0_10": 7,
        "title": "binary sequent calculi for truth invariance entailment of finite many valued logics",
        "abstract": "In this paper we consider the class of truth-functional many-valued logics with a finite set of truth-values. The main result of th is paper is the de- velopment of a new binary sequent calculi (each sequent is a pair of formulae) for many valued logic with a finite set of truth values, and of K ripke-like seman- tics for it that is both sound and complete. We did not use the logic entailment based on matrix with a strict subset of designated truth values, but a different new kind of semantics based on the generalization of the classic 2-valued truth- invariance entailment. In order to define this non-matrix ba sed sequent calculi, we transform many-valued logic into positive 2-valued multi-modal logic with classic conjunction, disjunction and finite set of modal con nectives. In this al- gebraic framework we define an uniquely determined axiom sys tem, by extend- ing the classic 2-valued distributive lattice logic (DLL) b y a new set of sequent axioms for many-valued logic connectives. Dually, in an autoreferential Kripke- style framework we obtain a uniquely determined frame, where each possible world is an equivalence class of Lindenbaum algebra for a many-valued logic as well, represented by a truth value."
      },
      {
        "node_idx": 74401,
        "score_0_10": 7,
        "title": "on paraconsistent weakening of intuitionistic negation",
        "abstract": "In [1], systems of weakening of intuitionistic negation logic called Z_n and CZ_n were developed in the spirit of da Costa's approach(c.f. [2]) by preserving, differently from da Costa, its fundamental properties: antitonicity, inversion and additivity for distributive lattices. However, according to [3], those systems turned out to be not paraconsistent but extensions of intuitionistic logic. Taking into account of this result, we shall here make some observations on the modified systems of Z_n and CZ_n, that are paraconsistent as well."
      },
      {
        "node_idx": 34657,
        "score_0_10": 7,
        "title": "consistent transformations of belief functions",
        "abstract": "Consistent belief functions represent collections of coherent or non-contradictory pieces of evidence, but most of all they are the counterparts of consistent knowledge bases in belief calculus. The use of consistent transformations cs[ ] in a reasoning process to guarantee coherence can therefore be desirable, and generalizes similar techniques in classical logics. Transformations can be obtained by minimizing an appropriate distance measure between the original belief function and the collection of consistent ones. We focus here on the case in which distances are measured using classical Lp norms, in both the \\mass space\" and the \\belief space\" representation of belief functions. While mass consistent approximations reassign the mass not focussed on a chosen element of the frame either to the whole frame or to all supersets of the element on an equal basis, approximations in the belief space do distinguish these focal elements according to the \\focussed consistent transformation\" principle. The dierent approximations are interpreted and compared, with the help of examples."
      },
      {
        "node_idx": 153214,
        "score_0_10": 7,
        "title": "belief and surprise a belief function formulation",
        "abstract": "We motivate and describe a theory of belief in this paper. This theory is developed with the following view of human belief in mind. Consider the belief that an event E will occur (or has occurred or is occurring). An agent either entertains this belief or does not entertain this belief (i.e., there is no \"grade\" in entertaining the belief). If the agent chooses to exercise \"the will to believe\" and entertain this belief, he/she/it is entitled to a degree of confidence c (1 > c > 0) in doing so. Adopting this view of human belief, we conjecture that whenever an agent entertains the belief that E will occur with c degree of confidence, the agent will be surprised (to the extent c) upon realizing that E did not occur."
      },
      {
        "node_idx": 142463,
        "score_0_10": 7,
        "title": "the nature of the unnormalized beliefs encountered in the transferable belief model",
        "abstract": "Within the transferable belief model, positive basic belief masses can be allocated to the empty set, leading to unnormalized belief functions. The nature of these unnormalized beliefs is analyzed."
      },
      {
        "node_idx": 30401,
        "score_0_10": 7,
        "title": "reduction of many valued into two valued modal logics",
        "abstract": "In this paper we develop a 2-valued reduction of many-valued logics, into 2-valued multi-modal logics. Such an approach is based on the contextualization of many-valued logics with the introduction of higher-order Herbrand interpretation types, where we explicitly introduce the coexistence of a set of algebraic truth values of original many-valued logic, transformed as parameters (or possible worlds), and the set of classic two logic values. This approach is close to the approach used in annotated logics, but offers the possibility of using the standard semantics based on Herbrand interpretations. Moreover, it uses the properties of the higher-order Herbrand types, as their fundamental nature is based on autoreferential Kripke semantics where the possible worlds are algebraic truth-values of original many-valued logic. This autoreferential Kripke semantics, which has the possibility of flattening higher-order Herbrand interpretations into ordinary 2-valued Herbrand interpretations, gives us a clearer insight into the relationship between many-valued and 2-valued multi-modal logics. This methodology is applied to the class of many-valued Logic Programs, where reduction is done in a structural way, based on the logic structure (logic connectives) of original many-valued logics. Following this, we generalize the reduction to general structural many-valued logics, in an abstract way, based on Suszko's informal non-constructive idea. In all cases, by using developed 2-valued reductions we obtain a kind of non truth-valued modal meta-logics, where two-valued formulae are modal sentences obtained by application of particular modal operators to original many-valued formulae."
      },
      {
        "node_idx": 151785,
        "score_0_10": 7,
        "title": "distance semantics for belief revision",
        "abstract": "A vast and interesting family of natural semantics for belief revision is defined. Suppose one is given a distance d between any two models. One may then define the revision of a theory K by a formula a as the theory defined by the set of all those models of a that are closest, by d, to the set of models of K. This family is characterized by a set of rationality postulates that extends the AGM postulates. The new postulates describe properties of iterated revisions."
      },
      {
        "node_idx": 98234,
        "score_0_10": 7,
        "title": "the mythos of model interpretability",
        "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not."
      }
    ]
  },
  "58": {
    "explanation": "effective optimization and reinforcement learning methods for neural networks",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 11752,
        "score_0_10": 9,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 98234,
        "score_0_10": 9,
        "title": "the mythos of model interpretability",
        "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not."
      },
      {
        "node_idx": 84316,
        "score_0_10": 8,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 51364,
        "score_0_10": 8,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 29312,
        "score_0_10": 8,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 26180,
        "score_0_10": 8,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 27774,
        "score_0_10": 8,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 106315,
        "score_0_10": 8,
        "title": "soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor",
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
      }
    ]
  },
  "59": {
    "explanation": "network coding and error correction in communication systems",
    "topk": [
      {
        "node_idx": 41252,
        "score_0_10": 10,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 79165,
        "score_0_10": 9,
        "title": "optimal beamforming for two way multi antenna relay channel with analogue network coding",
        "abstract": "This paper studies the wireless two-way relay channel (TWRC), where two source nodes, S1 and S2, exchange information through an assisting relay node, R. It is assumed that R receives the sum signal from S1 and S2 in one time-slot, and then amplifies and forwards the received signal to both S1 and S2 in the next time-slot. By applying the principle of analogue network (ANC), each of S1 and S2 cancels the so-called \"self-interference\" in the received signal from R and then decodes the desired message. Assuming that S1 and S2 are each equipped with a single antenna and R with multi-antennas, this paper analyzes the capacity region of an ANC-based TWRC with linear processing (beamforming) at R. The capacity region contains all the achievable bidirectional rate-pairs of S1 and S2 under the given transmit power constraints at S1, S2, and R. We present the optimal relay beamforming structure as well as an efficient algorithm to compute the optimal beamforming matrix based on convex optimization techniques. Low-complexity suboptimal relay beamforming schemes are also presented, and their achievable rates are compared against the capacity with the optimal scheme."
      },
      {
        "node_idx": 140389,
        "score_0_10": 9,
        "title": "cyclic codes from cyclotomic sequences of order four",
        "abstract": "Cyclic codes are an interesting subclass of linear codes and have been used in consumer electronics, data transmission technologies, broadcast systems, and computer applications due to their efficient encoding and decoding algorithms. In this paper, three cyclotomic sequences of order four are employed to construct a number of classes of cyclic codes over $\\gf(q)$ with prime length. Under certain conditions lower bounds on the minimum weight are developed. Some of the codes obtained are optimal or almost optimal. In general, the cyclic codes constructed in this paper are very good. Some of the cyclic codes obtained in this paper are closely related to almost difference sets and difference sets. As a byproduct, the $p$-rank of these (almost) difference sets are computed."
      },
      {
        "node_idx": 58129,
        "score_0_10": 9,
        "title": "the weight distributions of cyclic codes and elliptic curves",
        "abstract": "Cyclic codes with two zeros and their dual codes as a practically and theoretically interesting class of linear codes, have been studied for many years. However, the weight distributions of cyclic codes are difficult to determine. From elliptic curves, this paper determines the weight distributions of dual codes of cyclic codes with two zeros for a few more cases."
      },
      {
        "node_idx": 167605,
        "score_0_10": 9,
        "title": "network protection codes providing self healing in autonomic networks using network coding",
        "abstract": "Agile recovery from link failures in autonomic communication networks is essential to increase robustness, accessibility, and reliability of data transmission. However, this must be done with the least amount of protection resources, while using simple management plane functionality. Recently, network coding has been proposed as a solution to provide agile and cost efficient network self-healing against link failures, in a manner that does not require data rerouting, packet retransmission, or failure localization, hence leading to simple control and management planes. To achieve this, separate paths have to be provisioned to carry encoded packets, hence requiring either the addition of extra links, or reserving some of the resources for this purpose. #R##N#In this paper we introduce autonomic self-healing strategies for autonomic networks in order to protect against link failures. The strategies are based on network coding and reduced capacity, which is a technique that we call network protection codes (NPC). In these strategies, an autonomic network is able to provide self-healing from various network failures affecting network operation. The techniques improve service and enhance reliability of autonomic communication. #R##N#Network protection codes are extended to provide self-healing from multiple link failures in autonomic networks. We provide implementation aspects of the proposed strategies. We present bounds and network protection code constructions. Finally, we study the construction of such codes over the binary field. The paper also develops an Integer Linear Program formulation to evaluate the cost of provisioning connections using the proposed strategies."
      },
      {
        "node_idx": 160337,
        "score_0_10": 9,
        "title": "network protection codes against link failures using network coding",
        "abstract": "Protecting against link failures in communication networks is essential to increase robustness, accessibility, and reliability of data transmission. Recently, network coding has been proposed as a solution to provide agile and cost efficient network protection against link failures, which does not require data rerouting, or packet retransmission. To achieve this, separate paths have to be provisioned to carry encoded packets, hence requiring either the addition of extra links, or reserving some of the resources for this purpose. In this paper, we propose network protection codes against a single link failure using network coding, where a separate path using reserved links is not needed. In this case portions of the link capacities are used to carry the encoded packets. The scheme is extended to protect against multiple link failures and can be implemented at an overlay layer. Although this leads to reducing the network capacity, the network capacity reduction is asymptotically small in most cases of practical interest. We demonstrate that such network protection codes are equivalent to error correcting codes for erasure channels. Finally, we study the encoding and decoding operations of such codes over the binary field."
      },
      {
        "node_idx": 3489,
        "score_0_10": 9,
        "title": "weight distributions of cyclic codes with respect to pairwise coprime order elements",
        "abstract": "Let $\\Bbb F_r$ be an extension of a finite field $\\Bbb F_q$ with $r=q^m$. Let each $g_i$ be of order $n_i$ in $\\Bbb F_r^*$ and $\\gcd(n_i, n_j)=1$ for $1\\leq i \\neq j \\leq u$. #R##N#We define a cyclic code over $\\Bbb F_q$ by #R##N#$$\\mathcal C_{(q, m, n_1,n_2, ..., n_u)}=\\{c(a_1, a_2, ..., a_u) : a_1, a_2, ..., a_u \\in \\Bbb F_r\\},$$ where #R##N#$$c(a_1, a_2, ..., a_u)=({Tr}_{r/q}(\\sum_{i=1}^ua_ig_i^0), ..., {Tr}_{r/q}(\\sum_{i=1}^ua_ig_i^{n-1}))$$ and $n=n_1n_2... n_u$. In this paper, we present a method to compute the weights of $\\mathcal C_{(q, m, n_1,n_2, ..., n_u)}$. Further, we determine the weight distributions of the cyclic codes $\\mathcal C_{(q, m, n_1,n_2)}$ and $\\mathcal C_{(q, m, n_1,n_2,1)}$."
      },
      {
        "node_idx": 32700,
        "score_0_10": 9,
        "title": "the secrecy capacity region of the gaussian mimo multi receiver wiretap channel",
        "abstract": "In this paper, we consider the Gaussian multiple-input multiple-output (MIMO) multi-receiver wiretap channel in which a transmitter wants to have confidential communication with an arbitrary number of users in the presence of an external eavesdropper. We derive the secrecy capacity region of this channel for the most general case. We first show that even for the single-input single-output (SISO) case, existing converse techniques for the Gaussian scalar broadcast channel cannot be extended to this secrecy context, to emphasize the need for a new proof technique. Our new proof technique makes use of the relationships between the minimum-mean-square-error and the mutual information, and equivalently, the relationships between the Fisher information and the differential entropy. Using the intuition gained from the converse proof of the SISO channel, we first prove the secrecy capacity region of the degraded MIMO channel, in which all receivers have the same number of antennas, and the noise covariance matrices can be arranged according to a positive semi-definite order. We then generalize this result to the aligned case, in which all receivers have the same number of antennas, however there is no order among the noise covariance matrices. We accomplish this task by using the channel enhancement technique. Finally, we find the secrecy capacity region of the general MIMO channel by using some limiting arguments on the secrecy capacity region of the aligned MIMO channel. We show that the capacity achieving coding scheme is a variant of dirty-paper coding with Gaussian signals."
      },
      {
        "node_idx": 61110,
        "score_0_10": 9,
        "title": "capacity analysis of one bit quantized mimo systems with transmitter channel state information",
        "abstract": "With bandwidths on the order of a gigahertz in emerging wireless systems, high-resolution analog-to-digital convertors (ADCs) become a power consumption bottleneck. One solution is to employ low resolution one-bit ADCs. In this paper, we analyze the flat fading multiple-input multiple-output (MIMO) channel with one-bit ADCs. Channel state information is assumed to be known at both the transmitter and receiver. For the multiple-input single-output channel, we derive the exact channel capacity. For the single-input multiple-output and MIMO channel, the capacity at infinite signal-to-noise ratio (SNR) is found. We also derive upper bound at finite SNR, which is tight when the channel has full row rank. In addition, we propose an efficient method to design the input symbols to approach the capacity achieving solution. We incorporate millimeter wave channel characteristics and find the bounds on the infinite SNR capacity. The results show how the number of paths and number of receive antennas impact the capacity."
      },
      {
        "node_idx": 105486,
        "score_0_10": 9,
        "title": "joint physical layer coding and network coding for bi directional relaying",
        "abstract": "We consider the problem of two transmitters wishing to exchange information through a relay in the middle. The channels between the transmitters and the relay are assumed to be synchronized, average power constrained additive white Gaussian noise channels with a real input with signal-to-noise ratio (SNR) of snr. An upper bound on the capacity is 1/2 log(1+ snr) bits per transmitter per use of the medium-access phase and broadcast phase of the bi-directional relay channel. We show that using lattice codes and lattice decoding, we can obtain a rate of 1/2 log(0.5 + snr) bits per transmitter, which is essentially optimal at high SNRs. The main idea is to decode the sum of the codewords modulo a lattice at the relay followed by a broadcast phase which performs Slepian-Wolf coding with structured codes. For asymptotically low SNR's, jointly decoding the two transmissions at the relay (MAC channel) is shown to be optimal. We also show that if the two transmitters use identical lattices with minimum angle decoding, we can achieve the same rate of 1/2 log(0.5 + snr). The proposed scheme can be thought of as a joint physical layer, network layer code which outperforms other recently proposed analog network coding schemes."
      }
    ]
  },
  "63": {
    "explanation": "generative adversarial networks for high-quality image synthesis",
    "topk": [
      {
        "node_idx": 88321,
        "score_0_10": 10,
        "title": "deep generative image models using a laplacian pyramid of adversarial networks",
        "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset."
      },
      {
        "node_idx": 12113,
        "score_0_10": 10,
        "title": "gans trained by a two time scale update rule converge to a nash equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Frechet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark."
      },
      {
        "node_idx": 143193,
        "score_0_10": 10,
        "title": "on deep holes of projective reed solomon codes",
        "abstract": "In this paper, we obtain new results on the covering radius and deep holes for projective Reed-Solomon (PRS) codes."
      },
      {
        "node_idx": 56248,
        "score_0_10": 10,
        "title": "explicit deep holes of reed solomon codes",
        "abstract": "In this paper, deep holes of Reed-Solomon (RS) codes are studied. A new class of deep holes for generalized affine RS codes is given if the evaluation set satisfies certain combinatorial structure. Three classes of deep holes for projective Reed-Solomon (PRS) codes are constructed explicitly. In particular, deep holes of PRS codes with redundancy three are completely obtained when the characteristic of the finite field is odd. Most (asymptotically of ratio $1$) of the deep holes of PRS codes with redundancy four are also obtained."
      },
      {
        "node_idx": 94809,
        "score_0_10": 10,
        "title": "deep holes and mds extensions of reed solomon codes",
        "abstract": "We study the problem of classifying deep holes of Reed-Solomon codes. We show that this problem is equivalent to the problem of classifying MDS extensions of Reed-Solomon codes by one digit. This equivalence allows us to improve recent results on the former problem. In particular, we classify deep holes of Reed-Solomon codes of dimension greater than half the alphabet size. #R##N#We also give a complete classification of deep holes of Reed Solomon codes with redundancy three in all dimensions."
      },
      {
        "node_idx": 85171,
        "score_0_10": 9,
        "title": "spectral normalization for generative adversarial networks",
        "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques."
      },
      {
        "node_idx": 163808,
        "score_0_10": 9,
        "title": "resolution of indirect anaphora in japanese sentences using examples x no y y of x",
        "abstract": "A noun phrase can indirectly refer to an entity that has already been mentioned. For example, ``I went into an old house last night. The roof was leaking badly and ...'' indicates that ``the roof'' is associated with `` an old house}'', which was mentioned in the previous sentence. This kind of reference (indirect anaphora) has not been studied well in natural language processing, but is important for coherence resolution, language understanding, and machine translation. In order to analyze indirect anaphora, we need a case frame dictionary for nouns that contains knowledge of the relationships between two nouns but no such dictionary presently exists. Therefore, we are forced to use examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead. We tried estimating indirect anaphora using this information and obtained a recall rate of 63% and a precision rate of 68% on test sentences. This indicates that the information of ``X no Y'' is useful to a certain extent when we cannot make use of a noun case frame dictionary. We estimated the results that would be given by a noun case frame dictionary, and obtained recall and precision rates of 71% and 82% respectively. Finally, we proposed a way to construct a noun case frame dictionary by using examples of ``X no Y.''"
      },
      {
        "node_idx": 140427,
        "score_0_10": 9,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 86850,
        "score_0_10": 9,
        "title": "least squares generative adversarial networks",
        "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs."
      },
      {
        "node_idx": 112726,
        "score_0_10": 9,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      }
    ]
  },
  "65": {
    "explanation": "wireless and cognitive radio network capacity optimization techniques",
    "topk": [
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 28821,
        "score_0_10": 10,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 46136,
        "score_0_10": 9,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      },
      {
        "node_idx": 131128,
        "score_0_10": 9,
        "title": "design and characterization of a full duplex multiantenna system for wifi networks",
        "abstract": "In this paper, we present an experiment- and simulation-based study to evaluate the use of full duplex (FD) as a potential mode in practical IEEE 802.11 networks. To enable the study, we designed a 20-MHz multiantenna orthogonal frequency-division-multiplexing (OFDM) FD physical layer and an FD media access control (MAC) protocol, which is backward compatible with current 802.11. Our extensive over-the-air experiments, simulations, and analysis demonstrate the following two results. First, the use of multiple antennas at the physical layer leads to a higher ergodic throughput than its hardware-equivalent multiantenna half-duplex (HD) counterparts for SNRs above the median SNR encountered in practical WiFi deployments. Second, the proposed MAC translates the physical layer rate gain into near doubling of throughput for multinode single-AP networks. The two results allow us to conclude that there are potentially significant benefits gained from including an FD mode in future WiFi standards."
      },
      {
        "node_idx": 35908,
        "score_0_10": 8,
        "title": "exploiting multi antennas for opportunistic spectrum sharing in cognitive radio networks",
        "abstract": "In cognitive radio (CR) networks, there are scenarios where the secondary (lower priority) users intend to communicate with each other by opportunistically utilizing the transmit spectrum originally allocated to the existing primary (higher priority) users. For such a scenario, a secondary user usually has to tradeoff between two conflicting goals at the same time: one is to maximize its own transmit throughput; and the other is to minimize the amount of interference it produces at each primary receiver. In this paper, we study this fundamental tradeoff from an information-theoretic perspective by characterizing the secondary user's channel capacity under both its own transmit-power constraint as well as a set of interference-power constraints each imposed at one of the primary receivers. In particular, this paper exploits multi-antennas at the secondary transmitter to effectively balance between spatial multiplexing for the secondary transmission and interference avoidance at the primary receivers. Convex optimization techniques are used to design algorithms for the optimal secondary transmit spatial spectrum that achieves the capacity of the secondary transmission. Suboptimal solutions for ease of implementation are also presented and their performances are compared with the optimal solution. Furthermore, algorithms developed for the single-channel transmission are also extended to the case of multichannel transmission whereby the secondary user is able to achieve opportunistic spectrum sharing via transmit adaptations not only in space, but in time and frequency domains as well. Simulation results show that even under stringent interference-power constraints, substantial capacity gains are achievable for the secondary transmission by employing multi-antennas at the secondary transmitter. This is true even when the number of primary receivers exceeds that of secondary transmit antennas in a CR network, where an interesting \"interference diversity\" effect can be exploited."
      },
      {
        "node_idx": 130596,
        "score_0_10": 8,
        "title": "cooperative relay broadcast channels",
        "abstract": "The capacity regions are investigated for two relay broadcast channels (RBCs), where relay links are incorporated into standard two-user broadcast channels to support user cooperation. In the first channel, the Partially Cooperative Relay Broadcast Channel, only one user in the system can act as a relay and transmit to the other user through a relay link. An achievable rate region is derived based on the relay using the decode-and-forward scheme. An outer bound on the capacity region is derived and is shown to be tighter than the cut-set bound. For the special case where the Partially Cooperative RBC is degraded, the achievable rate region is shown to be tight and provides the capacity region. Gaussian Partially Cooperative RBCs and Partially Cooperative RBCs with feedback are further studied. In the second channel model being studied in the paper, the Fully Cooperative Relay Broadcast Channel, both users can act as relay nodes and transmit to each other through relay links. This is a more general model than the Partially Cooperative RBC. All the results for Partially Cooperative RBCs are correspondingly generalized to the Fully Cooperative RBCs. It is further shown that the AWGN Fully Cooperative RBC has a larger achievable rate region than the AWGN Partially Cooperative RBC. The results illustrate that relaying and user cooperation are powerful techniques in improving the capacity of broadcast channels."
      },
      {
        "node_idx": 83522,
        "score_0_10": 8,
        "title": "optimal multiband joint detection for spectrum sensing in cognitive radio networks",
        "abstract": "Spectrum sensing is an essential functionality that enables cognitive radios to detect spectral holes and to opportunistically use under-utilized frequency bands without causing harmful interference to legacy (primary) networks. In this paper, a novel wideband spectrum sensing technique referred to as multiband joint detection is introduced, which jointly detects the primary signals over multiple frequency bands rather than over one band at a time. Specifically, the spectrum sensing problem is formulated as a class of optimization problems, which maximize the aggregated opportunistic throughput of a cognitive radio system under some constraints on the interference to the primary users. By exploiting the hidden convexity in the seemingly nonconvex problems, optimal solutions can be obtained for multiband joint detection under practical conditions. The situation in which individual cognitive radios might not be able to reliably detect weak primary signals due to channel fading/shadowing is also considered. To address this issue by exploiting the spatial diversity, a cooperative wideband spectrum sensing scheme refereed to as spatial-spectral joint detection is proposed, which is based on a linear combination of the local statistics from multiple spatially distributed cognitive radios. The cooperative sensing problem is also mapped into an optimization problem, for which suboptimal solutions can be obtained through mathematical transformation under conditions of practical interest. Simulation results show that the proposed spectrum sensing schemes can considerably improve system performance. This paper establishes useful principles for the design of distributed wideband spectrum sensing algorithms in cognitive radio networks."
      },
      {
        "node_idx": 145848,
        "score_0_10": 8,
        "title": "lecture notes on network information theory",
        "abstract": "These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at this http URL The previous (and obsolete) version of the lecture notes can be found at this http URL"
      },
      {
        "node_idx": 82954,
        "score_0_10": 8,
        "title": "on peak versus average interference power constraints for protecting primary users in cognitive radio networks",
        "abstract": "This paper considers spectrum sharing for wireless communication between a cognitive radio (CR) link and a primary radio (PR) link. It is assumed that the CR protects the PR transmission by applying the so-called interference-temperature constraint, whereby the CR is allowed to transmit regardless of the PR's on/off status provided that the resultant interference power level at the PR receiver is kept below some predefined threshold. For the fading PR and CR channels, the interference-power constraint at the PR receiver is usually one of the following two types: One is to regulate the average interference power (AIP) over all the fading states, while the other is to limit the peak interference power (PIP) at each fading state. From the CR's perspective, given the same average and peak power threshold, the AIP constraint is more favorable than the PIP counterpart because of its more flexibility for dynamically allocating transmit powers over the fading states. On the contrary, from the perspective of protecting the PR, the more restrictive PIP constraint appears at a first glance to be a better option than the AIP. Some surprisingly, this paper shows that in terms of various forms of capacity limits achievable for the PR fading channel, e.g., the ergodic and outage capacities, the AIP constraint is also superior over the PIP. This result is based upon an interesting interference diversity phenomenon, i.e., randomized interference powers over the fading states in the AIP case are more advantageous over deterministic ones in the PIP case for minimizing the resultant PR capacity losses. Therefore, the AIP constraint results in larger fading channel capacities than the PIP for both the CR and PR transmissions."
      },
      {
        "node_idx": 149803,
        "score_0_10": 8,
        "title": "inner and outer bounds for the gaussian cognitive interference channel and new capacity results",
        "abstract": "The capacity of the Gaussian cognitive interference channel, a variation of the classical two-user interference channel where one of the transmitters (referred to as cognitive) has knowledge of both messages, is known in several parameter regimes but remains unknown in general. This paper provides a comparative overview of this channel model as it proceeds through the following contributions. First, several outer bounds are presented: (a) a new outer bound based on the idea of a broadcast channel with degraded message sets, and (b) an outer bound obtained by transforming the channel into channels with known capacity. Next, a compact Fourier-Motzkin eliminated version of the largest known inner bound derived for the discrete memoryless cognitive interference channel is presented and specialized to the Gaussian noise case, where several simplified schemes with jointly Gaussian input are evaluated in closed form and later used to prove a number of results. These include a new set of capacity results for: (a) the \u201cprimary decodes cognitive\u201d regime, a subset of the \u201cstrong interference\u201d regime that is not included in the \u201cvery strong interference\u201d regime for which capacity was known, and (b) the \u201cS-channel in strong interference\u201d in which the primary transmitter does not interfere with the cognitive receiver and the primary receiver experiences strong interference. Next, for a general Gaussian channel the capacity is determined to within one bit/s/Hz and to within a factor two regardless of the channel parameters, thus establishing rate performance guarantees at high and low SNR, respectively. The paper concludes with numerical evaluations and comparisons of the various simplified achievable rate regions and outer bounds in parameter regimes where capacity is unknown, leading to further insight on the capacity region."
      }
    ]
  },
  "66": {
    "explanation": "optimizing routing and prediction in dynamic networks and logistics",
    "topk": [
      {
        "node_idx": 84237,
        "score_0_10": 10,
        "title": "aggregate estimation over dynamic hidden web databases",
        "abstract": "Many databases on the web are \"hidden\" behind (i.e., accessible only through) their restrictive, form-like, search interfaces. Recent studies have shown that it is possible to estimate aggregate query answers over such hidden web databases by issuing a small number of carefully designed search queries through the restrictive web interface. A problem with these existing work, however, is that they all assume the underlying database to be static, while most real-world web databases (e.g., Amazon, eBay) are frequently updated. In this paper, we study the novel problem of estimating/tracking aggregates over dynamic hidden web databases while adhering to the stringent query-cost limitation they enforce (e.g., at most 1,000 search queries per day). Theoretical analysis and extensive real-world experiments demonstrate the effectiveness of our proposed algorithms and their superiority over baseline solutions (e.g., the repeated execution of algorithms designed for static web databases)."
      },
      {
        "node_idx": 131218,
        "score_0_10": 10,
        "title": "hdbscan density based clustering over location based services",
        "abstract": "Location Based Services (LBS) have become extremely popular and used by millions of users. Popular LBS run the entire gamut from mapping services (such as Google Maps) to restaurants (such as Yelp) and real-estate (such as Redfin). The public query interfaces of LBS can be abstractly modeled as a kNN interface over a database of two dimensional points: given an arbitrary query point, the system returns the k points in the database that are nearest to the query point. Often, k is set to a small value such as 20 or 50. In this paper, we consider the novel problem of enabling density based clustering over an LBS with only a limited, kNN query interface. Due to the query rate limits imposed by LBS, even retrieving every tuple once is infeasible. Hence, we seek to construct a cluster assignment function f(.) by issuing a small number of kNN queries, such that for any given tuple t in the database which may or may not have been accessed, f(.) outputs the cluster assignment of t with high accuracy. We conduct a comprehensive set of experiments over benchmark datasets and popular real-world LBS such as Yahoo! Flickr, Zillow, Redfin and Google Maps."
      },
      {
        "node_idx": 135461,
        "score_0_10": 10,
        "title": "the electric two echelon vehicle routing problem",
        "abstract": "Abstract   Two-echelon distribution systems are attractive from an economical standpoint and help to keep large vehicles out of densely populated city centers. Large trucks can be used to deliver goods to intermediate facilities in accessible locations, whereas smaller vehicles allow to reach the final customers. Due to their reduced size, pollution, and noise, multiple companies consider using an electric fleet of terrestrial or aerial vehicles for last-mile deliveries.  Route planning in multi-tier logistics leads to notoriously difficult problems. This difficulty is accrued in the presence of an electric fleet since each vehicle operates on a smaller range and may require planned visits to recharging stations. To study these challenges, we introduce the electric two-echelon vehicle routing problem (E2EVRP) as a prototypical problem. We propose a large neighborhood search (LNS) metaheuristic as well as an exact mathematical programming algorithm, which uses decomposition techniques to enumerate promising first-level solutions in conjunction with bounding functions and route enumeration for the second-level routes. These algorithms produce optimal or near-optimal solutions for the problem and allow us to evaluate the impact of several defining features of optimized battery-powered distribution networks.  We created representative E2EVRP benchmark instances to simulate realistic metropolitan areas. In particular, we observe that the detour miles due to recharging decrease proportionally to 1/\u03c1x with x\u202f\u2248\u202f5/4 as a function of the charging stations density \u03c1; e.g., in a scenario where the density of charging stations is doubled, recharging detours are reduced by 58%. Finally, we evaluate the trade-off between battery capacity and detour miles. This estimate is critical for strategic fleet-acquisition decisions, in a context where large batteries are generally more costly and less environment-friendly."
      },
      {
        "node_idx": 32016,
        "score_0_10": 10,
        "title": "link prediction in foursquare network",
        "abstract": "Foursquare is an online social network and can be represented with a bipartite network of users and venues. A user-venue pair is connected if a user has checked-in at that venue. In the case of Foursquare, network analysis techniques can be used to enhance the user experience. One such technique is link prediction, which can be used to build a personalized recommendation system of venues. Recommendation systems in bipartite networks are very often designed using the global ranking method and collaborative filtering. A less known method- network based inference is also a feasible choice for link prediction in bipartite networks and sometimes performs better than the previous two. In this paper we test these techniques on the Foursquare network. The best technique proves to be the network based inference. We also show that taking into account the available metadata can be beneficial."
      },
      {
        "node_idx": 82433,
        "score_0_10": 10,
        "title": "a large neighbourhood based heuristic for two echelon routing problems",
        "abstract": "In this paper, we address two optimisation problems arising in the context of city logistics and two-level transportation systems. The two-echelon vehicle routing problem and the two-echelon location routing problem seek to produce vehicle itineraries to deliver goods to customers, with transits through intermediate facilities. To efficiently solve these problems, we propose a hybrid metaheuristic which combines enumerative local searches with destroy-and-repair principles, as well as some tailored operators to optimise the selections of intermediate facilities. We conduct extensive computational experiments to investigate the contribution of these operators to the search performance, and measure the performance of the method on both problem classes. The proposed algorithm finds the current best known solutions, or better ones, for 95% of the two-echelon vehicle routing problem benchmark instances. Overall, for both problems, it achieves high-quality solutions within short computing times. Finally, for future reference, we resolve inconsistencies between different versions of benchmark instances, document their differences, and provide them all online in a unified format. HighlightsWe introduce a simple metaheuristic based on LNS for the 2E-VRP and the 2E-LRP.The algorithm is tested with benchmark instances form literature, and outperforms previous methods.We clarified several mistakes and inconsistencies in the benchmark instances."
      },
      {
        "node_idx": 119222,
        "score_0_10": 10,
        "title": "web services dependency networks analysis",
        "abstract": "Along with a continuously growing number of publicly available Web services (WS), we are witnessing a rapid development in semantic-related web technologies, which lead to the apparition of semantically described WS. In this work, we perform a comparative analysis of the syntactic and semantic approaches used to describe WS, from a complex network perspective. First, we extract syntactic and semantic WS dependency networks from a collection of publicly available WS descriptions. Then, we take advantage of tools from the complex network field to analyze them and determine their topological properties. We show WS dependency networks exhibit some of the typical characteristics observed in real-world networks, such as small world and scale free properties, as well as community structure. By comparing syntactic and semantic networks through their topological properties, we show the introduction of semantics in WS description allows modeling more accurately the dependencies between parameters, which in turn could lead to improved composition mining methods."
      },
      {
        "node_idx": 125685,
        "score_0_10": 10,
        "title": "benefits of semantics on web service composition from a complex network perspective",
        "abstract": "The number of publicly available Web services (WS) is continuously growing, and in parallel, we are witnessing a rapid development in semantic-related web technologies. The intersection of the semantic web and WS allows the development of semantic WS. In this work, we adopt a complex network perspective to perform a comparative analysis of the syntactic and semantic approaches used to describe WS. From a collection of publicly available WS descriptions, we extract syntactic and semantic WS interaction networks. We take advantage of tools from the complex network field to analyze them and determine their properties. We show that WS interaction networks exhibit some of the typical characteristics observed in real-world networks, such as short average distance between nodes and community structure. By comparing syntactic and semantic networks through their properties, we show the introduction of semantics in WS descriptions should improve the composition process."
      },
      {
        "node_idx": 169148,
        "score_0_10": 9,
        "title": "naming the pain in requirements engineering a design for a global family of surveys and first results from germany",
        "abstract": "Abstract   Context  For many years, we have observed industry struggling in defining a high quality requirements engineering (RE) and researchers trying to understand industrial expectations and problems. Although we are investigating the discipline with a plethora of empirical studies, they still do not allow for empirical generalisations.    Objective  To lay an empirical and externally valid foundation about the state of the practice in RE, we aim at a series of open and reproducible surveys that allow us to steer future research in a problem-driven manner.    Method  We designed a globally distributed family of surveys in joint collaborations with different researchers and completed the first run in Germany. The instrument is based on a theory in the form of a set of hypotheses inferred from our experiences and available studies. We test each hypothesis in our theory and identify further candidates to extend the theory by correlation and Grounded Theory analysis.    Results  In this article, we report on the design of the family of surveys, its underlying theory, and the full results obtained from Germany with participants from 58 companies. The results reveal, for example, a tendency to improve RE via internally defined qualitative methods rather than relying on normative approaches like CMMI. We also discovered various RE problems that are statistically significant in practice. For instance, we could corroborate communication flaws or moving targets as problems in practice. Our results are not yet fully representative but already give first insights into current practices and problems in RE, and they allow us to draw lessons learnt for future replications.    Conclusion  Our results obtained from this first run in Germany make us confident that the survey design and instrument are well-suited to be replicated and, thereby, to create a generalisable empirical basis of RE in practice."
      },
      {
        "node_idx": 41210,
        "score_0_10": 9,
        "title": "when crowdsourcing meets mobile sensing a social network perspective",
        "abstract": "Mobile sensing is an emerging technology that utilizes agent-participatory data for decision making or state estimation, including multimedia applications. This article investigates the structure of mobile sensing schemes and introduces crowdsourcing methods for mobile sensing. Inspired by social networks, one can establish trust among participatory agents to leverage the wisdom of crowds for mobile sensing. A prototype of social-network-inspired mobile multimedia and sensing application is presented for illustrative purposes. Numerical experiments on real-world datasets show improved performance of mobile sensing via crowdsourcing. Challenges for mobile sensing with respect to Internet layers are discussed."
      },
      {
        "node_idx": 72548,
        "score_0_10": 9,
        "title": "reasoning with individuals for the description logic shiq",
        "abstract": "While there has been a great deal of work on the development of reasoning algorithms for expressive description logics, in most cases only Tbox reasoning is considered. In this paper we present an algorithm for combined Tbox and Abox reasoning in the SHIQ description logic. This algorithm is of particular interest as it can be used to decide the problem of (database) conjunctive query containment w.r.t. a schema. Moreover, the realisation of an efficient implementation should be relatively straightforward as it can be based on an existing highly optimised implementation of the Tbox algorithm in the FaCT system."
      }
    ]
  },
  "67": {
    "explanation": "scholarly impact measurement using citations and social media metrics",
    "topk": [
      {
        "node_idx": 40714,
        "score_0_10": 10,
        "title": "tweeting biomedicine an analysis of tweets and citations in the biomedical literature",
        "abstract": "Data collected by social media platforms have recently been introduced as a new source for indicators to help measure the impact of scholarly research in ways that are complementary to traditional citation-based indicators. Data generated from social media activities related to scholarly content can be used to reflect broad types of impact. This paper aims to provide systematic evidence regarding how often Twitter is used to diffuse journal articles in the biomedical and life sciences. The analysis is based on a set of 1.4 million documents covered by both PubMed and Web of Science (WoS) and published between 2010 and 2012. The number of tweets containing links to these documents was analyzed to evaluate the degree to which certain journals, disciplines, and specialties were represented on Twitter. It is shown that, with less than 10% of PubMed articles mentioned on Twitter, its uptake is low in general. The relationship between tweets and WoS citations was examined for each document at the level of journals and specialties. The results show that tweeting behavior varies between journals and specialties and correlations between tweets and citations are low, implying that impact metrics based on tweets are different from those based on citations. A framework utilizing the coverage of articles and the correlation between Twitter mentions and citations is proposed to facilitate the evaluation of novel social-media based metrics and to shed light on the question in how far the number of tweets is a valid metric to measure research impact."
      },
      {
        "node_idx": 151774,
        "score_0_10": 10,
        "title": "a new methodology for constructing a publication level classification system of science",
        "abstract": "Classifying journals or publications into research areas is an essential element of many bibliometric analyses. Classification usually takes place at the level of journals, where the Web of Science subject categories are the most popular classification system. However, journal-level classification systems have two important limitations: They offer only a limited amount of detail, and they have difficulties with multidisciplinary journals. To avoid these limitations, we introduce a new methodology for constructing classification systems at the level of individual publications. In the proposed methodology, publications are clustered into research areas based on citation relations. The methodology is able to deal with very large numbers of publications. We present an application in which a classification system is produced that includes almost ten million publications. Based on an extensive analysis of this classification system, we discuss the strengths and the limitations of the proposed methodology. Important strengths are the transparency and relative simplicity of the methodology and its fairly modest computing and memory requirements. The main limitation of the methodology is its exclusive reliance on direct citation relations between publications. The accuracy of the methodology can probably be increased by also taking into account other types of relations, for instance based on bibliographic coupling."
      },
      {
        "node_idx": 160175,
        "score_0_10": 9,
        "title": "altmetrics in the wild using social media to explore scholarly impact",
        "abstract": "In growing numbers, scholars are integrating social media tools like blogs, Twitter, and Mendeley into their professional communications. The online, public nature of these tools exposes and reifies scholarly processes once hidden and ephemeral. Metrics based on this activities could inform broader, faster measures of impact, complementing traditional citation metrics. This study explores the properties of these social media-based metrics or \"altmetrics\", sampling 24,331 articles published by the Public Library of Science. #R##N#We find that that different indicators vary greatly in activity. Around 5% of sampled articles are cited in Wikipedia, while close to 80% have been included in at least one Mendeley library. There is, however, an encouraging diversity; a quarter of articles have nonzero data from five or more different sources. Correlation and factor analysis suggest citation and altmetrics indicators track related but distinct impacts, with neither able to describe the complete picture of scholarly use alone. There are moderate correlations between Mendeley and Web of Science citation, but many altmetric indicators seem to measure impact mostly orthogonal to citation. Articles cluster in ways that suggest five different impact \"flavors\", capturing impacts of different types on different audiences; for instance, some articles may be heavily read and saved by scholars but seldom cited. Together, these findings encourage more research into altmetrics as complements to traditional citation measures."
      },
      {
        "node_idx": 119139,
        "score_0_10": 9,
        "title": "how well developed are altmetrics a cross disciplinary analysis of the presence of alternative metrics in scientific publications",
        "abstract": "In this paper an analysis of the presence and possibilities of altmetrics for bibliometric and performance analysis is carried out. Using the web based tool Impact Story, we collected metrics for 20,000 random publications from the Web of Science. We studied both the presence and distribution of altmetrics in the set of publications, across fields, document types and over publication years, as well as the extent to which altmetrics correlate with citation indicators. The main result of the study is that the altmetrics source that provides the most metrics is Mendeley, with metrics on readerships for 62.6 % of all the publications studied, other sources only provide marginal information. In terms of relation with citations, a moderate spearman correlation (r = 0.49) has been found between Mendeley readership counts and citation indicators. Other possibilities and limitations of these indicators are discussed and future research lines are outlined."
      },
      {
        "node_idx": 164856,
        "score_0_10": 9,
        "title": "coverage and adoption of altmetrics sources in the bibliometric community",
        "abstract": "Altmetrics, indices based on social media platforms and tools, have recently emerged as alternative means of measuring scholarly impact. Such indices assume that scholars in fact populate online social environments, and interact with scholarly products there. We tested this assumption by examining the use and coverage of social media environments amongst a sample of bibliometricians. As expected, coverage varied: 82% of articles published by sampled bibliometricians were included in Mendeley libraries, while only 28% were included in CiteULike. Mendeley bookmarking was moderately correlated (.45) with Scopus citation. Over half of respondents asserted that social media tools were affecting their professional lives, although uptake of online tools varied widely. 68% of those surveyed had LinkedIn accounts, while Academia.edu, Mendeley, and ResearchGate each claimed a fifth of respondents. Nearly half of those responding had Twitter accounts, which they used both personally and professionally. Surveyed bibliometricians had mixed opinions on altmetrics' potential; 72% valued download counts, while a third saw potential in tracking articles' influence in blogs, Wikipedia, reference managers, and social media. Altogether, these findings suggest that some online tools are seeing substantial use by bibliometricians, and that they present a potentially valuable source of impact data."
      },
      {
        "node_idx": 17333,
        "score_0_10": 9,
        "title": "analyzing the citation characteristics of books edited books book series and publisher types in the book citation index",
        "abstract": "This paper presents a first approach to analyzing the factors that determine the citation characteristics of books. For this we use the Thomson Reuters' book citation index, a novel multidisciplinary database launched in 2011 which offers bibliometric data on books. We analyze three possible factors which are considered to affect the citation impact of books: the presence of editors, the inclusion in series and the type of publisher. Also, we focus on highly cited books to see if these factors may affect them as well. We considered as highly cited books, those in the top 5 % of those most highly cited in the database. We define these three aspects and present results for four major scientific areas in order to identify differences by area (science, engineering and technology, social sciences and arts and humanities). Finally, we report differences for edited books and publisher type, however books included in series showed higher impact in two areas."
      },
      {
        "node_idx": 98146,
        "score_0_10": 9,
        "title": "a principal component analysis of 39 scientific impact measures",
        "abstract": "Background: The impact of scientific publications has traditionally been expressed in terms of citation counts. However, scientific activity has moved online over the past decade. To better capture scientific impact in the digital era, a variety of new impact measures has been proposed on the basis of social network analysis and usage log data. Here we investigate how these new measures relate to each other, and how accurately and completely they express scientific impact. Methodology: We performed a principal component analysis of the rankings produced by 39 existing and proposed measures of scholarly impact that were calculated on the basis of both citation and usage log data. Conclusions: Our results indicate that the notion of scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator, although some measures are more suitable than others. The commonly used citation Impact Factor is not positioned at the core of this construct, but at its periphery, and should thus be used with caution."
      },
      {
        "node_idx": 168873,
        "score_0_10": 9,
        "title": "towards a book publishers citation reports first approach using the book citation index",
        "abstract": "The absence of books and book chapters in the Web of Science Citation Indexes (SCI, SSCI and A&HCI) has always been considered an important flaw but the Thomson Reuters 'Book Citation Index' database was finally available in October of 2010 indexing 29,618 books and 379,082 book chapters. The Book Citation Index opens a new window of opportunities for analyzing these fields from a bibliometric point of view. The main objective of this article is to analyze different impact indicators referred to the scientific publishers included in the Book Citation Index for the Social Sciences and Humanities fields during 2006-2011. This way we construct what we have called the 'Book Publishers Citation Reports'. For this, we present a total of 19 rankings according to the different disciplines in Humanities & Arts and Social Sciences & Law with six indicators for scientific publishers"
      },
      {
        "node_idx": 36666,
        "score_0_10": 9,
        "title": "the role of social networks in information diffusion",
        "abstract": "Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these technologies on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed."
      },
      {
        "node_idx": 35005,
        "score_0_10": 9,
        "title": "twitter sentiment analysis system",
        "abstract": "Social media is increasingly used by humans to express their feelings and opinions in the form of short text messages. Detecting sentiments in the text has a wide range of applications including identifying anxiety or depression of individuals and measuring well-being or mood of a community. Sentiments can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Sentiment Analysis in text documents is essentially a content-based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning. In this paper, sentiment recognition based on textual data and the techniques used in sentiment analysis are discussed."
      }
    ]
  },
  "72": {
    "explanation": "neural network methods for language understanding and text processing tasks",
    "topk": [
      {
        "node_idx": 140427,
        "score_0_10": 10,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 11752,
        "score_0_10": 9,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 68485,
        "score_0_10": 9,
        "title": "learning algorithms for keyphrase extraction",
        "abstract": "Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a generalpurpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by Extractor suggests that about 80% of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications."
      },
      {
        "node_idx": 30031,
        "score_0_10": 8,
        "title": "simlex 999 evaluating semantic models with genuine similarity estimation",
        "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness, so that pairs of entities that are associated but not actually similar [Freud, psychology] have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures."
      },
      {
        "node_idx": 160949,
        "score_0_10": 8,
        "title": "a convolutional neural network for modelling sentences",
        "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."
      },
      {
        "node_idx": 145937,
        "score_0_10": 8,
        "title": "diachronic word embeddings reveal statistical laws of semantic change",
        "abstract": "Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change."
      },
      {
        "node_idx": 48659,
        "score_0_10": 8,
        "title": "nrc canada building the state of the art in sentiment analysis of tweets",
        "abstract": "In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated us available resources."
      },
      {
        "node_idx": 51364,
        "score_0_10": 8,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 79971,
        "score_0_10": 8,
        "title": "negpspan efficient extraction of negative sequential patterns with embedding constraints",
        "abstract": "Mining frequent sequential patterns consists in extracting recurrent behaviors, modeled as patterns, in a big sequence dataset. Such patterns inform about which events are frequently observed in sequences, i.e. what does really happen. Sometimes, knowing that some specific event does not happen is more informative than extracting a lot of observed events. Negative sequential patterns (NSP) formulate recurrent behaviors by patterns containing both observed events and absent events. Few approaches have been proposed to mine such NSPs. In addition, the syntax and semantics of NSPs differ in the different methods which makes it difficult to compare them. This article provides a unified framework for the formulation of the syntax and the semantics of NSPs. Then, we introduce a new algorithm, NegPSpan, that extracts NSPs using a PrefixSpan depth-first scheme and enabling maxgap constraints that other approaches do not take into account. The formal framework allows for highlighting the differences between the proposed approach wrt to the methods from the literature, especially wrt the state of the art approach eNSP. Intensive experiments on synthetic and real datasets show that NegPSpan can extract meaningful NSPs and that it can process bigger datasets than eNSP thanks to significantly lower memory requirements and better computation times."
      }
    ]
  },
  "74": {
    "explanation": "Temporal deep learning for video and sequential data modeling",
    "topk": [
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      }
    ]
  },
  "77": {
    "explanation": "indexing and impact analysis methods for scholarly and moving object data",
    "topk": [
      {
        "node_idx": 70146,
        "score_0_10": 10,
        "title": "shapebots shape changing swarm robots",
        "abstract": "We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces."
      },
      {
        "node_idx": 149204,
        "score_0_10": 10,
        "title": "boosting moving object indexing through velocity partitioning",
        "abstract": "There have been intense research interests in moving object indexing in the past decade. However, existing work did not exploit the important property of skewed velocity distributions. In many real world scenarios, objects travel predominantly along only a few directions. Examples include vehicles on road networks, flights, people walking on the streets, etc. The search space for a query is heavily dependent on the velocity distribution of the objects grouped in the nodes of an index tree. Motivated by this observation, we propose the velocity partitioning (VP) technique, which exploits the skew in velocity distribution to speed up query processing using moving object indexes. The VP technique first identifies the \"dominant velocity axes (DVAs)\" using a combination of principal components analysis (PCA) and k-means clustering. Then, a moving object index (e.g., a TPR-tree) is created based on each DVA, using the DVA as an axis of the underlying coordinate system. An object is maintained in the index whose DVA is closest to the object's current moving direction. Thus, all the objects in an index are moving in a near 1-dimensional space instead of a 2-dimensional space. As a result, the expansion of the search space with time is greatly reduced, from a quadratic function of the maximum speed (of the objects in the search range) to a near linear function of the maximum speed. The VP technique can be applied to a wide range of moving object index structures. We have implemented the VP technique on two representative ones, the TPR*-tree and the Bx-tree. Extensive experiments validate that the VP technique consistently improves the performance of those index structures."
      },
      {
        "node_idx": 122540,
        "score_0_10": 10,
        "title": "bibliometric enhanced information retrieval",
        "abstract": "Bibliometric techniques are not yet widely used to enhance retrieval processes in digital libraries, although they offer value-added effects for users. In this workshop we will explore how statistical modelling of scholarship, such as Bradfordizing or network analysis of coauthorship network, can improve retrieval services for specific communities, as well as for large, cross-domain collections. This workshop aims to raise awareness of the missing link between information retrieval (IR) and bibliometrics/scientometrics and to create a common ground for the incorporation of bibliometric-enhanced services into retrieval at the digital library interface."
      },
      {
        "node_idx": 160175,
        "score_0_10": 10,
        "title": "altmetrics in the wild using social media to explore scholarly impact",
        "abstract": "In growing numbers, scholars are integrating social media tools like blogs, Twitter, and Mendeley into their professional communications. The online, public nature of these tools exposes and reifies scholarly processes once hidden and ephemeral. Metrics based on this activities could inform broader, faster measures of impact, complementing traditional citation metrics. This study explores the properties of these social media-based metrics or \"altmetrics\", sampling 24,331 articles published by the Public Library of Science. #R##N#We find that that different indicators vary greatly in activity. Around 5% of sampled articles are cited in Wikipedia, while close to 80% have been included in at least one Mendeley library. There is, however, an encouraging diversity; a quarter of articles have nonzero data from five or more different sources. Correlation and factor analysis suggest citation and altmetrics indicators track related but distinct impacts, with neither able to describe the complete picture of scholarly use alone. There are moderate correlations between Mendeley and Web of Science citation, but many altmetric indicators seem to measure impact mostly orthogonal to citation. Articles cluster in ways that suggest five different impact \"flavors\", capturing impacts of different types on different audiences; for instance, some articles may be heavily read and saved by scholars but seldom cited. Together, these findings encourage more research into altmetrics as complements to traditional citation measures."
      },
      {
        "node_idx": 16997,
        "score_0_10": 10,
        "title": "speed partitioning for indexing moving objects",
        "abstract": "Indexing moving objects has been extensively studied in the past decades. Moving objects, such as vehicles and mobile device users, usually exhibit some patterns on their velocities, which can be utilized for velocity-based partitioning to improve performance of the indexes. Existing velocity-based partitioning techniques rely on some kinds of heuristics rather than analytically calculate the optimal solution. In this paper, we propose a novel speed partitioning technique based on a formal analysis over speed values of the moving objects. We first show that speed partitioning will significantly reduce the search space expansion which has direct impacts on query performance of the indexes. Next we formulate the optimal speed partitioning problem based on search space expansion analysis and then compute the optimal solution using dynamic programming. We then build the partitioned indexing system where queries are duplicated and processed in each index partition. Extensive experiments demonstrate that our method dramatically improves the performance of indexes for moving objects and outperforms other state-of-the-art velocity-based partitioning approaches."
      },
      {
        "node_idx": 20843,
        "score_0_10": 9,
        "title": "computational geometry column 43",
        "abstract": "The concept of pointed pseudo-triangulations is defined and a few of its applications described."
      },
      {
        "node_idx": 40714,
        "score_0_10": 9,
        "title": "tweeting biomedicine an analysis of tweets and citations in the biomedical literature",
        "abstract": "Data collected by social media platforms have recently been introduced as a new source for indicators to help measure the impact of scholarly research in ways that are complementary to traditional citation-based indicators. Data generated from social media activities related to scholarly content can be used to reflect broad types of impact. This paper aims to provide systematic evidence regarding how often Twitter is used to diffuse journal articles in the biomedical and life sciences. The analysis is based on a set of 1.4 million documents covered by both PubMed and Web of Science (WoS) and published between 2010 and 2012. The number of tweets containing links to these documents was analyzed to evaluate the degree to which certain journals, disciplines, and specialties were represented on Twitter. It is shown that, with less than 10% of PubMed articles mentioned on Twitter, its uptake is low in general. The relationship between tweets and WoS citations was examined for each document at the level of journals and specialties. The results show that tweeting behavior varies between journals and specialties and correlations between tweets and citations are low, implying that impact metrics based on tweets are different from those based on citations. A framework utilizing the coverage of articles and the correlation between Twitter mentions and citations is proposed to facilitate the evaluation of novel social-media based metrics and to shed light on the question in how far the number of tweets is a valid metric to measure research impact."
      },
      {
        "node_idx": 166900,
        "score_0_10": 9,
        "title": "exploring the academic invisible web",
        "abstract": "Purpose \u2013 The purpose of this article is to provide a critical review of Bergman's study on the deep web. In addition, this study brings a new concept into the discussion, the academic invisible web (AIW). The paper defines the academic invisible web as consisting of all databases and collections relevant to academia but not searchable by the general\u2010purpose internet search engines. Indexing this part of the invisible web is central to scientific search engines. This paper provides an overview of approaches followed thus far.Design/methodology/approach \u2013 Provides a discussion of measures and calculations, estimation based on informetric laws. Also gives a literature review on approaches for uncovering information from the invisible web.Findings \u2013 Bergman's size estimate of the invisible web is highly questionable. This paper demonstrates some major errors in the conceptual design of the Bergman paper. A new (raw) size estimate is given.Research limitations/implications \u2013 The precision of this estimate is ..."
      },
      {
        "node_idx": 164856,
        "score_0_10": 9,
        "title": "coverage and adoption of altmetrics sources in the bibliometric community",
        "abstract": "Altmetrics, indices based on social media platforms and tools, have recently emerged as alternative means of measuring scholarly impact. Such indices assume that scholars in fact populate online social environments, and interact with scholarly products there. We tested this assumption by examining the use and coverage of social media environments amongst a sample of bibliometricians. As expected, coverage varied: 82% of articles published by sampled bibliometricians were included in Mendeley libraries, while only 28% were included in CiteULike. Mendeley bookmarking was moderately correlated (.45) with Scopus citation. Over half of respondents asserted that social media tools were affecting their professional lives, although uptake of online tools varied widely. 68% of those surveyed had LinkedIn accounts, while Academia.edu, Mendeley, and ResearchGate each claimed a fifth of respondents. Nearly half of those responding had Twitter accounts, which they used both personally and professionally. Surveyed bibliometricians had mixed opinions on altmetrics' potential; 72% valued download counts, while a third saw potential in tracking articles' influence in blogs, Wikipedia, reference managers, and social media. Altogether, these findings suggest that some online tools are seeing substantial use by bibliometricians, and that they present a potentially valuable source of impact data."
      },
      {
        "node_idx": 87433,
        "score_0_10": 9,
        "title": "generating preview tables for entity graphs",
        "abstract": "Users are tapping into massive, heterogeneous entity graphs for many applications. It is challenging to select entity graphs for a particular need, given abundant datasets from many sources and the oftentimes scarce information for them. We propose methods to produce preview tables for compact presentation of important entity types and relationships in entity graphs. The preview tables assist users in attaining a quick and rough preview of the data. They can be shown in a limited display space for a user to browse and explore, before she decides to spend time and resources to fetch and investigate the complete dataset. We formulate several optimization problems that look for previews with the highest scores according to intuitive goodness measures, under various constraints on preview size and distance between preview tables. The optimization problem under distance constraint is NP-hard. We design a dynamic-programming algorithm and an Apriori-style algorithm for finding optimal previews. Results from experiments, comparison with related work and user studies demonstrated the scoring measures' accuracy and the discovery algorithms' efficiency."
      }
    ]
  },
  "80": {
    "explanation": "wireless channel capacity and backhaul optimization techniques",
    "topk": [
      {
        "node_idx": 7733,
        "score_0_10": 10,
        "title": "low snr capacity of noncoherent fading channels",
        "abstract": "Discrete-time Rayleigh-fading single-input single-output (SISO) and multiple-input multiple-output (MIMO) channels are considered, with no channel state information at the transmitter or the receiver. The fading is assumed to be stationary and correlated in time, but independent from antenna to antenna. Peak-power and average-power constraints are imposed on the transmit antennas. For MIMO channels, these constraints are either imposed on the sum over antennas, or on each individual antenna. For SISO channels and MIMO channels with sum power constraints, the asymptotic capacity as the peak signal-to-noise ratio (SNR) goes to zero is identified; for MIMO channels with individual power constraints, this asymptotic capacity is obtained for a class of channels called transmit separable channels. The results for MIMO channels with individual power constraints are carried over to SISO channels with delay spread (i.e., frequency-selective fading)."
      },
      {
        "node_idx": 58620,
        "score_0_10": 10,
        "title": "capacity per unit energy of fading channels with a peak constraint",
        "abstract": "A discrete-time single-user scalar channel with temporally correlated Rayleigh fading is analyzed. There is no side information at the transmitter or the receiver. A simple expression is given for the capacity per unit energy, in the presence of a peak constraint. The simple formula of Verdu/spl acute/ for capacity per unit cost is adapted to a channel with memory, and is used in the proof. In addition to bounding the capacity of a channel with correlated fading, the result gives some insight into the relationship between the correlation in the fading process and the channel capacity. The results are extended to a channel with side information, showing that the capacity per unit energy is one nat per joule, independently of the peak power constraint. A continuous-time version of the model is also considered. The capacity per unit energy subject to a peak constraint (but no bandwidth constraint) is given by an expression similar to that for discrete time, and is evaluated for Gauss-Markov and Clarke fading channels."
      },
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 17426,
        "score_0_10": 10,
        "title": "noncoherent capacity of underspread fading channels",
        "abstract": "We derive bounds on the noncoherent capacity of wide-sense stationary uncorrelated scattering (WSSUS) channels that are selective both in time and frequency, and are underspread, i.e., the product of the channel's delay spread and Doppler spread is small. The underspread assumption is satisfied by virtually all wireless communication channels. For input signals that are peak constrained in time and frequency, we obtain upper and lower bounds on capacity that are explicit in the channel's scattering function, are accurate for a large range of bandwidth, and allow to coarsely identify the capacity-optimal bandwidth as a function of the peak power and the channel's scattering function. We also obtain a closed-form expression for the first-order Taylor series expansion of capacity in the infinite-bandwidth limit, and show that our bounds are tight in the wideband regime. For input signals that are peak constrained in time only (and, hence, allowed to be peaky in frequency), we provide upper and lower bounds on the infinite-bandwidth capacity. Our lower bound is closely related to a result by Viterbi (1967). We find cases where the bounds coincide and, hence, the infinite-bandwidth capacity is characterized exactly. The analysis in this paper is based on a discrete-time discrete-frequency approximation of WSSUS time- and frequency-selective channels. This discretization takes the underspread property of the channel explicitly into account."
      },
      {
        "node_idx": 4254,
        "score_0_10": 9,
        "title": "perturbation based regularization for signal estimation in linear discrete ill posed problems",
        "abstract": "Estimating the values of unknown parameters from corrupted measured data faces a lot of challenges in ill-posed problems. In such problems, many fundamental estimation methods fail to provide a meaningful stabilized solution. In this work, we propose a new regularization approach and a new regularization parameter selection approach for linear least-squares discrete ill-posed problems. The proposed approach is based on enhancing the singular-value structure of the ill-posed model matrix to acquire a better solution. Unlike many other regularization algorithms that seek to minimize the estimated data error, the proposed approach is developed to minimize the mean-squared error of the estimator which is the objective in many typical estimation scenarios. The performance of the proposed approach is demonstrated by applying it to a large set of real-world discrete ill-posed problems. Simulation results demonstrate that the proposed approach outperforms a set of benchmark regularization methods in most cases. In addition, the approach also enjoys the lowest runtime and offers the highest level of robustness amongst all the tested benchmark regularization methods."
      },
      {
        "node_idx": 109561,
        "score_0_10": 9,
        "title": "fast steerable wireless backhaul reconfiguration",
        "abstract": "Future mobile traffic growth will require 5G cellular networks to densify the deployment of small cell base stations (BS). As it is not feasible to form a backhaul (BH) by wiring all BSs to the core network, directional mmWave links can be an attractive solution to form BH links, due to their large available capacity. When small cells are powered on/off or traffic demands change, the BH may require reconfiguration, leading to topology and traffic routing changes. Ideally, such reconfiguration should be seamless and should not impact existing traffic. However, when using highly directional BH antennas which can be dynamically rotated to form new links, this can become time-consuming, requiring the coordination of BH interface movements, link establishment and traffic routing. In this paper, we propose greedy-based heuristic algorithms to solve the BH reconfiguration problem in real-time. We numerically compare the proposed algorithms with the optimal solution obtained by solving a mixed integer linear program (MILP) for smaller instances, and with a sub-optimal reduced MILP for larger instances. The obtained results indicate that the greedy-based algorithms achieve good quality solutions with significantly decreased execution time."
      },
      {
        "node_idx": 124006,
        "score_0_10": 9,
        "title": "miso capacity with per antenna power constraint",
        "abstract": "We establish in closed-form the capacity and the optimal signaling scheme for a MISO channel with per-antenna power constraint. Two cases of channel state information are considered: constant channel known at both the transmitter and receiver, and Rayleigh fading channel known only at the receiver. For the first case, the optimal signaling scheme is beamforming with the phases of the beam weights matched to the phases of the channel coefficients, but the amplitudes independent of the channel coefficients and dependent only on the constrained powers. For the second case, the optimal scheme is to send independent signals from the antennas with the constrained powers. In both cases, the capacity with per-antenna power constraint is usually less than that with sum power constraint."
      },
      {
        "node_idx": 54143,
        "score_0_10": 9,
        "title": "robust regularized least squares beamforming approach to signal estimation",
        "abstract": "In this paper, we address the problem of robust adaptive beamforming of signals received by a linear array. The challenge associated with the beamforming problem is twofold. Firstly, the process requires the inversion of the usually ill-conditioned covariance matrix of the received signals. Secondly, the steering vector pertaining to the direction of arrival of the signal of interest is not known precisely. To tackle these two challenges, the standard capon beamformer is manipulated to a form where the beamformer output is obtained as a scaled version of the inner product of two vectors. The two vectors are linearly related to the steering vector and the received signal snapshot, respectively. The linear operator, in both cases, is the square root of the covariance matrix. A regularized least-squares (RLS) approach is proposed to estimate these two vectors and to provide robustness without exploiting prior information. Simulation results show that the RLS beamformer using the proposed regularization algorithm outperforms state-of-the-art beamforming algorithms, as well as another RLS beamformers using a standard regularization approaches."
      },
      {
        "node_idx": 28821,
        "score_0_10": 9,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 163378,
        "score_0_10": 8,
        "title": "optimal steerable mmwave mesh backhaul reconfiguration",
        "abstract": "Future 5G mobile networks will require increased backhaul (BH) capacity to connect a massive amount of high capacity small cells (SCs) to the network. Because having an optical connection to each SC might be infeasible, mmWave-based (e.g. 60 GHz) BH links are an interesting alternative due to their large available bandwidth. To cope with the increased path loss, mmWave links require directional antennas that should be able to direct their beams to different neighbors, to dynamically change the BH topology, in case new nodes are powered on&#x002F;off or the traffic demand has changed. Such BH adaptation needs to be orchestrated to minimize the impact on existing traffic. This paper develops a Software-defined networking-based framework that guides the optimal reconfiguration of mesh BH networks composed by mmWave links, where antennas need to be mechanically aligned. By modelling the problem as a Mixed Integer Linear Program (MILP), its solution returns the optimal ordering of events necessary to transition between two BH network configurations. The model creates backup paths whenever it is possible, while minimizing the packet loss of ongoing flows. A numerical evaluation with different topologies and traffic demands shows that increasing the number of BH interfaces per SC from 2 to 4 can decrease the total loss by more than 50%. Moreover, when increasing the total reconfiguration time, additional backup paths can be created, consequently reducing the reconfiguration impact on existing traffic."
      }
    ]
  },
  "81": {
    "explanation": "sequence labeling with bidirectional LSTM and CNN integration",
    "topk": [
      {
        "node_idx": 88803,
        "score_0_10": 10,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 63929,
        "score_0_10": 10,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 112674,
        "score_0_10": 10,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 47858,
        "score_0_10": 9,
        "title": "instantaneously trained neural networks",
        "abstract": "This paper presents a review of instantaneously trained neural networks (ITNNs). These networks trade learning time for size and, in the basic model, a new hidden node is created for each training sample. Various versions of the corner-classification family of ITNNs, which have found applications in artificial intelligence (AI), are described. Implementation issues are also considered."
      },
      {
        "node_idx": 128298,
        "score_0_10": 9,
        "title": "devnagari handwritten numeral recognition using geometric features and statistical combination classifier",
        "abstract": "This paper presents a Devnagari Numerical recognition method based on statistical discriminant functions. 17 geometric features based on pixel connectivity, lines, line directions, holes, image area, perimeter, eccentricity, solidity, orientation etc. are used for representing the numerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear, Diagquadratic and Mahalanobis distance are used for classification. 1500 handwritten numerals are used for training. Another 1500 handwritten numerals are used for testing. Experimental results show that Linear, Quadratic and Mahalanobis discriminant functions provide better results. Results of these three Discriminants are fed to a majority voting type Combination classifier. It is found that Combination classifier offers better results over individual classifiers."
      },
      {
        "node_idx": 82343,
        "score_0_10": 9,
        "title": "improved accent classification combining phonetic vowels with acoustic features",
        "abstract": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field."
      },
      {
        "node_idx": 26180,
        "score_0_10": 9,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      }
    ]
  },
  "87": {
    "explanation": "multichannel speech enhancement and dereverberation in acoustic environments",
    "topk": [
      {
        "node_idx": 166121,
        "score_0_10": 10,
        "title": "on acoustic modeling for broadband beamforming",
        "abstract": "In this work, we describe limitations of the free-field propagation model for designing broadband beamformers for microphone arrays on a rigid surface. Towards this goal, we describe a general framework for quantifying the microphone array performance in a general wave-field by directly solving the acoustic wave equation. The model utilizes Finite-Element-Method (FEM) for evaluating the response of the microphone array surface to background 3D planar and spherical waves. The effectiveness of the framework is established by designing and evaluating a representative broadband beamformer under realistic acoustic conditions."
      },
      {
        "node_idx": 10269,
        "score_0_10": 10,
        "title": "multichannel speech separation and enhancement using the convolutive transfer function",
        "abstract": "This paper addresses the problem of speech separation and enhancement from multichannel convolutive and noisy mixtures, \\emph{assuming known mixing filters}. We propose to perform the speech separation and enhancement task in the short-time Fourier transform domain, using the convolutive transfer function (CTF) approximation. Compared to time-domain filters, CTF has much less taps, consequently it has less near-common zeros among channels and less computational complexity. The work proposes three speech-source recovery methods, namely: i) the multichannel inverse filtering method, i.e. the multiple input/output inverse theorem (MINT), is exploited in the CTF domain, and for the multi-source case, ii) a beamforming-like multichannel inverse filtering method applying single source MINT and using power minimization, which is suitable whenever the source CTFs are not all known, and iii) a constrained Lasso method, where the sources are recovered by minimizing the $\\ell_1$-norm to impose their spectral sparsity, with the constraint that the $\\ell_2$-norm fitting cost, between the microphone signals and the mixing model involving the unknown source signals, is less than a tolerance. The noise can be reduced by setting a tolerance onto the noise power. Experiments under various acoustic conditions are carried out to evaluate the three proposed methods. The comparison between them as well as with the baseline methods is presented."
      },
      {
        "node_idx": 62676,
        "score_0_10": 9,
        "title": "monaural multi talker speech recognition using factorial speech processing models",
        "abstract": "A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past super-human results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first super-human system and 2.7% absolute task performance improvement compared to its recent competitor."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 96805,
        "score_0_10": 9,
        "title": "coherent to diffuse power ratio estimation for dereverberation",
        "abstract": "The estimation of the time- and frequency-dependent coherent-to-diffuse power ratio (CDR) from the measured spatial coherence between two omnidirectional microphones is investigated. Known CDR estimators are formulated in a common framework, illustrated using a geometric interpretation in the complex plane, and investigated with respect to bias and robustness towards model errors. Several novel unbiased CDR estimators are proposed, and it is shown that knowledge of either the direction of arrival (DOA) of the target source or the coherence of the noise field is sufficient for unbiased CDR estimation. The validity of the model for the application of CDR estimates to dereverberation is investigated using measured and simulated impulse responses. A CDR-based dereverberation system is presented and evaluated using signal-based quality measures as well as automatic speech recognition accuracy. The results show that the proposed unbiased estimators have a practical advantage over existing estimators, and that the proposed DOA-independent estimator can be used for effective blind dereverberation."
      },
      {
        "node_idx": 8969,
        "score_0_10": 9,
        "title": "glottal closure and opening instant detection from speech signals",
        "abstract": "This paper proposes a new procedure to detect Glottal Closure and Opening Instants (GCIs and GOIs) directly from speech waveforms. The procedure is divided into two successive steps. First a mean-based signal is computed, and intervals where speech events are expected to occur are extracted from it. Secondly, at each interval a precise position of the speech event is assigned by locating a discontinuity in the Linear Prediction residual. The proposed method is compared to the DYPSA algorithm on the CMU ARCTIC database. A significant improvement as well as a better noise robustness are reported. Besides, results of GOI identification accuracy are promising for the glottal source characterization."
      },
      {
        "node_idx": 112968,
        "score_0_10": 9,
        "title": "information measures for microphone arrays",
        "abstract": "We propose a novel information-theoretic approach for evaluating microphone arrays that relies on the array physics and geometry rather than the underlying beamforming algorithm. The analogy between Multiple-Input-Multiple-Output (MIMO) wireless communication channel and the acoustic channel of microphone arrays is exploited to define information measures of microphone arrays, which provide upper bounds of the information rate of the microphone array system."
      },
      {
        "node_idx": 145770,
        "score_0_10": 9,
        "title": "design and stability of load side primary frequency control in power systems",
        "abstract": "We present a systematic method to design ubiquitous continuous fast-acting distributed load control for primary frequency regulation in power networks, by formulating an optimal load control (OLC) problem where the objective is to minimize the aggregate cost of tracking an operating point subject to power balance over the network. We prove that the swing dynamics and the branch power flows, coupled with frequency-based load control, serve as a distributed primal-dual algorithm to solve OLC. We establish the global asymptotic stability of a multimachine network under such type of load-side primary frequency control. These results imply that the local frequency deviations on each bus convey exactly the right information about the global power imbalance for the loads to make individual decisions that turn out to be globally optimal. Simulations confirm that the proposed algorithm can rebalance power and resynchronize bus frequencies after a disturbance with significantly improved transient performance."
      },
      {
        "node_idx": 160955,
        "score_0_10": 9,
        "title": "multichannel online dereverberation based on spectral magnitude inverse filtering",
        "abstract": "This paper addresses the problem of multichannel online dereverberation. The proposed method is carried out in the short-time Fourier transform (STFT) domain, and for each frequency band independently. In the STFT domain, the time-domain room impulse response is approximately represented by the convolutive transfer function (CTF). The multichannel CTFs are adaptively identified based on the cross-relation method, and using the recursive least square criterion. Instead of the complex-valued CTF convolution model, we use a nonnegative convolution model between the STFT magnitude of the source signal and the CTF magnitude, which is just a coarse approximation of the former model, but is shown to be more robust against the CTF perturbations. Based on this nonnegative model, we propose an online STFT magnitude inverse filtering method. The inverse filters of the CTF magnitude are formulated based on the multiple-input/output inverse theorem (MINT), and adaptively estimated based on the gradient descent criterion. Finally, the inverse filtering is applied to the STFT magnitude of the microphone signals, obtaining an estimate of the STFT magnitude of the source signal. Experiments regarding both speech enhancement and automatic speech recognition are conducted, which demonstrate that the proposed method can effectively suppress reverberation, even for the difficult case of a moving speaker."
      }
    ]
  },
  "88": {
    "explanation": "video action recognition and spatio-temporal feature learning",
    "topk": [
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 112726,
        "score_0_10": 9,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 115271,
        "score_0_10": 9,
        "title": "algebres de realisabilite un programme pour bien ordonner r",
        "abstract": "We give a method to transform into programs, classical proofs using a well ordering of the reals. The technics uses a generalization of Cohen's forcing and the theory of classical realizability introduced by the author."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 150492,
        "score_0_10": 9,
        "title": "iterative approximate byzantine consensus in arbitrary directed graphs",
        "abstract": "In this paper, we explore the problem of iterative approximate Byzantine consensus in arbitrary directed graphs. In particular, we prove a necessary and sufficient condition for the existence of iterative byzantine consensus algorithms. Additionally, we use our sufficient condition to examine whether such algorithms exist for some specific graphs."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 39237,
        "score_0_10": 8,
        "title": "emg controlled non anthropomorphic hand teleoperation using a continuous teleoperation subspace",
        "abstract": "We present a method for EMG-driven teleoperation of non-anthropomorphic robot hands. EMG sensors are appealing as a wearable, inexpensive, and unobtrusive way to gather information about the teleoperator's hand pose. However, mapping from EMG signals to the pose space of a non-anthropomorphic hand presents multiple challenges. We present a method that first projects from forearm EMG into a subspace relevant to teleoperation. To increase robustness, we use a model which combines continuous and discrete predictors along different dimensions of this subspace. We then project from the teleoperation subspace into the pose space of the robot hand. Our method is effective and intuitive, as it enables novice users to teleoperate pick and place tasks faster and more robustly than state-of-the-art EMG teleoperation methods when applied to a non-anthropomorphic, multi-DOF robot hand."
      }
    ]
  },
  "90": {
    "explanation": "efficient neural network architectures and optimization techniques",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 21714,
        "score_0_10": 9,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 26874,
        "score_0_10": 9,
        "title": "learning transferable architectures for scalable image recognition",
        "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 88803,
        "score_0_10": 8,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 153811,
        "score_0_10": 8,
        "title": "pruning filters for efficient convnets",
        "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
      }
    ]
  },
  "93": {
    "explanation": "crowd behavior and counting in high density scenarios",
    "topk": [
      {
        "node_idx": 157723,
        "score_0_10": 10,
        "title": "crowd behavior analysis",
        "abstract": "Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision. HighlightsReview crowd behavior studies in computer vision from physics and biology outlooks.Overview of the key attributes of crowd from the perspectives of the two sciences.General attributes of crowd: decentralized, collective motion, emergent behavior.Contradicting attributes of crowd: thinking/non-thinking, bias/non-bias.Discuss sample applications of crowd based on attributes and benchmarked datasets."
      },
      {
        "node_idx": 72516,
        "score_0_10": 10,
        "title": "impact of gate assignment on gate holding departure control strategies",
        "abstract": "Gate holding reduces congestion by reducing the number of aircraft present on the airport surface at any time, while not starving the runway. Because some departing flights are held at gates, there is a possibility that arriving flights cannot access the gates and have to wait until the gates are cleared. This is called a gate conflict. Robust gate assignment is an assignment that minimizes gate conflicts by assigning gates to aircraft to maximize the time gap between two consecutive flights at the same gate; it makes gate assignment robust, but passengers may walk longer to transfer flights. In order to simulate the airport departure process, a queuing model is introduced. The model is calibrated and validated with actual data from New York La Guardia Airport (LGA) and a U.S. hub airport. Then, the model simulates the airport departure process with the current gate assignment and a robust gate assignment to assess the impact of gate assignment on gate-holding departure control. The results show that the robust gate assignment reduces the number of gate conflicts caused by gate holding compared to the current gate assignment. Therefore, robust gate assignment can be combined with gate-holding departure control to improve operations at congested airports with limited gate resources."
      },
      {
        "node_idx": 100557,
        "score_0_10": 10,
        "title": "people counting in high density crowds from still images",
        "abstract": "We present a method of estimating the number of people in high density crowds from still images. The method estimates counts by fusing information from multiple sources. Most of the existing work on crowd counting deals with very small crowds (tens of individuals) and use temporal information from videos. Our method uses only still images to estimate the counts in high density images (hundreds to thousands of individuals). At this scale, we cannot rely on only one set of features for count estimation. We, therefore, use multiple sources, viz. interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM features and low confidence head detections, to estimate the counts. Each of these sources gives a separate estimate of the count along with confidences and other statistical measures which are then combined to obtain the final estimate. We test our method on an existing dataset of fifty images containing over 64000 individuals. Further, we added another fifty annotated images of crowds and tested on the complete dataset of hundred images containing over 87000 individuals. The counts per image range from 81 to 4633. We report the performance in terms of mean absolute error, which is a measure of accuracy of the method, and mean normalised absolute error, which is a measure of the robustness."
      },
      {
        "node_idx": 4734,
        "score_0_10": 10,
        "title": "distributed traffic signal control for maximum network throughput",
        "abstract": "We propose a distributed algorithm for controlling traffic signals. Our algorithm is adapted from backpressure routing, which has been mainly applied to communication and power networks. We formally prove that our algorithm ensures global optimality as it leads to maximum network throughput even though the controller is constructed and implemented in a completely distributed manner. Simulation results show that our algorithm significantly outperforms SCATS, an adaptive traffic signal control system that is being used in many cities."
      },
      {
        "node_idx": 145121,
        "score_0_10": 10,
        "title": "simple causes of complexity in hedonic games",
        "abstract": "Hedonic games provide a natural model of coalition formation among self-interested agents. The associated problem of finding stable outcomes in such games has been extensively studied. In this paper, we identify simple conditions on expressivity of hedonic games that are sufficient for the problem of checking whether a given game admits a stable outcome to be computationally hard. Somewhat surprisingly, these conditions are very mild and intuitive. Our results apply to a wide range of stability concepts (core stability, individual stability, Nash stability, etc.) and to many known formalisms for hedonic games (additively separable games, games with W-preferences, fractional hedonic games, etc.), and unify and extend known results for these formalisms. They also have broader applicability: for several classes of hedonic games whose computational complexity has not been explored in prior work, we show that our framework immediately implies a number of hardness results for them."
      },
      {
        "node_idx": 22009,
        "score_0_10": 10,
        "title": "mechanism design in social networks",
        "abstract": "This paper studies an auction design problem for a seller to sell a commodity in a social network, where each individual (the seller or a buyer) can only communicate with her neighbors. The challenge to the seller is to design a mechanism to incentivize the buyers, who are aware of the auction, to further propagate the information to their neighbors so that more buyers will participate in the auction and hence, the seller will be able to make a higher revenue. We propose a novel auction mechanism, called information diffusion mechanism (IDM), which incentivizes the buyers to not only truthfully report their valuations on the commodity to the seller, but also further propagate the auction information to all their neighbors. In comparison, the direct extension of the well-known Vickrey-Clarke-Groves (VCG) mechanism in social networks can also incentivize the information diffusion, but it will decrease the seller's revenue or even lead to a deficit sometimes. The formalization of the problem has not yet been addressed in the literature of mechanism design and our solution is very significant in the presence of large-scale online social networks."
      },
      {
        "node_idx": 58780,
        "score_0_10": 10,
        "title": "collaborative visual area coverage",
        "abstract": "Abstract   This article examines the problem of visual area coverage by a network of Mobile Aerial Agents (MAAs). Each MAA is assumed to be equipped with a downwards facing camera with a conical field of view which covers all points within a circle on the ground. The diameter of that circle is proportional to the altitude of the MAA, whereas the quality of the covered area decreases with the altitude. A distributed control law that maximizes a joint coverage-quality criterion by adjusting the MAAs\u2019 spatial coordinates is developed. The effectiveness of the proposed control scheme is evaluated through simulation studies."
      },
      {
        "node_idx": 34710,
        "score_0_10": 9,
        "title": "segmented and directional impact detection for parked vehicles using mobile devices",
        "abstract": "Mutual usage of vehicles as well as car sharing became more and more attractive during the last years. Especially in urban environments with limited parking possibilities and a higher risk for traffic jams, car rentals and sharing services may save time and money. But when renting a vehicle it could already be damaged (e.g., scratches or bumps inflicted by a previous user) without the damage being perceived by the service provider. In order to address such problems, we present an automated, motion-based system for impact detection, that facilitates a common smartphone as a sensor platform. The system is capable of detecting the impact segment and the point of time of an impact event on a vehicle's surface, as well as its direction of origin. With this additional specific knowledge, it may be possible to reconstruct the circumstances of an impact event, e.g., to prove possible innocence of a service's customer."
      },
      {
        "node_idx": 79827,
        "score_0_10": 9,
        "title": "human skin detection using rgb hsv and ycbcr color models",
        "abstract": "Human Skin detection deals with the recognition of skin-colored pixels and regions in a given image. Skin color is often used in human skin detection because it is invariant to orientation and size and is fast to process. A new human skin detection algorithm is proposed in this paper. The three main parameters for recognizing a skin pixel are RGB (Red, Green, Blue), HSV (Hue, Saturation, Value) and YCbCr (Luminance, Chrominance) color models. The objective of proposed algorithm is to improve the recognition of skin pixels in given images. The algorithm not only considers individual ranges of the three color parameters but also takes into ac- count combinational ranges which provide greater accuracy in recognizing the skin area in a given image."
      },
      {
        "node_idx": 163287,
        "score_0_10": 9,
        "title": "nash equilibria in routing games with edge priorities",
        "abstract": "In this paper we present a new competitive packet routing model with edge priorities. We consider players that route selfishly through a network over time and try to reach their destinations as fast as possible. If the number of players who want to enter an edge at the same time exceeds the inflow capacity of this edge, edge priorities with respect to the preceding edge solve these conflicts. Our edge priorities are well-motivated by applications in traffic. For this class of games, we show the existence of equilibrium solutions for single-source-single-sink games and we analyze structural properties of these solutions. We present an algorithm that computes Nash equilibria and we prove bounds both on the Price of Stability and on the Price of Anarchy. Moreover, we introduce the new concept of a Price of Mistrust. Finally, we also study the relations to earliest arrival flows."
      }
    ]
  },
  "94": {
    "explanation": "Neural network models for natural language processing and argumentation",
    "topk": [
      {
        "node_idx": 34798,
        "score_0_10": 10,
        "title": "abstract argumentation persuasion dynamics",
        "abstract": "The act of persuasion, a key component in rhetoric argumentation, may be viewed as a dynamics modifier. We extend Dung's frameworks with acts of persuasion among agents, and consider interactions among attack, persuasion and defence that have been largely unheeded so far. We characterise basic notions of admissibilities in this framework, and show a way of enriching them through, effectively, CTL (computation tree logic) encoding, which also permits importation of the theoretical results known to the logic into our argumentation frameworks. Our aim is to complement the growing interest in coordination of static and dynamic argumentation."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 29312,
        "score_0_10": 9,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 140427,
        "score_0_10": 9,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 84319,
        "score_0_10": 9,
        "title": "defeasible logic programming an argumentative approach",
        "abstract": "The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions. #R##N#In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query q will succeed when there is an argument A for q that is warranted, ie, the argument A that supports q is found undefeated by a warrant procedure that implements a dialectical analysis. #R##N#The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents."
      },
      {
        "node_idx": 168237,
        "score_0_10": 9,
        "title": "the symbol grounding problem",
        "abstract": "There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the \"symbol grounding problem\": How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) \"iconic representations\" , which are analogs of the proximal sensory projections of distal objects and events, and (2) \"categorical representations\" , which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) \"symbolic representations\" , grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., \"An X is a Y that is Z\"). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic \"module,\" however; the symbolic functions would emerge as an intrinsically \"dedicated\" symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded."
      },
      {
        "node_idx": 157548,
        "score_0_10": 9,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 112674,
        "score_0_10": 9,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 78341,
        "score_0_10": 9,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      }
    ]
  },
  "98": {
    "explanation": "distributed system specification monitoring with decentralized LTL evaluation",
    "topk": [
      {
        "node_idx": 125927,
        "score_0_10": 10,
        "title": "compositionality decompositionality and refinement in input output conformance testing",
        "abstract": "We propose an input/output conformance testing theory utilizing Modal Interface Automata with Input Refusals (IR-MIA) as novel behavioral formalism for both the specification and the implementation under test. A modal refinement relation on IR-MIA allows distinguishing between obligatory and allowed output behaviors, as well as between implicitly underspecified and explicitly forbidden input behaviors. The theory therefore supports positive and negative conformance testing with optimistic and pessimistic environmental assumptions. We further show that the resulting conformance relation on IR-MIA, called modal-irioco, enjoys many desirable properties concerning component-based behaviors. First, modal-irioco is preserved under modal refinement and constitutes a preorder under certain restrictions which can be ensured by a canonical input completion for IR-MIA. Second, under the same restrictions, modal-irioco is compositional with respect to parallel composition of IR-MIA with multi-cast and hiding. Finally, the quotient operator on IR-MIA, as the inverse to parallel composition, facilitates decompositionality in conformance testing to solve the unknown-component problem."
      },
      {
        "node_idx": 64978,
        "score_0_10": 10,
        "title": "me love syn cookies syn flood mitigation in programmable data planes",
        "abstract": "The SYN flood attack is a common attack strategy on the Internet, which tries to overload services with requests leading to a Denial-of-Service (DoS). Highly asymmetric costs for connection setup - putting the main burden on the attackee - make SYN flooding an efficient and popular DoS attack strategy. Abusing the widely used TCP as an attack vector complicates the detection of malicious traffic and its prevention utilizing naive connection blocking strategies. Modern programmable data plane devices are capable of handling traffic in the 10 Gbit/s range without overloading. We discuss how we can harness their performance to defend entire networks against SYN flood attacks. Therefore, we analyze different defense strategies, SYN authentication and SYN cookie, and discuss implementation difficulties when ported to different target data planes: software, network processors, and FPGAs. We provide prototype implementations and performance figures for all three platforms. Further, we fully disclose the artifacts leading to the experiments described in this work."
      },
      {
        "node_idx": 95061,
        "score_0_10": 10,
        "title": "decentralised ltl monitoring",
        "abstract": "Users wanting to monitor distributed or component-based systems often perceive them as monolithic systems which, seen from the outside, exhibit a uniform behaviour as opposed to many components displaying many local behaviours that together constitute the system's global behaviour. This level of abstraction is often reasonable, hiding implementation details from users who may want to specify the system's global behaviour in terms of an LTL formula. However, the problem that arises then is how such a specification can actually be monitored in a distributed system that has no central data collection point, where all the components' local behaviours are observable. In this case, the LTL specification needs to be decomposed into sub-formulae which, in turn, need to be distributed amongst the components' locally attached monitors, each of which sees only a distinct part of the global behaviour. The main contribution of this paper is an algorithm for distributing and monitoring LTL formulae, such that satisfac- tion or violation of specifications can be detected by local monitors alone. We present an implementation and show that our algorithm introduces only a minimum delay in detecting satisfaction/violation of a specification. Moreover, our practical results show that the communication overhead introduced by the local monitors is considerably lower than the number of messages that would need to be sent to a central data collection point."
      },
      {
        "node_idx": 144377,
        "score_0_10": 10,
        "title": "a secure infrastructure for system console and reset access",
        "abstract": "During the last years large farms have been built using commodity hardware. This hardware lacks components for remote and automated administration. Products that can be retrofitted to these systems are either costly or inherently insecure. We present a system based on serial ports and simple machine controlled relays. We report on experience gained by setting up a 50-machine test environment as well as current work in progress in the area."
      },
      {
        "node_idx": 44782,
        "score_0_10": 10,
        "title": "control flow integrity precision security and performance",
        "abstract": "Memory corruption errors in C/C++ programs remain the most common source of security vulnerabilities in today's systems. Control-flow hijacking attacks exploit memory corruption vulnerabilities to divert program execution away from the intended control flow. Researchers have spent more than a decade studying and refining defenses based on Control-Flow Integrity (CFI), and this technique is now integrated into several production compilers. However, so far no study has systematically compared the various proposed CFI mechanisms, nor is there any protocol on how to compare such mechanisms. #R##N#We compare a broad range of CFI mechanisms using a unified nomenclature based on (i) a qualitative discussion of the conceptual security guarantees, (ii) a quantitative security evaluation, and (iii) an empirical evaluation of their performance in the same test environment. For each mechanism, we evaluate (i) protected types of control-flow transfers, (ii) the precision of the protection for forward and backward edges. For open-source compiler-based implementations, we additionally evaluate (iii) the generated equivalence classes and target sets, and (iv) the runtime performance."
      },
      {
        "node_idx": 13910,
        "score_0_10": 9,
        "title": "hydra hybrid design for remote attestation using a formally verified microkernel",
        "abstract": "Remote Attestation (RA) allows a trusted entity (verifier) to securely measure internal state of a remote untrusted hardware platform (prover). RA can be used to establish a static or dynamic root of trust in embedded and cyber-physical systems. It can also be used as a building block for other security services and primitives, such as software updates and patches, verifiable deletion and memory resetting. There are three major classes of RA designs: hardware-based, software-based, and hybrid, each with its own set of benefits and drawbacks. This paper presents the first hybrid RA design, called HYDRA, that builds upon formally verified software components that ensure memory isolation and protection, as well as enforce access control to memory and other resources. HYDRA obtains these properties by using the formally verified seL4 microkernel. (Until now, this was only attainable with purely hardware-based designs.) Using seL4 requires fewer hardware modifications to the underlying microprocessor. Building upon a formally verified software component increases confidence in security of the overall design of HYDRA and its implementation. We instantiate HYDRA on two commodity hardware platforms and assess the performance and overhead of performing RA on such platforms via experimentation; we show that HYDRA can attest 10MB of memory in less than 500msec when using a Speck-based message authentication code (MAC) to compute a cryptographic checksum over the memory to be attested."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 98739,
        "score_0_10": 9,
        "title": "redesigning op2 compiler to use hpx runtime asynchronous techniques",
        "abstract": "Maximizing parallelism level in applications can be achieved by minimizing overheads due to load imbalances and waiting time due to memory latencies. Compiler optimization is one of the most effective solutions to tackle this problem. The compiler is able to detect the data dependencies in an application and is able to analyze the specific sections of code for parallelization potential. However, all of these techniques provided with a compiler are usually applied at compile time, so they rely on static analysis, which is insufficient for achieving maximum parallelism and producing desired application scalability. One solution to address this challenge is the use of runtime methods. This strategy can be implemented by delaying certain amount of code analysis to be done at runtime. In this research, we improve the parallel application performance generated by the OP2 compiler by leveraging HPX, a C++ runtime system, to provide runtime optimizations. These optimizations include asynchronous tasking, loop interleaving, dynamic chunk sizing, and data prefetching. The results of the research were evaluated using an Airfoil application which showed a 40-50% improvement in parallel performance."
      },
      {
        "node_idx": 65984,
        "score_0_10": 9,
        "title": "c flat control flow attestation for embedded systems software",
        "abstract": "Remote attestation is a crucial security service particularly relevant to increasingly popular IoT (and other embedded) devices. It allows a trusted party (verifier) to learn the state of a remote, and potentially malware-infected, device (prover). Most existing approaches are static in nature and only check whether benign software is initially loaded on the prover. However, they are vulnerable to run-time attacks that hijack the application's control or data flow, e.g., via return-oriented programming or data-oriented exploits. As a concrete step towards more comprehensive run-time remote attestation, we present the design and implementation of Control- FLow ATtestation (C-FLAT) that enables remote attestation of an application's control-flow path, without requiring the source code. We describe a full prototype implementation of C-FLAT on Raspberry Pi using its ARM TrustZone hardware security extensions. We evaluate C-FLAT's performance using a real-world embedded (cyber-physical) application, and demonstrate its efficacy against control-flow hijacking attacks."
      },
      {
        "node_idx": 19942,
        "score_0_10": 9,
        "title": "eu datagrid testbed management and support at cern",
        "abstract": "In this paper we report on the first two years of running the CERN testbed site for the EU DataGRID project. The site consists of about 120 dual-processor PCs distributed over several testbeds used for different purposes: software development, system integration, and application tests. Activities at the site included test productions of MonteCarlo data for LHC experiments, tutorials and demonstrations of GRID technologies, and support for individual users analysis. This paper focuses on node installation and configuration techniques, service management, user support in a gridified environment, and includes considerations on scalability and security issues and comparisons with \"traditional\" production systems, as seen from the administrator point of view."
      }
    ]
  },
  "102": {
    "explanation": "impact of developer emotions on programming performance and productivity",
    "topk": [
      {
        "node_idx": 155003,
        "score_0_10": 10,
        "title": "happy software developers solve problems better psychological measurements in empirical software engineering",
        "abstract": "For more than thirty years, it has been claimed that a way to improve software developers\u2019 productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states\u2014emotions and moods\u2014deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint."
      },
      {
        "node_idx": 17415,
        "score_0_10": 10,
        "title": "understanding the affect of developers theoretical background and guidelines for psychoempirical software engineering",
        "abstract": "Affects--emotions and moods--have an impact on cognitive processing activities and the working performance of individuals. It has been established that software development tasks are undertaken through cognitive processing activities. Therefore, we have proposed to employ psychology theory and measurements in software engineering (SE) research. We have called it \"psychoempirical software engineering\". However, we found out that existing SE research has often fallen into misconceptions about the affect of developers, lacking in background theory and how to successfully employ psychological measurements in studies. The contribution of this paper is threefold. (1) It highlights the challenges to conduct proper affect-related studies with psychology; (2) it provides a comprehensive literature review in affect theory; and (3) it proposes guidelines for conducting psychoempirical software engineering."
      },
      {
        "node_idx": 100588,
        "score_0_10": 10,
        "title": "in pursuit of spreadsheet excellence",
        "abstract": "The first fully-documented study into the quantitative impact of errors in operational spreadsheets identified an interesting anomaly. One of the five participating organisations involved in the study contributed a set of five spreadsheets of such quality that they set the organisation apart in a statistical sense. This virtuoso performance gave rise to a simple sampling test - The Clean Sheet Test - which can be used to objectively evaluate if an organisation is in control of the spreadsheets it is using in important processes such as financial reporting."
      },
      {
        "node_idx": 96990,
        "score_0_10": 10,
        "title": "what happens when software developers are un happy",
        "abstract": "Abstract   The growing literature on affect among software developers mostly reports on the linkage between happiness, software quality, and developer productivity. Understanding happiness and unhappiness in all its components \u2013 positive and negative emotions and moods \u2013 is an attractive and important endeavor. Scholars in industrial and organizational psychology have suggested that understanding happiness and unhappiness could lead to cost-effective ways of enhancing working conditions, job performance, and to limiting the occurrence of psychological disorders. Our comprehension of the consequences of (un)happiness among developers is still too shallow, being mainly expressed in terms of development productivity and software quality. In this paper, we study what happens when developers are happy and unhappy while developing software. Qualitative data analysis of responses given by 317 questionnaire participants identified 42 consequences of unhappiness and 32 of happiness. We found consequences of happiness and unhappiness that are beneficial and detrimental for developers\u2019 mental well-being, the software development process, and the produced artifacts. Our classification scheme, available as open data enables new happiness research opportunities of cause-effect type, and it can act as a guideline for practitioners for identifying damaging effects of unhappiness and for fostering happiness on the job."
      },
      {
        "node_idx": 919,
        "score_0_10": 10,
        "title": "an introduction to deep learning for the physical layer",
        "abstract": "We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. The paper is concluded with a discussion of open challenges and areas for future investigation."
      },
      {
        "node_idx": 87393,
        "score_0_10": 10,
        "title": "how do you feel developer an explanatory theory of the impact of affects on programming performance",
        "abstract": "Affects---emotions and moods---have an impact on cognitive activities and the working performance of individuals. Development tasks are undertaken through cognitive processes, yet software engineering research lacks theory on affects and their impact on software development activities. In this paper, we report on an interpretive study aimed at broadening our understanding of the psychology of programming in terms of the experience of affects while programming, and the impact of affects on programming performance. We conducted a qualitative interpretive study based on: face-to-face open-ended interviews, in-field observations, and e-mail exchanges. This enabled us to construct a novel explanatory theory of the impact of affects on development performance. The theory is explicated using an established taxonomy framework. The proposed theory builds upon the concepts of events, affects, attractors, focus, goals, and performance. Theoretical and practical implications are given."
      },
      {
        "node_idx": 36762,
        "score_0_10": 10,
        "title": "consequences of unhappiness while developing software",
        "abstract": "The proliferating literature on the affect of software developers consists mostly of studies investigating the linkage between happiness, software quality, and developers' productivity. Understanding the positive side of happiness - positive emotions and moods - is an attractive and important endeavour. Yet, scholars in industrial and organizational psychology have suggested that studying unhappiness could lead to cost-effective ways of enhancing working conditions, job performance, and to limiting the occurrence of psychological disorders. Furthermore, our comprehension of the consequences of (un)happiness of developers is still too shallow, and it is mainly expressed in terms of development productivity and software quality. In this paper, we attempt to uncover the experienced consequences of unhappiness among programmers while developing software. Using qualitative data analysis of the responses given by 181 questionnaire participants, we identified 49 consequences of unhappiness of software engineers. We found detrimental consequences on developers' mental well-being, the software development process, and the produced artifacts. Our classification scheme, available as open data, will spawn new happiness research opportunities of cause-effect type, and it can act as a guideline for practitioners for identifying detrimental effects of unhappiness and for fostering happiness on the job."
      },
      {
        "node_idx": 146035,
        "score_0_10": 9,
        "title": "on the unhappiness of software developers",
        "abstract": "The happy-productive worker thesis states that happy workers are more productive. Recent research in software engineering supports the thesis, and the ideal of flourishing happiness among software developers is often expressed among industry practitioners. However, the literature suggests that a cost-effective way to foster happiness and productivity among workers could be to limit unhappiness. Psychological disorders such as job burnout and anxiety could also be reduced by limiting the negative experiences of software developers. Simultaneously, a baseline assessment of (un)happiness and knowledge about how developers experience it are missing. In this paper, we broaden the understanding of unhappiness among software developers in terms of (1) the software developer population distribution of (un)happiness, and (2) the causes of unhappiness while developing software. We conducted a large-scale quantitative and qualitative survey, incorporating a psychometrically validated instrument for measuring (un)happiness, with 2 220 developers, yielding a rich and balanced sample of 1318 complete responses. Our results indicate that software developers are a slightly happy population, but the need for limiting the unhappiness of developers remains. We also identified 219 factors representing causes of unhappiness while developing software. Our results, which are available as open data, can act as guidelines for practitioners in management positions and developers in general for fostering happiness on the job. We suggest considering happiness in future studies of both human and technical aspects in software engineering."
      },
      {
        "node_idx": 46557,
        "score_0_10": 9,
        "title": "excel 2013 spreadsheet inquire",
        "abstract": "Excel 2013 (version 15) includes an add-in 'Inquire ' 1 for auditing spreadsheets. We describe the evolution of such tools in the third-party mark etplace and assess the usefulness of Microsoft's own add-in in this context. We compare in detail the features of Inquire with similar products and make suggestions for how it co uld be enhanced. We offer a free helper add-in that in our opinion corrects one major short coming of Inquire."
      },
      {
        "node_idx": 166778,
        "score_0_10": 9,
        "title": "recommended practices for spreadsheet testing",
        "abstract": "This paper presents the authors recommended practices for spreadsheet testing. Documented spreadsheet error rates are unacceptable in corporations today. Although improvements are needed throughout the systems development life cycle, credible improvement programs must include comprehensive testing. Several forms of testing are possible, but logic inspection is recommended for module testing. Logic inspection appears to be feasible for spreadsheet developers to do, and logic inspection appears to be safe and effective."
      }
    ]
  },
  "103": {
    "explanation": "complexity and capacity bounds in computational and communication models",
    "topk": [
      {
        "node_idx": 149124,
        "score_0_10": 10,
        "title": "the complexity of the comparator circuit value problem",
        "abstract": "In 1990 Subramanian defined the complexity class CC as the set of problems log-space reducible to the comparator circuit value problem (CCV). He and Mayr showed that NL \\subseteq CC \\subseteq P, and proved that in addition to CCV several other problems are complete for CC, including the stable marriage problem, and finding the lexicographically first maximal matching in a bipartite graph. We are interested in CC because we conjecture that it is incomparable with the parallel class NC which also satisfies NL \\subseteq NC \\subseteq P, and note that this conjecture implies that none of the CC-complete problems has an efficient polylog time parallel algorithm. We provide evidence for our conjecture by giving oracle settings in which relativized CC and relativized NC are incomparable. #R##N#We give several alternative definitions of CC, including (among others) the class of problems computed by uniform polynomial-size families of comparator circuits supplied with copies of the input and its negation, the class of problems AC^0-reducible to CCV, and the class of problems computed by uniform AC^0 circuits with CCV gates. We also give a machine model for CC, which corresponds to its characterization as log-space uniform polynomial-size families of comparator circuits. These various characterizations show that CC is a robust class. The main technical tool we employ is universal comparator circuits. #R##N#Other results include a simpler proof of NL \\subseteq CC, and an explanation of the relation between the Gale-Shapley algorithm and Subramanian's algorithm for stable marriage. #R##N#This paper continues the previous work of Cook, L\\^e and Ye which focused on Cook-Nguyen style uniform proof complexity, answering several open questions raised in that paper."
      },
      {
        "node_idx": 42633,
        "score_0_10": 9,
        "title": "singular and plural functions for functional logic programming",
        "abstract": "Functional logic programming (FLP) languages use non-terminating and non-confluent constructor systems (CS's) as programs in order to define non-strict non-determi-nistic functions. Two semantic alternatives have been usually considered for parameter passing with this kind of functions: call-time choice and run-time choice. While the former is the standard choice of modern FLP languages, the latter lacks some properties---mainly compositionality---that have prevented its use in practical FLP systems. Traditionally it has been considered that call-time choice induces a singular denotational semantics, while run-time choice induces a plural semantics. We have discovered that this latter identification is wrong when pattern matching is involved, and thus we propose two novel compositional plural semantics for CS's that are different from run-time choice. #R##N#We study the basic properties of our plural semantics---compositionality, polarity, monotonicity for substitutions, and a restricted form of the bubbling property for constructor systems---and the relation between them and to previous proposals, concluding that these semantics form a hierarchy in the sense of set inclusion of the set of computed values. We have also identified a class of programs characterized by a syntactic criterion for which the proposed plural semantics behave the same, and a program transformation that can be used to simulate one of them by term rewriting. At the practical level, we study how to use the expressive capabilities of these semantics for improving the declarative flavour of programs. We also propose a language which combines call-time choice and our plural semantics, that we have implemented in Maude. The resulting interpreter is employed to test several significant examples showing the capabilities of the combined semantics. #R##N#To appear in Theory and Practice of Logic Programming (TPLP)"
      },
      {
        "node_idx": 154425,
        "score_0_10": 9,
        "title": "complexity limitations on quantum computation",
        "abstract": "We use the powerful tools of counting complexity and generic oracles to help understand the limitations of the complexity of quantum computation. We show several results for the probabilistic quantum class BQP. #R##N#1. BQP is low for PP, i.e., PP^BQP=PP. #R##N#2. There exists a relativized world where P=BQP and the polynomial-time hierarchy is infinite. #R##N#3. There exists a relativized world where BQP does not have complete sets. #R##N#4. There exists a relativized world where P=BQP but P is not equal to UP intersect coUP and one-way functions exist. This gives a relativized answer to an open question of Simon."
      },
      {
        "node_idx": 145848,
        "score_0_10": 9,
        "title": "lecture notes on network information theory",
        "abstract": "These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at this http URL The previous (and obsolete) version of the lecture notes can be found at this http URL"
      },
      {
        "node_idx": 55398,
        "score_0_10": 9,
        "title": "the capacity of channels with feedback",
        "abstract": "We introduce a general framework for treating channels with memory and feedback. First, we generalize Massey's concept of directed information and use it to characterize the feedback capacity of general channels. Second, we present coding results for Markov channels. This requires determining appropriate sufficient statistics at the encoder and decoder. Third, a dynamic programming framework for computing the capacity of Markov channels is presented. Fourth, it is shown that the average cost optimality equation (ACOE) can be viewed as an implicit single-letter characterization of the capacity. Fifth, scenarios with simple sufficient statistics are described."
      },
      {
        "node_idx": 93618,
        "score_0_10": 9,
        "title": "feedback capacity of the gaussian interference channel to within 2 bits",
        "abstract": "We characterize the capacity region to within 2 bits/s/Hz and the symmetric capacity to within 1 bit/s/Hz for the two-user Gaussian interference channel (IC) with feedback. We develop achievable schemes and derive a new outer bound to arrive at this conclusion. One consequence of the result is that feedback provides multiplicative gain, i.e., the gain becomes arbitrarily large for certain channel parameters. It is a surprising result because feedback has been so far known to provide no gain in memoryless point-to-point channels and only bounded additive gain in multiple access channels. The gain comes from using feedback to maximize resource utilization, thereby enabling more efficient resource sharing between the interfering users. The result makes use of a deterministic model to provide insights into the Gaussian channel. This deterministic model is a special case of El Gamal-Costa deterministic model and as a side-generalization, we establish the exact feedback capacity region of this general class of deterministic ICs."
      },
      {
        "node_idx": 71257,
        "score_0_10": 9,
        "title": "the two user gaussian interference channel a deterministic view",
        "abstract": "This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel."
      },
      {
        "node_idx": 80899,
        "score_0_10": 9,
        "title": "monoids and maximal codes",
        "abstract": "In recent years codes that are not Uniquely Decipherable (UD) are been studied partitioning them in classes that localize the ambiguities of the code. A natural question is how we can extend the notion of maximality to codes that are not UD. In this paper we give an answer to this question. To do this we introduce a partial order in the set of submonoids of a monoid showing the existence, in this poset, of maximal elements that we call full monoids. Then a set of generators of a full monoid is, by definition, a maximal code. We show how this definition extends, in a natural way, the existing definition concerning UD codes and we find a characteristic property of a monoid generated by a maximal UD code."
      },
      {
        "node_idx": 76418,
        "score_0_10": 9,
        "title": "zeno machines and hypercomputation",
        "abstract": "This paper reviews the Church-Turing Thesis (or rather, theses) with reference to their origin and application and considers some models of \"hypercomputation\", concentrating on perhaps the most straight-forward option: Zeno machines (Turing machines with accelerating clock). The halting problem is briefly discussed in a general context and the suggestion that it is an inevitable companion of any reasonable computational model is emphasised. It is hinted that claims to have \"broken the Turing barrier\" could be toned down and that the important and well-founded role of Turing computability in the mathematical sciences stands unchallenged."
      },
      {
        "node_idx": 149803,
        "score_0_10": 9,
        "title": "inner and outer bounds for the gaussian cognitive interference channel and new capacity results",
        "abstract": "The capacity of the Gaussian cognitive interference channel, a variation of the classical two-user interference channel where one of the transmitters (referred to as cognitive) has knowledge of both messages, is known in several parameter regimes but remains unknown in general. This paper provides a comparative overview of this channel model as it proceeds through the following contributions. First, several outer bounds are presented: (a) a new outer bound based on the idea of a broadcast channel with degraded message sets, and (b) an outer bound obtained by transforming the channel into channels with known capacity. Next, a compact Fourier-Motzkin eliminated version of the largest known inner bound derived for the discrete memoryless cognitive interference channel is presented and specialized to the Gaussian noise case, where several simplified schemes with jointly Gaussian input are evaluated in closed form and later used to prove a number of results. These include a new set of capacity results for: (a) the \u201cprimary decodes cognitive\u201d regime, a subset of the \u201cstrong interference\u201d regime that is not included in the \u201cvery strong interference\u201d regime for which capacity was known, and (b) the \u201cS-channel in strong interference\u201d in which the primary transmitter does not interfere with the cognitive receiver and the primary receiver experiences strong interference. Next, for a general Gaussian channel the capacity is determined to within one bit/s/Hz and to within a factor two regardless of the channel parameters, thus establishing rate performance guarantees at high and low SNR, respectively. The paper concludes with numerical evaluations and comparisons of the various simplified achievable rate regions and outer bounds in parameter regimes where capacity is unknown, leading to further insight on the capacity region."
      }
    ]
  },
  "106": {
    "explanation": "energy-efficient multiprocessor task scheduling and replication mechanisms",
    "topk": [
      {
        "node_idx": 44123,
        "score_0_10": 10,
        "title": "myrmics scalable dependency aware task scheduling on heterogeneous manycores",
        "abstract": "Task-based programming models have become very popular, as they offer an attractive solution to parallelize serial application code with task and data annotations. They usually depend on a runtime system that schedules the tasks to multiple cores in parallel while resolving any data hazards. However, existing runtime system implementations are not ready to scale well on emerging manycore processors, as they often rely on centralized structures and/or locks on shared structures in a cache-coherent memory. We propose design choices, policies and mechanisms to enhance runtime system scalability for single-chip processors with hundreds of cores. Based on these concepts, we create and evaluate Myrmics, a runtime system for a dependency-aware, task-based programming model on a heterogeneous hardware prototype platform that emulates a single-chip processor of 8 latency-optimized and 512 throughput-optimized CPUs. We find that Myrmics scales successfully to hundreds of cores. Compared to MPI versions of the same benchmarks with hand-tuned message passing, Myrmics achieves similar scalability with a 10-30% performance overhead, but with less programming effort. We analyze the scalability of the runtime system in detail and identify the key factors that contribute to it."
      },
      {
        "node_idx": 66543,
        "score_0_10": 10,
        "title": "energy efficient scheduling for homogeneous multiprocessor systems",
        "abstract": "We present a number of novel algorithms, based on mathematical optimization formulations, in order to solve a homogeneous multiprocessor scheduling problem, while minimizing the total energy consumption. In particular, for a system with a discrete speed set, we propose solving a tractable linear program. Our formulations are based on a fluid model and a global scheduling scheme, i.e. tasks are allowed to migrate between processors. The new methods are compared with three global energy/feasibility optimal workload allocation formulations. Simulation results illustrate that our methods achieve both feasibility and energy optimality and outperform existing methods for constrained deadline tasksets. Specifically, the results provided by our algorithm can achieve up to an 80% saving compared to an algorithm without a frequency scaling scheme and up to 70% saving compared to a constant frequency scaling scheme for some simulated tasksets. Another benefit is that our algorithms can solve the scheduling problem in one step instead of using a recursive scheme. Moreover, our formulations can solve a more general class of scheduling problems, i.e. any periodic real-time taskset with arbitrary deadline. Lastly, our algorithms can be applied to both online and offline scheduling schemes."
      },
      {
        "node_idx": 2923,
        "score_0_10": 10,
        "title": "feedback scheduling for energy efficient real time homogeneous multiprocessor systems",
        "abstract": "Real-time scheduling algorithms proposed in the literature are often based on worst-case estimates of task parameters and the performance of an open-loop scheme can therefore be poor. To improve on such a situation, one can instead apply a closed-loop scheme, where feedback is exploited to dynamically adjust the system parameters at run-time. We propose an optimal control framework that takes advantage of feeding back information of finished tasks to solve a real-time multiprocessor scheduling problem with uncertainty in task execution times, with the objective of minimizing the total energy consumption. Specifically, we propose a linear programming-based algorithm to solve a workload partitioning problem and adopt McNaughton's wrap around algorithm to find the task execution order. Simulation results for a PowerPC 405LP and an XScale processor illustrate that our feedback scheduling algorithm can result in an energy saving of approximately 40% compared to an open-loop method."
      },
      {
        "node_idx": 39739,
        "score_0_10": 9,
        "title": "creek low latency mixed consistency transactional replication scheme",
        "abstract": "In this paper we introduce Creek, a low-latency, eventually consistent replication scheme that also enables execution of strongly consistent operations (akin to ACID transactions). Operations can have arbitrary complex (but deterministic) semantics. Similarly to state machine replication (SMR), Creek totally-orders all operations, but does so using two different broadcast mechanisms: a timestamp-based one and our novel conditional atomic broadcast (CAB). The former is used to establish a tentative order of all operations for speculative execution, and it can tolerate network partitions. On the other hand, CAB is only used to ensure linearizable execution of the strongly consistent operations, whenever distributed consensus can be solved. The execution of strongly consistent operations also stabilizes the execution order of the causally related weakly consistent operations. Creek uses multiversion concurrency control to efficiently handle operations' rollbacks and reexecutions resulting from the mismatch between the tentative and the final execution orders. In the TPC-C benchmark, Creek offers up to 2.5 times lower latency in returning client responses compared to the state-of-the-art speculative SMR scheme, while maintaining high accuracy of the speculative execution (92-100%)."
      },
      {
        "node_idx": 97175,
        "score_0_10": 9,
        "title": "compiler phase ordering as an orthogonal approach for reducing energy consumption",
        "abstract": "Compiler writers typically focus primarily on the performance of the generated program binaries when selecting the passes and the order in which they are applied in the standard optimization levels, such as GCC -O3. In some domains, such as embedded systems and High-Performance Computing (HPC), it might be sometimes acceptable to slowdown computations if the energy consumed can be significantly decreased. Embedded systems often rely on a battery and besides energy also have power dissipation limitations, while HPC centers have a growing concern with electricity and cooling costs. Relying on power policies to apply frequency/voltage scaling and/or change the CPU to idle states (e.g., alternate between power levels in bursts) as the main method to reduce energy leaves potential for improvement using other orthogonal approaches. In this work we evaluate the impact of compiler pass sequences specialization (also known as compiler phase ordering) as a means to reduce the energy consumed by a set of programs/functions when comparing with the use of the standard compiler phase orders provided by, e.g., -OX flags. We use our phase selection and ordering framework to explore the design space in the context of a Clang+LLVM compiler targeting a multicore ARM processor in an ODROID board and a dual x86 desktop representative of a node in a Supercomputing center. Our experiments with a set of representative kernels show that there we can reduce energy consumption by up to 24% and that some of these improvements can only be partially explained by improvements to execution time. The experiments show cases where applications that run faster consume more energy. Additionally, we make an effort to characterize the compiler sequence exploration space in terms of their impact on performance and energy."
      },
      {
        "node_idx": 62536,
        "score_0_10": 9,
        "title": "dotted version vectors logical clocks for optimistic replication",
        "abstract": "In cloud computing environments, a large number of users access data stored in highly available storage systems. To provide good performance to geographically disperse users and allow operation even in the presence of failures or network partitions, these systems often rely on optimistic replication solutions that guarantee only eventual consistency. In this scenario, it is important to be able to accurately and efficiently identify updates executed concurrently. In this paper, first we review, and expose problems with current approaches to causality tracking in optimistic replication: these either lose information about causality or do not scale, as they require replicas to maintain information that grows linearly with the number of clients or updates. Then, we propose a novel solution that fully captures causality while being very concise in that it maintains information that grows linearly only with the number of servers that register updates for a given data element, bounded by the degree of replication."
      },
      {
        "node_idx": 152568,
        "score_0_10": 8,
        "title": "church rosser systems codes with bounded synchronization delay and local rees extensions",
        "abstract": "What is the common link, if there is any, between Church-Rosser systems, prefix codes with bounded synchronization delay, and local Rees extensions? The first obvious answer is that each of these notions relates to topics of interest for WORDS: Church-Rosser systems are certain rewriting systems over words, codes are given by sets of words which form a basis of a free submonoid in the free monoid of all words (over a given alphabet) and local Rees extensions provide structural insight into regular languages over words. So, it seems to be a legitimate title for an extended abstract presented at the conference WORDS 2017. However, this work is more ambitious, it outlines some less obvious but much more interesting link between these topics. This link is based on a structure theory of finite monoids with varieties of groups and the concept of local divisors playing a prominent role. Parts of this work appeared in a similar form in conference proceedings where proofs and further material can be found."
      },
      {
        "node_idx": 160812,
        "score_0_10": 8,
        "title": "dynamic eee coalescing techniques and bounds",
        "abstract": "Frame coalescing is one of the most efficient techniques to manage the low power idle (LPI) mode supported by energy efficient Ethernet (EEE) interfaces. This technique enables EEE interfaces to remain in the LPI mode for a certain amount of time upon the arrival of the first frame (time-based coalescing) or until a predefined amount of traffic accumulates in the transmission buffer (size-based coalescing). This paper provides new insights on the practical efficiency limits of both coalescing techniques. In particular, we derive the fundamental limits on the maximum energy savings considering a target average frame delay. Additionally, we present new open-loop adaptive variants of both time-based and size-based coalescing techniques. These proposals dynamically adjust the length of the sleeping periods in accordance with actual traffic conditions to reduce energy consumption while keeping the average delay near a predefined value simultaneously. Analytical and simulation results show that the energy consumption of both proposals is comparable to the fundamental limits. Consequently, we recommend the usage of the time-based algorithm in most scenarios because of its simplicity as well as its ability to bound the maximum frame delay at the same time."
      },
      {
        "node_idx": 11402,
        "score_0_10": 8,
        "title": "in datacenter performance analysis of a tensor processing unit",
        "abstract": "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."
      },
      {
        "node_idx": 52884,
        "score_0_10": 8,
        "title": "regular languages are church rosser congruential",
        "abstract": "This paper proves a long standing conjecture in formal language theory. It shows that all regular languages are Church-Rosser congruential. The class of Church-Rosser congruential languages was introduced by McNaughton, Narendran, and Otto in 1988. A language L is Church-Rosser congruential, if there exists a finite confluent, and length-reducing semi-Thue system S such that L is a finite union of congruence classes modulo S. It was known that there are deterministic linear context-free languages which are not Church-Rosser congruential, but on the other hand it was strongly believed that all regular language are of this form. Actually, this paper proves a more general result."
      }
    ]
  },
  "108": {
    "explanation": "hybrid analog-digital beamforming and sensor calibration in mmWave systems",
    "topk": [
      {
        "node_idx": 82210,
        "score_0_10": 10,
        "title": "observability of strapdown ins alignment a global perspective",
        "abstract": "Alignment of the strapdown inertial navigation system (INS) has strong nonlinearity, even worse when maneuvers, e.g., tumbling techniques, are employed to improve the alignment. There is no general rule to attack the observability of a nonlinear system, so most previous works addressed the observability of the corresponding linearized system by implicitly assuming that the original nonlinear system and the linearized one have identical observability characteristics. Strapdown INS alignment is a nonlinear system that has its own characteristics. Using the inherent properties of strapdown INS, e.g., the attitude evolution on the SO(3) manifold, we start from the basic definition and develop a global and constructive approach to investigate the observability of strapdown INS static and tumbling alignment, highlighting the effects of the attitude maneuver on observability. We prove that strapdown INS alignment, considering the unknown constant sensor biases, will be completely observable if the strapdown INS is rotated successively about two different axes and will be nearly observable for finite known unobservable states (no more than two) if it is rotated about a single axis. Observability from a global perspective provides us with insights into and a clearer picture of the problem, shedding light on previous theoretical results on strapdown INS alignment that were not comprehensive or consistent. The reporting of inconsistencies calls for a review of all linearization-based observability studies in the vast literature. Extensive simulations with constructed ideal observers and an extended Kalman filter are carried out, and the numerical results accord with the analysis. The conclusions can also assist in designing the optimal tumbling strategy and the appropriate state observer in practice to maximize the alignment performance."
      },
      {
        "node_idx": 21298,
        "score_0_10": 9,
        "title": "limited feedback hybrid precoding for multi user millimeter wave systems",
        "abstract": "Antenna arrays will be an important ingredient in millimeter-wave (mmWave) cellular systems. A natural application of antenna arrays is simultaneous transmission to multiple users. Unfortunately, the hardware constraints in mmWave systems make it difficult to apply conventional lower frequency multiuser MIMO precoding techniques at mmWave. This paper develops low-complexity hybrid analog/digital precoding for downlink multiuser mmWave systems. Hybrid precoding involves a combination of analog and digital processing that is inspired by the power consumption of complete radio frequency and mixed signal hardware. The proposed algorithm configures hybrid precoders at the transmitter and analog combiners at multiple receivers with a small training and feedback overhead. The performance of the proposed algorithm is analyzed in the large dimensional regime and in single-path channels. When the analog and digital precoding vectors are selected from quantized codebooks, the rate loss due to the joint quantization is characterized, and insights are given into the performance of hybrid precoding compared with analog-only beamforming solutions. Analytical and simulation results show that the proposed techniques offer higher sum rates compared with analog-only beamforming solutions, and approach the performance of the unconstrained digital beamforming with relatively small codebooks."
      },
      {
        "node_idx": 16751,
        "score_0_10": 9,
        "title": "dynamic magnetometer calibration and alignment to inertial sensors by kalman filtering",
        "abstract": "Magnetometer and inertial sensors are widely used for orientation estimation. Magnetometer usage is often troublesome, as it is prone to be interfered by onboard or ambient magnetic disturbance. The onboard soft-iron material distorts not only the magnetic field, but the magnetometer sensor frame coordinate and the cross-sensor misalignment relative to inertial sensors. It is desirable to conveniently put magnetic and inertial sensors information in a common frame. Existing methods either split the problem into successive intrinsic and cross-sensor calibrations, or rely on stationary accelerometer measurements which is infeasible in dynamic conditions. This paper formulates the magnetometer calibration and alignment to inertial sensors as a state estimation problem, and collectively solves the magnetometer intrinsic and cross-sensor calibrations, as well as the gyroscope bias estimation. Sufficient conditions are derived for the problem to be globally observable, even when no accelerometer information is used at all. An extended Kalman filter is designed to implement the state estimation and comprehensive test data results show the superior performance of the proposed approach. It is immune to acceleration disturbance and applicable potentially in any dynamic conditions."
      },
      {
        "node_idx": 123204,
        "score_0_10": 9,
        "title": "hybrid digital and analog beamforming design for large scale antenna arrays",
        "abstract": "The potential of using of millimeter wave (mmWave) frequency for future wireless cellular communication systems has motivated the study of large-scale antenna arrays for achieving highly directional beamforming. However, the conventional fully digital beamforming methods which require one radio frequency (RF) chain per antenna element is not viable for large-scale antenna arrays due to the high cost and high power consumption of RF chain components in high frequencies. To address the challenge of this hardware limitation, this paper considers a hybrid beamforming architecture in which the overall beamformer consists of a low-dimensional digital beamformer followed by an RF beamformer implemented using analog phase shifters. Our aim is to show that such an architecture can approach the performance of a fully digital scheme with much fewer number of RF chains. Specifically, this paper establishes that if the number of RF chains is twice the total number of data streams, the hybrid beamforming structure can realize any fully digital beamformer exactly, regardless of the number of antenna elements. For cases with fewer number of RF chains, this paper further considers the hybrid beamforming design problem for both the transmission scenario of a point-to-point multiple-input multiple-output (MIMO) system and a downlink multi-user multiple-input single-output (MU-MISO) system. For each scenario, we propose a heuristic hybrid beamforming design that achieves a performance close to the performance of the fully digital beamforming baseline. Finally, the proposed algorithms are modified for the more practical setting in which only finite resolution phase shifters are available. Numerical simulations show that the proposed schemes are effective even when phase shifters with very low resolution are used."
      },
      {
        "node_idx": 127370,
        "score_0_10": 9,
        "title": "gyroscope calibration via magnetometer",
        "abstract": "Magnetometers, gyroscopes and accelerometers are commonly used sensors in a variety of applications. The paper proposes a novel gyroscope calibration method in the homogeneous magnetic field by the help of magnetometer. It is shown that, with sufficient rotation excitation, the homogeneous magnetic field vector can be exploited to serve as a good reference for calibrating low-cost gyroscopes. The calibration parameters include the gyroscope scale factor, non-orthogonal coefficient and bias for three axes, as well as its misalignment to the magnetometer frame. Simulation and field test results demonstrate the method's effectiveness."
      },
      {
        "node_idx": 109290,
        "score_0_10": 9,
        "title": "hybrid mimo architectures for millimeter wave communications phase shifters or switches",
        "abstract": "Hybrid analog/digital MIMO architectures were recently proposed as an alternative for fully-digitalprecoding in millimeter wave (mmWave) wireless communication systems. This is motivated by the possible reduction in the number of RF chains and analog-to-digital converters. In these architectures, the analog processing network is usually based on variable phase shifters. In this paper, we propose hybrid architectures based on switching networks to reduce the complexity and the power consumption of the structures based on phase shifters. We define a power consumption model and use it to evaluate the energy efficiency of both structures. To estimate the complete MIMO channel, we propose an open loop compressive channel estimation technique which is independent of the hardware used in the analog processing stage. We analyze the performance of the new estimation algorithm for hybrid architectures based on phase shifters and switches. Using the estimated, we develop two algorithms for the design of the hybrid combiner based on switches and analyze the achieved spectral efficiency. Finally, we study the trade-offs between power consumption, hardware complexity, and spectral efficiency for hybrid architectures based on phase shifting networks and switching networks. Numerical results show that architectures based on switches obtain equal or better channel estimation performance to that obtained using phase shifters, while reducing hardware complexity and power consumption. For equal power consumption, all the hybrid architectures provide similar spectral efficiencies."
      },
      {
        "node_idx": 133719,
        "score_0_10": 8,
        "title": "resilience for exascale enabled multigrid methods",
        "abstract": "With the increasing number of components and further miniaturization the mean time between faults in supercomputers will decrease. System level fault tolerance techniques are expensive and cost energy, since they are often based on redundancy. Also classical check-point-restart techniques reach their limits when the time for storing the system state to backup memory becomes excessive. Therefore, algorithm-based fault tolerance mechanisms can become an attractive alternative. This article investigates the solution process for elliptic partial differential equations that are discretized by finite elements. Faults that occur in the parallel geometric multigrid solver are studied in various model scenarios. In a standard domain partitioning approach, the impact of a failure of a core or a node will affect one or several subdomains. Different strategies are developed to compensate the effect of such a failure algorithmically. The recovery is achieved by solving a local subproblem with Dirichlet boundary conditions using local multigrid cycling algorithms. Additionally, we propose a superman strategy where extra compute power is employed to minimize the time of the recovery process."
      },
      {
        "node_idx": 164196,
        "score_0_10": 8,
        "title": "low complexity hybrid precoding in massive multiuser mimo systems",
        "abstract": "Massive multiple-input multiple-output (MIMO) is envisioned to offer considerable capacity improvement, but at the cost of high complexity of the hardware. In this paper, we propose a low-complexity hybrid precoding scheme to approach the performance of the traditional baseband zero-forcing (ZF) precoding (referred to as full-complexity ZF), which is considered a virtually optimal linear precoding scheme in massive MIMO systems. The proposed hybrid precoding scheme, named phased-ZF (PZF), essentially applies phase-only control at the RF domain and then performs a low-dimensional baseband ZF precoding based on the effective channel seen from baseband. Heavily quantized RF phase control up to 2 bits of precision is also considered and shown to incur very limited degradation. The proposed scheme is simulated in both ideal Rayleigh fading channels and sparsely scattered millimeter wave (mmWave) channels, both achieving highly desirable performance."
      },
      {
        "node_idx": 45381,
        "score_0_10": 8,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 119246,
        "score_0_10": 8,
        "title": "stiffness modeling of robotic manipulator with gravity compensator",
        "abstract": "The paper focuses on the stiffness modeling of robotic manipulators with gravity compensators. The main attention is paid to the development of the stiffness model of a spring-based compensator located between sequential links of a serial structure. The derived model allows us to describe the compensator as an equivalent non-linear virtual spring integrated in the corresponding actuated joint. The obtained results have been efficiently applied to the stiffness modeling of a heavy industrial robot of the Kuka family."
      }
    ]
  },
  "111": {
    "explanation": "graph orientation and reachability optimization",
    "topk": [
      {
        "node_idx": 73065,
        "score_0_10": 10,
        "title": "orienting graphs to optimize reachability",
        "abstract": "It is well known that every 2-edge-connected graph can be oriented so that the resulting digraph is strongly connected. Here we study the problem of orienting a connected graph with cut edges in order to maximize the number of ordered vertex pairs (x, y) such that there is a directed path from x to y. After transforming this problem, we prove a key theorem about the transformed problem that allows us to obtain a quadratic algorithm for the original orientation problem. We also consider how to orient graphs to minimize the number of ordered vertex pairs joined by a directed path. After showing this problem is equivalent to the comparability graph completion problem, we show both problems are NP-hard, and even NP-hard to approximate to within a factor of 1 + e, for some e > 0."
      },
      {
        "node_idx": 140115,
        "score_0_10": 10,
        "title": "label dependent session types",
        "abstract": "Session types have emerged as a typing discipline for communication protocols. Existing calculi with session types come equipped with many different primitives that combine communication with the introduction or elimination of the transmitted value. #R##N#We present a foundational session type calculus with a lightweight operational semantics. It fully decouples communication from the introduction and elimination of data and thus features a single communication reduction, which acts as a rendezvous between senders and receivers. We achieve this decoupling by introducing label-dependent session types, a minimalist value-dependent session type system with subtyping. The system is sufficiently powerful to simulate existing functional session type systems. Compared to such systems, label-dependent session types place fewer restrictions on the code. We further introduce primitive recursion over natural numbers at the type level, thus allowing to describe protocols whose behaviour depends on numbers exchanged in messages. An algorithmic type checking system is introduced and proved equivalent to its declarative counterpart. The new calculus showcases a novel lightweight integration of dependent types and linear typing, with has uses beyond session type systems."
      },
      {
        "node_idx": 151710,
        "score_0_10": 10,
        "title": "an efficient algorithm for enumerating chordless cycles and chordless paths",
        "abstract": "A chordless cycle (induced cycle) $C$ of a graph is a cycle without any chord, meaning that there is no edge outside the cycle connecting two vertices of the cycle. A chordless path is defined similarly. In this paper, we consider the problems of enumerating chordless cycles/paths of a given graph $G=(V,E),$ and propose algorithms taking $O(|E|)$ time for each chordless cycle/path. In the existing studies, the problems had not been deeply studied in the theoretical computer science area, and no output polynomial time algorithm has been proposed. Our experiments showed that the computation time of our algorithms is constant per chordless cycle/path for non-dense random graphs and real-world graphs. They also show that the number of chordless cycles is much smaller than the number of cycles. We applied the algorithm to prediction of NMR (Nuclear Magnetic Resonance) spectra, and increased the accuracy of the prediction."
      },
      {
        "node_idx": 92885,
        "score_0_10": 10,
        "title": "vertex coloring with star defects",
        "abstract": "Defective coloring is a variant of traditional vertex-coloring, according to which adjacent vertices are allowed to have the same color, as long as the monochromatic components induced by the corresponding edges have a certain structure. Due to its important applications, as for example in the bipartisation of graphs, this type of coloring has been extensively studied, mainly with respect to the size, degree, and acyclicity of the monochromatic components. #R##N#In this paper we focus on defective colorings in which the monochromatic components are acyclic and have small diameter, namely, they form stars. For outerplanar graphs, we give a linear-time algorithm to decide if such a defective coloring exists with two colors and, in the positive case, to construct one. Also, we prove that an outerpath (i.e., an outerplanar graph whose weak-dual is a path) always admits such a two-coloring. Finally, we present NP-completeness results for non-planar and planar graphs of bounded degree for the cases of two and three colors."
      },
      {
        "node_idx": 137633,
        "score_0_10": 10,
        "title": "cubic graphs with large circumference deficit",
        "abstract": "The circumference $c(G)$ of a graph $G$ is the length of a longest cycle. By exploiting our recent results on resistance of snarks, we construct infinite classes of cyclically $4$-, $5$- and $6$-edge-connected cubic graphs with circumference ratio $c(G)/|V(G)|$ bounded from above by $0.876$, $0.960$ and $0.990$, respectively. In contrast, the dominating cycle conjecture implies that the circumference ratio of a cyclically $4$-edge-connected cubic graph is at least $0.75$. #R##N#In addition, we construct snarks with large girth and large circumference deficit, solving Problem 1 proposed in [J. H\\\"agglund and K. Markstr\\\"om, On stable cycles and cycle double covers of graphs with large circumference, Disc. Math. 312 (2012), 2540--2544]."
      },
      {
        "node_idx": 14508,
        "score_0_10": 10,
        "title": "taking the initiative with extempore exploring out of turn interactions with websites",
        "abstract": "We present the first study to explore the use of out-of-turn interaction in websites. Out-of-turn interaction is a technique which empowers the user to supply unsolicited information while browsing. This approach helps flexibly bridge any mental mismatch between the user and the website, in a manner fundamentally different from faceted browsing and site-specific search tools. We built a user interface (Extempore) which accepts out-of-turn input via voice or text; and employed it in a US congressional website, to determine if users utilize out-of-turn interaction for information-finding tasks, and their rationale for doing so. The results indicate that users are adept at discerning when out-of-turn interaction is necessary in a particular task, and actively interleaved it with browsing. However, users found cascading information across information-finding subtasks challenging. Therefore, this work not only improves our understanding of out-of-turn interaction, but also suggests further opportunities to enrich browsing experiences for users."
      },
      {
        "node_idx": 24657,
        "score_0_10": 10,
        "title": "automatically generating interfaces for personalized interaction with digital libraries",
        "abstract": "We present an approach to automatically generate interfaces supporting personalized interaction with digital libraries; these interfaces augment the user-DL dialog by empowering the user to (optionally) supply out-of-turn information during an interaction, flatten or restructure the dialog, and enquire about dialog options. Interfaces generated using this approach for CITIDEL are described."
      },
      {
        "node_idx": 64672,
        "score_0_10": 10,
        "title": "staging transformations for multimodal web interaction management",
        "abstract": "Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. In addition to improving access and delivery capabilities, such interfaces enable flexible and personalized dialogs with websites, much like a conversation between humans. In this paper, we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites. A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction. The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input. It supports multiple interaction interfaces, and offers sessioning, caching, and co-ordination functions through the use of an interaction manager. Two case studies are presented to illustrate the promise of this approach."
      },
      {
        "node_idx": 16420,
        "score_0_10": 10,
        "title": "a survey on graph drawing beyond planarity",
        "abstract": "Graph Drawing Beyond Planarity is a rapidly growing research area that classifies and studies geometric representations of non-planar graphs in terms of forbidden crossing configurations. Aim of this survey is to describe the main research directions in this area, the most prominent known results, and some of the most challenging open problems."
      },
      {
        "node_idx": 103461,
        "score_0_10": 9,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      }
    ]
  },
  "116": {
    "explanation": "end-to-end deep learning for autonomous driving control",
    "topk": [
      {
        "node_idx": 45381,
        "score_0_10": 10,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 45355,
        "score_0_10": 10,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 16751,
        "score_0_10": 9,
        "title": "dynamic magnetometer calibration and alignment to inertial sensors by kalman filtering",
        "abstract": "Magnetometer and inertial sensors are widely used for orientation estimation. Magnetometer usage is often troublesome, as it is prone to be interfered by onboard or ambient magnetic disturbance. The onboard soft-iron material distorts not only the magnetic field, but the magnetometer sensor frame coordinate and the cross-sensor misalignment relative to inertial sensors. It is desirable to conveniently put magnetic and inertial sensors information in a common frame. Existing methods either split the problem into successive intrinsic and cross-sensor calibrations, or rely on stationary accelerometer measurements which is infeasible in dynamic conditions. This paper formulates the magnetometer calibration and alignment to inertial sensors as a state estimation problem, and collectively solves the magnetometer intrinsic and cross-sensor calibrations, as well as the gyroscope bias estimation. Sufficient conditions are derived for the problem to be globally observable, even when no accelerometer information is used at all. An extended Kalman filter is designed to implement the state estimation and comprehensive test data results show the superior performance of the proposed approach. It is immune to acceleration disturbance and applicable potentially in any dynamic conditions."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 74065,
        "score_0_10": 9,
        "title": "convolutional lstm network a machine learning approach for precipitation nowcasting",
        "abstract": "The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 155778,
        "score_0_10": 9,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      }
    ]
  },
  "117": {
    "explanation": "formal logic and decidability in XML static analysis",
    "topk": [
      {
        "node_idx": 47795,
        "score_0_10": 10,
        "title": "logics for xml",
        "abstract": "This work describes the theoretical and practical foundations of a system for the static analysis of XML processing languages. The system relies on a fixpoint modal logic with converse where models are finite trees. This calculus is expressive enough to capture regular tree types along with multi-directional navigation in trees. The decidability of the logic is proved in time 2^O(n) where n is the size of the input formula. XPath expressions and XML schemas are linearly translated into the logic. Based on these embeddings, several problems of major importance in XML applications are reduced to logical satisfiability. The focus is then given to a sound and complete algorithm for deciding the logic, along with crucial implementation techniques for building an effective solver. Practical experiments using a full system implementation are presented. The system appears efficient in practice for several realistic scenarios. The main application of this work is a new class of static analyzers for programs manipulating XML data. Such analyzers allow to ensure at compile-time valuable properties such as type-safety and optimizations, for safer and more efficient XML processing."
      },
      {
        "node_idx": 36873,
        "score_0_10": 9,
        "title": "adaptive branching for constraint satisfaction problems",
        "abstract": "The two standard branching schemes for CSPs are d-way and 2-way branching. Although it has been shown that in theory the latter can be exponentially more effective than the former, there is a lack of empirical evidence showing such differences. To investigate this, we initially make an experimental comparison of the two branching schemes over a wide range of benchmarks. Experimental results verify the theoretical gap between d-way and 2-way branching as we move from a simple variable ordering heuristic like dom to more sophisticated ones like dom/ddeg. However, perhaps surprisingly, experiments also show that when state-of-the-art variable ordering heuristics like dom/wdeg are used then d-way can be clearly more efficient than 2-way branching in many cases. Motivated by this observation, we develop two generic heuristics that can be applied at certain points during search to decide whether 2-way branching or a restricted version of 2-way branching, which is close to d-way branching, will be followed. The application of these heuristics results in an adaptive branching scheme. Experiments with instantiations of the two generic heuristics confirm that search with adaptive branching outperforms search with a fixed branching scheme on a wide range of problems."
      },
      {
        "node_idx": 144946,
        "score_0_10": 9,
        "title": "on asymptotic characterization of destabilizing switching signals for switched linear systems",
        "abstract": "This paper deals with classes of (de)stabilizing switching signals for switched systems. Most of the available conditions for stability of switched systems are sufficient in nature, and consequently, their violation does not conclude instability of a switched system. The study of instability is, however, important for obvious reasons. Our contributions are twofold: Firstly, we propose a class of switching signals under which a continuous-time switched linear system is unstable. Our characterization of instability depends solely on the asymptotic behaviour of frequency of switching, frequency of transition between subsystems, and fraction of activation of subsystems. Secondly, we show that our class of destabilizing switching signals is a strict subset of the class of switching signals that does not satisfy asymptotic characterization of stability recently proposed in the literature. This observation identifies a gap between asymptotic characterizations of stabilizing and destabilizing switching signals for switched linear systems. The main apparatus for our analysis is multiple Lyapunov-like functions."
      },
      {
        "node_idx": 49333,
        "score_0_10": 9,
        "title": "a graph theoretic approach to input to state stability of switched systems",
        "abstract": "This article deals with input-to-state stability (ISS) of discrete-time switched systems. Given a family of nonlinear systems with exogenous inputs, we present a class of switching signals under which the resulting switched system is ISS. We allow non-ISS systems in the family and our analysis involves graph-theoretic arguments. A weighted digraph is associated to the switched system, and a switching signal is expressed as an infinite walk on this digraph, both in a natural way. Our class of stabilizing switching signals (infinite walks) is periodic in nature and affords simple algorithmic construction."
      },
      {
        "node_idx": 138522,
        "score_0_10": 9,
        "title": "dependence and independence",
        "abstract": "We introduce an atomic formula intuitively saying that given variables are independent from given other variables if a third set of variables is kept constant. We contrast this with dependence logic. We show that our independence atom gives rise to a natural logic capable of formalizing basic intuitions about independence and dependence."
      },
      {
        "node_idx": 50137,
        "score_0_10": 9,
        "title": "linear logic based analysis of constraint handling rules with disjunction",
        "abstract": "Constraint Handling Rules (CHR) is a declarative committed-choice programming language with a strong relationship to linear logic. Its generalization CHR with Disjunction (CHRv) is a multi-paradigm declarative programming language that allows the embedding of horn programs. We analyse the assets and the limitations of the classical declarative semantics of CHR before we motivate and develop a linear-logic declarative semantics for CHR and CHRv. We show how to apply the linear-logic semantics to decide program properties and to prove operational equivalence of CHRv programs across the boundaries of language paradigms."
      },
      {
        "node_idx": 75302,
        "score_0_10": 9,
        "title": "bpa bisimilarity is exptime hard",
        "abstract": "Given a basic process algebra (BPA) and two stack symbols, the BPA bisimilarity problem asks whether the two stack symbols are bisimilar. We show that this problem is EXPTIME-hard."
      },
      {
        "node_idx": 136970,
        "score_0_10": 9,
        "title": "robust stability conditions for switched linear systems under restricted switching",
        "abstract": "We propose matrix commutator based stability characterization for discrete-time switched linear systems under restricted switching. Given an admissible minimum dwell time, we identify sufficient conditions on subsystems such that a switched system is stable under all switching signals that obey the given restriction. The primary tool for our analysis is commutation relations between the subsystem matrices. Our stability conditions are robust with respect to small perturbations in the elements of these matrices. In case of arbitrary switching (i.e., given minimum dwell time = 1), we recover the prior result [1,Proposition 1] as a special case of our result."
      },
      {
        "node_idx": 135340,
        "score_0_10": 9,
        "title": "annotation of car trajectories based on driving patterns",
        "abstract": "Nowadays, the ubiquity of various sensors enables the collection of voluminous datasets of car trajectories. Such datasets enable analysts to make sense of driving patterns and behaviors: in order to understand the behavior of drivers, one approach is to break a trajectory into its underlying patterns and then analyze that trajectory in terms of derived patterns. The process of trajectory segmentation is a function of various resources including a set of ground truth trajectories with their driving patterns. To the best of our knowledge, no such ground-truth dataset exists in the literature. In this paper, we describe a trajectory annotation framework and report our results to annotate a dataset of personal car trajectories. Our annotation methodology consists of a crowd-sourcing task followed by a precise process of aggregation. Our annotation process consists of two granularity levels, one to specify the annotation (segment border) and the other one to describe the type of the segment (e.g. speed-up, turn, merge, etc.). The output of our project, Dataset of Annotated Car Trajectories (DACT), is available online at this https URL ."
      },
      {
        "node_idx": 49307,
        "score_0_10": 9,
        "title": "deciding probabilistic automata weak bisimulation in polynomial time",
        "abstract": "Deciding in an efficient way weak probabilistic bisimulation in the context of Probabilistic Automata is an open problem for about a decade. In this work we close this problem by proposing a procedure that checks in polynomial time the existence of a weak combined transition satisfying the step condition of the bisimulation. We also present several extensions of weak combined transitions, such as hyper-transitions and the new concepts of allowed weak combined and hyper-transitions and of equivalence matching, that turn out to be verifiable in polynomial time as well. These results set the ground for the development of more effective compositional analysis algorithms for probabilistic systems."
      }
    ]
  },
  "118": {
    "explanation": "algebraic and computational complexity lower bounds and barriers",
    "topk": [
      {
        "node_idx": 135305,
        "score_0_10": 10,
        "title": "towards an algebraic natural proofs barrier via polynomial identity testing",
        "abstract": "We observe that a certain kind of algebraic proof - which covers essentially all known algebraic circuit lower bounds to date - cannot be used to prove lower bounds against VP if and only if what we call succinct hitting sets exist for VP. This is analogous to the Razborov-Rudich natural proofs barrier in Boolean circuit complexity, in that we rule out a large class of lower bound techniques under a derandomization assumption. We also discuss connections between this algebraic natural proofs barrier, geometric complexity theory, and (algebraic) proof complexity."
      },
      {
        "node_idx": 8773,
        "score_0_10": 10,
        "title": "feasible interpolation for qbf resolution calculi",
        "abstract": "In sharp contrast to classical proof complexity we are currently short of lower bound techniques for QBF proof systems. In this paper we establish the feasible interpolation technique for all resolution-based QBF systems, whether modelling CDCL or expansion-based solving. This both provides the first general lower bound method for QBF proof systems as well as largely extends the scope of classical feasible interpolation. We apply our technique to obtain new exponential lower bounds to all resolution-based QBF systems for a new class of QBF formulas based on the clique problem. Finally, we show how feasible interpolation relates to the recently established lower bound method based on strategy extraction."
      },
      {
        "node_idx": 154425,
        "score_0_10": 10,
        "title": "complexity limitations on quantum computation",
        "abstract": "We use the powerful tools of counting complexity and generic oracles to help understand the limitations of the complexity of quantum computation. We show several results for the probabilistic quantum class BQP. #R##N#1. BQP is low for PP, i.e., PP^BQP=PP. #R##N#2. There exists a relativized world where P=BQP and the polynomial-time hierarchy is infinite. #R##N#3. There exists a relativized world where BQP does not have complete sets. #R##N#4. There exists a relativized world where P=BQP but P is not equal to UP intersect coUP and one-way functions exist. This gives a relativized answer to an open question of Simon."
      },
      {
        "node_idx": 82182,
        "score_0_10": 10,
        "title": "covert communication over noisy channels a resolvability perspective",
        "abstract": "We consider the situation in which a transmitter attempts to communicate reliably over a discrete memoryless channel, while simultaneously ensuring covertness (low probability of detection) with respect to a warden, who observes the signals through another discrete memoryless channel. We develop a coding scheme based on the principle of channel resolvability, which generalizes and extends prior work in several directions. First, it shows that irrespective of the quality of the channels, it is possible to communicate on the order of    $\\sqrt {n}$    reliable and covert bits over    $n$    channel uses if the transmitter and the receiver share on the order of    $\\sqrt {n}$    key bits. This improves upon earlier results requiring on the order of    $\\sqrt {n}\\log n$    key bits. Second, it proves that if the receiver\u2019s channel is better than the warden\u2019s channel in a sense that we make precise, it is possible to communicate on the order of    $\\sqrt {n}$    reliable and covert bits over    $n$    channel uses without a secret key. This generalizes earlier results established for binary symmetric channels. We also identify the fundamental limits of covert and secret communications in terms of the optimal asymptotic scaling of the message size and key size, and we extend the analysis to Gaussian channels. The main technical problem that we address is how to develop concentration inequalities for low-weight sequences. The crux of our approach is to define suitably modified typical sets that are amenable to concentration inequalities."
      },
      {
        "node_idx": 129761,
        "score_0_10": 10,
        "title": "renyi divergence and kullback leibler divergence",
        "abstract": "Renyi divergence is related to Renyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by Renyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the Renyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of Renyi divergence and Kullback-Leibler divergence, including convexity, continuity, limits of    \\(\\sigma \\)   -algebras, and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results."
      },
      {
        "node_idx": 30590,
        "score_0_10": 10,
        "title": "exponential decreasing rate of leaked information in universal random privacy amplification",
        "abstract": "We derive a new upper bound for Eve's information in secret key generation from a common random number without communication. This bound improves on Bennett 's bound based on the Renyi entropy of order 2 because the bound obtained here uses the Renyi entropy of order 1+s for s \u2208 [0,1]. This bound is applied to a wire-tap channel. Then, we derive an exponential upper bound for Eve's information. Our exponent is compared with Hayashi 's exponent. For the additive case, the bound obtained here is better. The result is applied to secret key agreement by public discussion."
      },
      {
        "node_idx": 149184,
        "score_0_10": 9,
        "title": "resource bounded measure",
        "abstract": "A general theory of resource-bounded measurability and measure is developed. Starting from any feasible probability measure $\\nu$ on the Cantor space $\\C$ and any suitable complexity class $C \\subseteq \\C$, the theory identifies the subsets of $\\C$ that are $\\nu$-measurable in $C$ and assigns measures to these sets, thereby endowing $C$ with internal measure-theoretic structure. Classes to which the theory applies include various exponential time and space complexity classes, the class of all decidable languages, and the Cantor space itself, on which the resource-bounded theory is shown to agree with the classical theory. #R##N#The sets that are $\\nu$-measurable in $C$ are shown to form an algebra relative to which $\\nu$-measure is well-behaved. This algebra is also shown to be complete and closed under sufficiently uniform infinitary unions and intersections, and $\\nu$-measure in $C$ is shown to have the appropriate additivity and monotone convergence properties with respect to such infinitary operations. #R##N#A generalization of the classical Kolmogorov zero-one law is proven, showing that when $\\nu$ is any feasible coin-toss probability measure on $\\C$, every set that is $\\nu$-measurable in $C$ and (like most complexity classes) invariant under finite alterations must have $\\nu$-measure 0 or $\\nu$-measure 1 in $C$. #R##N#The theory is presented here is based on resource-bounded martingale splitting operators, which are type-2 functionals, each of which maps $\\N \\times {\\cal D}_\\nu$ into ${\\cal D}_\\nu \\times {\\cal D}_\\nu$, where ${\\cal D}_\\nu$ is the set of all $\\nu$-martingales. This type-2 aspect of the theory appears to be essential for general $\\nu$-measure in complexity classes $C$, but the sets of $\\nu$-measure 0 or 1 in C are shown to be characterized by the success conditions for martingales (type-1 functions) that have been used in resource-bounded measure to date."
      },
      {
        "node_idx": 8618,
        "score_0_10": 9,
        "title": "capacity results for arbitrarily varying wiretap channels",
        "abstract": "In this work the arbitrarily varying wiretap channel AVWC is studied. We derive a lower bound on the random code secrecy capacity for the average error criterion and the strong secrecy criterion in the case of a best channel to the eavesdropper by using Ahlswede's robustification technique for ordinary AVCs. We show that in the case of a non-symmetrisable channel to the legitimate receiver the deterministic code secrecy capacity equals the random code secrecy capacity, a result similar to Ahlswede's dichotomy result for ordinary AVCs. Using this we can derive that the lower bound is also valid for the deterministic code capacity of the AVWC. The proof of the dichotomy result is based on the elimination technique introduced by Ahlswede for ordinary AVCs. We further prove upper bounds on the deterministic code secrecy capacity in the general case, which results in a multi-letter expression for the secrecy capacity in the case of a best channel to the eavesdropper. Using techniques of Ahlswede, developed to guarantee the validity of a reliability criterion, the main contribution of this work is to integrate the strong secrecy criterion into these techniques."
      },
      {
        "node_idx": 53291,
        "score_0_10": 9,
        "title": "limits of reliable communication with low probability of detection on awgn channels",
        "abstract": "We present a square root limit on the amount of information transmitted reliably and with low probability of detection (LPD) over additive white Gaussian noise (AWGN) channels. Specifically, if the transmitter has AWGN channels to an intended receiver and a warden, both with non-zero noise power, we prove that o(\u221an) bits can be sent from the transmitter to the receiver in n channel uses while lower-bounding \u03b1 + \u03b2 \u2265 1-e for any e > 0, where \u03b1 and \u03b2 respectively denote the warden's probabilities of a false alarm when the sender is not transmitting and a missed detection when the sender is transmitting. Moreover, in most practical scenarios, a lower bound on the noise power on the channel between the transmitter and the warden is known and O(\u221an) bits can be sent in n LPD channel uses. Conversely, attempting to transmit more than O(\u221an) bits either results in detection by the warden with probability one or a non-zero probability of decoding error at the receiver as n\u2192\u221e."
      },
      {
        "node_idx": 93095,
        "score_0_10": 9,
        "title": "small depth multilinear formula lower bounds for iterated matrix multiplication with applications",
        "abstract": "In this paper, we study the algebraic formula complexity of multiplying $d$ many $2\\times 2$ matrices, denoted $\\mathrm{IMM}_{d}$, and show that the well-known divide-and-conquer algorithm cannot be significantly improved at any depth, as long as the formulas are multilinear. #R##N#Formally, for each depth $\\Delta \\leq \\log d$, we show that any product-depth $\\Delta$ multilinear formula for $\\mathrm{IMM}_d$ must have size $\\exp(\\Omega(\\Delta d^{1/\\Delta})).$ It also follows from this that any multilinear circuit of product-depth $\\Delta$ for the same polynomial of the above form must have a size of $\\exp(\\Omega(d^{1/\\Delta})).$ In particular, any polynomial-sized multilinear formula for $\\mathrm{IMM}_d$ must have depth $\\Omega(\\log d)$, and any polynomial-sized multilinear circuit for $\\mathrm{IMM}_d$ must have depth $\\Omega(\\log d/\\log \\log d).$ Both these bounds are tight up to constant factors. #R##N#1. Depth-reduction: A well-known result of Brent (JACM 1974) implies that any formula of size $s$ can be converted to one of size $s^{O(1)}$ and depth $O(\\log s)$; further, this reduction continues to hold for multilinear formulas. Our lower bound implies that any depth-reduction in the multilinear setting cannot reduce the depth to $o(\\log s)$ without a superpolynomial blow-up in size. #R##N#2. Separations from general formulas: Our result, along with a non-trivial upper bound for $\\mathrm{IMM}_{d}$ implied by a result of Gupta, Kamath, Kayal and Saptharishi (SICOMP 2016), shows that for any size $s$ and product-depth $\\Delta = o(\\log s),$ general formulas of size $s$ and product-depth $\\Delta$ cannot be converted to multilinear formulas of size $s^{\\omega(1)}$ and product-depth $\\Delta,$ when the underlying field has characteristic zero."
      }
    ]
  },
  "119": {
    "explanation": "wireless communication system design and capacity optimization",
    "topk": [
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 46136,
        "score_0_10": 10,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      },
      {
        "node_idx": 100650,
        "score_0_10": 10,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 92281,
        "score_0_10": 9,
        "title": "cell free massive mimo versus small cells",
        "abstract": "A Cell-Free Massive MIMO (multiple-input multiple-output) system comprises a very large number of distributed access points (APs)which simultaneously serve a much smaller number of users over the same time/frequency resources based on directly measured channel characteristics. The APs and users have only one antenna each. The APs acquire channel state information through time-division duplex operation and the reception of uplink pilot signals transmitted by the users. The APs perform multiplexing/de-multiplexing through conjugate beamforming on the downlink and matched filtering on the uplink. Closed-form expressions for individual user uplink and downlink throughputs lead to max-min power control algorithms. Max-min power control ensures uniformly good service throughout the area of coverage. A pilot assignment algorithm helps to mitigate the effects of pilot contamination, but power control is far more important in that regard. #R##N#Cell-Free Massive MIMO has considerably improved performance with respect to a conventional small-cell scheme, whereby each user is served by a dedicated AP, in terms of both 95%-likely per-user throughput and immunity to shadow fading spatial correlation. Under uncorrelated shadow fading conditions, the cell-free scheme provides nearly 5-fold improvement in 95%-likely per-user throughput over the small-cell scheme, and 10-fold improvement when shadow fading is correlated."
      },
      {
        "node_idx": 124006,
        "score_0_10": 9,
        "title": "miso capacity with per antenna power constraint",
        "abstract": "We establish in closed-form the capacity and the optimal signaling scheme for a MISO channel with per-antenna power constraint. Two cases of channel state information are considered: constant channel known at both the transmitter and receiver, and Rayleigh fading channel known only at the receiver. For the first case, the optimal signaling scheme is beamforming with the phases of the beam weights matched to the phases of the channel coefficients, but the amplitudes independent of the channel coefficients and dependent only on the constrained powers. For the second case, the optimal scheme is to send independent signals from the antennas with the constrained powers. In both cases, the capacity with per-antenna power constraint is usually less than that with sum power constraint."
      },
      {
        "node_idx": 28821,
        "score_0_10": 9,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 137708,
        "score_0_10": 8,
        "title": "cooperative interference management with miso beamforming",
        "abstract": "In this correspondence, we study the downlink transmission in a multi-cell system, where multiple base stations (BSs) each with multiple antennas cooperatively design their respective transmit beamforming vectors to optimize the overall system performance. For simplicity, it is assumed that all mobile stations (MSs) are equipped with a single antenna each, and there is one active MS in each cell at one time. Accordingly, the system of interests can be modeled by a multiple-input single-output (MISO) Gaussian interference channel (IC), termed as MISO-IC, with interference treated as noise. We propose a new method to characterize different rate-tuples for active MSs on the Pareto boundary of the achievable rate region for the MISO-IC, by exploring the relationship between the MISO-IC and the cognitive radio (CR) MISO channel. We show that each Pareto-boundary rate-tuple of the MISO-IC can be achieved in a decentralized manner when each of the BSs attains its own channel capacity subject to a certain set of interference-power constraints (also known as interference-temperature constraints in the CR system) at the other MS receivers. Furthermore, we show that this result leads to a new decentralized algorithm for implementing the multi-cell cooperative downlink beamforming."
      },
      {
        "node_idx": 38209,
        "score_0_10": 8,
        "title": "distributed data storage for modern astroparticle physics experiments",
        "abstract": "The German-Russian Astroparticle Data Life Cycle Initiative is an international project launched in 2018. The Initiative aims to develop technologies that provide a unified approach to data management, as well as to demonstrate their applicability on the example of two large astrophysical experiments - KASCADE and TAIGA. One of the key points of the project is the development of a distributed storage, which, on the one hand, will allow data of several experiments to be combined into a single repository with unified interface, and on the other hand, will provide data to all participants of experimental groups for multi-messenger analysis. Our approach to storage design is based on the single write-multiple read (SWMR) model for accessing raw or centrally processed data for further analysis. The main feature of the distributed storage is the ability to extract data either as a collection of files or as aggregated events from different sources. In the last case the storage provides users with a special service that aggregates data from different storages into a single sample. Thanks to this feature, multi-messenger methods used for more sophisticated data exploration can be applied. Users can use both Web-interface and Application Programming Interface (API) for accessing the storage. In this paper we describe the architecture of a distributed data storage for astroparticle physics and discuss the current status of our work."
      },
      {
        "node_idx": 61238,
        "score_0_10": 8,
        "title": "an overview of the transmission capacity of wireless networks",
        "abstract": "This paper surveys and unifies a number of recent contributions that have collectively developed a metric for decentralized wireless network analysis known as transmission capacity. Although it is notoriously difficult to derive general end-to-end capacity results for multi-terminal or adhoc networks, the transmission capacity (TC) framework allows for quantification of achievable single-hop rates by focusing on a simplified physical/MAC-layer model. By using stochastic geometry to quantify the multi-user interference in the network, the relationship between the optimal spatial density and success probability of transmissions in the network can be determined, and expressed-often fairly simply-in terms of the key network parameters. The basic model and analytical tools are first discussed and applied to a simple network with path loss only and we present tight upper and lower bounds on transmission capacity (via lower and upper bounds on outage probability). We then introduce random channels (fading/shadowing) and give TC and outage approximations for an arbitrary channel distribution, as well as exact results for the special cases of Rayleigh and Nakagami fading. We then apply these results to show how TC can be used to better understand scheduling, power control, and the deployment of multiple antennas in a decentralized network. The paper closes by discussing shortcomings in the model as well as future research directions."
      },
      {
        "node_idx": 71257,
        "score_0_10": 8,
        "title": "the two user gaussian interference channel a deterministic view",
        "abstract": "This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel."
      }
    ]
  },
  "120": {
    "explanation": "efficient neural architecture design and optimization methods",
    "topk": [
      {
        "node_idx": 26874,
        "score_0_10": 10,
        "title": "learning transferable architectures for scalable image recognition",
        "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
      },
      {
        "node_idx": 100147,
        "score_0_10": 10,
        "title": "communication efficient learning of deep networks from decentralized data",
        "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. #R##N#We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 21714,
        "score_0_10": 9,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 155778,
        "score_0_10": 9,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 104043,
        "score_0_10": 9,
        "title": "darts differentiable architecture search",
        "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms."
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 43354,
        "score_0_10": 8,
        "title": "neural turing machines",
        "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."
      }
    ]
  },
  "122": {
    "explanation": "anti-phishing personalized device-based fraud detection",
    "topk": [
      {
        "node_idx": 140101,
        "score_0_10": 10,
        "title": "papg personalized anti phishing guard",
        "abstract": "Security and privacy have been considered a corner stone in all electronic transactions nowadays. People are becoming very cautious when conducting electronic transactions over internet. One of the major issues that frightens them is identity theft. Identity theft might be conducted using phishing techniques that aims to trick the user to provide his credentials in a well-organized tactic. Efforts have been done towards fighting against phishing attacks and hence identify theft. However, most of these efforts are either computationally exhaustive to the electronic device or depend on a third party to perform the task. In this paper, we propose a plugin called Personalized Anti-Phishing Guard - PAPG that is managed personally on the device and is used to guard the user against phishing attacks. The plugin maintains data locally and may not need to synchronize with a third party. Besides, PAPG depends on the user's feedback to build the local knowledge base that is used to support the decision. The user might also store his profile and reuse it with other devices and from different locations without having to configure it again"
      },
      {
        "node_idx": 155778,
        "score_0_10": 9,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 157548,
        "score_0_10": 9,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 106315,
        "score_0_10": 8,
        "title": "soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor",
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
      },
      {
        "node_idx": 493,
        "score_0_10": 8,
        "title": "robust integral action of port hamiltonian systems",
        "abstract": "Interconnection and damping assignment, passivity-based control (IDA-PBC) has proven to be a successful control technique for the stabilisation of many nonlinear systems. In this paper, we propose a method to robustify a system which has been stabilised using IDA-PBC with respect to constant, matched disturbances via the addition of integral action. The proposed controller extends previous work on the topic by being robust against the damping of the system, a quantity which may not be known in many applications."
      },
      {
        "node_idx": 27774,
        "score_0_10": 8,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 35756,
        "score_0_10": 8,
        "title": "building end to end dialogue systems using generative hierarchical neural network models",
        "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings."
      },
      {
        "node_idx": 116521,
        "score_0_10": 8,
        "title": "recurrent neural network regularization",
        "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation."
      },
      {
        "node_idx": 155391,
        "score_0_10": 8,
        "title": "a portuguese native language identification dataset",
        "abstract": "In this paper we present NLI-PT, the first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author's first language based on their second language writing. The dataset includes 1,868 student essays written by learners of European Portuguese, native speakers of the following L1s: Chinese, English, Spanish, German, Russian, French, Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean, Romanian, and Swedish. NLI-PT includes the original student text and four different types of annotation: POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of Second Language Acquisition and educational NLP. We discuss possible applications of this dataset and present the results obtained for the first lexical baseline system for Portuguese NLI."
      },
      {
        "node_idx": 29312,
        "score_0_10": 8,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      }
    ]
  },
  "123": {
    "explanation": "residual connections and network architecture improvements in deep learning",
    "topk": [
      {
        "node_idx": 53950,
        "score_0_10": 10,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 21251,
        "score_0_10": 10,
        "title": "relaxed disk packing",
        "abstract": "Motivated by biological questions, we study configurations of equal-sized disks in the Euclidean plane that neither pack nor cover. Measuring the quality by the probability that a random point lies in exactly one disk, we show that the regular hexagonal grid gives the maximum among lattice configurations."
      },
      {
        "node_idx": 52018,
        "score_0_10": 10,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 28650,
        "score_0_10": 9,
        "title": "sphere packing with limited overlap",
        "abstract": "The classical sphere packing problem asks for the best (infinite) arrangement of non-overlapping unit balls which cover as much space as possible. We define a generalized version of the problem, where we allow each ball a limited amount of overlap with other balls. We study two natural choices of overlap measures and obtain the optimal lattice packings in a parameterized family of lattices which contains the FCC, BCC, and integer lattice."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 55272,
        "score_0_10": 9,
        "title": "image super resolution using deep convolutional networks",
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."
      },
      {
        "node_idx": 121343,
        "score_0_10": 8,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 111390,
        "score_0_10": 8,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 35496,
        "score_0_10": 8,
        "title": "aggregated residual transformations for deep neural networks",
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
      }
    ]
  },
  "125": {
    "explanation": "6D object pose and multi-person pose estimation techniques",
    "topk": [
      {
        "node_idx": 540,
        "score_0_10": 10,
        "title": "deepim deep iterative matching for 6d pose estimation",
        "abstract": "Estimating 6D poses of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the input image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using a disentangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects."
      },
      {
        "node_idx": 5611,
        "score_0_10": 10,
        "title": "robust physical world attacks on deep learning models",
        "abstract": "Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8%of the captured video frames obtained on a moving vehicle(field test) for the target classifier."
      },
      {
        "node_idx": 71243,
        "score_0_10": 10,
        "title": "deepcut joint subset partition and labeling for multi person pose estimation",
        "abstract": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation. Models and code available at this http URL."
      },
      {
        "node_idx": 167758,
        "score_0_10": 10,
        "title": "associative embedding end to end learning for joint detection and grouping",
        "abstract": "We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to both multi-person pose estimation and instance segmentation and report state-of-the-art performance for multi-person pose on the MPII and MS-COCO datasets."
      },
      {
        "node_idx": 53105,
        "score_0_10": 10,
        "title": "a survey of simultaneous localization and mapping with an envision in 6g wireless networks",
        "abstract": "Simultaneous Localization and Mapping (SLAM) achieves the purpose of simultaneous positioning and map construction based on self-perception. The paper makes an overview in SLAM including Lidar SLAM, visual SLAM, and their fusion. For Lidar or visual SLAM, the survey illustrates the basic type and product of sensors, open source system in sort and history, deep learning embedded, the challenge and future. Additionally, visual inertial odometry is supplemented. For Lidar and visual fused SLAM, the paper highlights the multi-sensors calibration, the fusion in hardware, data, task layer. The open question and forward thinking with an envision in 6G wireless networks end the paper. The contributions of this paper can be summarized as follows: the paper provides a high quality and full-scale overview in SLAM. It's very friendly for new researchers to hold the development of SLAM and learn it very obviously. Also, the paper can be considered as a dictionary for experienced researchers to search and find new interesting orientation."
      },
      {
        "node_idx": 157880,
        "score_0_10": 10,
        "title": "feature squeezing detecting adversarial examples in deep neural networks",
        "abstract": "Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks."
      },
      {
        "node_idx": 123412,
        "score_0_10": 10,
        "title": "posecnn a convolutional neural network for 6d object pose estimation in cluttered scenes",
        "abstract": "Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at this https URL."
      },
      {
        "node_idx": 133339,
        "score_0_10": 9,
        "title": "implicit 3d orientation learning for 6d object detection from rgb images",
        "abstract": "We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results."
      },
      {
        "node_idx": 161063,
        "score_0_10": 9,
        "title": "multi view 3d object detection network for autonomous driving",
        "abstract": "This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods."
      },
      {
        "node_idx": 146012,
        "score_0_10": 9,
        "title": "multi context attention for human pose estimation",
        "abstract": "In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on the detailed description for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic-consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive fields, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts."
      }
    ]
  },
  "127": {
    "explanation": "immune-inspired recommendation and social media misinformation detection",
    "topk": [
      {
        "node_idx": 11587,
        "score_0_10": 10,
        "title": "on the effects of idiotypic interactions for recommendation communities in artificial immune systems",
        "abstract": "It has previously been shown that a recommender based on immune system idiotypic principles can out perform one based on correlation alone. This paper reports the results of work in progress, where we undertake some investigations into the nature of this beneficial effect. The initial findings are that the immune system recommender tends to produce different neighbourhoods, and that the superior performance of this recommender is due partly to the different neighbourhoods, and partly to the way that the idiotypic effect is used to weight each neighbours recommendations."
      },
      {
        "node_idx": 113298,
        "score_0_10": 10,
        "title": "fake news detection on social media a data mining perspective",
        "abstract": "Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of \"fake news\", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ineffective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media."
      },
      {
        "node_idx": 140101,
        "score_0_10": 10,
        "title": "papg personalized anti phishing guard",
        "abstract": "Security and privacy have been considered a corner stone in all electronic transactions nowadays. People are becoming very cautious when conducting electronic transactions over internet. One of the major issues that frightens them is identity theft. Identity theft might be conducted using phishing techniques that aims to trick the user to provide his credentials in a well-organized tactic. Efforts have been done towards fighting against phishing attacks and hence identify theft. However, most of these efforts are either computationally exhaustive to the electronic device or depend on a third party to perform the task. In this paper, we propose a plugin called Personalized Anti-Phishing Guard - PAPG that is managed personally on the device and is used to guard the user against phishing attacks. The plugin maintains data locally and may not need to synchronize with a third party. Besides, PAPG depends on the user's feedback to build the local knowledge base that is used to support the decision. The user might also store his profile and reuse it with other devices and from different locations without having to configure it again"
      },
      {
        "node_idx": 109276,
        "score_0_10": 9,
        "title": "the author topic model for authors and documents",
        "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 92275,
        "score_0_10": 9,
        "title": "predicting the popularity of online content",
        "abstract": "We present a method for accurately predicting the long time popularity of online content from early measurements of user access. Using two content sharing portals, Youtube and Digg, we show that by modeling the accrual of views and votes on content offered by these services we can predict the long-term dynamics of individual submissions from initial data. In the case of Digg, measuring access to given stories during the first two hours allows us to forecast their popularity 30 days ahead with remarkable accuracy, while downloads of Youtube videos need to be followed for 10 days to attain the same performance. The differing time scales of the predictions are shown to be due to differences in how content is consumed on the two portals: Digg stories quickly become outdated, while Youtube videos are still found long after they are initially submitted to the portal. We show that predictions are more accurate for submissions for which attention decays quickly, whereas predictions for evergreen content will be prone to larger errors."
      },
      {
        "node_idx": 104629,
        "score_0_10": 9,
        "title": "movie recommendation systems using an artificial immune system",
        "abstract": "We apply the Artificial Immune System (AIS) technology to the Collaborative Filtering (CF) technology when we build the movie recommendation system. Two different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa, are used to calculate the correlation coefficients for this movie recommendation system. From the testing we think that Weighted Kappa is more suitable than Kendall tau for movie problems."
      },
      {
        "node_idx": 49212,
        "score_0_10": 9,
        "title": "on affinity measures for artificial immune system movie recommenders",
        "abstract": "We combine Artificial Immune Systems 'AIS', technology with Collaborative Filtering 'CF' and use it to build a movie recommendation system. We already know that Artificial Immune Systems work well as movie recommenders from previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate the effect of different affinity measure algorithms for the AIS. Two different affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the correlation coefficients for the movie recommender. We compare the results with those published previously and show that Weighted Kappa is more suitable than others for movie problems. We also show that AIS are generally robust movie recommenders and that, as long as a suitable affinity measure is chosen, results are good."
      },
      {
        "node_idx": 57972,
        "score_0_10": 9,
        "title": "an experimental comparison of naive bayesian and keyword based anti spam filtering with personal e mail messages",
        "abstract": "The growing problem of unsolicited bulk e-mail, also known as \"spam\", has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in \"encrypted\" form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader."
      },
      {
        "node_idx": 167109,
        "score_0_10": 9,
        "title": "online human bot interactions detection estimation and characterization",
        "abstract": "Increasing evidence suggests that a growing amount of social media content is generated by autonomous entities known as social bots. In this work we present a framework to detect such entities on Twitter. We leverage more than a thousand features extracted from public data and meta-data about users: friends, tweet content and sentiment, network patterns, and activity time series. We benchmark the classification framework by using a publicly available dataset of Twitter bots. This training data is enriched by a manually annotated collection of active Twitter users that include both humans and bots of varying sophistication. Our models yield high accuracy and agreement with each other and can detect bots of different nature. Our estimates suggest that between 9% and 15% of active Twitter accounts are bots. Characterizing ties among accounts, we observe that simple bots tend to interact with bots that exhibit more human-like behaviors. Analysis of content flows reveals retweet and mention strategies adopted by bots to interact with different target groups. Using clustering analysis, we characterize several subclasses of accounts, including spammers, self promoters, and accounts that post content from connected applications."
      }
    ]
  },
  "128": {
    "explanation": "decentralized multi-agent system abstractions and control",
    "topk": [
      {
        "node_idx": 43544,
        "score_0_10": 10,
        "title": "decentralized abstractions for feedback interconnected multi agent systems",
        "abstract": "The purpose of this report is to define abstractions for multi-agent systems under coupled constraints. In the proposed decentralized framework, we specify a finite or countable transition system for each agent which only takes into account the discrete positions of its neighbors. The dynamics of the considered systems consist of two components. An appropriate feedback law which guarantees that certain performance requirements (eg. connectivity) are preserved and induces the coupled constraints and additional free inputs which we exploit in order to accomplish high level tasks. In this work we provide sufficient conditions on the space and time discretization of the system which ensure that we can extract a well posed and hence meaningful finite transition system."
      },
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 13090,
        "score_0_10": 10,
        "title": "deludedly agreeing to agree",
        "abstract": "We study conditions relating to the impossibility of agreeing to disagree in models of interactive KD45 belief (in contrast to models of S5 knowledge, which are used in nearly all the agreements literature). We show that even when the truth axiom is not assumed it turns out that players will find it impossible to agree to disagree under fairly broad conditions."
      },
      {
        "node_idx": 152267,
        "score_0_10": 9,
        "title": "automated reasoning for robot ethics",
        "abstract": "Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are considered for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be used to model ethical codes for multi-agent systems. Furthermore we show how Hyper, a high performance theorem prover, can be used to prove properties of these ethical codes."
      },
      {
        "node_idx": 61221,
        "score_0_10": 9,
        "title": "point set registration coherent point drift",
        "abstract": "Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods."
      },
      {
        "node_idx": 162434,
        "score_0_10": 9,
        "title": "robust decentralized abstractions for multiple mobile manipulators",
        "abstract": "This paper addresses the problem of decentralized abstractions for multiple mobile manipulators with 2nd order dynamics. In particular, we propose decentralized controllers for the navigation of each agent among predefined regions of interest in the workspace, while guaranteeing at the same time inter-agent collision avoidance and connectivity maintenance for a subset of initially connected agents. In that way, the motion of the coupled multi-agent system is abstracted into multiple finite transition systems for each agent, which are then suitable for the application of temporal logic-based high level plans. The proposed methodology is decentralized, since each agent uses local information based on limited sensing capabilities. Finally, simulation studies verify the validity of the approach."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 25224,
        "score_0_10": 9,
        "title": "control contraction metrics convex and intrinsic criteria for nonlinear feedback design",
        "abstract": "We introduce the concept of a control contraction metric, extending contraction analysis to constructive nonlinear control design. We derive sufficient conditions for exponential stabilizability of all trajectories of a nonlinear control system. The conditions have a simple geometrical interpretation, can be written as a convex feasibility problem, and are invariant under coordinate changes. We show that these conditions are necessary and sufficient for feedback linearizable systems, and also derive novel convex criteria for exponential stabilization of a nonlinear submanifold of state space. We illustrate the benefits of convexity by constructing a controller for an unstable polynomial system that combines local optimality and global stability, using a metric found via sum-of-squares programming."
      },
      {
        "node_idx": 126953,
        "score_0_10": 9,
        "title": "automated reasoning in deontic logic",
        "abstract": "Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are discussed for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be stepwise transformed into description logic and DL-clauses, such that it can be processed by Hyper, a high performance theorem prover which uses a hypertableau calculus. Two use cases, one from multi-agent research and one from the development of normative system are investigated."
      },
      {
        "node_idx": 62609,
        "score_0_10": 9,
        "title": "autonomous scanning for endomicroscopic mosaicing and 3d fusion",
        "abstract": "Robot-assisted minimally invasive surgery can benefit from the automation of common, repetitive or well-defined but ergonomically difficult tasks. One such task is the scanning of a pick-up endomicroscopy probe over a complex, undulating tissue surface to enhance the effective field-of-view through video mosaicing. In this paper, the da Vinci\u00ae surgical robot, through the dVRK framework, is used for autonomous scanning and 2D mosaicing over a user-defined region of interest. To achieve the level of precision required for high quality mosaic generation, which relies on sufficient overlap between consecutive image frames, visual servoing is performed using a combination of a tracking marker attached to the probe and the endomicroscopy images themselves. The resulting sub-millimetre accuracy of the probe motion allows for the generation of large mosaics with minimal intervention from the surgeon. Images are streamed from the endomicroscope and overlaid live onto the surgeons view, while 2D mosaics are generated in real-time, and fused into a 3D stereo reconstruction of the surgical scene, thus providing intuitive visualisation and fusion of the multi-scale images. The system therefore offers significant potential to enhance surgical procedures, by providing the operator with cellular-scale information over a larger area than could typically be achieved by manual scanning."
      }
    ]
  },
  "129": {
    "explanation": "distributed and coded data repair in storage systems",
    "topk": [
      {
        "node_idx": 27035,
        "score_0_10": 10,
        "title": "conflict free replicated data types crdts",
        "abstract": "A conflict-free replicated data type (CRDT) is an abstract data type, with a well defined interface, designed to be replicated at multiple processes and exhibiting the following properties: (1) any replica can be modified without coordinating with another replicas; (2) when any two replicas have received the same set of updates, they reach the same state, deterministically, by adopting mathematically sound rules to guarantee state convergence."
      },
      {
        "node_idx": 27273,
        "score_0_10": 10,
        "title": "self repairing homomorphic codes for distributed storage systems",
        "abstract": "Erasure codes provide a storage efficient alternative to replication based redundancy in (networked) storage systems. They however entail high communication overhead for maintenance, when some of the encoded fragments are lost and need to be replenished. Such overheads arise from the fundamental need to recreate (or keep separately) first a copy of the whole object before any individual encoded fragment can be generated and replenished. There has been recently intense interest to explore alternatives, most prominent ones being regenerating codes (RGC) and hierarchical codes (HC). We propose as an alternative a new family of codes to improve the maintenance process, which we call self-repairing codes (SRC), with the following salient features: (a) encoded fragments can be repaired directly from other subsets of encoded fragments without having to reconstruct first the original data, ensuring that (b) a fragment is repaired from a fixed number of encoded fragments, the number depending only on how many encoded blocks are missing and independent of which specific blocks are missing. These properties allow for not only low communication overhead to recreate a missing fragment, but also independent reconstruction of different missing fragments in parallel, possibly in different parts of the network. We analyze the static resilience of SRCs with respect to traditional erasure codes, and observe that SRCs incur marginally larger storage overhead in order to achieve the aforementioned properties. The salient SRC properties naturally translate to low communication overheads for reconstruction of lost fragments, and allow reconstruction with lower latency by facilitating repairs in parallel. These desirable properties make self-repairing codes a good and practical candidate for networked distributed storage systems."
      },
      {
        "node_idx": 84319,
        "score_0_10": 10,
        "title": "defeasible logic programming an argumentative approach",
        "abstract": "The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions. #R##N#In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query q will succeed when there is an argument A for q that is warranted, ie, the argument A that supports q is found undefeated by a warrant procedure that implements a dialectical analysis. #R##N#The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents."
      },
      {
        "node_idx": 71276,
        "score_0_10": 9,
        "title": "repairing multiple failures with coordinated and adaptive regenerating codes",
        "abstract": "Erasure correcting codes are widely used to ensure data persistence in distributed storage systems. This paper addresses the simultaneous repair of multiple failures in such codes. We go beyond existing work (i.e., regenerating codes by Dimakis et al.) by describing (i) coordinated regenerating codes (also known as cooperative regenerating codes) which support the simultaneous repair of multiple devices, and (ii) adaptive regenerating codes which allow adapting the parameters at each repair. Similarly to regenerating codes by Dimakis et al., these codes achieve the optimal tradeoff between storage and the repair bandwidth. Based on these extended regenerating codes, we study the impact of lazy repairs applied to regenerating codes and conclude that lazy repairs cannot reduce the costs in term of network bandwidth but allow reducing the disk-related costs (disk bandwidth and disk I/O)."
      },
      {
        "node_idx": 133135,
        "score_0_10": 9,
        "title": "matrix f5 algorithms over finite precision complete discrete valuation fields",
        "abstract": "Let $(f\\_1,\\dots, f\\_s) \\in \\mathbb{Q}\\_p [X\\_1,\\dots, X\\_n]^s$ be a sequence of homogeneous polynomials with $p$-adic coefficients. Such system may happen, for example, in arithmetic geometry. Yet, since $\\mathbb{Q}\\_p$ is not an effective field, classical algorithm does not apply.We provide a definition for an approximate Gr{\\\"o}bner basis with respect to a monomial order $w.$ We design a strategy to compute such a basis, when precision is enough and under the assumption that the input sequence is regular and the ideals $\\langle f\\_1,\\dots,f\\_i \\rangle$ are weakly-$w$-ideals. The conjecture of Moreno-Socias states that for the grevlex ordering, such sequences are generic.Two variants of that strategy are available, depending on whether one lean more on precision or time-complexity. For the analysis of these algorithms, we study the loss of precision of the Gauss row-echelon algorithm, and apply it to an adapted Matrix-F5 algorithm. Numerical examples are provided.Moreover, the fact that under such hypotheses, Gr{\\\"o}bner bases can be computed stably has many applications. Firstly, the mapping sending $(f\\_1,\\dots,f\\_s)$ to the reduced Gr{\\\"o}bner basis of the ideal they span is differentiable, and its differential can be given explicitly. Secondly, these hypotheses allows to perform lifting on the Grobner bases, from $\\mathbb{Z}/p^k \\mathbb{Z}$ to $\\mathbb{Z}/p^{k+k'} \\mathbb{Z}$ or $\\mathbb{Z}.$ Finally, asking for the same hypotheses on the highest-degree homogeneous components of the entry polynomials allows to extend our strategy to the affine case."
      },
      {
        "node_idx": 157213,
        "score_0_10": 9,
        "title": "cooperative regenerating codes",
        "abstract": "One of the design objectives in distributed storage system is the minimization of the data traffic during the repair of failed storage nodes. By repairing multiple failures simultaneously and cooperatively, further reduction of repair traffic is made possible. A closed-form expression of the optimal tradeoff between the repair traffic and the amount of storage in each node for cooperative repair is given. We show that the points on the tradeoff curve can be achieved by linear cooperative regenerating codes, with an explicit bound on the required finite field size. The proof relies on a max-flow-min-cut-type theorem for submodular flow from combinatorial optimization. Two families of explicit constructions are given."
      },
      {
        "node_idx": 106294,
        "score_0_10": 9,
        "title": "software distribution transparency and auditability",
        "abstract": "A large user base relies on software updates provided through package managers. This provides a unique lever for improving the security of the software update process. We propose a transparency system for software updates and implement it for a widely deployed Linux package manager, namely APT. Our system is capable of detecting targeted backdoors without producing overhead for maintainers. In addition, in our system, the availability of source code is ensured, the binding between source and binary code is verified using reproducible builds, and the maintainer responsible for distributing a specific package can be identified. We describe a novel \"hidden version\" attack against current software transparency systems and propose as well as integrate a suitable defense. To address equivocation attacks by the transparency log server, we introduce tree root cross logging, where the log's Merkle tree root is submitted into a separately operated log server. This significantly relaxes the inter-operator cooperation requirements compared to other systems. Our implementation is evaluated by replaying over 3000 updates of the Debian operating system over the course of two years, demonstrating its viability and identifying numerous irregularities."
      },
      {
        "node_idx": 112312,
        "score_0_10": 9,
        "title": "a survey and critique of multiagent deep reinforcement learning",
        "abstract": "Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community."
      },
      {
        "node_idx": 85020,
        "score_0_10": 9,
        "title": "deep reinforcement learning an overview",
        "abstract": "We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. #R##N#Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update."
      },
      {
        "node_idx": 28029,
        "score_0_10": 9,
        "title": "speeding up distributed machine learning using codes",
        "abstract": "Codes are widely used in many engineering applications to offer  robustness  against  noise . In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms\u2014straggler nodes, system failures, or communication bottlenecks\u2014but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how  coded  solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms:  matrix multiplication  and  data shuffling . For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is    $n$   , and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of    $\\log n$   . For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction    $\\alpha $    of the data matrix can be cached at each worker, and    $n$    is the number of workers,  coded shuffling  reduces the communication cost by a factor of    $\\left({\\alpha + \\frac {1}{n}}\\right)\\gamma (n)$    compared with uncoded shuffling, where    $\\gamma (n)$    is the ratio of the cost of unicasting    $n$    messages to    $n$    users to multicasting a common message (of the same size) to    $n$    users. For instance,    $\\gamma (n) \\simeq n$    if multicasting a message to    $n$    users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms."
      }
    ]
  },
  "130": {
    "explanation": "private information retrieval capacity and database coding tradeoffs",
    "topk": [
      {
        "node_idx": 140495,
        "score_0_10": 10,
        "title": "the capacity of private information retrieval",
        "abstract": "In the private information retrieval (PIR) problem a user wishes to retrieve, as efficiently as possible, one out of $K$ messages from $N$ non-communicating databases (each holds all $K$ messages) while revealing nothing about the identity of the desired message index to any individual database. The information theoretic capacity of PIR is the maximum number of bits of desired information that can be privately retrieved per bit of downloaded information. For $K$ messages and $N$ databases, we show that the PIR capacity is $(1+1/N+1/N^2+\\cdots+1/N^{K-1})^{-1}$. A remarkable feature of the capacity achieving scheme is that if we eliminate any subset of messages (by setting the message symbols to zero), the resulting scheme also achieves the PIR capacity for the remaining subset of messages."
      },
      {
        "node_idx": 145973,
        "score_0_10": 9,
        "title": "the capacity of private information retrieval from coded databases",
        "abstract": "We consider the problem of private information retrieval (PIR) over a distributed storage system. The storage system consists of $N$ non-colluding databases, each storing a coded version of $M$ messages. In the PIR problem, the user wishes to retrieve one of the available messages without revealing the message identity to any individual database. We derive the information-theoretic capacity of this problem, which is defined as the maximum number of bits of the desired message that can be privately retrieved per one bit of downloaded information. We show that the PIR capacity in this case is $C=\\left(1+\\frac{K}{N}+\\frac{K^2}{N^2}+\\cdots+\\frac{K^{M-1}}{N^{M-1}}\\right)^{-1}=(1+R_c+R_c^2+\\cdots+R_c^{M-1})^{-1}=\\frac{1-R_c}{1-R_c^M}$, where $R_c$ is the rate of the $(N,K)$ code used. The capacity is a function of the code rate and the number of messages only regardless of the explicit structure of the storage code. The result implies a fundamental tradeoff between the optimal retrieval cost and the storage cost. The result generalizes the achievability and converse results for the classical PIR with replicating databases to the case of coded databases."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 21714,
        "score_0_10": 9,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 62619,
        "score_0_10": 8,
        "title": "the capacity of robust private information retrieval with colluding databases",
        "abstract": "Private information retrieval (PIR) is the problem of retrieving as efficiently as possible, one out of $K$ messages from $N$ non-communicating replicated databases (each holds all $K$ messages) while keeping the identity of the desired message index a secret from each individual database. The information theoretic capacity of PIR (equivalently, the reciprocal of minimum download cost) is the maximum number of bits of desired information that can be privately retrieved per bit of downloaded information. $T$-private PIR is a generalization of PIR to include the requirement that even if any $T$ of the $N$ databases collude, the identity of the retrieved message remains completely unknown to them. Robust PIR is another generalization that refers to the scenario where we have $M \\geq N$ databases, out of which any $M - N$ may fail to respond. For $K$ messages and $M\\geq N$ databases out of which at least some $N$ must respond, we show that the capacity of $T$-private and Robust PIR is $\\left(1+T/N+T^2/N^2+\\cdots+T^{K-1}/N^{K-1}\\right)^{-1}$. The result includes as special cases the capacity of PIR without robustness ($M=N$) or $T$-privacy constraints ($T=1$)."
      },
      {
        "node_idx": 2954,
        "score_0_10": 8,
        "title": "the exact rate memory tradeoff for caching with uncoded prefetching",
        "abstract": "We consider a basic cache network, in which a single server is connected to multiple users via a shared bottleneck link. The server has a database of files (content). Each user has an isolated memory that can be used to cache content in a prefetching phase. In a following delivery phase, each user requests a file from the database, and the server needs to deliver users\u2019 demands as efficiently as possible by taking into account their cache contents. We focus on an important and commonly used class of prefetching schemes, where the caches are filled with uncoded data. We provide the exact characterization of the rate-memory tradeoff for this problem, by deriving both the  minimum average rate  (for a uniform file popularity) and the  minimum peak rate  required on the bottleneck link for a given cache size available at each user. In particular, we propose a novel caching scheme, which strictly improves the state of the art by exploiting commonality among user demands. We then demonstrate the exact optimality of our proposed scheme through a matching converse, by dividing the set of all demands into types, and showing that the placement phase in the proposed caching scheme is universally optimal for all types. Using these techniques, we also fully characterize the rate-memory tradeoff for a decentralized setting, in which users fill out their cache content without any coordination."
      },
      {
        "node_idx": 33272,
        "score_0_10": 8,
        "title": "the approximate capacity of the many to one and one to many gaussian interference channels",
        "abstract": "Recently, Etkin, Tse, and Wang found the capacity region of the two-user Gaussian interference channel to within 1 bit/s/Hz. A natural goal is to apply this approach to the Gaussian interference channel with an arbitrary number of users. We make progress towards this goal by finding the capacity region of the many-to-one and one-to-many Gaussian interference channels to within a constant number of bits. The result makes use of a deterministic model to provide insight into the Gaussian channel. The deterministic model makes explicit the dimension of signal level. A central theme emerges: the use of lattice codes for alignment of interfering signals on the signal level."
      },
      {
        "node_idx": 35496,
        "score_0_10": 8,
        "title": "aggregated residual transformations for deep neural networks",
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
      },
      {
        "node_idx": 119987,
        "score_0_10": 8,
        "title": "xception deep learning with depthwise separable convolutions",
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
      }
    ]
  },
  "131": {
    "explanation": "multilingual word translation and embedding alignment",
    "topk": [
      {
        "node_idx": 140427,
        "score_0_10": 10,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 112674,
        "score_0_10": 9,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 63929,
        "score_0_10": 9,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 26180,
        "score_0_10": 9,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 78341,
        "score_0_10": 9,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 11752,
        "score_0_10": 9,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 30031,
        "score_0_10": 8,
        "title": "simlex 999 evaluating semantic models with genuine similarity estimation",
        "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness, so that pairs of entities that are associated but not actually similar [Freud, psychology] have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures."
      },
      {
        "node_idx": 153102,
        "score_0_10": 8,
        "title": "embedding entities and relations for learning and inference in knowledge bases",
        "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning."
      },
      {
        "node_idx": 53478,
        "score_0_10": 8,
        "title": "opennmt open source toolkit for neural machine translation",
        "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques."
      }
    ]
  },
  "132": {
    "explanation": "distributed rigid formation shape and orientation control",
    "topk": [
      {
        "node_idx": 31383,
        "score_0_10": 10,
        "title": "distributed rotational and translational maneuvering of rigid formations and their applications",
        "abstract": "Recently it has been reported that range-measurement inconsistency, or equivalently mismatches in prescribed inter-agent distances, may prevent the popular gradient controllers from guiding rigid formations of mobile agents to converge to their desired shape, and even worse from standing still at any location. In this paper, instead of treating mismatches as the source of ill performance, we take them as design parameters and show that by introducing such a pair of parameters per distance constraint, distributed controller achieving simultaneously both formation and motion control can be designed that not only encompasses the popular gradient control, but more importantly allows us to achieve constant collective translation, rotation or their combination while guaranteeing asymptotically no distortion in the formation shape occurs. Such motion control results are then applied to (a) the alignment of formations orientations and (b) enclosing and tracking a moving target. Besides rigorous mathematical proof, experiments using mobile robots are demonstrated to show the satisfying performances of the proposed formation-motion distributed controller."
      },
      {
        "node_idx": 78493,
        "score_0_10": 10,
        "title": "analysis and control of aircraft longitudinal dynamics subject to steady aerodynamic forces",
        "abstract": "The paper contributes towards the development of a unified control approach for longitudinal aircraft dynamics. Prior to the control design, we analyze the existence and the uniqueness of the equilibrium orientation along a desired reference velocity. We show that shape symmetries and aerodynamic stall phenomena imply the existence of the equilibrium orientation irrespective of the reference velocity. The equilibrium orientation, however, is not in general unique, and this may trigger an aircraft loss-of-control for specific reference velocities. Conditions that ensure the local and the global uniqueness of the equilibrium orientation are stated. We show that the uniqueness of the equilibrium orientation is intimately related to the possibility of applying the spherical equivalency, i.e. a thrust change of variable rendering the direction of the transformed external force independent of the vehicle's orientation, as in the case of spherical shapes. Once this transformation is applied, control laws for reference velocities can be designed. We show that these laws extend the thrust direction control paradigm developed for systems with orientation-independent external forces, e.g. spherical shapes, to orientation-dependent external forces, e.g. generic shapes."
      },
      {
        "node_idx": 133900,
        "score_0_10": 10,
        "title": "actin automata with memory",
        "abstract": "Actin is a globular protein which forms long polar filaments in eukaryotic. The actin filaments play the roles of cytoskeleton, motility units, information processing and learning. We model actin filament as a double chain of finite state machines, nodes, which take states \u201c0\u201d and \u201c1\u201d. The states are abstractions of absence and presence of a subthreshold charge on actin units corresponding to the nodes. All nodes update their state in parallel to discrete time. A node updates its current state depending on states of two closest neighbors in the node chain and two closest neighbors in the complementary chain. Previous models of actin automata consider momentary state transitions of nodes. We enrich the actin automata model by assuming that states of nodes depend not only on the current states of neighboring node but also on their past states. Thus, we assess the effect of memory of past states on the dynamics of acting automata. We demonstrate in computational experiments that memory slows down propagation..."
      },
      {
        "node_idx": 65022,
        "score_0_10": 10,
        "title": "modeling for control of symmetric aerial vehicles subjected to aerodynamic forces",
        "abstract": "This paper participates in the development of a unified approach to the control of aerial vehicles with extended flight envelopes. More precisely, modeling for control purposes of a class of thrust-propelled aerial vehicles subjected to lift and drag aerodynamic forces is addressed assuming a rotational symmetry of the vehicle's shape about the thrust force axis. A condition upon aerodynamic characteristics that allows one to recast the control problem into the simpler case of a spherical vehicle is pointed out. Beside showing how to adapt nonlinear controllers developed for this latter case, the paper extends a previous work by the authors in two directions. First, the 3D case is addressed whereas only motions in a single vertical plane was considered. Secondly, the family of models of aerodynamic forces for which the aforementioned transformation holds is enlarged."
      },
      {
        "node_idx": 28262,
        "score_0_10": 9,
        "title": "on the properties of the compound nodal admittance matrix of polyphase power systems",
        "abstract": "Most techniques for power system analysis model the grid by exact electrical circuits. For instance, in power flow study, state estimation, and voltage stability assessment, the use of admittance parameters (i.e., the nodal admittance matrix) and hybrid parameters is common. Moreover, network reduction techniques (e.g., Kron reduction) are often applied to decrease the size of large grid models (i.e., with hundreds or thousands of state variables), thereby alleviating the computational burden. However, researchers normally disregard the fact that the applicability of these methods is not generally guaranteed. In reality, the nodal admittance must satisfy certain properties in order for hybrid parameters to exist and Kron reduction to be feasible. Recently, this problem was solved for the particular cases of monophase and balanced triphase grids. This paper investigates the general case of unbalanced polyphase grids. Firstly, conditions determining the rank of the so-called compound nodal admittance matrix and its diagonal subblocks are deduced from the characteristics of the electrical components and the network graph. Secondly, the implications of these findings concerning the feasibility of Kron reduction and the existence of hybrid parameters are discussed. In this regard, this paper provides a rigorous theoretical foundation for various applications in power system analysis."
      },
      {
        "node_idx": 27958,
        "score_0_10": 9,
        "title": "decentralized event triggering for control of nonlinear systems",
        "abstract": "This paper considers nonlinear systems with full state feedback, a central controller and distributed sensors not co-located with the central controller. We present a methodology for designing decentralized asynchronous event-triggers, which utilize only locally available information, for determining the time instants of transmission from the sensors to the central controller. The proposed design guarantees a positive lower bound for the inter-transmission times of each sensor, while ensuring asymptotic stability of the origin of the system with an arbitrary, but priorly fixed, compact region of attraction. In the special case of Linear Time Invariant (LTI) systems, global asymptotic stability is guaranteed and scale invariance of inter-transmission times is preserved. A modified design method is also proposed for nonlinear systems, with the addition of event-triggered communication from the controller to the sensors, that promises to significantly increase the average sensor inter-transmission times compared to the case where the controller does not transmit data to the sensors. The proposed designs are illustrated through simulations of a linear and a nonlinear example."
      },
      {
        "node_idx": 147294,
        "score_0_10": 9,
        "title": "discovering boolean gates in slime mould",
        "abstract": "Slime mould of Physarum polycephalum is a large cell exhibiting rich spatial non-linear electrical characteristics. We exploit the electrical properties of the slime mould to implement logic gates using a flexible hardware platform designed for investigating the electrical properties of a substrate (Mecobo). We apply arbitrary electrical signals to \u2018configure\u2019 the slime mould, i.e. change shape of its body and, measure the slime mould\u2019s electrical response. We show that it is possible to find configurations that allow the Physarum to act as any 2-input Boolean gate. The occurrence frequency of the gates discovered in the slime was analysed and compared to complexity hierarchies of logical gates obtained in other unconventional materials. The search for gates was performed by both sweeping across configurations in the real material as well as training a neural network-based model and searching the gates therein using gradient descent."
      },
      {
        "node_idx": 54087,
        "score_0_10": 9,
        "title": "optimal steering of a linear stochastic system to a final probability distribution part iii",
        "abstract": "We consider the problem to steer a linear dynamical system with full state observation from an initial gaussian distribution in state-space to a final one with minimum energy control. The system is stochastically driven through the control channels; an example for such a system is that of an inertial particle experiencing random \"white noise\" forcing. We show that a target probability distribution can always be achieved in finite time. The optimal control is given in state-feedback form and is computed explicitely by solving a pair of differential Lyapunov equations that are coupled through their boundary values. This result, given its attractive algorithmic nature, appears to have several potential applications such as to active control of nanomechanical systems and molecular cooling. The problem to steer a diffusion process between end-point marginals has a long history (Schr\\\"odinger bridges) and therefore, the present case of steering a linear stochastic system constitutes a Schr\\\"odinger bridge for possibly degenerate diffusions. Our results, however, provide the first implementable form of the optimal control for a general Gauss-Markov process. Illustrative examples of the optimal evolution and control for inertial particles and a stochastic oscillator are provided. A final result establishes directly the property of Schr\\\"{o}dinger bridges as the most likely random evolution between given marginals to the present context of linear stochastic systems."
      },
      {
        "node_idx": 31361,
        "score_0_10": 9,
        "title": "event triggered control of nonlinear singularly perturbed systems based only on the slow dynamics",
        "abstract": "Controllers are often designed based on a reduced or simplified model of the plant dynamics. In this context, we investigate whether it is possible to synthesize a stabilizing event-triggered feedback law for networked control systems (NCS) which have two time-scales, based only on an approximate model of the slow dynamics. We follow an emulation-like approach as we assume that we know how to solve the problem in the absence of sampling and then we study how to design the event-triggering rule under communication constraints. The NCS is modeled as a hybrid singularly perturbed system which exhibits the feature to generate jumps for both the fast variable and the error variable induced by the sampling. The first conclusion is that a triggering law which guarantees the stability and the existence of a uniform minimum amount of time between two transmissions for the slow model may not ensure the existence of such a time for the overall system, which makes the controller not implementable in practice. The objective of this contribution is twofold. We first show that existing event-triggering conditions can be adapted to singularly perturbed systems and semiglobal practical stability can be ensured in this case. Second, we propose another technique that combines event-triggered and time-triggered results in the sense that transmissions are only allowed after a predefined amount of time has elapsed since the last transmission. This technique has the advantage, under an additional assumption, to ensure a global asymptotic stability property and to allow the user to directly tune the minimum inter-transmission interval. We believe that this technique is of its own interest independently of the two-time scale nature of the addressed problem. The results are shown to be applicable to a class of globally Lipschitz systems."
      },
      {
        "node_idx": 144539,
        "score_0_10": 9,
        "title": "distributed stabilization control of rigid formations with prescribed orientation",
        "abstract": "Most rigid formation controllers reported in the literature aim to only stabilize a rigid formation shape, while the formation orientation is not controlled. This paper studies the problem of controlling rigid formations with prescribed orientations in both 2-D and 3-D spaces. The proposed controllers involve the commonly-used gradient descent control for shape stabilization, and an additional term to control the directions of certain relative position vectors associated with certain chosen agents. In this control framework, we show the minimal number of agents which should have knowledge of a global coordinate system (2 agents for a 2-D rigid formation and 3 agents for a 3-D rigid formation), while all other agents do not require any global coordinate knowledge or any coordinate frame alignment to implement the proposed control. The exponential convergence to the desired rigid shape and formation orientation is also proved. Typical simulation examples are shown to support the analysis and performance of the proposed formation controllers."
      }
    ]
  },
  "136": {
    "explanation": "visual-semantic alignment for image captioning and recognition",
    "topk": [
      {
        "node_idx": 130623,
        "score_0_10": 10,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 29312,
        "score_0_10": 9,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 57930,
        "score_0_10": 9,
        "title": "show attend and tell neural image caption generation with visual attention",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 14201,
        "score_0_10": 8,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      },
      {
        "node_idx": 139975,
        "score_0_10": 8,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 135057,
        "score_0_10": 8,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      }
    ]
  },
  "137": {
    "explanation": "formal methods for program verification and ontology reasoning",
    "topk": [
      {
        "node_idx": 104591,
        "score_0_10": 10,
        "title": "evaluating the competency of a first order ontology",
        "abstract": "We report on the results of evaluating the competency of a first-order ontology for its use with automated theorem provers (ATPs). The evaluation follows the adaptation of the methodology based on competency questions (CQs) [4] to the framework of first-order logic, which is presented in [2], and is applied to Adimen-SUMO [1]. The set of CQs used for this evaluation has been automatically generated from a small set of semantic patterns and the mapping of WordNet to SUMO. Analysing the results, we can conclude that it is feasible to use ATPs for working with Adimen-SUMO v2.4, enabling the resolution of goals by means of performing non-trivial inferences."
      },
      {
        "node_idx": 58375,
        "score_0_10": 10,
        "title": "concurrent kleene algebra with observations from hypotheses to completeness",
        "abstract": "Concurrent Kleene Algebra (CKA) extends basic Kleene algebra with a parallel composition operator, which enables reasoning about concurrent programs. However, CKA fundamentally misses tests, which are needed to model standard programming constructs such as conditionals and $\\mathsf{while}$-loops. It turns out that integrating tests in CKA is subtle, due to their interaction with parallelism. In this paper we provide a solution in the form of Concurrent Kleene Algebra with Observations (CKAO). Our main contribution is a completeness theorem for CKAO. Our result resorts on a more general study of CKA \"with hypotheses\", of which CKAO turns out to be an instance: this analysis is of independent interest, as it can be applied to extensions of CKA other than CKAO."
      },
      {
        "node_idx": 127768,
        "score_0_10": 10,
        "title": "on partial order semantics for sat smt based symbolic encodings of weak memory concurrency",
        "abstract": "Concurrent systems are notoriously difficult to analyze, and technological advances such as weak memory architectures greatly compound this problem. This has renewed interest in partial order semantics as a theoretical foundation for formal verification techniques. Among these, symbolic techniques have been shown to be particularly effective at finding concurrency-related bugs because they can leverage highly optimized decision procedures such as SAT/SMT solvers. This paper gives new fundamental results on partial order semantics for SAT/SMT-based symbolic encodings of weak memory concurrency. In particular, we give the theoretical basis for a decision procedure that can handle a fragment of concurrent programs endowed with least fixed point operators. In addition, we show that a certain partial order semantics of relaxed sequential consistency is equivalent to the conjunction of three extensively studied weak memory axioms by Alglave et al. An important consequence of this equivalence is an asymptotically smaller symbolic encoding for bounded model checking which has only a quadratic number of partial order constraints compared to the state-of-the-art cubic-size encoding."
      },
      {
        "node_idx": 167796,
        "score_0_10": 10,
        "title": "kleene algebra with domain",
        "abstract": "We propose Kleene algebra with domain (KAD), an extension of Kleene algebra with two equational axioms for a domain and a codomain operation, respectively. KAD considerably augments the expressiveness of Kleene algebra, in particular for the specification and analysis of state transition systems. We develop the basic calculus, discuss some related theories and present the most important models of KAD. We demonstrate applicability by two examples: First, an algebraic reconstruction of Noethericity and well-foundedness; second, an algebraic reconstruction of propositional Hoare logic."
      },
      {
        "node_idx": 55946,
        "score_0_10": 10,
        "title": "improving the competency of first order ontologies",
        "abstract": "We introduce a new framework to evaluate and improve first-order (FO) ontologies using automated theorem provers (ATPs) on the basis of competency questions (CQs). Our framework includes both the adaptation of a methodology for evaluating ontologies to the framework of first-order logic and a new set of non-trivial CQs designed to evaluate FO versions of SUMO, which significantly extends the very small set of CQs proposed in the literature. Most of these new CQs have been automatically generated from a small set of patterns and the mapping of WordNet to SUMO. Applying our framework, we demonstrate that Adimen-SUMO v2.2 outperforms TPTP-SUMO. In addition, using the feedback provided by ATPs we have set an improved version of Adimen-SUMO (v2.4). This new version outperforms the previous ones in terms of competency. For instance, \"Humans can reason\" is automatically inferred from Adimen-SUMO v2.4, while it is neither deducible from TPTP-SUMO nor Adimen-SUMO v2.2."
      },
      {
        "node_idx": 91753,
        "score_0_10": 10,
        "title": "verification of programs via intermediate interpretation",
        "abstract": "We explore an approach to verification of programs via program transformation applied to an interpreter of a programming language. A specialization technique known as Turchin's supercompilation is used to specialize some interpreters with respect to the program models. We show that several safety properties of functional programs modeling a class of cache coherence protocols can be proved by a supercompiler and compare the results with our earlier work on direct verification via supercompilation not using intermediate interpretation. #R##N#Our approach was in part inspired by an earlier work by E. De Angelis et al. (2014-2015) where verification via program transformation and intermediate interpretation was studied in the context of specialization of constraint logic programs."
      },
      {
        "node_idx": 131248,
        "score_0_10": 10,
        "title": "comparing weakest precondition and weakest liberal precondition",
        "abstract": "In this article we investigate the relationships between the classical notions of weakest precondition and weakest liberal precondition, and provide several results, namely that in general, weakest liberal precondition is neither stronger nor weaker than weakest precondition, however, given a deterministic and terminating sequential while program and a postcondition, they are equivalent. Hence, in such situation, it does not matter which definition is used."
      },
      {
        "node_idx": 100691,
        "score_0_10": 10,
        "title": "decidability and undecidability results for propositional schemata",
        "abstract": "We define a logic of propositional formula schemata adding to the syntax of propositional logic indexed propositions (e.g., pi) and iterated connectives \u2228 or \u2227 ranging over intervals parameterized by arithmetic variables (e.g., \u2227i-1n pi, where n is a parameter). The satisfiability problem is shown to be undecidable for this new logic, but we introduce a very general class of schemata, called bound-linear, for which this problem becomes decidable. This result is obtained by reduction to a particular class of schemata called regular, for which we provide a sound and complete terminating proof procedure. This schemata calculus (called STAB) allows one to capture proof patterns corresponding to a large class of problems specified in propositional logic. We also show that the satisfiability problem becomes again undecidable for slight extensions of this class, thus demonstrating that bound-linear schemata represent a good compromise between expressivity and decidability"
      },
      {
        "node_idx": 113156,
        "score_0_10": 9,
        "title": "proving erasure",
        "abstract": "It seems impossible to certify that a remote hosting service does not leak its users' data --- or does quantum mechanics make it possible? We investigate if a server hosting data can information-theoretically prove its definite deletion using a \"BB84-like\" protocol. To do so, we first rigorously introduce an alternative to privacy by encryption: privacy delegation. We then apply this novel concept to provable deletion and remote data storage. For both tasks, we present a protocol, sketch its partial security, and display its vulnerability to eavesdropping attacks targeting only a few bits."
      },
      {
        "node_idx": 154992,
        "score_0_10": 9,
        "title": "soft concurrent constraint programming",
        "abstract": "Soft constraints extend classical constraints to represent multiple consistency levels, and thus provide a way to express preferences, fuzziness, and uncertainty. While there are many soft constraint solving formalisms, even distributed ones, as yet there seems to be no concurrent programming framework where soft constraints can be handled. In this article we show how the classical concurrent constraint (cc) programming framework can work with soft constraints, and we also propose an extension of cc languages which can use soft constraints to prune and direct the search for a solution. We believe that this new programming paradigm, called soft cc (scc), can be also very useful in many Web-related scenarios. In fact, the language level allows Web agents to express their interaction and negotiation protocols, and also to post their requests in terms of preferences, and the underlying soft constraint solver can find an agreement among the agents even if their requests are incompatible."
      }
    ]
  },
  "138": {
    "explanation": "channel feedback capacity and coding strategies",
    "topk": [
      {
        "node_idx": 55398,
        "score_0_10": 10,
        "title": "the capacity of channels with feedback",
        "abstract": "We introduce a general framework for treating channels with memory and feedback. First, we generalize Massey's concept of directed information and use it to characterize the feedback capacity of general channels. Second, we present coding results for Markov channels. This requires determining appropriate sufficient statistics at the encoder and decoder. Third, a dynamic programming framework for computing the capacity of Markov channels is presented. Fourth, it is shown that the average cost optimality equation (ACOE) can be viewed as an implicit single-letter characterization of the capacity. Fifth, scenarios with simple sufficient statistics are described."
      },
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 157681,
        "score_0_10": 10,
        "title": "degrees of freedom of time correlated miso broadcast channel with delayed csit",
        "abstract": "We consider the time correlated multiple-input single-output (MISO) broadcast channel where the transmitter has imperfect knowledge of the current channel state, in addition to delayed channel state information. By representing the quality of the current channel state information as P-\u03b1 for the signal-to-noise ratio P and some constant \u03b1 \u2265 0, we characterize the optimal degrees of freedom region for this more general two-user MISO broadcast correlated channel. The essential ingredients of the proposed scheme lie in the quantization and multicast of the overheard interferences, while broadcasting new private messages. Our proposed scheme smoothly bridges between the scheme recently proposed by Maddah-Ali and Tse with no current state information and a simple zero-forcing beamforming with perfect current state information."
      },
      {
        "node_idx": 70472,
        "score_0_10": 9,
        "title": "ontology based approach for video transmission over the network",
        "abstract": "With the increase in the bandwidth & the transmission speed over the internet, transmission of multimedia objects like video, audio, images has become an easier work. In this paper we provide an approach that can be useful for transmission of video objects over the internet without much fuzz. The approach provides a ontology based framework that is used to establish an automatic deployment of video transmission system. Further the video is compressed using the structural flow mechanism that uses the wavelet principle for compression of video frames. Finally the video transmission algorithm known as RRDBFSF algorithm is provided that makes use of the concept of restrictive flooding to avoid redundancy thereby increasing the efficiency."
      },
      {
        "node_idx": 44820,
        "score_0_10": 9,
        "title": "an end to end probabilistic network calculus with moment generating functions",
        "abstract": "Network calculus is a min-plus system theory for performance evaluation of queuing networks. Its elegance stems from intuitive convolution formulas for concatenation of deterministic servers. Recent research dispenses with the worst-case assumptions of network calculus to develop a probabilistic equivalent that benefits from statistical multiplexing. Significant achievements have been made, owing for example to the theory of effective bandwidths, however, the outstanding scalability set up by concatenation of deterministic servers has not been shown. #R##N#This paper establishes a concise, probabilistic network calculus with moment generating functions. The presented work features closed-form, end-to-end, probabilistic performance bounds that achieve the objective of scaling linearly in the number of servers in series. The consistent application of moment generating functions put forth in this paper utilizes independence beyond the scope of current statistical multiplexing of flows. A relevant additional gain is demonstrated for tandem servers with independent cross-traffic."
      },
      {
        "node_idx": 31334,
        "score_0_10": 9,
        "title": "feedback capacity of stationary gaussian channels",
        "abstract": "The feedback capacity of additive stationary Gaussian noise channels is characterized as the solution to a variational problem. Toward this end, it is proved that the optimal feedback coding scheme is stationary. When specialized to the first-order autoregressive moving average noise spectrum, this variational characterization yields a closed-form expression for the feedback capacity. In particular, this result shows that the celebrated Schalkwijk-Kailath coding scheme achieves the feedback capacity for the first-order autoregressive moving average Gaussian channel, positively answering a long-standing open problem studied by Butman, Schalkwijk-Tiernan, Wolfowitz, Ozarow, Ordentlich, Yang-Kavcic-Tatikonda, and others. More generally, it is shown that a k-dimensional generalization of the Schalkwijk-Kailath coding scheme achieves the feedback capacity for any autoregressive moving average noise spectrum of order k. Simply put, the optimal transmitter iteratively refines the receiver's knowledge of the intended message."
      },
      {
        "node_idx": 63929,
        "score_0_10": 9,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 19940,
        "score_0_10": 9,
        "title": "output synchronization of nonlinear systems under input disturbances",
        "abstract": "We study synchronization of nonlinear systems that satisfy an incremental passivity property. We consider the case where the control input is subject to a class of disturbances, including constant and sinusoidal disturbances with unknown phases and magnitudes and known frequencies. We design a distributed control law that recovers the synchronization of the nonlinear systems in the presence of the disturbances. Simulation results of Goodwin oscillators illustrate the effectiveness of the control law. Finally, we highlight the connection of the proposed control law to the dynamic average consensus estimator developed in [1]."
      },
      {
        "node_idx": 47313,
        "score_0_10": 9,
        "title": "optimal use of current and outdated channel state information degrees of freedom of the miso bc with mixed csit",
        "abstract": "We consider a multiple-input-single-output (MISO) broadcast channel with mixed channel state information at the transmitter (CSIT) that consists of imperfect current CSIT and perfect outdated CSIT. Recent work by Kobayashi et al. presented a scheme which exploits both imperfect current CSIT and perfect outdated CSIT and achieves higher degrees of freedom (DoF) than possible with only imperfect current CSIT or only outdated CSIT individually. In this work, we further improve the achievable DoF in this setting by incorporating additional private messages, and provide a tight information theoretic DoF outer bound, thereby identifying the DoF optimal use of mixed CSIT. The new result is stronger even in the original setting of only delayed CSIT, because it allows us to remove the restricting assumption of statistically equivalent fading for all users."
      }
    ]
  },
  "143": {
    "explanation": "motion barcode-based epipolar geometry estimation with dynamic silhouettes",
    "topk": [
      {
        "node_idx": 102743,
        "score_0_10": 10,
        "title": "fundamental matrices from moving objects using line motion barcodes",
        "abstract": "Computing the epipolar geometry between cameras with very different viewpoints is often very difficult. The appearance of objects can vary greatly, and it is difficult to find corresponding feature points. Prior methods searched for corresponding epipolar lines using points on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the \"Motion Barcodes\", a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction."
      },
      {
        "node_idx": 145320,
        "score_0_10": 10,
        "title": "touch less interactive augmented reality game on vision based wearable device",
        "abstract": "There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction."
      },
      {
        "node_idx": 42633,
        "score_0_10": 10,
        "title": "singular and plural functions for functional logic programming",
        "abstract": "Functional logic programming (FLP) languages use non-terminating and non-confluent constructor systems (CS's) as programs in order to define non-strict non-determi-nistic functions. Two semantic alternatives have been usually considered for parameter passing with this kind of functions: call-time choice and run-time choice. While the former is the standard choice of modern FLP languages, the latter lacks some properties---mainly compositionality---that have prevented its use in practical FLP systems. Traditionally it has been considered that call-time choice induces a singular denotational semantics, while run-time choice induces a plural semantics. We have discovered that this latter identification is wrong when pattern matching is involved, and thus we propose two novel compositional plural semantics for CS's that are different from run-time choice. #R##N#We study the basic properties of our plural semantics---compositionality, polarity, monotonicity for substitutions, and a restricted form of the bubbling property for constructor systems---and the relation between them and to previous proposals, concluding that these semantics form a hierarchy in the sense of set inclusion of the set of computed values. We have also identified a class of programs characterized by a syntactic criterion for which the proposed plural semantics behave the same, and a program transformation that can be used to simulate one of them by term rewriting. At the practical level, we study how to use the expressive capabilities of these semantics for improving the declarative flavour of programs. We also propose a language which combines call-time choice and our plural semantics, that we have implemented in Maude. The resulting interpreter is employed to test several significant examples showing the capabilities of the combined semantics. #R##N#To appear in Theory and Practice of Logic Programming (TPLP)"
      },
      {
        "node_idx": 117358,
        "score_0_10": 10,
        "title": "making abstract domains condensing",
        "abstract": "In this paper we show that reversible analysis of logic languages by abstract interpretation can be performed without loss of precision by systematically refining abstract domains. The idea is to include semantic structures into abstract domains in such a way that the refined abstract domain becomes rich enough to allow approximate bottom-up and top-down semantics to agree. These domains are known as condensing abstract domains. Substantially, an abstract domain is condensing if goal-driven and goal-independent analyses agree, namely no loss of precision is introduced by approximating queries in a goal-independent analysis. We prove that condensation is an abstract domain property and that the problem of making an abstract domain condensing boils down to the problem of making the domain complete with respect to unification. In a general abstract interpretation setting we show that when concrete domains and operations give rise to quantales, i.e. models of propositional linear logic, objects in a complete refined abstract domain can be explicitly characterized by linear logic-based formulations. This is the case for abstract domains for logic program analysis approximating computed answer substitutions where unification plays the role of multiplicative conjunction in a quantale of idempotent substitutions. Condensing abstract domains can therefore be systematically derived by minimally extending any, generally non-condensing domain, by a simple domain refinement operator."
      },
      {
        "node_idx": 104099,
        "score_0_10": 10,
        "title": "review of the use of electroencephalography as an evaluation method for human computer interaction",
        "abstract": "Evaluating human-computer interaction is essential as a broadening population uses machines, sometimes in sensitive contexts. However, traditional evaluation methods may fail to combine real-time measures, an \"objective\" approach and data contextualization. In this review we look at how adding neuroimaging techniques can respond to such needs. We focus on electroencephalography (EEG), as it could be handled effectively during a dedicated evaluation phase. We identify workload, attention, vigilance, fatigue, error recognition, emotions, engagement, flow and immersion as being recognizable by EEG. We find that workload, attention and emotions assessments would benefit the most from EEG. Moreover, we advocate to study further error recognition through neuroimaging to enhance usability and increase user experience."
      },
      {
        "node_idx": 93694,
        "score_0_10": 10,
        "title": "slime mould electronic oscillators",
        "abstract": "We construct electronic oscillator from acellular slime mould Physarum polycephalum. The slime mould oscillator is made of two electrodes connected by a protoplasmic tube of the living slime mould. A protoplasmic tube has an average resistance of 3~MOhm. The tube's resistance is changing over time due to peristaltic contractile activity of the tube. The resistance of the protoplasmic tube oscillates with average period of 73~sec and average amplitude of 0.6~MOhm. We present experimental laboratory results on dynamics of Physarum oscillator under direct current voltage up to 15~V and speculate that slime mould P. polycephalum can be employed as a living electrical oscillator in biological and hybrid circuits."
      },
      {
        "node_idx": 90193,
        "score_0_10": 10,
        "title": "color texture classification approach based on combination of primitive pattern units and statistical features",
        "abstract": "Texture classification became one of the problems which has been paid much attention on by image processing scientists since late 80s. Consequently, since now many different methods have been proposed to solve this problem. In most of these methods the researchers attempted to describe and discriminate textures based on linear and non-linear patterns. The linear and non-linear patterns on any window are based on formation of Grain Components in a particular order. Grain component is a primitive unit of morphology that most meaningful information often appears in the form of occurrence of that. The approach which is proposed in this paper could analyze the texture based on its grain components and then by making grain components histogram and extracting statistical features from that would classify the textures. Finally, to increase the accuracy of classification, proposed approach is expanded to color images to utilize the ability of approach in analyzing each RGB channels, individually. Although, this approach is a general one and it could be used in different applications, the method has been tested on the stone texture and the results can prove the quality of approach."
      },
      {
        "node_idx": 116677,
        "score_0_10": 10,
        "title": "model checking contractual protocols",
        "abstract": "This paper discusses how model checking, a technique used for the verification of behavioural requirements of dynamic systems, can be usefully deployed for the verification of contracts. A process view of agreements between parties is taken, whereby a contract is modelled as it evolves over time in terms of actions or more generally events that effect changes in its state. Modelling is done with Petri Nets in the spirit of other research work on the representation of trade procedures. The paper illustrates all the phases of the verification technique through an example and argues that the approach is useful particularly in the context of pre-contractual negotiation and contract drafting. The work reported here is part of a broader project on the development of logic-based tools for the analysis and representation of legal contracts."
      },
      {
        "node_idx": 121079,
        "score_0_10": 9,
        "title": "texture image analysis and texture classification methods a review",
        "abstract": "Tactile texture refers to the tangible feel of a surface and visual texture refers to see the shape or contents of the image. In the image processing, the texture can be defined as a function of spatial variation of the brightness intensity of the pixels. Texture is the main term used to define objects or concepts of a given image. Texture analysis plays an important role in computer vision cases such as object recognition, surface defect detection, pattern recognition, medical image analysis, etc. Since now many approaches have been proposed to describe texture images accurately. Texture analysis methods usually are classified into four categories: statistical methods, structural, model-based and transform-based methods. This paper discusses the various methods used for texture or analysis in details. New researches shows the power of combinational methods for texture analysis, which can't be in specific category. This paper provides a review on well known combinational methods in a specific section with details. This paper counts advantages and disadvantages of well-known texture image descriptors in the result part. Main focus in all of the survived methods is on discrimination performance, computational complexity and resistance to challenges such as noise, rotation, etc. A brief review is also made on the common classifiers used for texture image classification. Also, a survey on texture image benchmark datasets is included."
      },
      {
        "node_idx": 49376,
        "score_0_10": 9,
        "title": "camera calibration from dynamic silhouettes using motion barcodes",
        "abstract": "Computing the epipolar geometry between cameras with very different viewpoints is often problematic as matching points are hard to find. In these cases, it has been proposed to use information from dynamic objects in the scene for suggesting point and line correspondences. #R##N#We propose a speed up of about two orders of magnitude, as well as an increase in robustness and accuracy, to methods computing epipolar geometry from dynamic silhouettes. This improvement is based on a new temporal signature: motion barcode for lines. Motion barcode is a binary temporal sequence for lines, indicating for each frame the existence of at least one foreground pixel on that line. The motion barcodes of two corresponding epipolar lines are very similar, so the search for corresponding epipolar lines can be limited only to lines having similar barcodes. The use of motion barcodes leads to increased speed, accuracy, and robustness in computing the epipolar geometry."
      }
    ]
  },
  "144": {
    "explanation": "wireless communication techniques and caching optimization in 5G networks",
    "topk": [
      {
        "node_idx": 74729,
        "score_0_10": 10,
        "title": "a survey on non orthogonal multiple access for 5g networks research challenges and future trends",
        "abstract": "Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed."
      },
      {
        "node_idx": 80893,
        "score_0_10": 9,
        "title": "fairness for non orthogonal multiple access in 5g systems",
        "abstract": "In non-orthogonal multiple access (NOMA) downlink, multiple data flows are superimposed in the power domain and user decoding is based on successive interference cancellation. NOMA\u2019s performance highly depends on the power split among the data flows and the associated power allocation (PA) problem. In this letter, we study NOMA from a fairness standpoint and we investigate PA techniques that ensure fairness for the downlink users under i)\u00a0instantaneous channel state information (CSI) at the transmitter, and ii)\u00a0average CSI. Although the formulated problems are non-convex, we have developed low-complexity polynomial algorithms that yield the optimal solution in both cases considered."
      },
      {
        "node_idx": 143592,
        "score_0_10": 9,
        "title": "molecular computers",
        "abstract": "We propose the chemlambda artificial chemistry, whose behavior strongly suggests that real molecules which embed Interaction Nets patterns and real chemical reactions which resemble Interaction Nets graph rewrites could be a realistic path towards molecular computers, in the sense explained in the article."
      },
      {
        "node_idx": 109425,
        "score_0_10": 9,
        "title": "ergodic interference alignment",
        "abstract": "This paper develops a new communication strategy, ergodic interference alignment, for the K-user interference channel with time-varying fading. At any particular time, each receiver will see a superposition of the transmitted signals plus noise. The standard approach to such a scenario results in each transmitter-receiver pair achieving a rate proportional to 1/K its interference-free ergodic capacity. However, given two well-chosen time indices, the channel coefficients from interfering users can be made to exactly cancel. By adding up these two observations, each receiver can obtain its desired signal without any interference. If the channel gains have independent, uniform phases, this technique allows each user to achieve at least 1/2 its interference-free ergodic capacity at any signal-to-noise ratio. Prior interference alignment techniques were only able to attain this performance as the signal-to-noise ratio tended to infinity. Extensions are given for the case where each receiver wants a message from more than one transmitter as well as the \"X channel\" case (with two receivers) where each transmitter has an independent message for each receiver. Finally, it is shown how to generalize this strategy beyond Gaussian channel models. For a class of finite field interference channels, this approach yields the ergodic capacity region."
      },
      {
        "node_idx": 125134,
        "score_0_10": 9,
        "title": "linear probing and graphs",
        "abstract": "Mallows and Riordan showed in 1968 that labeled trees with a small number of inversions are related to labeled graphs that are connected and sparse. Wright enumerated sparse connected graphs in 1977, and Kreweras related the inversions of trees to the so-called ``parking problem'' in 1980. A~combination of these three results leads to a surprisingly simple analysis of the behavior of hashing by linear probing, including higher moments of the cost of successful search."
      },
      {
        "node_idx": 94987,
        "score_0_10": 9,
        "title": "fundamental limits of caching in wireless d2d networks",
        "abstract": "We consider a wireless Device-to-Device (D2D) network where communication is restricted to be single-hop. Users make arbitrary requests from a finite library of files and have pre-cached information on their devices, subject to a per-node storage capacity constraint. A similar problem has already been considered in an ``infrastructure'' setting, where all users receive a common multicast (coded) message from a single omniscient server (e.g., a base station having all the files in the library) through a shared bottleneck link. In this work, we consider a D2D ``infrastructure-less'' version of the problem. We propose a caching strategy based on deterministic assignment of subpackets of the library files, and a coded delivery strategy where the users send linearly coded messages to each other in order to collectively satisfy their demands. We also consider a random caching strategy, which is more suitable to a fully decentralized implementation. Under certain conditions, both approaches can achieve the information theoretic outer bound within a constant multiplicative factor. In our previous work, we showed that a caching D2D wireless network with one-hop communication, random caching, and uncoded delivery, achieves the same throughput scaling law of the infrastructure-based coded multicasting scheme, in the regime of large number of users and files in the library. This shows that the spatial reuse gain of the D2D network is order-equivalent to the coded multicasting gain of single base station transmission. It is therefore natural to ask whether these two gains are cumulative, i.e.,if a D2D network with both local communication (spatial reuse) and coded multicasting can provide an improved scaling law. Somewhat counterintuitively, we show that these gains do not cumulate (in terms of throughput scaling law)."
      },
      {
        "node_idx": 16140,
        "score_0_10": 9,
        "title": "modeling and analyzing millimeter wave cellular systems",
        "abstract": "We provide a comprehensive overview of mathematical models and analytical techniques for millimeter wave (mmWave) cellular systems. The two fundamental physical differences from conventional sub-6-GHz cellular systems are: 1) vulnerability to blocking and 2) the need for significant directionality at the transmitter and/or receiver, which is achieved through the use of large antenna arrays of small individual elements. We overview and compare models for both of these factors, and present a baseline analytical approach based on stochastic geometry that allows the computation of the statistical distributions of the downlink signal-to-interference-plus-noise ratio (SINR) and also the per link data rate, which depends on the SINR as well as the average load. There are many implications of the models and analysis: 1) mmWave systems are significantly more noise-limited than at sub-6 GHz for most parameter configurations; 2) initial access is much more difficult in mmWave; 3) self-backhauling is more viable than in sub-6-GHz systems, which makes ultra-dense deployments more viable, but this leads to increasingly interference-limited behavior; and 4) in sharp contrast to sub-6-GHz systems cellular operators can mutually benefit by sharing their spectrum licenses despite the uncontrolled interference that results from doing so. We conclude by outlining several important extensions of the baseline model, many of which are promising avenues for future research."
      },
      {
        "node_idx": 157681,
        "score_0_10": 9,
        "title": "degrees of freedom of time correlated miso broadcast channel with delayed csit",
        "abstract": "We consider the time correlated multiple-input single-output (MISO) broadcast channel where the transmitter has imperfect knowledge of the current channel state, in addition to delayed channel state information. By representing the quality of the current channel state information as P-\u03b1 for the signal-to-noise ratio P and some constant \u03b1 \u2265 0, we characterize the optimal degrees of freedom region for this more general two-user MISO broadcast correlated channel. The essential ingredients of the proposed scheme lie in the quantization and multicast of the overheard interferences, while broadcasting new private messages. Our proposed scheme smoothly bridges between the scheme recently proposed by Maddah-Ali and Tse with no current state information and a simple zero-forcing beamforming with perfect current state information."
      },
      {
        "node_idx": 7889,
        "score_0_10": 9,
        "title": "algebraic semantics for a modal logic close to s1",
        "abstract": "The modal systems S1--S3 were introduced by C. I. Lewis as logics for strict implication. While there are Kripke semantics for S2 and S3, there is no known natural semantics for S1. We extend S1 by a Substitution Principle SP which generalizes a reference rule of S1. In system S1+SP, the relation of strict equivalence $\\varphi\\equiv\\psi$ satisfies the identity axioms of R. Suszko's non-Fregean logic adapted to the language of modal logic (we call these axioms the axioms of propositional identity). This enables us to develop a framework of algebraic semantics which captures S1+SP as well as the Lewis systems S3--S5. So from the viewpoint of algebraic semantics, S1+SP turns out to be an interesting modal logic. We show that S1+SP is strictly contained between S1 and S3 and differs from S2. It is the weakest modal logic containing S1 such that strict equivalence is axiomatized by propositional identity."
      },
      {
        "node_idx": 2954,
        "score_0_10": 9,
        "title": "the exact rate memory tradeoff for caching with uncoded prefetching",
        "abstract": "We consider a basic cache network, in which a single server is connected to multiple users via a shared bottleneck link. The server has a database of files (content). Each user has an isolated memory that can be used to cache content in a prefetching phase. In a following delivery phase, each user requests a file from the database, and the server needs to deliver users\u2019 demands as efficiently as possible by taking into account their cache contents. We focus on an important and commonly used class of prefetching schemes, where the caches are filled with uncoded data. We provide the exact characterization of the rate-memory tradeoff for this problem, by deriving both the  minimum average rate  (for a uniform file popularity) and the  minimum peak rate  required on the bottleneck link for a given cache size available at each user. In particular, we propose a novel caching scheme, which strictly improves the state of the art by exploiting commonality among user demands. We then demonstrate the exact optimality of our proposed scheme through a matching converse, by dividing the set of all demands into types, and showing that the placement phase in the proposed caching scheme is universally optimal for all types. Using these techniques, we also fully characterize the rate-memory tradeoff for a decentralized setting, in which users fill out their cache content without any coordination."
      }
    ]
  },
  "145": {
    "explanation": "constructing and analyzing self-dual and cyclic error-correcting codes",
    "topk": [
      {
        "node_idx": 59818,
        "score_0_10": 10,
        "title": "multipath tcp in ns 3",
        "abstract": "In this paper we present our work on designing and implementing an NS3 model for MultiPath TCP (MPTCP). Our MPTCP model closely follows MPTCP specifications, as described in RFC 6824, and supports TCP NewReno loss recovery on a per subflow basis. Subflow management is based on MPTCP's kernel implementation. We briefly describe how we integrate our MPTCP model with NS3 and present example simulation results to showcase its working state."
      },
      {
        "node_idx": 56083,
        "score_0_10": 10,
        "title": "various constructions for self dual codes over rings and new binary self dual codes",
        "abstract": "In this work, extension theorems are used for self-dual codes over rings and as applications many new binary self-dual extremal codes are found from self-dual codes over F 2 m + u F 2 m for m = 1 , 2 . The duality and distance preserving Gray maps from F 4 + u F 4 to ( F 2 + u F 2 ) 2 and F 2 4 are used to obtain self-dual codes whose binary Gray images are 64 , 32 , 12 -extremal self-dual. An F 2 + u F 2 -extension is used and as binary images, 178 extremal binary self-dual codes of length 68 with new weight enumerators are obtained. Especially the first examples of codes with \u03b3 = 3 and many codes with the rare \u03b3 = 4 , 6 parameters are obtained. In addition to these, two hundred fifty doubly even self dual 96 , 48 , 16 -codes with new weight enumerators are obtained from four-circulant codes over F 4 + u F 4 . New extremal doubly even binary codes of lengths 80 and 88 are also found by the F 2 + u F 2 -lifts of binary four circulant codes and thus a lower bound on the number of non-isomorphic 3-(80, 16, 665) designs is modified."
      },
      {
        "node_idx": 74321,
        "score_0_10": 10,
        "title": "new extremal binary self dual codes from a modified four circulant construction",
        "abstract": "In this work, we propose a modified four circulant construction for self-dual codes and a bordered version of the construction using the properties of \u03bb -circulant and \u03bb -reverse circulant matrices. By using the constructions on F 2 , we obtain new binary codes of lengths 64 and 68. We also apply the constructions to the ring R 2 and considering the F 2 and R 1 -extensions, we obtain new singly-even extremal binary self-dual codes of lengths 66 and 68. More precisely, we find 3 new codes of length 64, 13 new codes of length 66 and 21 new codes of length 68. These codes all have weight enumerators with parameters that were not known to exist in the literature."
      },
      {
        "node_idx": 156364,
        "score_0_10": 10,
        "title": "a comprehensive survey of recent advancements in molecular communication",
        "abstract": "With much advancement in the field of nanotechnology, bioengineering, and synthetic biology over the past decade, microscales and nanoscales devices are becoming a reality. Yet the problem of engineering a reliable communication system between tiny devices is still an open problem. At the same time, despite the prevalence of radio communication, there are still areas where traditional electromagnetic waves find it difficult or expensive to reach. Points of interest in industry, cities, and medical applications often lie in embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or they are located in such a way that directional high frequency systems are ineffective. Inspired by nature, one solution to these problems is molecular communication (MC), where chemical signals are used to transfer information. Although biologists have studied MC for decades, it has only been researched for roughly 10 year from a communication engineering lens. Significant number of papers have been published to date, but owing to the need for interdisciplinary work, much of the results are preliminary. In this survey, the recent advancements in the field of MC engineering are highlighted. First, the biological, chemical, and physical processes used by an MC system are discussed. This includes different components of the MC transmitter and receiver, as well as the propagation and transport mechanisms. Then, a comprehensive survey of some of the recent works on MC through a communication engineering lens is provided. The survey ends with a technology readiness analysis of MC and future research directions."
      },
      {
        "node_idx": 115478,
        "score_0_10": 10,
        "title": "weierstrass semigroups from kummer extensions",
        "abstract": "Abstract   The Weierstrass semigroups and pure gaps can be helpful in constructing codes with better parameters. In this paper, we investigate explicitly the minimal generating set of the Weierstrass semigroups associated with several totally ramified places over arbitrary Kummer extensions. Applying the techniques provided by Matthews in her previous work, we extend the results of specific Kummer extensions studied in the literature. Some examples are included to illustrate our results."
      },
      {
        "node_idx": 56645,
        "score_0_10": 10,
        "title": "a note on cyclic codes from apn functions",
        "abstract": "Cyclic codes, as linear block error-correcting codes in coding theory, play a vital role and have wide applications. Ding in \\cite{D} constructed a number of classes of cyclic codes from almost perfect nonlinear (APN) functions and planar functions over finite fields and presented ten open problems on cyclic codes from highly nonlinear functions. In this paper, we consider two open problems involving the inverse APN functions $f(x)=x^{q^m-2}$ and the Dobbertin APN function $f(x)=x^{2^{4i}+2^{3i}+2^{2i}+2^{i}-1}$. From the calculation of linear spans and the minimal polynomials of two sequences generated by these two classes of APN functions, the dimensions of the corresponding cyclic codes are determined and lower bounds on the minimum weight of these cyclic codes are presented. Actually, we present a framework for the minimal polynomial and linear span of the sequence $s^{\\infty}$ defined by $s_t=Tr((1+\\alpha^t)^e)$, where $\\alpha$ is a primitive element in $GF(q)$. These techniques can also be applied into other open problems in \\cite{D}."
      },
      {
        "node_idx": 40198,
        "score_0_10": 9,
        "title": "amp a better multipath tcp for data center networks",
        "abstract": "In recent years several multipath data transport mechanisms, such as MPTCP and XMP, have been introduced to effectively exploit the path diversity of data center networks (DCNs). However, these multipath schemes have not been widely deployed in DCNs. We argue that two key factors among others impeded their adoption: TCP incast and minimum window syndrome. First, these mechanisms are ill-suited for workloads with a many-to-one communication pattern, commonly found in DCNs, causing frequent TCP incast collapses. Second, the syndrome we discover for the first time, results in 2-5 times lower throughput for single-path flows than multipath flows, thus severely violating network fairness. #R##N#To effectively tackle these problems, we propose AMP: an adaptive multipath congestion control mechanism that quickly detects the onset of these problems and transforms its multipath flow into a single-path flow. Once these problems disappear, AMP safely reverses this transformation and continues its data transmission via multiple paths. Our evaluation results under a diverse set of scenarios in a fat-tree topology with realistic workloads demonstrate that AMP is robust to the TCP incast problem and improves network fairness between multipath and single-path flows significantly with little performance loss."
      },
      {
        "node_idx": 79127,
        "score_0_10": 9,
        "title": "on self dual double circulant codes",
        "abstract": "Self-dual double circulant codes of odd dimension are shown to be dihedral in even characteristic and consta-dihedral in odd characteristic. Exact counting formulae are derived for them and used to show they contain families of codes with relative distance satisfying a modified Gilbert-Varshamov bound."
      },
      {
        "node_idx": 29467,
        "score_0_10": 9,
        "title": "weierstrass pure gaps from a quotient of the hermitian curve",
        "abstract": "In this paper, by employing the results over Kummer extensions, we give an arithmetic characterization of pure gaps at many totally ramified places over the quotients of Hermitian curves, including the well-studied Hermitian curves as special cases. The cardinality of these pure gaps is explicitly investigated. In particular, the numbers of gaps and pure gaps at a pair of distinct places are determined precisely, which can be regarded as an extension of the previous work by Matthews (2001) considered Hermitian curves. Additionally, some concrete examples are provided to illustrate our results."
      },
      {
        "node_idx": 3489,
        "score_0_10": 9,
        "title": "weight distributions of cyclic codes with respect to pairwise coprime order elements",
        "abstract": "Let $\\Bbb F_r$ be an extension of a finite field $\\Bbb F_q$ with $r=q^m$. Let each $g_i$ be of order $n_i$ in $\\Bbb F_r^*$ and $\\gcd(n_i, n_j)=1$ for $1\\leq i \\neq j \\leq u$. #R##N#We define a cyclic code over $\\Bbb F_q$ by #R##N#$$\\mathcal C_{(q, m, n_1,n_2, ..., n_u)}=\\{c(a_1, a_2, ..., a_u) : a_1, a_2, ..., a_u \\in \\Bbb F_r\\},$$ where #R##N#$$c(a_1, a_2, ..., a_u)=({Tr}_{r/q}(\\sum_{i=1}^ua_ig_i^0), ..., {Tr}_{r/q}(\\sum_{i=1}^ua_ig_i^{n-1}))$$ and $n=n_1n_2... n_u$. In this paper, we present a method to compute the weights of $\\mathcal C_{(q, m, n_1,n_2, ..., n_u)}$. Further, we determine the weight distributions of the cyclic codes $\\mathcal C_{(q, m, n_1,n_2)}$ and $\\mathcal C_{(q, m, n_1,n_2,1)}$."
      }
    ]
  },
  "146": {
    "explanation": "neural network modules enabling spatial transformation and invariance",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 14201,
        "score_0_10": 9,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      }
    ]
  },
  "147": {
    "explanation": "sequence modeling for image captioning and visual recognition tasks",
    "topk": [
      {
        "node_idx": 130623,
        "score_0_10": 10,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 57930,
        "score_0_10": 8,
        "title": "show attend and tell neural image caption generation with visual attention",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO."
      },
      {
        "node_idx": 51364,
        "score_0_10": 8,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 135057,
        "score_0_10": 8,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 55399,
        "score_0_10": 8,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 14201,
        "score_0_10": 8,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      },
      {
        "node_idx": 71636,
        "score_0_10": 8,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 112726,
        "score_0_10": 8,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 43376,
        "score_0_10": 8,
        "title": "skip thought vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
      }
    ]
  },
  "148": {
    "explanation": "wireless energy and information transmission optimization",
    "topk": [
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 28821,
        "score_0_10": 9,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 35908,
        "score_0_10": 8,
        "title": "exploiting multi antennas for opportunistic spectrum sharing in cognitive radio networks",
        "abstract": "In cognitive radio (CR) networks, there are scenarios where the secondary (lower priority) users intend to communicate with each other by opportunistically utilizing the transmit spectrum originally allocated to the existing primary (higher priority) users. For such a scenario, a secondary user usually has to tradeoff between two conflicting goals at the same time: one is to maximize its own transmit throughput; and the other is to minimize the amount of interference it produces at each primary receiver. In this paper, we study this fundamental tradeoff from an information-theoretic perspective by characterizing the secondary user's channel capacity under both its own transmit-power constraint as well as a set of interference-power constraints each imposed at one of the primary receivers. In particular, this paper exploits multi-antennas at the secondary transmitter to effectively balance between spatial multiplexing for the secondary transmission and interference avoidance at the primary receivers. Convex optimization techniques are used to design algorithms for the optimal secondary transmit spatial spectrum that achieves the capacity of the secondary transmission. Suboptimal solutions for ease of implementation are also presented and their performances are compared with the optimal solution. Furthermore, algorithms developed for the single-channel transmission are also extended to the case of multichannel transmission whereby the secondary user is able to achieve opportunistic spectrum sharing via transmit adaptations not only in space, but in time and frequency domains as well. Simulation results show that even under stringent interference-power constraints, substantial capacity gains are achievable for the secondary transmission by employing multi-antennas at the secondary transmitter. This is true even when the number of primary receivers exceeds that of secondary transmit antennas in a CR network, where an interesting \"interference diversity\" effect can be exploited."
      },
      {
        "node_idx": 100650,
        "score_0_10": 8,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 14646,
        "score_0_10": 8,
        "title": "wireless powered communication opportunities and challenges",
        "abstract": "The performance of wireless communication is fundamentally constrained by the limited battery life of wireless devices, the operations of which are frequently disrupted due to the need of manual battery replacement/recharging. The recent advance in RF-enabled wireless energy transfer (WET) technology provides an attractive solution named wireless powered communication (WPC), where the wireless devices are powered by dedicated wireless power transmitters to provide continuous and stable microwave energy over the air. As a key enabling technology for truly perpetual communications, WPC opens up the potential to build a network with larger throughput, higher robustness, and increased flexibility compared to its battery-powered counterpart. However, the combination of wireless energy and information transmissions also raises many new research problems and implementation issues that need to be addressed. In this article, we provide an overview of stateof- the-art RF-enabled WET technologies and their applications to wireless communications, highlighting the key design challenges, solutions, and opportunities ahead."
      },
      {
        "node_idx": 79165,
        "score_0_10": 7,
        "title": "optimal beamforming for two way multi antenna relay channel with analogue network coding",
        "abstract": "This paper studies the wireless two-way relay channel (TWRC), where two source nodes, S1 and S2, exchange information through an assisting relay node, R. It is assumed that R receives the sum signal from S1 and S2 in one time-slot, and then amplifies and forwards the received signal to both S1 and S2 in the next time-slot. By applying the principle of analogue network (ANC), each of S1 and S2 cancels the so-called \"self-interference\" in the received signal from R and then decodes the desired message. Assuming that S1 and S2 are each equipped with a single antenna and R with multi-antennas, this paper analyzes the capacity region of an ANC-based TWRC with linear processing (beamforming) at R. The capacity region contains all the achievable bidirectional rate-pairs of S1 and S2 under the given transmit power constraints at S1, S2, and R. We present the optimal relay beamforming structure as well as an efficient algorithm to compute the optimal beamforming matrix based on convex optimization techniques. Low-complexity suboptimal relay beamforming schemes are also presented, and their achievable rates are compared against the capacity with the optimal scheme."
      },
      {
        "node_idx": 158471,
        "score_0_10": 7,
        "title": "communications and signals design for wireless power transmission",
        "abstract": "Radiative wireless power transfer (WPT) is a promising technology to provide cost-effective and real-time power supplies to wireless devices. Although radiative WPT shares many similar characteristics with the extensively studied wireless information transfer or communication, they also differ significantly in terms of design objectives, transmitter/receiver architectures and hardware constraints, etc. In this article, we first give an overview on the various WPT technologies, the historical development of the radiative WPT technology and the main challenges in designing contemporary radiative WPT systems. Then, we focus on discussing the new communication and signal processing techniques that can be applied to tackle these challenges. Topics discussed include energy harvester modeling, energy beamforming for WPT, channel acquisition, power region characterization in multi-user WPT, waveform design with linear and non-linear energy receiver model, safety and health issues of WPT, massive MIMO (multiple-input multiple-output) and millimeter wave (mmWave) enabled WPT, wireless charging control, and wireless power and communication systems co-design. We also point out directions that are promising for future research."
      },
      {
        "node_idx": 74729,
        "score_0_10": 7,
        "title": "a survey on non orthogonal multiple access for 5g networks research challenges and future trends",
        "abstract": "Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed."
      },
      {
        "node_idx": 137708,
        "score_0_10": 7,
        "title": "cooperative interference management with miso beamforming",
        "abstract": "In this correspondence, we study the downlink transmission in a multi-cell system, where multiple base stations (BSs) each with multiple antennas cooperatively design their respective transmit beamforming vectors to optimize the overall system performance. For simplicity, it is assumed that all mobile stations (MSs) are equipped with a single antenna each, and there is one active MS in each cell at one time. Accordingly, the system of interests can be modeled by a multiple-input single-output (MISO) Gaussian interference channel (IC), termed as MISO-IC, with interference treated as noise. We propose a new method to characterize different rate-tuples for active MSs on the Pareto boundary of the achievable rate region for the MISO-IC, by exploring the relationship between the MISO-IC and the cognitive radio (CR) MISO channel. We show that each Pareto-boundary rate-tuple of the MISO-IC can be achieved in a decentralized manner when each of the BSs attains its own channel capacity subject to a certain set of interference-power constraints (also known as interference-temperature constraints in the CR system) at the other MS receivers. Furthermore, we show that this result leads to a new decentralized algorithm for implementing the multi-cell cooperative downlink beamforming."
      },
      {
        "node_idx": 36142,
        "score_0_10": 7,
        "title": "joint trajectory and communication design for multi uav enabled wireless networks",
        "abstract": "Unmanned aerial vehicles (UAVs) have attracted significant interest recently in assisting wireless communication due to their high maneuverability, flexible deployment, and low cost. This paper considers a multi-UAV enabled wireless communication system, where multiple UAV-mounted aerial base stations (BSs) are employed to serve a group of users on the ground. To achieve fair performance among users, we maximize the minimum throughput over all ground users in the downlink communication by optimizing the multiuser communication scheduling and association jointly with the UAVs' trajectory and power control. The formulated problem is a mixed integer non-convex optimization problem that is challenging to solve. As such, we propose an efficient iterative algorithm for solving it by applying the block coordinate descent and successive convex optimization techniques. Specifically, the user scheduling and association, UAV trajectory, and transmit power are alternately optimized in each iteration. In particular, for the non-convex UAV trajectory and transmit power optimization problems, two approximate convex optimization problems are solved, respectively. We further show that the proposed algorithm is guaranteed to converge to at least a locally optimal solution. To speed up the algorithm convergence and achieve good throughput, a low-complexity and systematic initialization scheme is also proposed for the UAV trajectory design based on the simple circular trajectory and the circle packing scheme. Extensive simulation results are provided to demonstrate the significant throughput gains of the proposed design as compared to other benchmark schemes."
      }
    ]
  },
  "152": {
    "explanation": "neural networks for sequence modeling and translation tasks",
    "topk": [
      {
        "node_idx": 124702,
        "score_0_10": 10,
        "title": "convolutional sequence to sequence learning",
        "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 111368,
        "score_0_10": 9,
        "title": "grounding bound founded answer set programs",
        "abstract": "To appear in Theory and Practice of Logic Programming (TPLP) #R##N#Bound Founded Answer Set Programming (BFASP) is an extension of Answer Set Programming (ASP) that extends stable model semantics to numeric variables. While the theory of BFASP is defined on ground rules, in practice BFASP programs are written as complex non-ground expressions. Flattening of BFASP is a technique used to simplify arbitrary expressions of the language to a small and well defined set of primitive expressions. In this paper, we first show how we can flatten arbitrary BFASP rule expressions, to give equivalent BFASP programs. Next, we extend the bottom-up grounding technique and magic set transformation used by ASP to BFASP programs. Our implementation shows that for BFASP problems, these techniques can significantly reduce the ground program size, and improve subsequent solving."
      },
      {
        "node_idx": 53478,
        "score_0_10": 9,
        "title": "opennmt open source toolkit for neural machine translation",
        "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques."
      },
      {
        "node_idx": 135351,
        "score_0_10": 9,
        "title": "six challenges for neural machine translation",
        "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation."
      },
      {
        "node_idx": 78341,
        "score_0_10": 8,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 43376,
        "score_0_10": 8,
        "title": "skip thought vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 85704,
        "score_0_10": 8,
        "title": "characterization of negabent functions and construction of bent negabent functions with maximum algebraic degree",
        "abstract": "We present necessary and sufficient conditions for a Boolean function to be a negabent function for both even and odd number of variables, which demonstrate the relationship between negabent functions and bent functions. By using these necessary and sufficient conditions for Boolean functions to be negabent, we obtain that the nega spectrum of a negabent function has at most 4 values. We determine the nega spectrum distribution of negabent functions. Further, we provide a method to construct bent-negabent functions in $n$ variables ($n$ even) of algebraic degree ranging from 2 to $\\frac{n}{2}$, which implies that the maximum algebraic degree of an $n$-variable bent-negabent function is equal to $\\frac{n}{2}$. Thus, we answer two open problems proposed by Parker and Pott and by St\\v{a}nic\\v{a} \\textit{et al.} respectively."
      },
      {
        "node_idx": 100165,
        "score_0_10": 8,
        "title": "quaternary constant amplitude codes for multicode cdma",
        "abstract": "A constant-amplitude code is a code that reduces the peak-to-average power ratio (PAPR) in multicode code-division multiple access (MC-CDMA) systems to the favorable value 1. In this paper quaternary constant-amplitude codes (codes over Z_4) of length 2^m with error-correction capabilities are studied. These codes exist for every positive integer m, while binary constant-amplitude codes cannot exist if m is odd. Every word of such a code corresponds to a function from the binary m-tuples to Z_4 having the bent property, i.e., its Fourier transform has magnitudes 2^{m/2}. Several constructions of such functions are presented, which are exploited in connection with algebraic codes over Z_4 (in particular quaternary Reed-Muller, Kerdock, and Delsarte-Goethals codes) to construct families of quaternary constant-amplitude codes. Mappings from binary to quaternary constant-amplitude codes are presented as well."
      }
    ]
  },
  "156": {
    "explanation": "multimodal alignment for image captioning and visual language translation",
    "topk": [
      {
        "node_idx": 130623,
        "score_0_10": 10,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 26369,
        "score_0_10": 10,
        "title": "verbal programming of robot behavior",
        "abstract": "Home robots may come with many sophisticated built-in abilities, however there will always be a degree of customization needed for each user and environment. Ideally this should be accomplished through one-shot learning, as collecting the large number of examples needed for statistical inference is tedious. A particularly appealing approach is to simply explain to the robot, via speech, what it should be doing. In this paper we describe the ALIA cognitive architecture that is able to effectively incorporate user-supplied advice and prohibitions in this manner. The functioning of the implemented system on a small robot is illustrated by an associated video."
      },
      {
        "node_idx": 113299,
        "score_0_10": 10,
        "title": "child sized 3d printed igus humanoid open platform",
        "abstract": "The use of standard platforms in the field of humanoid robotics can accelerate research, and lower the entry barrier for new research groups. While many affordable humanoid standard platforms exist in the lower size ranges of up to 60cm, beyond this the few available standard platforms quickly become significantly more expensive, and difficult to operate and maintain. In this paper, the igus Humanoid Open Platform is presented---a new, affordable, versatile and easily customisable standard platform for humanoid robots in the child-sized range. At 90cm, the robot is large enough to interact with a human-scale environment in a meaningful way, and is equipped with enough torque and computing power to foster research in many possible directions. The structure of the robot is entirely 3D printed, allowing for a lightweight and appealing design. The electrical and mechanical designs of the robot are presented, and the main features of the corresponding open-source ROS software are discussed. The 3D CAD files for all of the robot parts have been released open-source in conjunction with this paper."
      },
      {
        "node_idx": 104596,
        "score_0_10": 9,
        "title": "why robots a survey on the roles and benefits of social robots in the therapy of children with autism",
        "abstract": "This paper reviews the use of socially interactive robots to assist in the therapy of children with autism. The extent to which the robots were successful in helping the children in their social, emotional and communication deficits was investigated. Child\u2013robot interactions were scrutinized with respect to the different target behaviors that are to be elicited from a child during therapy. These behaviors were thoroughly examined with respect to a child\u2019s development needs. Most importantly, experimental data from the surveyed works were extracted and analysed in terms of the target behaviors and of how each robot was used during a therapy session to achieve these behaviors. The study concludes by categorizing the different therapeutic roles that these robots were observed to play, and highlights the important design features that enable them to achieve high levels of effectiveness in autism therapy."
      },
      {
        "node_idx": 71152,
        "score_0_10": 9,
        "title": "application software domain specific languages and language design assistants",
        "abstract": "While application software does the real work, domain-specific languages (DSLs) are tools to help produce it efficiently, and language design assistants in turn are meta-tools to help produce DSLs quickly. DSLs are already in wide use (HTML for web pages, Excel macros for spreadsheet applications, VHDL for hardware design, ...), but many more will be needed for both new as well as existing application domains. Language design assistants to help develop them currently exist only in the basic form of language development systems. After a quick look at domain-specific languages, and especially their relationship to application libraries, we survey existing language development systems and give an outline of future language design assistants."
      },
      {
        "node_idx": 57930,
        "score_0_10": 9,
        "title": "show attend and tell neural image caption generation with visual attention",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 150904,
        "score_0_10": 9,
        "title": "statistical sign language machine translation from english written text to american sign language gloss",
        "abstract": "This works aims to design a statistical machine translation from English text to American Sign Language (ASL). The system is based on Moses tool with some modifications and the results are synthesized through a 3D avatar for interpretation. First, we translate the input text to gloss, a written form of ASL. Second, we pass the output to the WebSign Plug-in to play the sign. Contributions of this work are the use of a new couple of language English/ASL and an improvement of statistical machine translation based on string matching thanks to Jaro-distance."
      },
      {
        "node_idx": 63929,
        "score_0_10": 9,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 164989,
        "score_0_10": 9,
        "title": "google s multilingual neural machine translation system enabling zero shot translation",
        "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages."
      }
    ]
  },
  "157": {
    "explanation": "bio-electronic circuits and sensing with Physarum slime mould",
    "topk": [
      {
        "node_idx": 16420,
        "score_0_10": 10,
        "title": "a survey on graph drawing beyond planarity",
        "abstract": "Graph Drawing Beyond Planarity is a rapidly growing research area that classifies and studies geometric representations of non-planar graphs in terms of forbidden crossing configurations. Aim of this survey is to describe the main research directions in this area, the most prominent known results, and some of the most challenging open problems."
      },
      {
        "node_idx": 93694,
        "score_0_10": 10,
        "title": "slime mould electronic oscillators",
        "abstract": "We construct electronic oscillator from acellular slime mould Physarum polycephalum. The slime mould oscillator is made of two electrodes connected by a protoplasmic tube of the living slime mould. A protoplasmic tube has an average resistance of 3~MOhm. The tube's resistance is changing over time due to peristaltic contractile activity of the tube. The resistance of the protoplasmic tube oscillates with average period of 73~sec and average amplitude of 0.6~MOhm. We present experimental laboratory results on dynamics of Physarum oscillator under direct current voltage up to 15~V and speculate that slime mould P. polycephalum can be employed as a living electrical oscillator in biological and hybrid circuits."
      },
      {
        "node_idx": 143170,
        "score_0_10": 10,
        "title": "thirty eight things to do with live slime mould",
        "abstract": "Slime mould \\emph{Physarum polycephalum} is a large single cell capable for distributed sensing, concurrent information processing, parallel computation and decentralised actuation. The ease of culturing and experimenting with Physarum makes this slime mould an ideal substrate for real-world implementations of unconventional sensing and computing devices. In the last decade the Physarum became a swiss knife of the unconventional computing: give the slime mould a problem it will solve it. We provide a concise summary of what exact computing and sensing operations are implemented with live slime mould. The Physarum devices range from morphological processors for computational geometry to experimental archeology tools, from self-routing wires to memristors, from devices approximating a shortest path to analog physical models of space exploration."
      },
      {
        "node_idx": 130647,
        "score_0_10": 10,
        "title": "practical circuits with physarum wires",
        "abstract": "Purpose: Protoplasmic tubes of Physarum polycephalum, also know as Physarum Wires (PW), have been previously suggested as novel bio- electronic components. Until recently, practical examples of electronic circuits using PWs have been limited. These PWs have been shown to be self repairing, offering significant advantage over traditional electronic components. This article documents work performed to produce practical circuits using PWs. Method: We have demonstrated through manufacture and testing of hybrid circuits that PWs can be used to produce a variety of practical electronic circuits. A purality of different applications of PWs have been tested to show the universality of PWs in analogue and digital electronics. Results: Voltage dividers can be produced using a pair of PWs in series with an output voltage accurate to within 12%. PWs can also transmit analogue and digital data with a frequency of up to 19 kHz, which with the addition of a buffer, can drive high current circuits. We have demonstrated that PWs can last approximately two months, a 4 fold increase on previous literature. Protoplasmic tubes can be modified with the addition of conductive or magnetic nano-particles to provide changes in functionality. Conclusion This work has documented novel macro-scale data transmission through biological material; it has advanced the field of bio-electronics by providing a cheap and easy to grow conducting bio-material which may be used in future hybrid electronic technology."
      },
      {
        "node_idx": 80444,
        "score_0_10": 9,
        "title": "on upward drawings of trees on a given grid",
        "abstract": "Computing a minimum-area planar straight-line drawing of a graph is known to be NP-hard for planar graphs, even when restricted to outerplanar graphs. However, the complexity question is open for trees. Only a few hardness results are known for straight-line drawings of trees under various restrictions such as edge length or slope constraints. On the other hand, there exist polynomial-time algorithms for computing minimum-width (resp., minimum-height) upward drawings of trees, where the height (resp., width) is unbounded. #R##N#In this paper we take a major step in understanding the complexity of the area minimization problem for strictly-upward drawings of trees, which is one of the most common styles for drawing rooted trees. We prove that given a rooted tree $T$ and a $W\\times H$ grid, it is NP-hard to decide whether $T$ admits a strictly-upward (unordered) drawing in the given grid."
      },
      {
        "node_idx": 21982,
        "score_0_10": 9,
        "title": "non clairvoyant scheduling to minimize max flow time on a machine with setup times",
        "abstract": "Consider a problem in which $n$ jobs that are classified into $k$ types arrive over time at their release times and are to be scheduled on a single machine so as to minimize the maximum flow time. The machine requires a setup taking $s$ time units whenever it switches from processing jobs of one type to jobs of a different type. We consider the problem as an online problem where each job is only known to the scheduler as soon as it arrives and where the processing time of a job only becomes known upon its completion (non-clairvoyance). #R##N#We are interested in the potential of simple \"greedy-like\" algorithms. We analyze a modification of the FIFO strategy and show its competitiveness to be $\\Theta(\\sqrt{n})$, which is optimal for the considered class of algorithms. For $k=2$ types it achieves a constant competitiveness. Our main insight is obtained by an analysis of the smoothed competitiveness. If processing times $p_j$ are independently perturbed to $\\hat p_j = (1+X_j)p_j$, we obtain a competitiveness of $O(\\sigma^{-2} \\log^2 n)$ when $X_j$ is drawn from a uniform or a (truncated) normal distribution with standard deviation $\\sigma$. The result proves that bad instances are fragile and \"practically\" one might expect a much better performance than given by the $\\Omega(\\sqrt{n})$-bound."
      },
      {
        "node_idx": 92858,
        "score_0_10": 9,
        "title": "towards slime mould colour sensor recognition of colours by physarum polycephalum",
        "abstract": "Acellular slime mould Physarum polycephalum is a popular now user-friendly living substrate for designing of future and emergent sensing and computing devices. P. polycephalum exhibits regular patterns of oscillations of its surface electrical potential. The oscillation patterns are changed when the slime mould is subjected to mechanical, chemical, electrical or optical stimuli. We evaluate feasibility of slime-mould based colour sensors by illuminating Physarum with red, green, blue and white colours and analysing patterns of the slime mould's electrical potential oscillations. We define that the slime mould recognises a colour if it reacts to illumination with the colour by a unique changes in amplitude and periods of oscillatory activity. In laboratory experiments we found that the slime mould recognises red and blue colour. The slime mould does not differentiate between green and white colours. The slime mould also recognises when red colour is switched off. We also map colours to diversity of the oscillations: illumination with a white colour increases diversity of amplitudes and periods of oscillations, other colours studied increase diversity either of amplitude or period."
      },
      {
        "node_idx": 11209,
        "score_0_10": 9,
        "title": "non preemptive scheduling on machines with setup times",
        "abstract": "Consider the problem in which n jobs that are classified into k types are to be scheduled on m identical machines without preemption. A machine requires a proper setup taking s time units before processing jobs of a given type. The objective is to minimize the makespan of the resulting schedule. We design and analyze an approximation algorithm that runs in time polynomial in n, m and k and computes a solution with an approximation factor that can be made arbitrarily close to 3/2."
      },
      {
        "node_idx": 127909,
        "score_0_10": 9,
        "title": "optimal polygonal representation of planar graphs",
        "abstract": "In this paper, we consider the problem of representing graphs by polygons whose sides touch. We show that at least six sides per polygon are necessary by constructing a class of planar graphs that cannot be represented by pentagons. We also show that the lower bound of six sides is matched by an upper bound of six sides with a linear-time algorithm for representing any planar graph by touching hexagons. Moreover, our algorithm produces convex polygons with edges having at most three slopes and with all vertices lying on an O(n)xO(n) grid."
      },
      {
        "node_idx": 9264,
        "score_0_10": 9,
        "title": "towards physarum robots computing and manipulating on water surface",
        "abstract": "Plasmodium of Physarum polycephalum is an ideal biological substrate for implementing concurrent and parallel computation, including combinatorial geometry and optimization on graphs. The scoping experiments on Physarum computing in conditions of minimal friction, on the water surface were performed. The laboratory and computer experimental results show that plasmodium of Physarum is capable of computing a basic spanning tree and manipulating of light-weight objects. We speculate that our results pave the pathways towards the design and implementation of amorphous biological robots."
      }
    ]
  },
  "160": {
    "explanation": "multi-scale spatial transformation and context aggregation",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 22502,
        "score_0_10": 8,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 66578,
        "score_0_10": 8,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 121343,
        "score_0_10": 8,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 69942,
        "score_0_10": 8,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 14201,
        "score_0_10": 8,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      }
    ]
  },
  "162": {
    "explanation": "self-healing algorithms preserving edges and network connectivity",
    "topk": [
      {
        "node_idx": 106719,
        "score_0_10": 10,
        "title": "edge preserving self healing keeping network backbones densely connected",
        "abstract": "Healing algorithms play a crucial part in distributed P2P networks where failures occur continuously and frequently. Several self-healing algorithms have been suggested recently [IPDPS'08, PODC'08, PODC'09, PODC'11] in a line of work that has yielded gradual improvements in the properties ensured on the graph. This work motivates a strong general phenomenon of edge-preserving healing that aims at obtaining self-healing algorithms with the constraint that all original edges in the graph (not deleted by the adversary), be retained in every intermediate graph. #R##N#The previous algorithms, in their nascent form, are not explicitly edge preserving. In this paper, we show they can be suitably modified (We introduce Xheal+, an edge-preserving version of Xheal[PODC'11]). Towards this end, we present a general self-healing model that unifies the previous models. The main contribution of this paper is not in the technical complexity, rather in the simplicity with which the edge-preserving property can be ensured and the message that this is a crucial property with several benefits. In particular, we highlight this by showing that, almost as an immediate corollary, subgraph densities are preserved or increased. Maintaining density is a notion motivated by the fact that in certain distributed networks, certain nodes may require and initially have a larger number of inter-connections. It is vital that a healing algorithm, even amidst failures, respect these requirements. Our suggested modifications yield such subgraph density preservation as a by product. In addition, edge preservation helps maintain any subgraph induced property that is monotonic. Also, algorithms that are edge-preserving require minimal alteration of edges which can be an expensive cost in healing - something that has not been modeled in any of the past work."
      },
      {
        "node_idx": 166849,
        "score_0_10": 10,
        "title": "integrated structure and semantics for reo connectors and petri nets",
        "abstract": "In this paper, we present an integrated structural and behavioral model of Reo connectors and Petri nets, allowing a direct comparison of the two concurrency models. For this purpose, we introduce a notion of connectors which consist of a number of interconnected, user-defined primitives with fixed behavior. While the structure of connectors resembles hypergraphs, their semantics is given in terms of so-called port automata. We define both models in a categorical setting where composition operations can be elegantly defined and integrated. Specifically, we formalize structural gluings of connectors as pushouts, and joins of port automata as pullbacks. We then define a semantical functor from the connector to the port automata category which preserves this composition. We further show how to encode Reo connectors and Petri nets into this model and indicate applications to dynamic reconfigurations modeled using double pushout graph transformation."
      },
      {
        "node_idx": 84212,
        "score_0_10": 10,
        "title": "a constructive proof of the general lovasz local lemma",
        "abstract": "The Lovasz Local Lemma [EL75] is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. In his breakthrough paper [Bec91], Beck demonstrated that a constructive variant can be given under certain more restrictive conditions. Simplifications of his procedure and relaxations of its restrictions were subsequently exhibited in several publications [Alo91, MR98, CS00, Mos06, Sri08, Mos08]. In [Mos09], a constructive proof was presented that works under negligible restrictions, formulated in terms of the Bounded Occurrence Satisfiability problem. In the present paper, we reformulate and improve upon these findings so as to directly apply to almost all known applications of the general Local Lemma."
      },
      {
        "node_idx": 13936,
        "score_0_10": 10,
        "title": "xheal localized self healing using expanders",
        "abstract": "We consider the problem of self-healing in reconfigurable networks (e.g. peer-to-peer and wireless mesh networks) that are under repeated attack by an omniscient adversary and propose a fully distributed algorithm, Xheal that maintains good expansion and spectral properties of the network, also keeping the network connected. Moreover, Xheal does this while allowing only low stretch and degree increase per node. Thus, the algorithm heals global properties while only doing local changes and using only local information. #R##N#Our work improves over the self-healing algorithms 'Forgiving tree'[PODC 2008] and 'Forgiving graph'[PODC 2009] (using a similar model) in that we are able to give guarantees on degree and stretch, while at the same time preserving the expansion and spectral properties of the network. These repairs preserve the invariants in the following sense. At any point in the algorithm, the expansion of the graph will be either `better' than the expansion of the graph formed by considering only the adversarial insertions (not the adversarial deletions) or the expansion will be, at least, a constant. Also, the stretch i.e. the distance between any pair of nodes in the healed graph is no more than a $O(\\log n)$ factor. Similarly, at any point, a node $v$ whose degree would have been $d$ in the graph with adversarial insertions only, will have degree at most $O(\\kappa d)$ in the actual graph, for a small parameter $\\kappa$. We also provide bounds on the second smallest eigenvalue of the Laplacian which captures key properties such as mixing time, conductance, congestion in routing etc. Our distributed data structure has low amortized latency and bandwidth requirements."
      },
      {
        "node_idx": 116690,
        "score_0_10": 10,
        "title": "an algorithmic proof of the lovasz local lemma via resampling oracles",
        "abstract": "The Lovasz Local Lemma is a seminal result in probabilistic combinatorics. It gives a sufficient condition on a probability space and a collection of events for the existence of an outcome that simultaneously avoids all of those events. Finding such an outcome by an efficient algorithm has been an active research topic for decades. Breakthrough work of Moser and Tardos (2009) presented an efficient algorithm for a general setting primarily characterized by a product structure on the probability space. #R##N#In this work we present an efficient algorithm for a much more general setting. Our main assumption is that there exist certain functions, called resampling oracles, that can be invoked to address the undesired occurrence of the events. We show that, in all scenarios to which the original Lovasz Local Lemma applies, there exist resampling oracles, although they are not necessarily efficient. Nevertheless, for essentially all known applications of the Lovasz Local Lemma and its generalizations, we have designed efficient resampling oracles. As applications of these techniques, we present new results for packings of Latin transversals, rainbow matchings and rainbow spanning trees."
      },
      {
        "node_idx": 147143,
        "score_0_10": 10,
        "title": "memetic search in differential evolution algorithm",
        "abstract": "Differential Evolution (DE) is a renowned optimization stratagem that can easily solve nonlinear and comprehensive problems. DE is a well known and uncomplicated population based probabilistic approach for comprehensive optimization. It has apparently outperformed a number of Evolutionary Algorithms and further search heuristics in the vein of Particle Swarm Optimization at what time of testing over both yardstick and actual world problems. Nevertheless, DE, like other probabilistic optimization algorithms, from time to time exhibits precipitate convergence and stagnates at suboptimal position. In order to stay away from stagnation behavior while maintaining an excellent convergence speed, an innovative search strategy is introduced, named memetic search in DE. In the planned strategy, positions update equation customized as per a memetic search stratagem. In this strategy a better solution participates more times in the position modernize procedure. The position update equation is inspired from the memetic search in artificial bee colony algorithm. The proposed strategy is named as Memetic Search in Differential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it is tested over 8 benchmark optimization problems and three real world optimization problems. A comparative analysis has also been carried out among proposed MSDE and original DE. Results show that the anticipated algorithm go one better than the basic DE and its recent deviations in a good number of the experiments."
      },
      {
        "node_idx": 101577,
        "score_0_10": 10,
        "title": "compact routing messages in self healing trees",
        "abstract": "Existing compact routing schemes, e.g., Thorup and Zwick [SPAA 2001] and Chechik [PODC 2013], often have no means to tolerate failures, once the system has been setup and started. This paper presents, to our knowledge, the first self-healing compact routing scheme. Besides, our schemes are developed for low memory nodes, i.e., nodes need only $O(\\log^2 n)$ memory, and are thus, compact schemes. #R##N#We introduce two algorithms of independent interest: The first is CompactFT, a novel compact version (using only $O(\\log n)$ local memory) of the self-healing algorithm Forgiving Tree of Hayes et al. [PODC 2008]. The second algorithm (CompactFTZ) combines CompactFT with Thorup-Zwick's tree-based compact routing scheme [SPAA 2001] to produce a fully compact self-healing routing scheme. In the self-healing model, the adversary deletes nodes one at a time with the affected nodes self-healing locally by adding few edges. CompactFT recovers from each attack in only $O(1)$ time and $\\Delta$ messages, with only +3 degree increase and $O(log \\Delta)$ graph diameter increase, over any sequence of deletions ($\\Delta$ is the initial maximum degree). #R##N#Additionally, CompactFTZ guarantees delivery of a packet sent from sender s as long as the receiver t has not been deleted, with only an additional $O(y \\log \\Delta)$ latency, where $y$ is the number of nodes that have been deleted on the path between $s$ and $t$. If $t$ has been deleted, $s$ gets informed and the packet removed from the network."
      },
      {
        "node_idx": 2852,
        "score_0_10": 10,
        "title": "termination analysis of polynomial programs with equality conditions",
        "abstract": "In this paper, we investigate the termination problem of a family of polynomial programs, in which all assignments to program variables are polynomials, and test conditions of loops and conditional statements are polynomial equations. Our main result is that the non-terminating inputs of such a polynomial program is algorithmically computable according to a strictly descending chain of algebraic sets, which implies that the termination problem of these programs is decidable. The complexity of the algorithm follows immediately from the length of the chain, which can be computed by Hilbert's function and Macaulay's theorem. To the best of our knowledge, the considered family of polynomial programs should be the largest one with a decidable termination problem so far. The experimental results indicate the efficiency of our approach."
      },
      {
        "node_idx": 16048,
        "score_0_10": 10,
        "title": "picking up the pieces self healing in reconfigurable networks",
        "abstract": "We consider the problem of self-healing in networks that are reconfigurable in the sense that they can change their topology during an attack. Our goal is to maintain connectivity in these networks, even in the presence of repeated adversarial node deletion, by carefully adding edges after each attack. We present a new algorithm, DASH, that provably ensures that: 1) the network stays connected even if an adversary deletes up to all nodes in the network; and 2) no node ever increases its degree by more than 2 log n, where n is the number of nodes initially in the network. DASH is fully distributed; adds new edges only among neighbors of deleted nodes; and has average latency and bandwidth costs that are at most logarithmic in n. DASH has these properties irrespective of the topology of the initial network, and is thus orthogonal and complementary to traditional topology- based approaches to defending against attack. We also prove lower-bounds showing that DASH is asymptotically optimal in terms of minimizing maximum degree increase over multiple attacks. Finally, we present empirical results on power-law graphs that show that DASH performs well in practice, and that it significantly outperforms naive algorithms in reducing maximum degree increase."
      },
      {
        "node_idx": 44718,
        "score_0_10": 10,
        "title": "verification of railway interlocking compositional approach with ocra",
        "abstract": "In the railway domain, an electronic interlocking is a computerised system that controls the railway signalling components (e.g. switches or signals) in order to allow a safe operation of the train traffic. Interlockings are controlled by a software logic that relies on a generic software and a set of application data particular to the station under control. The verification of the application data is time consuming and error prone as it is mostly performed by human testers. In the first stage of our research, we built a model of a small Belgian railway station and we performed the verification of the application data with the nusmv model checker. However, the verification of larger stations fails due to the state space explosion problem. The intuition is that large stations can be split into smaller components that can be verified separately. This concept is known as compositional verification. This article explains how we used the ocra tool in order to model a medium size station and how we verified safety properties by mean of contracts. We also took advantage of new algorithms (k-liveness and ic3) recently implemented in nuxmv in order to verify LTL properties on our model."
      }
    ]
  },
  "163": {
    "explanation": "adaptive learning rate optimization and transferability in deep neural networks",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 14201,
        "score_0_10": 9,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      }
    ]
  },
  "164": {
    "explanation": "dichotomy theorems and complexity classifications for quantified boolean formulas",
    "topk": [
      {
        "node_idx": 27812,
        "score_0_10": 10,
        "title": "dichotomy theorems for alternation bounded quantified boolean formulas",
        "abstract": "In 1978, Schaefer proved his famous dichotomy theorem for generalized satisfiability problems. He defined an infinite number of propositional satisfiability problems, showed that all these problems are either in P or NP-complete, and gave a simple criterion to determine which of the two cases holds. This result is surprising in light of Ladner's theorem, which implies that there are an infinite number of complexity classes between P and NP-complete (under the assumption that P is not equal to NP). #R##N#Schaefer also stated a dichotomy theorem for quantified generalized Boolean formulas, but this theorem was only recently proven by Creignou, Khanna, and Sudan, and independently by Dalmau: Determining truth of quantified Boolean formulas is either PSPACE-complete or in P. #R##N#This paper looks at alternation-bounded quantified generalized Boolean formulas. In their unrestricted forms, these problems are the canonical problems complete for the levels of the polynomial hierarchy. In this paper, we prove dichotomy theorems for alternation-bounded quantified generalized Boolean formulas, by showing that these problems are either $\\Sigma_i^p$-complete or in P, and we give a simple criterion to determine which of the two cases holds. This is the first result that obtains dichotomy for an infinite number of classes at once."
      },
      {
        "node_idx": 155778,
        "score_0_10": 10,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 142463,
        "score_0_10": 10,
        "title": "the nature of the unnormalized beliefs encountered in the transferable belief model",
        "abstract": "Within the transferable belief model, positive basic belief masses can be allocated to the empty set, leading to unnormalized belief functions. The nature of these unnormalized beliefs is analyzed."
      },
      {
        "node_idx": 128365,
        "score_0_10": 10,
        "title": "belief revision and rational inference",
        "abstract": "The (extended) AGM postulates for belief revision seem to deal with the revision of a given theory K by an arbitrary formula ', but not to constrain the revisions of two different theories by the same formula. A new postulate is proposed and compared with other similar postulates that have been proposed in the literature. The AGM revisions that satisfy this new postulate stand in one-to-one correspondence with the rational, consistency-preserving relations. This correspondence is described explicitly. Two viewpoints on iterative revisions are distinguished and discussed."
      },
      {
        "node_idx": 34657,
        "score_0_10": 10,
        "title": "consistent transformations of belief functions",
        "abstract": "Consistent belief functions represent collections of coherent or non-contradictory pieces of evidence, but most of all they are the counterparts of consistent knowledge bases in belief calculus. The use of consistent transformations cs[ ] in a reasoning process to guarantee coherence can therefore be desirable, and generalizes similar techniques in classical logics. Transformations can be obtained by minimizing an appropriate distance measure between the original belief function and the collection of consistent ones. We focus here on the case in which distances are measured using classical Lp norms, in both the \\mass space\" and the \\belief space\" representation of belief functions. While mass consistent approximations reassign the mass not focussed on a chosen element of the frame either to the whole frame or to all supersets of the element on an equal basis, approximations in the belief space do distinguish these focal elements according to the \\focussed consistent transformation\" principle. The dierent approximations are interpreted and compared, with the help of examples."
      },
      {
        "node_idx": 80789,
        "score_0_10": 9,
        "title": "adversarial satisfiability problem",
        "abstract": "We study the adversarial satisfiability problem, where the adversary can choose whether the variables are negated in clauses or not, in order to make the resulting formula unsatisfiable. This problem belongs to a general class of adversarial optimization problems that often arise in practice and are algorithmically much harder than the standard optimization problems. We use the cavity method to compute large deviations of the entropy in the random satisfiability problem with respect to the configurations of negations. We conclude that in the thermodynamic limit the best strategy the adversary can adopt is to simply balance the number of times every variable is negated and the number of times it is not negated. We also conduct a numerical study of the problem, and find that there are very strong pre-asymptotic effects that may be due to the fact that for small sizes exponential and factorial growth is hardly distinguishable. As a side result we compute the satisfiability threshold for balanced configurations of negations, and also the random regular satisfiability, i.e. when all variables belong to the same number of clauses."
      },
      {
        "node_idx": 66081,
        "score_0_10": 9,
        "title": "a new approach to updating beliefs",
        "abstract": "We define a new notion of conditional belief, which plays the same role for Dempster-Shafer belief functions as conditional probability does for probability functions. Our definition is different from the standard definition given by Dempster, and avoids many of the well-known problems of that definition. Just as the conditional probability Pr (lB) is a probability function which is the result of conditioning on B being true, so too our conditional belief function Bel (lB) is a belief function which is the result of conditioning on B being true. We define the conditional belief as the lower envelope (that is, the inf) of a family of conditional probability functions, and provide a closed form expression for it. An alternate way of understanding our definition of conditional belief is provided by considering ideas from an earlier paper [Fagin and Halpern, 1989], where we connect belief functions with inner measures. In particular, we show here how to extend the definition of conditional probability to non measurable sets, in order to get notions of inner and outer conditional probabilities, which can be viewed as best approximations to the true conditional probability, given our lack of information. Our definition of conditional belief turns out to be an exact analogue of our definition of inner conditional probability."
      },
      {
        "node_idx": 2383,
        "score_0_10": 9,
        "title": "elementary iterated revision and the levi identity",
        "abstract": "Recent work has considered the problem of extending to the case of iterated belief change the so-called `Harper Identity' (HI), which defines single-shot contraction in terms of single-shot revision. The present paper considers the prospects of providing a similar extension of the Levi Identity (LI), in which the direction of definition runs the other way. We restrict our attention here to the three classic iterated revision operators--natural, restrained and lexicographic, for which we provide here the first collective characterisation in the literature, under the appellation of `elementary' operators. We consider two prima facie plausible ways of extending (LI). The first proposal involves the use of the rational closure operator to offer a `reductive' account of iterated revision in terms of iterated contraction. The second, which doesn't commit to reductionism, was put forward some years ago by Nayak et al. We establish that, for elementary revision operators and under mild assumptions regarding contraction, Nayak's proposal is equivalent to a new set of postulates formalising the claim that contraction by $\\neg A$ should be considered to be a kind of `mild' revision by $A$. We then show that these, in turn, under slightly weaker assumptions, jointly amount to the conjunction of a pair of constraints on the extension of (HI) that were recently proposed in the literature. Finally, we consider the consequences of endorsing both suggestions and show that this would yield an identification of rational revision with natural revision. We close the paper by discussing the general prospects for defining iterated revision in terms of iterated contraction."
      },
      {
        "node_idx": 106114,
        "score_0_10": 9,
        "title": "matching networks for one shot learning",
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
      },
      {
        "node_idx": 153214,
        "score_0_10": 9,
        "title": "belief and surprise a belief function formulation",
        "abstract": "We motivate and describe a theory of belief in this paper. This theory is developed with the following view of human belief in mind. Consider the belief that an event E will occur (or has occurred or is occurring). An agent either entertains this belief or does not entertain this belief (i.e., there is no \"grade\" in entertaining the belief). If the agent chooses to exercise \"the will to believe\" and entertain this belief, he/she/it is entitled to a degree of confidence c (1 > c > 0) in doing so. Adopting this view of human belief, we conjecture that whenever an agent entertains the belief that E will occur with c degree of confidence, the agent will be surprised (to the extent c) upon realizing that E did not occur."
      }
    ]
  },
  "167": {
    "explanation": "Mathematical and computational methods for graph, process, and control systems analysis",
    "topk": [
      {
        "node_idx": 143958,
        "score_0_10": 10,
        "title": "space exploration via proximity search",
        "abstract": "We investigate what computational tasks can be performed on a point set in $\\Re^d$, if we are only given black-box access to it via nearest-neighbor search. This is a reasonable assumption if the underlying point set is either provided implicitly, or it is stored in a data structure that can answer such queries. In particular, we show the following: (A) One can compute an approximate bi-criteria $k$-center clustering of the point set, and more generally compute a greedy permutation of the point set. (B) One can decide if a query point is (approximately) inside the convex-hull of the point set. #R##N#We also investigate the problem of clustering the given point set, such that meaningful proximity queries can be carried out on the centers of the clusters, instead of the whole point set."
      },
      {
        "node_idx": 104520,
        "score_0_10": 10,
        "title": "bubbles are rational",
        "abstract": "As we show using the notion of equilibrium in the theory of infinite sequential games, bubbles and escalations are rational for economic and environmental agents, who believe in an infinite world. This goes against a vision of a self regulating, wise and pacific economy in equilibrium. In other words, in this context, equilibrium is not a synonymous of stability. We attempt to draw from this statement methodological consequences and a new approach to economics. To the mindware of economic agents (a concept due to cognitive psychology) we propose to add coinduction to properly reason on infinite games. This way we refine the notion of rationality."
      },
      {
        "node_idx": 100809,
        "score_0_10": 10,
        "title": "a unifying framework for the electrical structure based approach to pmu placement in electric power systems",
        "abstract": "The electrical structure of the power grid is utilized to address the phasor measurement unit (PMU) placement problem. First, we derive the connectivity matrix of the network using the resistance distance metric and employ it in the linear program formulation to obtain the optimal number of PMUs, for complete network observability without zero injection measurements. This approach was developed by the author in an earlier work, but the solution methodology to address the location problem did not fully utilize the electrical properties of the network, resulting in an ambiguity. In this paper, we settle this issue by exploiting the coupling structure of the grid derived using the singular value decomposition (SVD)-based analysis of the resistance distance matrix to solve the location problem. Our study, which is based on recent advances in complex networks that promote the electrical structure of the grid over its topological structure and the SVD analysis which throws light on the electrical coupling of the network, results in a unified framework for the electrical structure-based PMU placement. The proposed method is tested on IEEE bus systems, and the results uncover intriguing connections between the singular vectors and average resistance distance between buses in the network."
      },
      {
        "node_idx": 159367,
        "score_0_10": 10,
        "title": "natural deduction and the isabelle proof assistant",
        "abstract": "We describe our Natural Deduction Assistant (NaDeA) and the interfaces between the Isabelle proof assistant and NaDeA. In particular, we explain how NaDeA, using a generated prover that has been verified in Isabelle, provides feedback to the student, and also how NaDeA, for each formula proved by the student, provides a generated theorem that can be verified in Isabelle."
      },
      {
        "node_idx": 117988,
        "score_0_10": 10,
        "title": "electrical reduction homotopy moves and defect",
        "abstract": "We prove the first nontrivial worst-case lower bounds for two closely related problems. First, $\\Omega(n^{3/2})$ degree-1 reductions, series-parallel reductions, and $\\Delta$Y transformations are required in the worst case to reduce an $n$-vertex plane graph to a single vertex or edge. The lower bound is achieved by any planar graph with treewidth $\\Theta(\\sqrt{n})$. Second, $\\Omega(n^{3/2})$ homotopy moves are required in the worst case to reduce a closed curve in the plane with $n$ self-intersection points to a simple closed curve. For both problems, the best upper bound known is $O(n^2)$, and the only lower bound previously known was the trivial $\\Omega(n)$. #R##N#The first lower bound follows from the second using medial graph techniques ultimately due to Steinitz, together with more recent arguments of Noble and Welsh [J. Graph Theory 2000]. The lower bound on homotopy moves follows from an observation by Haiyashi et al. [J. Knot Theory Ramif. 2012] that the standard projections of certain torus knots have large defect, a topological invariant of generic closed curves introduced by Aicardi and Arnold. Finally, we prove that every closed curve in the plane with $n$ crossings has defect $O(n^{3/2})$, which implies that better lower bounds for our algorithmic problems will require different techniques."
      },
      {
        "node_idx": 77754,
        "score_0_10": 10,
        "title": "untangling planar curves",
        "abstract": "Any generic closed curve in the plane can be transformed into a simple closed curve by a finite sequence of local transformations called homotopy moves. We prove that simplifying a planar closed curve with $n$ self-crossings requires $\\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the best previous upper bound $O(n^2)$, which is already implicit in the classical work of Steinitz; the matching lower bound follows from the construction of closed curves with large defect, a topological invariant of generic closed curves introduced by Aicardi and Arnold. Our lower bound also implies that $\\Omega(n^{3/2})$ facial electrical transformations are required to reduce any plane graph with treewidth $\\Omega(\\sqrt{n})$ to a single vertex, matching known upper bounds for rectangular and cylindrical grid graphs. More generally, we prove that transforming one immersion of $k$ circles with at most $n$ self-crossings into another requires $\\Theta(n^{3/2} + nk + k^2)$ homotopy moves in the worst case. Finally, we prove that transforming one noncontractible closed curve to another on any orientable surface requires $\\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if the curve is homotopic to a simple closed curve."
      },
      {
        "node_idx": 34422,
        "score_0_10": 10,
        "title": "momentum control with hierarchical inverse dynamics on a torque controlled humanoid",
        "abstract": "Hierarchical inverse dynamics based on cascades of quadratic programs have been proposed for the control of legged robots. They have important benefits but to the best of our knowledge have never been implemented on a torque controlled humanoid where model inaccuracies, sensor noise and real-time computation requirements can be problematic. Using a reformulation of existing algorithms, we propose a simplification of the problem that allows to achieve real-time control. Momentum-based control is integrated in the task hierarchy and a LQR design approach is used to compute the desired associated closed-loop behavior and improve performance. Extensive experiments on various balancing and tracking tasks show very robust performance in the face of unknown disturbances, even when the humanoid is standing on one foot. Our results demonstrate that hierarchical inverse dynamics together with momentum control can be efficiently used for feedback control under real robot conditions."
      },
      {
        "node_idx": 90838,
        "score_0_10": 10,
        "title": "investigating the process of process modeling with eye movement analysis",
        "abstract": "Research on quality issues of business process models has recently begun to explore the process of creating process models by analyzing the modeler's interactions with the modeling environment. In this paper we aim to complement previous insights on the modeler's modeling behavior with data gathered by tracking the modeler's eye movements when engaged in the act of modeling. We present preliminary results and outline directions for future research to triangulate toward a more comprehensive understanding of the process of process modeling. We believe that combining different views on the process of process modeling constitutes another building block in understanding this process that will ultimately enable us to support modelers in creating better process models."
      },
      {
        "node_idx": 161658,
        "score_0_10": 9,
        "title": "tying process model quality to the modeling process the impact of structuring movement and speed",
        "abstract": "In an investigation into the process of process modeling, we examined how modeling behavior relates to the quality of the process model that emerges from that. Specifically, we considered whether (i) a modeler's structured modeling style, (ii) the frequency of moving existing objects over the modeling canvas, and (iii) the overall modeling speed is in any way connected to the ease with which the resulting process model can be understood. In this paper, we describe the exploratory study to build these three conjectures, clarify the experimental set-up and infrastructure that was used to collect data, and explain the used metrics for the various concepts to test the conjectures empirically. We discuss various implications for research and practice from the conjectures, all of which were confirmed by the experiment."
      },
      {
        "node_idx": 75383,
        "score_0_10": 9,
        "title": "a categorical view on algebraic lattices in formal concept analysis",
        "abstract": "Formal concept analysis has grown from a new branch of the mathematical field of lattice theory to a widely recognized tool in Computer Science and elsewhere. In order to fully benefit from this theory, we believe that it can be enriched with notions such as approximation by computation or representability. The latter are commonly studied in denotational semantics and domain theory and captured most prominently by the notion of algebraicity, e.g. of lattices. In this paper, we explore the notion of algebraicity in formal concept analysis from a category-theoretical perspective. To this end, we build on the the notion of approximable concept with a suitable category and show that the latter is equivalent to the category of algebraic lattices. At the same time, the paper provides a relatively comprehensive account of the representation theory of algebraic lattices in the framework of Stone duality, relating well-known structures such as Scott information systems with further formalisms from logic, topology, domains and lattice theory."
      }
    ]
  },
  "168": {
    "explanation": "deep supervised hashing and feature learning for image retrieval",
    "topk": [
      {
        "node_idx": 63877,
        "score_0_10": 10,
        "title": "feature learning based deep supervised hashing with pairwise labels",
        "abstract": "Recent years have witnessed wide application of hashing for large-scale image retrieval. However, most existing hashing methods are based on handcrafted features which might not be optimally compatible with the hashing procedure. Recently, deep hashing methods have been proposed to perform simultaneous feature learning and hash-code learning with deep neural networks, which have shown better performance than traditional hashing methods with hand-crafted features. Most of these deep hashing methods are supervised whose supervised information is given with triplet labels. For another common application scenario with pairwise labels, there have not existed methods for simultaneous feature learning and hash-code learning. In this paper, we propose a novel deep hashing method, called deep pairwise-supervised hashing (DPSH), to perform simultaneous feature learning and hashcode learning for applications with pairwise labels. Experiments on real datasets show that our DPSH method can outperform other methods to achieve the state-of-the-art performance in image retrieval applications."
      },
      {
        "node_idx": 44027,
        "score_0_10": 10,
        "title": "barcode annotations for medical image retrieval a preliminary investigation",
        "abstract": "This paper proposes to generate and to use barcodes to annotate medical images and/or their regions of interest such as organs, tumors and tissue types. A multitude of efficient feature-based image retrieval methods already exist that can assign a query image to a certain image class. Visual annotations may help to increase the retrieval accuracy if combined with existing feature-based classification paradigms. Whereas with annotations we usually mean textual descriptions, in this paper barcode annotations are proposed. In particular, Radon barcodes (RBC) are introduced. As well, local binary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as barcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 test images is used to verify how barcodes could facilitate image retrieval."
      },
      {
        "node_idx": 39924,
        "score_0_10": 10,
        "title": "simultaneous feature learning and hash coding with deep neural networks",
        "abstract": "Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods."
      },
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 30510,
        "score_0_10": 9,
        "title": "return of the devil in the details delving deep into convolutional nets",
        "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available."
      },
      {
        "node_idx": 150473,
        "score_0_10": 9,
        "title": "star unfolding convex polyhedra via quasigeodesic loops",
        "abstract": "We extend the notion of star unfolding to be based on a quasigeodesic loop Q rather than on a point. This gives a new general method to unfold the surface of any convex polyhedron P to a simple (non-overlapping), planar polygon: cut along one shortest path from each vertex of P to Q, and cut all but one segment of Q."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 112726,
        "score_0_10": 9,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 84579,
        "score_0_10": 9,
        "title": "cameras viewing cameras geometry",
        "abstract": "A basic problem in computer vision is to understand the structure of a real-world scene given several images of it. Here we study several theoretical aspects of the intra multi-view geometry of calibrated cameras when all that they can reliably recognize is each other. With the proliferation of wearable cameras, autonomous vehicles and drones, the geometry of these multiple cameras is a timely and relevant problem to study."
      }
    ]
  },
  "169": {
    "explanation": "error correction codes and quantum error correction advances",
    "topk": [
      {
        "node_idx": 119073,
        "score_0_10": 10,
        "title": "ieee 802 11ah the wi fi approach for m2m communications",
        "abstract": "M2M communications are projected to be one of the fastest growing technology segments of the IT sector in the next years. Sensor and actuator networks connect communication machines and devices so that they automatically transmit information, serving the growing demand for environmental data acquisition. IEEE 802.11ah Task Group addresses the creation of a new standard for giving response to the particular requirements of this type of networks: large number of power-constrained stations, long transmission range, small and infrequent data messages, low data-rates and non-critical delay. This article explores the key features of this new standard under development, especially those related to the reduction of energy consumption in the MAC Layer. In this direction, a performance assessment of IEEE 802.11ah in four typical M2M scenarios has been performed."
      },
      {
        "node_idx": 162288,
        "score_0_10": 9,
        "title": "a construction of new quantum mds codes",
        "abstract": "It has been a great challenge to construct new quantum MDS codes. In particular, it is very hard to construct quantum MDS codes with relatively large minimum distance. So far, except for some sparse lengths, all known $q$-ary quantum MDS codes have minimum distance less than or equal to $q/2+1$. In the present paper, we provide a construction of quantum MDS codes with minimum distance bigger than $q/2+1$. In particular, we show existence of $q$-ary quantum MDS codes with length $n=q^2+1$ and minimum distance $d$ for any $d\\le q+1$ (this result extends those given in \\cite{Gu11,Jin1,KZ12}); and with length $(q^2+2)/3$ and minimum distance $d$ for any $d\\le (2q+2)/3$ if $3|(q+1)$. Our method is through Hermitian self-orthogonal codes. The main idea of constructing Hermitian self-orthogonal codes is based on the solvability in $\\F_q$ of a system of homogenous equations over $\\F_{q^2}$."
      },
      {
        "node_idx": 71804,
        "score_0_10": 9,
        "title": "constacyclic codes over finite fields",
        "abstract": "Abstract   An equivalence relation called isometry is introduced to classify constacyclic codes over a finite field; the polynomial generators of constacyclic codes of length      l    t      p    s      are characterized, where  p  is the characteristic of the finite field and  l  is a prime different from  p ."
      },
      {
        "node_idx": 130093,
        "score_0_10": 9,
        "title": "towards efficient coexistence of ieee 802 15 4e tsch and ieee 802 11",
        "abstract": "A major challenge in wide deployment of smart wireless devices, using different technologies and sharing the same 2.4 GHz spectrum, is to achieve coexistence across multiple technologies. The IEEE~802.11 (WLAN) and the IEEE 802.15.4e TSCH (WSN) where designed with different goals in mind and both play important roles for respective applications. However, they cause mutual interference and degraded performance while operating in the same space. To improve this situation we propose an approach to enable a cooperative control which type of network is transmitting at given time, frequency and place. #R##N#We recognize that TSCH based sensor network is expected to occupy only small share of time, and that the nodes are by design tightly synchronized. We develop mechanism enabling over-the-air synchronization of the Wi-Fi network to the TSCH based sensor network. Finally, we show that Wi-Fi network can avoid transmitting in the \"collision periods\". We provide full design and show prototype implementation based on the Commercial off-the-shelf (COTS) devices. Our solution does not require changes in any of the standards."
      },
      {
        "node_idx": 41252,
        "score_0_10": 9,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 68571,
        "score_0_10": 9,
        "title": "enhanced public key security for the mceliece cryptosystem",
        "abstract": "This paper studies a variant of the McEliece cryptosystem able to ensure that the code used as the public key is no longer permutation-equivalent to the secret code. This increases the security level of the public key, thus opening the way for reconsidering the adoption of classical families of codes, like Reed-Solomon codes, that have been longly excluded from the McEliece cryptosystem for security reasons. It is well known that codes of these classes are able to yield a reduction in the key size or, equivalently, an increased level of security against information set decoding; so, these are the main advantages of the proposed solution. We also describe possible vulnerabilities and attacks related to the considered system, and show what design choices are best suited to avoid them."
      },
      {
        "node_idx": 48329,
        "score_0_10": 9,
        "title": "entanglement assisted quantum mds codes from constacyclic codes with large minimum distance",
        "abstract": "The entanglement-assisted (EA) formalism allows arbitrary classical linear codes to transform into entanglement-assisted quantum error correcting codes (EAQECCs) by using pre-shared entanglement between the sender and the receiver. In this work, we propose a decomposition of the defining set of constacyclic codes. Using this method, we construct four classes of $q$-ary entanglement-assisted quantum MDS (EAQMDS) codes based on classical constacyclic MDS codes by exploiting less pre-shared maximally entangled states. We show that a class of $q$-ary EAQMDS have minimum distance upper limit greater than $3q-1$. Some of them have much larger minimum distance than the known quantum MDS (QMDS) codes of the same length. Most of these $q$-ary EAQMDS codes are new in the sense that their parameters are not covered by the codes available in the literature."
      },
      {
        "node_idx": 39332,
        "score_0_10": 9,
        "title": "xoring elephants novel erasure codes for big data",
        "abstract": "Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. #R##N#This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. #R##N#We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication."
      },
      {
        "node_idx": 49429,
        "score_0_10": 9,
        "title": "constructions of good entanglement assisted quantum error correcting codes",
        "abstract": "Entanglement-assisted quantum error correcting codes (EAQECCs) are a simple and fundamental class of codes. They allow for the construction of quantum codes from classical codes by relaxing the duality condition and using pre-shared entanglement between the sender and receiver. However, in general it is not easy to determine the number of shared pairs required to construct an EAQECC. In this paper, we show that this number is related to the hull of the classical code. Using this fact, we give methods to construct EAQECCs requiring desirable amount of entanglement. This leads to design families of EAQECCs with good error performance. Moreover, we construct maximal entanglement EAQECCs from LCD codes. Finally, we prove the existence of asymptotically good EAQECCs in the odd characteristic case."
      },
      {
        "node_idx": 110365,
        "score_0_10": 9,
        "title": "some new constructions of quantum mds codes",
        "abstract": "It is an important task to construct quantum MDS codes with good parameters. In the present paper, we provide six new classes of $q$-ary quantum MDS codes by using generalized Reed-Solomon codes and Hermitian construction. Most of our quantum MDS codes have minimum distance larger than $\\frac{q}{2}+1$. Three of these six classes of quantum MDS codes have larger length than the ones constructed in \\cite{5} and \\cite{6}, hence some of their results can be easily derived from ours via the propagation rule. Moreover, some quantum MDS codes of specific length can be seen as special cases of ours and the minimum distance of some known quantum MDS codes are also improved."
      }
    ]
  },
  "170": {
    "explanation": "hierarchical and multi-agent optimization for robotics and clustering",
    "topk": [
      {
        "node_idx": 87336,
        "score_0_10": 10,
        "title": "2 player nash and nonsymmetric bargaining via flexible budget markets",
        "abstract": "The solution to a Nash or a nonsymmetric bargaining game is obtained by maximizing a concave function over a convex set, i.e., it is the solution to a convex program. We show that each 2-player game whose convex program has linear constraints, admits a rational solution and such a solution can be found in polynomial time using only an LP solver. If in addition, the game is succinct, i.e., the coefficients in its convex program are ``small'', then its solution can be found in strongly polynomial time. We also give a non-succinct linear game whose solution can be found in strongly polynomial time. The notion of flexible budget markets, introduced in \\cite{va.NB}, plays a crucial role in the design of these algorithms."
      },
      {
        "node_idx": 19033,
        "score_0_10": 10,
        "title": "sliding suffix tree",
        "abstract": "We consider a sliding window over a stream of characters from some finite alphabet. The user wants to perform deterministic substring matching on the current sliding window content and obtain positions of the matches. We present an indexed version of the sliding window based on a suffix tree. The data structure has optimal time queries $\\Theta(m+occ)$ and amortized constant time updates, where $m$ is the length of the query string and $occ$ the number of occurrences."
      },
      {
        "node_idx": 42000,
        "score_0_10": 10,
        "title": "hierarchical clustering via spreading metrics",
        "abstract": "We study the cost function for hierarchical clusterings introduced by [arXiv:1510.05043] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [arXiv:1510.05043] that a top-down algorithm returns a hierarchical clustering of cost at most $O\\left(\\alpha_n \\log n\\right)$ times the cost of the optimal hierarchical clustering, where $\\alpha_n$ is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top down algorithm returns a hierarchical clustering of cost at most $O\\left(\\log^{3/2} n\\right)$ times the cost of the optimal solution. We improve this by giving an $O(\\log{n})$-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of \\emph{sphere growing} which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an $O(\\log{n})$-approximate hierarchical clustering for a generalization of this cost function also studied in [arXiv:1510.05043]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We also give constant factor inapproximability results for this problem."
      },
      {
        "node_idx": 32463,
        "score_0_10": 10,
        "title": "pose estimation of vehicles over uneven terrain",
        "abstract": "This paper presents a method for pose estimation of off-road vehicles moving over uneven terrain. It determines the contact points between the wheels and the terrain, assuming rigid contacts between an arbitrary number of wheels and ground. The terrain is represented by a 3D points cloud, interpolated by a B-patch to provide a continuous terrain representation. The pose estimation problem is formulated as a rigid body contact problem for a given location of the vehicle's center of mass over the terrain and a given yaw angle. The contact points between the wheels and ground are determined by releasing the vehicle from a given point above the terrain, until the contact forces between the wheels and ground, and the gravitational force, reach equilibrium. The contact forces are calculated using singular value decomposition (SVD) of the deduced contact matrix. The proposed method is computationally efficient, allowing real time computation during motion, as demonstrated in several examples. Accurate pose estimations can be used for motion planning, stability analyses and traversability analyses over uneven terrain."
      },
      {
        "node_idx": 9810,
        "score_0_10": 10,
        "title": "range closest pair search in higher dimensions",
        "abstract": "Range closest-pair (RCP) search is a range-search variant of the classical closest-pair problem, which aims to store a given set $S$ of points into some space-efficient data structure such that when a query range $Q$ is specified, the closest pair in $S \\cap Q$ can be reported quickly. RCP search has received attention over years, but the primary focus was only on $\\mathbb{R}^2$. In this paper, we study RCP search in higher dimensions. We give the first nontrivial RCP data structures for orthogonal, simplex, halfspace, and ball queries in $\\mathbb{R}^d$ for any constant $d$. Furthermore, we prove a conditional lower bound for orthogonal RCP search for $d \\geq 3$."
      },
      {
        "node_idx": 25550,
        "score_0_10": 10,
        "title": "a decentralized multi agent unmanned aerial system to search pick up and relocate objects",
        "abstract": "We present a fully integrated autonomous multi\u00adrobot aerial system for finding and collecting moving and static objects with unknown locations. This task addresses multiple relevant problems in search and rescue (SAR) robotics such as multi-agent aerial exploration, object detection and tracking, and aerial gripping. Usually, the community tackles these problems individually but the integration into a working system generates extra complexity which is rarely addressed. We show that this task can be solved reliably using only simple components. Our decentralized system uses accurate global state estimation, reactive collision avoidance, and sweep plan\u00adning for multi-agent exploration. Objects are detected, tracked, and picked up using blob detection, inverse 3D-projection, Kalman filtering, visual-servoing, and a magnetic gripper. We evaluate the individual components of our system on the real platform. The full system has been deployed successfully in various public demonstrations, field tests, and the Mohamed Bin Zayed International Robotics Challenge 2017 (MBZIRC). Among the contestants we showed reliable performances and reached second place out of 17 in the individual challenge."
      },
      {
        "node_idx": 106462,
        "score_0_10": 10,
        "title": "development of a steel bridge climbing robot",
        "abstract": "Motivated by a high demand for automated inspection of civil infrastructure, this work presents a new design and development of a tank-like robot for structural health monitoring. Unlike most existing magnetic wheeled mobile robot designs, which may be suitable for climbing on flat steel surface, our proposed tank-like robot design uses reciprocating mechanism and roller-chains to make it capable of climbing on different structural shapes (e.g., cylinder, cube) with coated or non-coated steel surfaces. The proposed robot is able to transition from one surface to the other (e.g., from flat surface to curving surface).  Taking into account of several strict considerations (including tight dimension, efficient adhesion and climbing flexibility) to adapt with various shapes of steel structures, a prototype tank-like robot incorporating multiple sensors (hall-effects, sonars, inertial measurement unit and camera), has been developed. Rigorous analysis of robot kinematics, adhesion force, sliding failure and turn-over failure has been conducted to demonstrate the stability of the proposed design. Mechanical and magnetic force analysis together with sliding/turn-over failure investigation can serve as an useful framework for designing various steel climbing robots in the future. Experimental results and field deployments confirm the adhesion and climbing capability of the developed robot."
      },
      {
        "node_idx": 34633,
        "score_0_10": 10,
        "title": "a cost function for similarity based hierarchical clustering",
        "abstract": "The development of algorithms for hierarchical clustering has been hampered by a shortage of precise objective functions. To help address this situation, we introduce a simple cost function on hierarchies over a set of points, given pairwise similarities between those points. We show that this criterion behaves sensibly in canonical instances and that it admits a top-down construction procedure with a provably good approximation ratio."
      },
      {
        "node_idx": 34891,
        "score_0_10": 10,
        "title": "simultaneous contact gait and motion planning for robust multi legged locomotion via mixed integer convex optimization",
        "abstract": "Traditional motion planning approaches for multilegged locomotion divide the problem into several stages, such as contact search and trajectory generation. However, reasoning about contacts and motions simultaneously is crucial for the generation of complex whole-body behaviors. Currently, coupling theses problems has required either the assumption of a fixed gait sequence and flat terrain condition, or nonconvex optimization with intractable computation time. In this letter, we propose a mixed-integer convex formulation to plan simultaneously contact locations, gait transitions, and motion, in a computationally efficient fashion. In contrast to previous works, our approach is not limited to flat terrain nor to a prespecified gait sequence. Instead, we incorporate the friction cone stability margin, approximate the robot's torque limits, and plan the gait using mixed-integer convex constraints. We experimentally validated our approach on the HyQ robot by traversing different challenging terrains, where nonconvexity and flat terrain assumptions might lead to suboptimal or unstable plans. Our method increases the motion robustness while keeping a low computation time."
      },
      {
        "node_idx": 93009,
        "score_0_10": 10,
        "title": "asymptotically stable walking of a five link underactuated 3 d bipedal robot",
        "abstract": "This paper presents three feedback controllers that achieve an asymptotically stable, periodic, and fast walking gait for a 3-D bipedal robot consisting of a torso, revolute knees, and passive (unactuated) point feet. The walking surface is assumed to be rigid and flat; the contact between the robot and the walking surface is assumed to inhibit yaw rotation. The studied robot has 8 DOF in the single support phase and six actuators. In addition to the reduced number of actuators, the interest of studying robots with point feet is that the feedback control solution must explicitly account for the robot's natural dynamics in order to achieve balance while walking. We use an extension of the method of virtual constraints and hybrid zero dynamics (HZD), a very successful method for planar bipeds, in order to simultaneously compute a periodic orbit and an autonomous feedback controller that realizes the orbit, for a 3-D (spatial) bipedal walking robot. This method allows the computations for the controller design and the periodic orbit to be carried out on a 2-DOF subsystem of the 8-DOF robot model. The stability of the walking gait under closed-loop control is evaluated with the linearization of the restricted Poincare map of the HZD. Most periodic walking gaits for this robot are unstable when the controlled outputs are selected to be the actuated coordinates. Three strategies are explored to produce stable walking. The first strategy consists of imposing a stability condition during the search of a periodic gait by optimization. The second strategy uses an event-based controller to modify the eigenvalues of the (linearized) Poincare map. In the third approach, the effect of output selection on the zero dynamics is discussed and a pertinent choice of outputs is proposed, leading to stabilization without the use of a supplemental event-based controller."
      }
    ]
  },
  "171": {
    "explanation": "adaptive learning rate optimization for neural networks",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 15365,
        "score_0_10": 10,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 151734,
        "score_0_10": 10,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 14201,
        "score_0_10": 9,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      }
    ]
  },
  "173": {
    "explanation": "multi-scale context aggregation and spatial transformation for image recognition",
    "topk": [
      {
        "node_idx": 151734,
        "score_0_10": 10,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 14201,
        "score_0_10": 9,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      },
      {
        "node_idx": 103461,
        "score_0_10": 9,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 80881,
        "score_0_10": 9,
        "title": "objective criteria for the evaluation of clustering methods",
        "abstract": "Abstract Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering."
      }
    ]
  },
  "181": {
    "explanation": "author-topic model for collaborative document generation",
    "topk": [
      {
        "node_idx": 109276,
        "score_0_10": 10,
        "title": "the author topic model for authors and documents",
        "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output."
      },
      {
        "node_idx": 105042,
        "score_0_10": 10,
        "title": "convexity shape prior for level set based image segmentation method",
        "abstract": "We propose a geometric convexity shape prior preservation method for variational level set based image segmentation methods. Our method is built upon the fact that the level set of a convex signed distanced function must be convex. This property enables us to transfer a complicated geometrical convexity prior into a simple inequality constraint on the function. An active set based Gauss-Seidel iteration is used to handle this constrained minimization problem to get an efficient algorithm. We apply our method to region and edge based level set segmentation models including Chan-Vese (CV) model with guarantee that the segmented region will be convex. Experimental results show the effectiveness and quality of the proposed model and algorithm."
      },
      {
        "node_idx": 85599,
        "score_0_10": 10,
        "title": "ontology based query expansion with latently related named entities for semantic text search",
        "abstract": "Traditional information retrieval systems represent documents and queries by keyword sets. However, the content of a document or a query is mainly defined by both keywords and named entities occurring in it. Named entities have ontological features, namely, their aliases, classes, and identifiers, which are hidden from their textual appearance. Besides, the meaning of a query may imply latent named entities that are related to the apparent ones in the query. We propose an ontology-based generalized vector space model to semantic text search. It exploits ontological features of named entities and their latently related ones to reveal the semantics of documents and queries. We also propose a framework to combine different ontologies to take their complementary advantages for semantic annotation and searching. Experiments on a benchmark dataset show better search quality of our model to other ones."
      },
      {
        "node_idx": 20225,
        "score_0_10": 10,
        "title": "texture and color based image retrieval using the local extrema features and riemannian distance",
        "abstract": "A novel efficient method for content-based image retrieval (CBIR) is developed in this paper using both texture and color features. Our motivation is to represent and characterize an input image by a set of local descriptors extracted at characteristic points (i.e. keypoints) within the image. Then, dissimilarity measure between images is calculated based on the geometric distance between the topological feature spaces (i.e. manifolds) formed by the sets of local descriptors generated from these images. In this work, we propose to extract and use the local extrema pixels as our feature points. Then, the so-called local extrema-based descriptor (LED) is generated for each keypoint by integrating all color, spatial as well as gradient information captured by a set of its nearest local extrema. Hence, each image is encoded by a LED feature point cloud and riemannian distances between these point clouds enable us to tackle CBIR. Experiments performed on Vistex, Stex and colored Brodatz texture databases using the proposed approach provide very efficient and competitive results compared to the state-of-the-art methods."
      },
      {
        "node_idx": 166750,
        "score_0_10": 10,
        "title": "texture retrieval via the scattering transform",
        "abstract": "This work studies the problem of content-based image retrieval, specifically, texture retrieval. It focuses on feature extraction and similarity measure for texture images. Our approach employs a recently developed method, the so-called Scattering transform, for the process of feature extraction in texture retrieval. It shares a distinctive property of providing a robust representation, which is stable with respect to spatial deformations. Recent work has demonstrated its capability for texture classification, and hence as a promising candidate for the problem of texture retrieval. #R##N#Moreover, we adopt a common approach of measuring the similarity of textures by comparing the subband histograms of a filterbank transform. To this end we derive a similarity measure based on the popular Bhattacharyya Kernel. Despite the popularity of describing histograms using parametrized probability density functions, such as the Generalized Gaussian Distribution, it is unfortunately not applicable for describing most of the Scattering transform subbands, due to the complex modulus performed on each one of them. In this work, we propose to use the Weibull distribution to model the Scattering subbands of descendant layers. #R##N#Our numerical experiments demonstrated the effectiveness of the proposed approach, in comparison with several state of the arts."
      },
      {
        "node_idx": 13987,
        "score_0_10": 9,
        "title": "augmented segmentation and visualization for presentation videos",
        "abstract": "We investigate methods of segmenting, visualizing, and indexing presentation videos by separately considering audio and visual data. The audio track is segmented by speaker, and augmented with key phrases which are extracted using an Automatic Speech Recognizer (ASR). The video track is segmented by visual dissimilarities and augmented by representative key frames. An interactive user interface combines a visual representation of audio, video, text, and key frames, and allows the user to navigate a presentation video. We also explore clustering and labeling of speaker data and present preliminary results."
      },
      {
        "node_idx": 61035,
        "score_0_10": 9,
        "title": "interactive visual exploration of topic models using graphs",
        "abstract": "Probabilistic topic modeling is a popular and powerful family of tools for uncovering thematic structure in large sets of unstructured text documents. While much attention has been directed towards the modeling algorithms and their various extensions, comparatively few studies have concerned how to present or visualize topic models in meaningful ways. In this paper, we present a novel design that uses graphs to visually communicate topic structure and meaning. By connecting topic nodes via descriptive keyterms, the graph representation reveals topic similarities, topic meaning and shared, ambiguous keyterms. At the same time, the graph can be used for information retrieval purposes, to find documents by topic or topic subsets. To exemplify the utility of the design, we illustrate its use for organizing and exploring corpora of financial patents."
      },
      {
        "node_idx": 82565,
        "score_0_10": 9,
        "title": "internal universes in models of homotopy type theory",
        "abstract": "We show that universes of fibrations in various models of homotopy type theory have an essentially global character: they cannot be described in the internal language of the presheaf topos from which the model is constructed. We get around this problem by extending the internal language with a modal operator for expressing properties of global elements. In this setting we show how to construct a universe that classifies the Cohen-Coquand-Huber-M\\\"ortberg (CCHM) notion of fibration from their cubical sets model, starting from the assumption that the interval is tiny - a property that the interval in cubical sets does indeed have. This leads to a completely internal development of models of homotopy type theory within what we call crisp type theory."
      },
      {
        "node_idx": 74506,
        "score_0_10": 9,
        "title": "analysis and visualization of index words from audio transcripts of instructional videos",
        "abstract": "We introduce new techniques for extracting, analyzing, and visualizing textual contents from instructional videos of low production quality. Using automatic speech recognition, approximate transcripts (/spl ap/75% word error rate) are obtained from the originally highly compressed videos of university courses, each comprising between 10 to 30 lectures. Text material in the form of books or papers that accompany the course are then used to filter meaningful phrases from the seemingly incoherent transcripts. The resulting index into the transcripts is tied together and visualized in 3 experimental graphs that help in understanding the overall course structure and provide a tool for localizing certain topics for indexing. We specifically discuss a transcript index map, which graphically lays out key phrases for a course, a textbook chapter to transcript match, and finally a lecture transcript similarity graph, which clusters semantically similar lectures. We test our methods and tools on 7 full courses with 230 hours of video and 273 transcripts. We are able to extract up to 98 unique key terms for a given transcript and up to 347 unique key terms for an entire course. The accuracy of the Textbook Chapter to Transcript Match exceeds 70% on average. The methods used can be applied to genres of video in which there are recurrent thematic words (news, sports, meetings, etc.)."
      },
      {
        "node_idx": 130510,
        "score_0_10": 9,
        "title": "analysis and interface for instructional video",
        "abstract": "We present a new method for segmenting, and a new user interface for indexing and visualizing, the semantic content of extended instructional videos. Using various visual filters, key frames are first assigned a media type (board, class, computer, illustration, podium, and sheet). Key frames of media type board and sheet are then clustered based on contents via an algorithm with near-linear cost. A novel user interface, the result of two user studies, displays related topics using icons linked topologically, allowing users to quickly locate semantically related portions of the video. We analyze the accuracy of the segmentation tool on 17 instructional videos, each of which is from 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%."
      }
    ]
  },
  "182": {
    "explanation": "wireless relay and broadcast channel capacity optimization techniques",
    "topk": [
      {
        "node_idx": 79165,
        "score_0_10": 10,
        "title": "optimal beamforming for two way multi antenna relay channel with analogue network coding",
        "abstract": "This paper studies the wireless two-way relay channel (TWRC), where two source nodes, S1 and S2, exchange information through an assisting relay node, R. It is assumed that R receives the sum signal from S1 and S2 in one time-slot, and then amplifies and forwards the received signal to both S1 and S2 in the next time-slot. By applying the principle of analogue network (ANC), each of S1 and S2 cancels the so-called \"self-interference\" in the received signal from R and then decodes the desired message. Assuming that S1 and S2 are each equipped with a single antenna and R with multi-antennas, this paper analyzes the capacity region of an ANC-based TWRC with linear processing (beamforming) at R. The capacity region contains all the achievable bidirectional rate-pairs of S1 and S2 under the given transmit power constraints at S1, S2, and R. We present the optimal relay beamforming structure as well as an efficient algorithm to compute the optimal beamforming matrix based on convex optimization techniques. Low-complexity suboptimal relay beamforming schemes are also presented, and their achievable rates are compared against the capacity with the optimal scheme."
      },
      {
        "node_idx": 130596,
        "score_0_10": 10,
        "title": "cooperative relay broadcast channels",
        "abstract": "The capacity regions are investigated for two relay broadcast channels (RBCs), where relay links are incorporated into standard two-user broadcast channels to support user cooperation. In the first channel, the Partially Cooperative Relay Broadcast Channel, only one user in the system can act as a relay and transmit to the other user through a relay link. An achievable rate region is derived based on the relay using the decode-and-forward scheme. An outer bound on the capacity region is derived and is shown to be tighter than the cut-set bound. For the special case where the Partially Cooperative RBC is degraded, the achievable rate region is shown to be tight and provides the capacity region. Gaussian Partially Cooperative RBCs and Partially Cooperative RBCs with feedback are further studied. In the second channel model being studied in the paper, the Fully Cooperative Relay Broadcast Channel, both users can act as relay nodes and transmit to each other through relay links. This is a more general model than the Partially Cooperative RBC. All the results for Partially Cooperative RBCs are correspondingly generalized to the Fully Cooperative RBCs. It is further shown that the AWGN Fully Cooperative RBC has a larger achievable rate region than the AWGN Partially Cooperative RBC. The results illustrate that relaying and user cooperation are powerful techniques in improving the capacity of broadcast channels."
      },
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 100650,
        "score_0_10": 9,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 53332,
        "score_0_10": 9,
        "title": "towards the secrecy capacity of the gaussian mimo wire tap channel the 2 2 1 channel",
        "abstract": "We find the secrecy capacity of the 2-2-1 Gaussian MIMO wiretap channel, which consists of a transmitter and a receiver with two antennas each, and an eavesdropper with a single antenna. We determine the secrecy capacity of this channel by proposing an achievable scheme and then developing a tight upper bound that meets the proposed achievable secrecy rate. We show that, for this channel, Gaussian signalling in the form of beam-forming is optimal, and no pre-processing of information is necessary."
      },
      {
        "node_idx": 157681,
        "score_0_10": 9,
        "title": "degrees of freedom of time correlated miso broadcast channel with delayed csit",
        "abstract": "We consider the time correlated multiple-input single-output (MISO) broadcast channel where the transmitter has imperfect knowledge of the current channel state, in addition to delayed channel state information. By representing the quality of the current channel state information as P-\u03b1 for the signal-to-noise ratio P and some constant \u03b1 \u2265 0, we characterize the optimal degrees of freedom region for this more general two-user MISO broadcast correlated channel. The essential ingredients of the proposed scheme lie in the quantization and multicast of the overheard interferences, while broadcasting new private messages. Our proposed scheme smoothly bridges between the scheme recently proposed by Maddah-Ali and Tse with no current state information and a simple zero-forcing beamforming with perfect current state information."
      },
      {
        "node_idx": 2884,
        "score_0_10": 9,
        "title": "sum rate maximization for linearly precoded downlink multiuser miso systems with partial csit a rate splitting approach",
        "abstract": "This paper considers the Sum-Rate (SR) maximization problem in downlink MU-MISO systems under imperfect Channel State Information at the Transmitter (CSIT). Contrary to existing works, we consider a rather unorthodox transmission scheme. In particular, the message intended to one of the users is split into two parts: a common part which can be recovered by all users, and a private part recovered by the corresponding user. On the other hand, the rest of users receive their information through private messages. This Rate-Splitting (RS) approach was shown to boost the achievable Degrees of Freedom (DoF) when CSIT errors decay with increased SNR. In this work, the RS strategy is married with linear precoder design and optimization techniques to achieve a maximized Ergodic SR (ESR) performance over the entire range of SNRs. Precoders are designed based on partial CSIT knowledge by solving a stochastic rate optimization problem using means of Sample Average Approximation (SAA) coupled with the Weighted Minimum Mean Square Error (WMMSE) approach. Numerical results show that in addition to the ESR gains, the benefits of RS also include relaxed CSIT quality requirements and enhanced achievable rate regions compared to conventional transmission with NoRS."
      },
      {
        "node_idx": 32700,
        "score_0_10": 9,
        "title": "the secrecy capacity region of the gaussian mimo multi receiver wiretap channel",
        "abstract": "In this paper, we consider the Gaussian multiple-input multiple-output (MIMO) multi-receiver wiretap channel in which a transmitter wants to have confidential communication with an arbitrary number of users in the presence of an external eavesdropper. We derive the secrecy capacity region of this channel for the most general case. We first show that even for the single-input single-output (SISO) case, existing converse techniques for the Gaussian scalar broadcast channel cannot be extended to this secrecy context, to emphasize the need for a new proof technique. Our new proof technique makes use of the relationships between the minimum-mean-square-error and the mutual information, and equivalently, the relationships between the Fisher information and the differential entropy. Using the intuition gained from the converse proof of the SISO channel, we first prove the secrecy capacity region of the degraded MIMO channel, in which all receivers have the same number of antennas, and the noise covariance matrices can be arranged according to a positive semi-definite order. We then generalize this result to the aligned case, in which all receivers have the same number of antennas, however there is no order among the noise covariance matrices. We accomplish this task by using the channel enhancement technique. Finally, we find the secrecy capacity region of the general MIMO channel by using some limiting arguments on the secrecy capacity region of the aligned MIMO channel. We show that the capacity achieving coding scheme is a variant of dirty-paper coding with Gaussian signals."
      },
      {
        "node_idx": 35908,
        "score_0_10": 8,
        "title": "exploiting multi antennas for opportunistic spectrum sharing in cognitive radio networks",
        "abstract": "In cognitive radio (CR) networks, there are scenarios where the secondary (lower priority) users intend to communicate with each other by opportunistically utilizing the transmit spectrum originally allocated to the existing primary (higher priority) users. For such a scenario, a secondary user usually has to tradeoff between two conflicting goals at the same time: one is to maximize its own transmit throughput; and the other is to minimize the amount of interference it produces at each primary receiver. In this paper, we study this fundamental tradeoff from an information-theoretic perspective by characterizing the secondary user's channel capacity under both its own transmit-power constraint as well as a set of interference-power constraints each imposed at one of the primary receivers. In particular, this paper exploits multi-antennas at the secondary transmitter to effectively balance between spatial multiplexing for the secondary transmission and interference avoidance at the primary receivers. Convex optimization techniques are used to design algorithms for the optimal secondary transmit spatial spectrum that achieves the capacity of the secondary transmission. Suboptimal solutions for ease of implementation are also presented and their performances are compared with the optimal solution. Furthermore, algorithms developed for the single-channel transmission are also extended to the case of multichannel transmission whereby the secondary user is able to achieve opportunistic spectrum sharing via transmit adaptations not only in space, but in time and frequency domains as well. Simulation results show that even under stringent interference-power constraints, substantial capacity gains are achievable for the secondary transmission by employing multi-antennas at the secondary transmitter. This is true even when the number of primary receivers exceeds that of secondary transmit antennas in a CR network, where an interesting \"interference diversity\" effect can be exploited."
      },
      {
        "node_idx": 46136,
        "score_0_10": 8,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      }
    ]
  },
  "183": {
    "explanation": "distributed stochastic variational inference with networked ADMM optimization",
    "topk": [
      {
        "node_idx": 38955,
        "score_0_10": 10,
        "title": "admm based networked stochastic variational inference",
        "abstract": "Owing to the recent advances in \"Big Data\" modeling and prediction tasks, variational Bayesian estimation has gained popularity due to their ability to provide exact solutions to approximate posteriors. One key technique for approximate inference is stochastic variational inference (SVI). SVI poses variational inference as a stochastic optimization problem and solves it iteratively using noisy gradient estimates. It aims to handle massive data for predictive and classification tasks by applying complex Bayesian models that have observed as well as latent variables. This paper aims to decentralize it allowing parallel computation, secure learning and robustness benefits. We use Alternating Direction Method of Multipliers in a top-down setting to develop a distributed SVI algorithm such that independent learners running inference algorithms only require sharing the estimated model parameters instead of their private datasets. Our work extends the distributed SVI-ADMM algorithm that we first propose, to an ADMM-based networked SVI algorithm in which not only are the learners working distributively but they share information according to rules of a graph by which they form a network. This kind of work lies under the umbrella of `deep learning over networks' and we verify our algorithm for a topic-modeling problem for corpus of Wikipedia articles. We illustrate the results on latent Dirichlet allocation (LDA) topic model in large document classification, compare performance with the centralized algorithm, and use numerical experiments to corroborate the analytical results."
      },
      {
        "node_idx": 135057,
        "score_0_10": 10,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 6175,
        "score_0_10": 10,
        "title": "on comparing sums of square roots of small integers",
        "abstract": "Let $k$ and $n$ be positive integers, $n>k$. Define $r(n,k)$ to be the minimum positive value of $$ |\\sqrt{a_1} + ... + \\sqrt{a_k} - \\sqrt{b_1} - >... -\\sqrt{b_k} | $$ where $ a_1, a_2, ..., a_k, b_1, b_2, ..., b_k $ are positive integers no larger than $n$. It is an important problem in computational geometry to determine a good upper bound of $-\\log r(n,k)$. In this paper we prove an upper bound of $ 2^{O(n/\\log n)} \\log n$, which is better than the best known result $O(2^{2k} \\log n)$ whenever $ n \\leq ck\\log k$ for some constant $c$. In particular, our result implies a {\\em subexponential} algorithm to compare two sums of square roots of integers of size $o(k\\log k)$."
      },
      {
        "node_idx": 136642,
        "score_0_10": 9,
        "title": "the dynamics of message passing on dense graphs with applications to compressed sensing",
        "abstract": "\u201cApproximate message passing\u201d (AMP) algorithms have proved to be effective in reconstructing sparse signals from a small number of incoherent linear measurements. Extensive numerical experiments further showed that their dynamics is accurately tracked by a simple one-dimensional iteration termed state evolution. In this paper, we provide rigorous foundation to state evolution. We prove that indeed it holds asymptotically in the large system limit for sensing matrices with independent and identically distributed Gaussian entries. While our focus is on message passing algorithms for compressed sensing, the analysis extends beyond this setting, to a general class of algorithms on dense graphs. In this context, state evolution plays the role that density evolution has for sparse graphs. The proof technique is fundamentally different from the standard approach to density evolution, in that it copes with a large number of short cycles in the underlying factor graph. It relies instead on a conditioning technique recently developed by Erwin Bolthausen in the context of spin glass theory."
      },
      {
        "node_idx": 157548,
        "score_0_10": 9,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 133632,
        "score_0_10": 9,
        "title": "understanding crowd flow movements using active langevin model",
        "abstract": "Crowd flow describes the elementary group behavior of crowds. Understanding the dynamics behind these movements can help to identify various abnormalities in crowds. However, developing a crowd model describing these flows is a challenging task. In this paper, a physics-based model is proposed to describe the movements in dense crowds. The crowd model is based on active Langevin equation where the motion points are assumed to be similar to active colloidal particles in fluids. The model is further augmented with computer-vision techniques to segment both linear and non-linear motion flows in a dense crowd. The evaluation of the active Langevin equation-based crowd segmentation has been done on publicly available crowd videos and on our own videos. The proposed method is able to segment the flow with lesser optical flow error and better accuracy in comparison to existing state-of-the-art methods."
      },
      {
        "node_idx": 106683,
        "score_0_10": 9,
        "title": "neural approaches to conversational ai",
        "abstract": "The present paper surveys neural approaches to conversational AI that have been developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies."
      },
      {
        "node_idx": 60281,
        "score_0_10": 9,
        "title": "a multilevel monte carlo method for high dimensional uncertainty quantification of low frequency electromagnetic devices",
        "abstract": "This paper addresses uncertainty quantification of electromagnetic devices determined by the eddy current problem. The multilevel Monte Carlo (MLMC) method is used for the treatment of uncertain parameters while the devices are discretized in space by the finite element method. Both methods yield numerical approximations such that the total error is split into stochastic and spatial contributions. We propose a particular implementation where the spatial error is controlled based on a Richardson extrapolation-based error indicator. The stochastic error, in turn, is efficiently reduced in the MLMC approach by distributing the samples on multiple grids. The method is applied to a toy problem with closed-form solution and to a permanent magnet synchronous machine with uncertainties. The uncertainties under consideration are related to the material properties in the stator and the magnets in the rotor. The examples show that the error indicator works reliably, the meshes used for the different levels do not have to be nested, and, most importantly, MLMC reduces the computational cost by at least one order of magnitude compared to standard Monte Carlo."
      },
      {
        "node_idx": 73918,
        "score_0_10": 9,
        "title": "generalized approximate message passing for estimation with random linear mixing",
        "abstract": "We consider the estimation of an i.i.d.\\ random vector observed through a linear transform followed by a componentwise, probabilistic (possibly nonlinear) measurement channel. A novel algorithm, called generalized approximate message passing (GAMP), is presented that provides computationally efficient approximate implementations of max-sum and sum-problem loopy belief propagation for such problems. The algorithm extends earlier approximate message passing methods to incorporate arbitrary distributions on both the input and output of the transform and can be applied to a wide range of problems in nonlinear compressed sensing and learning. #R##N#Extending an analysis by Bayati and Montanari, we argue that the asymptotic componentwise behavior of the GAMP method under large, i.i.d. Gaussian transforms is described by a simple set of state evolution (SE) equations. From the SE equations, one can \\emph{exactly} predict the asymptotic value of virtually any componentwise performance metric including mean-squared error or detection accuracy. Moreover, the analysis is valid for arbitrary input and output distributions, even when the corresponding optimization problems are non-convex. The results match predictions by Guo and Wang for relaxed belief propagation on large sparse matrices and, in certain instances, also agree with the optimal performance predicted by the replica method. The GAMP methodology thus provides a computationally efficient methodology, applicable to a large class of non-Gaussian estimation problems with precise asymptotic performance guarantees."
      },
      {
        "node_idx": 155778,
        "score_0_10": 9,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      }
    ]
  },
  "184": {
    "explanation": "overlapping community detection and information diffusion in networks",
    "topk": [
      {
        "node_idx": 160277,
        "score_0_10": 10,
        "title": "community detection in networks with node attributes",
        "abstract": "Community detection algorithms are fundamental tools that allow us to uncover organizational principles in networks. When detecting communities, there are two possible sources of information one can use: the network structure, and the features and attributes of nodes. Even though communities form around nodes that have common edges and common attributes, typically, algorithms have only focused on one of these two data modalities: community detection algorithms traditionally focus only on the network structure, while clustering algorithms mostly consider only node attributes. In this paper, we develop Communities from Edge Structure and Node Attributes (CESNA), an accurate and scalable algorithm for detecting overlapping communities in networks with node attributes. CESNA statistically models the interaction between the network structure and the node attributes, which leads to more accurate community detection as well as improved robustness in the presence of noise in the network structure. CESNA has a linear runtime in the network size and is able to process networks an order of magnitude larger than comparable approaches. Last, CESNA also helps with the interpretation of detected communities by finding relevant node attributes for each community."
      },
      {
        "node_idx": 36666,
        "score_0_10": 10,
        "title": "the role of social networks in information diffusion",
        "abstract": "Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these technologies on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed."
      },
      {
        "node_idx": 65154,
        "score_0_10": 10,
        "title": "community detection in bipartite networks using weighted symmetric binary matrix factorization",
        "abstract": "In this paper, we propose weighted symmetric binary matrix factorization (wSBMF) framework to detect overlapping communities in bipartite networks, which describes the relationships between two types of nodes. Our method improves performance by recognizing the distinction between two types of missing edges \u2014 ones among the nodes in each node type and the others between two node types. Our method can also explicitly assign community membership and distinguish outliers from overlapping nodes, as well as incorporating existing knowledge on the network. We propose a generalized partition density for bipartite networks as a quality function, which identifies the most appropriate number of communities. The experimental results on both synthetic and real-world networks demonstrate the effectiveness of our method."
      },
      {
        "node_idx": 87547,
        "score_0_10": 10,
        "title": "slpa uncovering overlapping communities in social networks via a speaker listener interaction dynamic process",
        "abstract": "Overlap is one of the characteristics of social networks, in which a person may belong to more than one social group. For this reason, discovering overlapping structures is necessary for realistic social analysis. In this paper, we present a novel, general framework to detect and analyze both individual overlapping nodes and entire communities. In this framework, nodes exchange labels according to dynamic interaction rules. A specific implementation called Speaker-listener Label Propagation Algorithm (SLPA1) demonstrates an excellent performance in identifying both overlapping nodes and overlapping communities with different degrees of diversity."
      },
      {
        "node_idx": 153630,
        "score_0_10": 9,
        "title": "overlapping community detection in complex networks using symmetric binary matrix factorization",
        "abstract": "Discovering overlapping community structures is a crucial step to understanding the structure and dynamics of many networks. In this paper we develop a symmetric binary matrix factorization model to identify overlapping communities. Our model allows us not only to assign community memberships explicitly to nodes, but also to distinguish outliers from overlapping nodes. In addition, we propose a modified partition density to evaluate the quality of community structures. We use this to determine the most appropriate number of communities. We evaluate our methods using both synthetic benchmarks and real-world networks, demonstrating the effectiveness of our approach."
      },
      {
        "node_idx": 121252,
        "score_0_10": 9,
        "title": "physical layer network coding tutorial survey and beyond",
        "abstract": "The concept of physical-layer network coding (PNC) was proposed in 2006 for application in wireless networks. Since then it has developed into a subfield of network coding with wide followings. The basic idea of PNC is to exploit the network coding operation that occurs naturally when electromagnetic (EM) waves are superimposed on one another. This simple idea turns out to have profound and fundamental ramifications. Subsequent works by various researchers have led to many new results in the domains of 1) wireless communication; 2) wireless information theory; and 3) wireless networking. The purpose of this paper is fourfold. First, we give a brief tutorial on the basic concept of PNC. Second, we survey and discuss recent key results in the three aforementioned areas. Third, we examine a critical issue in PNC: synchronization. It has been a common belief that PNC requires tight synchronization. Our recent results suggest, however, that PNC may actually benefit from asynchrony. Fourth, we propose that PNC is not just for wireless networks; it can also be useful in optical networks. We provide an example showing that the throughput of a passive optical network (PON) could potentially be raised by 100% with PNC."
      },
      {
        "node_idx": 161887,
        "score_0_10": 9,
        "title": "community detection in complex networks using link prediction",
        "abstract": "Community detection and link prediction are both of great significance in network analysis, which provide very valuable insights into topological structures of the network from different perspectives. In this paper, we propose a novel community detection algorithm with inclusion of link prediction, motivated by the question whether link prediction can be devoted to improving the accuracy of community partition. For link prediction, we propose two novel indices to compute the similarity between each pair of nodes, one of which aims to add missing links, and the other tries to remove spurious edges. Extensive experiments are conducted on benchmark data sets, and the results of our proposed algorithm are compared with two classes of baseline. In conclusion, our proposed algorithm is competitive, revealing that link prediction does improve the precision of community detection."
      },
      {
        "node_idx": 41918,
        "score_0_10": 9,
        "title": "inferring networks of diffusion and influence",
        "abstract": "Information diffusion and virus propagation are fundamental processes taking place in networks. While it is often possible to directly observe when nodes become infected with a virus or adopt the information, observing individual transmissions (i.e., who infects whom, or who influences whom) is typically very difficult. Furthermore, in many applications, the underlying network over which the diffusions and propagations spread is actually unobserved. We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate. Given the times when nodes adopt pieces of information or become infected, we identify the optimal network that best explains the observed infection times. Since the optimization problem is NP-hard to solve exactly, we develop an efficient approximation algorithm that scales to large datasets and finds provably near-optimal networks. #R##N#We demonstrate the effectiveness of our approach by tracing information diffusion in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space. We find that the diffusion network of news for the top 1,000 media sites and blogs tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web. These sites tend to have stable circles of influence with more general news media sites acting as connectors between them."
      },
      {
        "node_idx": 163086,
        "score_0_10": 9,
        "title": "leveraging microgrids for capturing uncertain distribution network net load ramping",
        "abstract": "In this paper, a flexibility-oriented microgrid optimal scheduling model is proposed to mitigate distribution network net load variability caused by large penetration distributed solar generation. The distributed solar generation variability, which is caused by increasing adoption of this technology by end-use consumers, is mainly addressed by electric utilities using grid reinforcement. Microgrids, however, provide viable and local solutions to this pressing challenge. The proposed model, which is developed using mixed-integer programming and employs robust optimization, not only can efficiently capture distribution network net load variations, mainly in terms of ramping, but also accounts for possible uncertainties in forecasting. Numerical simulations on a test distribution feeder with one microgrid and several consumers/prosumers indicate the effectiveness of the proposed model."
      },
      {
        "node_idx": 156364,
        "score_0_10": 9,
        "title": "a comprehensive survey of recent advancements in molecular communication",
        "abstract": "With much advancement in the field of nanotechnology, bioengineering, and synthetic biology over the past decade, microscales and nanoscales devices are becoming a reality. Yet the problem of engineering a reliable communication system between tiny devices is still an open problem. At the same time, despite the prevalence of radio communication, there are still areas where traditional electromagnetic waves find it difficult or expensive to reach. Points of interest in industry, cities, and medical applications often lie in embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or they are located in such a way that directional high frequency systems are ineffective. Inspired by nature, one solution to these problems is molecular communication (MC), where chemical signals are used to transfer information. Although biologists have studied MC for decades, it has only been researched for roughly 10 year from a communication engineering lens. Significant number of papers have been published to date, but owing to the need for interdisciplinary work, much of the results are preliminary. In this survey, the recent advancements in the field of MC engineering are highlighted. First, the biological, chemical, and physical processes used by an MC system are discussed. This includes different components of the MC transmitter and receiver, as well as the propagation and transport mechanisms. Then, a comprehensive survey of some of the recent works on MC through a communication engineering lens is provided. The survey ends with a technology readiness analysis of MC and future research directions."
      }
    ]
  },
  "186": {
    "explanation": "open-domain neural conversational dialogue systems",
    "topk": [
      {
        "node_idx": 157548,
        "score_0_10": 10,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 35756,
        "score_0_10": 9,
        "title": "building end to end dialogue systems using generative hierarchical neural network models",
        "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings."
      },
      {
        "node_idx": 112674,
        "score_0_10": 8,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 63929,
        "score_0_10": 8,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 57558,
        "score_0_10": 8,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 26180,
        "score_0_10": 8,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 137083,
        "score_0_10": 8,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 43464,
        "score_0_10": 8,
        "title": "bidirectional attention flow for machine comprehension",
        "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."
      },
      {
        "node_idx": 54474,
        "score_0_10": 8,
        "title": "the ubuntu dialogue corpus a large dataset for research in unstructured multi turn dialogue systems",
        "abstract": "This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response."
      }
    ]
  },
  "187": {
    "explanation": "advanced techniques for parallel, distributed, and structured deep learning models",
    "topk": [
      {
        "node_idx": 24362,
        "score_0_10": 10,
        "title": "demystifying parallel and distributed deep learning an in depth concurrency analysis",
        "abstract": "Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. Specifically, we present trends in DNN architectures and the resulting implications on parallelization strategies. We discuss the different types of concurrency in DNNs; synchronous and asynchronous stochastic gradient descent; distributed system architectures; communication schemes; and performance modeling. Based on these approaches, we extrapolate potential directions for parallelism in deep learning."
      },
      {
        "node_idx": 93518,
        "score_0_10": 10,
        "title": "provenance for aggregate queries",
        "abstract": "We study in this paper provenance information for queries with aggregation. Provenance information was studied in the context of various query languages that do not allow for aggregation, and recent work has suggested to capture provenance by annotating the different database tuples with elements of a commutative semiring and propagating the annotations through query evaluation. We show that aggregate queries pose novel challenges rendering this approach inapplicable. Consequently, we propose a new approach, where we annotate with provenance information not just tuples but also the individual values within tuples, using provenance to describe the values computation. We realize this approach in a concrete construction, first for \"simple\" queries where the aggregation operator is the last one applied, and then for arbitrary (positive) relational algebra queries with aggregation; the latter queries are shown to be more challenging in this context. Finally, we use aggregation to encode queries with difference, and study the semantics obtained for such queries on provenance annotated databases."
      },
      {
        "node_idx": 76595,
        "score_0_10": 10,
        "title": "on the parameterized complexity and kernelization of the workflow satisfiability problem",
        "abstract": "A workflow specification defines a set of steps and the order in which those steps must be executed. Security requirements may impose constraints on which groups of users are permitted to perform subsets of those steps. A workflow specification is said to be satisfiable if there exists an assignment of users to workflow steps that satisfies all the constraints. An algorithm for determining whether such an assignment exists is important, both as a static analysis tool for workflow specifications, and for the construction of run-time reference monitors for workflow management systems. Finding such an assignment is a hard problem in general, but work by Wang and Li in 2010 using the theory of parameterized complexity suggests that efficient algorithms exist under reasonable assumptions about workflow specifications. In this paper, we improve the complexity bounds for the workflow satisfiability problem. We also generalize and extend the types of constraints that may be defined in a workflow specification and prove that the satisfiability problem remains fixed-parameter tractable for such constraints. Finally, we consider preprocessing for the problem and prove that in an important special case, in polynomial time, we can reduce the given input into an equivalent one, where the number of users is at most the number of steps. We also show that no such reduction exists for two natural extensions of this case, which bounds the number of users by a polynomial in the number of steps, provided a widely-accepted complexity-theoretical assumption holds."
      },
      {
        "node_idx": 157819,
        "score_0_10": 10,
        "title": "reenactment for read committed snapshot isolation",
        "abstract": "Provenance for transactional updates is critical for many applications such as auditing and debugging of transactions. Recently, we have introduced MV-semirings, an extension of the semiring provenance model that supports updates and transactions. Furthermore, we have proposed reenactment, a declarative form of replay with provenance capture, as an efficient and non-invasive method for computing this type of provenance. However, this approach is limited to the snapshot isolation (SI) concurrency control protocol while many real world applications apply the read committed version of snapshot isolation (RC-SI) to improve performance at the cost of consistency. We present non trivial extensions of the model and reenactment approach to be able to compute provenance of RC-SI transactions efficiently. In addition, we develop techniques for applying reenactment across multiple RC-SI transactions. Our experiments demonstrate that our implementation in the GProM system supports efficient re-construction and querying of provenance."
      },
      {
        "node_idx": 143330,
        "score_0_10": 9,
        "title": "deepest neural networks",
        "abstract": "This paper shows that a long chain of perceptrons (that is, a multilayer perceptron, or MLP, with many hidden layers of width one) can be a universal classifier. The classification procedure is not necessarily computationally efficient, but the technique throws some light on the kind of computations possible with narrow and deep MLPs."
      },
      {
        "node_idx": 165403,
        "score_0_10": 9,
        "title": "scaliendb designing and implementing a distributed database using paxos",
        "abstract": "ScalienDB is a scalable, replicated database built on top of the Paxos algorithm. It was developed from 2010 to 2012, when the startup backing it failed. This paper discusses the design decisions of the distributed database, describes interesting parts of the C++ codebase and enumerates lessons learned putting ScalienDB into production at a handful of clients. The source code is available on Github under the AGPL license, but it is no longer developed or maintained."
      },
      {
        "node_idx": 65132,
        "score_0_10": 9,
        "title": "timed consistent network updates",
        "abstract": "Network updates such as policy and routing changes occur frequently in Software Defined Networks (SDN). Updates should be performed consistently, preventing temporary disruptions, and should require as little overhead as possible. Scalability is increasingly becoming an essential requirement in SDN. In this paper we propose to use time-triggered network updates to achieve consistent updates. Our proposed solution requires lower overhead than existing update approaches, without compromising the consistency during the update. We demonstrate that accurate time enables far more scalable consistent updates in SDN than previously available. In addition, it provides the SDN programmer with fine-grained control over the tradeoff between consistency and scalability."
      },
      {
        "node_idx": 77509,
        "score_0_10": 9,
        "title": "validating the claim defeating hatch building malicious ip cores",
        "abstract": "This paper defends the design of hardware Trojan proposed in the paper \"Defeating HaTCh: Building Malicious IP Cores\" by defining the four critical properties which are required to be satisfied to classify it to belong to class of Deterministic Hardware Trojan. This is to counter the claims of authors of HaTCh in their paper \"Comments on Defeating HaTCh\"."
      },
      {
        "node_idx": 78341,
        "score_0_10": 9,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 26398,
        "score_0_10": 9,
        "title": "definitions and semantic simulations based on object oriented analysis and modeling",
        "abstract": "We have proposed going beyond traditional ontologies to use rich semantics implemented in programming languages for modeling. In this paper, we discuss the application of executable semantic models to two examples, first a structured definition of a waterfall and second the cardiopulmonary system. We examine the components of these models and the way those components interact. Ultimately, such models should provide the basis for direct representation."
      }
    ]
  },
  "189": {
    "explanation": "network stability under fixed-route attacks",
    "topk": [
      {
        "node_idx": 19922,
        "score_0_10": 10,
        "title": "network destabilizing attacks",
        "abstract": "The Border Gateway Protocol (BGP) sets up routes between the smaller networks that make up the Internet. Despite its crucial role, BGP is notoriously vulnerable to serious problems, including (1) propagation of bogus routing information due to attacks or misconfigurations, and (2) network instabilities in the form of persistent routing oscillations. The conditions required to avoid BGP instabilities are quite delicate. How, then, can we explain the observed stability of today's Internet in the face of common configuration errors and attacks? This work explains this phenomenon by first noticing that almost every observed attack and misconfiguration to date shares a common characteristic: even when a router announces egregiously bogus information, it will continue to announce the same bogus information for the duration of its attack/misconfiguration. We call these the \"fixed-route attacks\", and show that, while even simple fixed-route attacks can destabilize a network, the commercial routing policies used in today's Internet prevent such attacks from creating instabilities."
      },
      {
        "node_idx": 130527,
        "score_0_10": 10,
        "title": "towards single face shortest vertex disjoint paths in undirected planar graphs",
        "abstract": "Given $k$ pairs of terminals $\\{(s_{1}, t_{1}), \\ldots, (s_{k}, t_{k})\\}$ in a graph $G$, the min-sum $k$ vertex-disjoint paths problem is to find a collection $\\{Q_{1}, Q_{2}, \\ldots, Q_{k}\\}$ of vertex-disjoint paths with minimum total length, where $Q_{i}$ is an $s_i$-to-$t_i$ path between $s_i$ and $t_i$. We consider the problem in planar graphs, where little is known about computational tractability, even in restricted cases. Kobayashi and Sommer propose a polynomial-time algorithm for $k \\le 3$ in undirected planar graphs assuming all terminals are adjacent to at most two faces. Colin de Verdiere and Schrijver give a polynomial-time algorithm when all the sources are on the boundary of one face and all the sinks are on the boundary of another face and ask about the existence of a polynomial-time algorithm provided all terminals are on a common face. We make progress toward Colin de Verdiere and Schrijver's open question by giving an $O(kn^5)$ time algorithm for undirected planar graphs when $\\{(s_{1}, t_{1}), \\ldots, (s_{k}, t_{k})\\}$ are in counter-clockwise order on a common face."
      },
      {
        "node_idx": 111978,
        "score_0_10": 10,
        "title": "covering a line segment with variable radius discs",
        "abstract": "The paper addresses the problem of locating sensors with a circular field of view so that a given line segment is under full surveillance, which is termed as the Disc Covering Problem on a Line. The cost of each sensor includes a fixed component, and a variable component that is proportional to the field-of-view area. When only one type of sensor or, in general, one type of disc, is available, then a simple polynomial algorithm solves the problem. When there are different types of sensors in terms of fixed and variable costs, the problem becomes NP-hard. A branch-and-bound algorithm as well as an efficient heuristic are developed. The heuristic very often obtains the optimal solution as shown in extensive computational testing."
      },
      {
        "node_idx": 60037,
        "score_0_10": 10,
        "title": "scor software defined constrained optimal routing platform for sdn",
        "abstract": "A Software-defined Constrained Optimal Routing (SCOR) platform is introduced as a Northbound interface in SDN architecture. It is based on constraint programming techniques and is implemented in MiniZinc modelling language. Using constraint programming techniques in this Northbound interface has created an efficient tool for implementing complex Quality of Service routing applications in a few lines of code. The code includes only the problem statement and the solution is found by a general solver program. A routing framework is introduced based on SDN's architecture model which uses SCOR as its Northbound interface and an upper layer of applications implemented in SCOR. Performance of a few implemented routing applications are evaluated in different network topologies, network sizes and various number of concurrent flows."
      },
      {
        "node_idx": 39332,
        "score_0_10": 10,
        "title": "xoring elephants novel erasure codes for big data",
        "abstract": "Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. #R##N#This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. #R##N#We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication."
      },
      {
        "node_idx": 47806,
        "score_0_10": 9,
        "title": "two compact incremental prime sieves",
        "abstract": "A prime sieve is an algorithm that finds the primes up to a bound  #N#                       #N#                          #N#                         #N#            #N#             #N#                 #N#                    $n$#N#                 #N#             #N#        #N#                       #N#                    . We say that a prime sieve is  incremental , if it can quickly determine if  #N#                       #N#                          #N#                         #N#            #N#             #N#                 #N#                    $n+1$#N#                 #N#             #N#        #N#                       #N#                     is prime after having found all primes up to\u00a0 #N#                       #N#                          #N#                         #N#            #N#             #N#                 #N#                    $n$#N#                 #N#             #N#        #N#                       #N#                    . We say a sieve is  compact  if it uses roughly  #N#                       #N#                          #N#                         #N#            #N#             #N#                 #N#                    $\\sqrt{n}$#N#                 #N#             #N#        #N#                       #N#                     space or less. In this paper, we present two new results. #N#                 #N#                    #N#                       #N#                          \u2013 #N#                          #N#                             We describe the  rolling sieve , a practical, incremental prime sieve that takes  #N#                                   #N#                                      #N#                                     #N#            #N#             #N#                 #N#                    $O(n\\log \\log n)$#N#                 #N#             #N#        #N#                                   #N#                                 time and  #N#                                   #N#                                      #N#                                     #N#            #N#             #N#                 #N#                    $O(\\sqrt{n}\\log n)$#N#                 #N#             #N#        #N#                                   #N#                                 bits of space. #N#                          #N#                       #N#                       #N#                          \u2013 #N#                          #N#                             We also show how to modify the sieve of Atkin and Bernstein from 2004 to obtain a sieve that is simultaneously sublinear, compact, and incremental. #N#                          #N#                       #N#                    #N#                 #N#                 The second result solves an open problem given by Paul Pritchard in 1994."
      },
      {
        "node_idx": 76614,
        "score_0_10": 9,
        "title": "on capacity computation for the two user binary multiple access channel",
        "abstract": "This paper deals with the problem of computing the boundary of the capacity region for the memoryless two-user binary-input binary-output multiple-access channel ((2,2;2)-MAC), or equivalently, the computation of input probability distributions maximizing weighted sum-rate. This is equivalent to solving a difficult nonconvex optimization problem. For a restricted class of (2,2;2)-MACs and weight vectors, it is shown that, depending on an ordering property of the channel matrix, the optimal solution is located on the boundary, or the objective function has at most one stationary point in the interior of the domain. For this, the problem is reduced to a pseudoconcave one-dimensional optimization and the single-user problem."
      },
      {
        "node_idx": 36688,
        "score_0_10": 9,
        "title": "a few more quadratic apn functions",
        "abstract": "We present two infinite families of APN functions where the degree of the field is divisible by 3 but not 9. Our families contain two already known families as special cases. We also discuss the inequivalence proof (by computation) which shows that these functions are new."
      },
      {
        "node_idx": 83490,
        "score_0_10": 9,
        "title": "a lower bound on seller revenue in single buyer monopoly auctions",
        "abstract": "We consider a monopoly seller who optimally auctions a single object to a single potential buyer, with a known distribution of valuations. We show that a tight lower bound on the seller\u2019s expected revenue is 1/e times the geometric expectation of the buyer\u2019s valuation, and that this bound is uniquely achieved for the equal revenue distribution. We show also that when the valuation\u2019s expectation and geometric expectation are close, then the seller\u2019s expected revenue is close to the expected valuation."
      },
      {
        "node_idx": 46136,
        "score_0_10": 9,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      }
    ]
  },
  "191": {
    "explanation": "state-of-the-art convolutional neural network architectures and modules",
    "topk": [
      {
        "node_idx": 53950,
        "score_0_10": 10,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 67928,
        "score_0_10": 10,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 103461,
        "score_0_10": 10,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      },
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 80414,
        "score_0_10": 9,
        "title": "3d u net learning dense volumetric segmentation from sparse annotation",
        "abstract": "This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases."
      },
      {
        "node_idx": 141525,
        "score_0_10": 9,
        "title": "selecting and designing grippers for an assembly task in a structured approach",
        "abstract": "In this paper, we present a structured approach of selecting and designing a set of grippers for an assembly task. Compared to current experience-based gripper design method, our approach accelerates the design process by automatically generating a set of initial design options on gripper type and parameters according to the CAD models of assembly components. We use mesh segmentation techniques to segment the assembly components and fit the segmented parts with shape primitives, according to the predefined correspondence between primitive shape and gripper type, suitable gripper types and parameters can be selected and extracted from the fitted shape primitives. Then considering the assembly constraints, applicable gripper types and parameters can be filtered from the initial options. Among the applicable gripper configurations, we further minimize the required number of grippers for performing the assembly task, by exploring the gripper that is able to handle multiple assembly components during the assembly. Finally, the feasibility of the designed grippers are experimentally verified by assembling a part of an industrial product."
      },
      {
        "node_idx": 117988,
        "score_0_10": 9,
        "title": "electrical reduction homotopy moves and defect",
        "abstract": "We prove the first nontrivial worst-case lower bounds for two closely related problems. First, $\\Omega(n^{3/2})$ degree-1 reductions, series-parallel reductions, and $\\Delta$Y transformations are required in the worst case to reduce an $n$-vertex plane graph to a single vertex or edge. The lower bound is achieved by any planar graph with treewidth $\\Theta(\\sqrt{n})$. Second, $\\Omega(n^{3/2})$ homotopy moves are required in the worst case to reduce a closed curve in the plane with $n$ self-intersection points to a simple closed curve. For both problems, the best upper bound known is $O(n^2)$, and the only lower bound previously known was the trivial $\\Omega(n)$. #R##N#The first lower bound follows from the second using medial graph techniques ultimately due to Steinitz, together with more recent arguments of Noble and Welsh [J. Graph Theory 2000]. The lower bound on homotopy moves follows from an observation by Haiyashi et al. [J. Knot Theory Ramif. 2012] that the standard projections of certain torus knots have large defect, a topological invariant of generic closed curves introduced by Aicardi and Arnold. Finally, we prove that every closed curve in the plane with $n$ crossings has defect $O(n^{3/2})$, which implies that better lower bounds for our algorithmic problems will require different techniques."
      },
      {
        "node_idx": 73053,
        "score_0_10": 9,
        "title": "semantic image segmentation with deep convolutional nets and fully connected crfs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
      }
    ]
  },
  "192": {
    "explanation": "robotic manipulation and agile software development methodologies",
    "topk": [
      {
        "node_idx": 37100,
        "score_0_10": 10,
        "title": "roles in software development using domain specific modeling languages",
        "abstract": "Domain-specific modelling languages (DSMLs) successfully separate the conceptual and technical design of a software system by modelling requirements in the DSML and adding technical elements by appropriate generator technology. In this paper we describe the roles within an agile development process that allows us to implement a software system by using a combination of domain specific models and source code. We describe the setup of such a process using the MontiCore framework and demonstrate the advantages by describing how a group of developers with diverse individual skills can develop automotive HMI software."
      },
      {
        "node_idx": 134817,
        "score_0_10": 10,
        "title": "model predictive control with inverse statics optimization for tensegrity spine robots",
        "abstract": "Robots with flexible spines based on tensegrity structures have potential advantages over traditional designs with rigid torsos. However, these robots can be difficult to control due to their high-dimensional nonlinear dynamics and actuator constraints. This work presents two controllers for tensegrity spine robots, using model-predictive control (MPC) and inverse statics optimization. The controllers introduce two different approaches to making the control problem computationally tractable. The first utilizes smoothing terms in the MPC problem. The second uses a new inverse statics optimization algorithm, which gives the first feasible solutions to the problem for certain tensegrity robots, to generate reference input trajectories in combination with MPC. Tracking the inverse statics reference input trajectory significantly reduces the number of tuning parameters. The controllers are validated against simulations of two-dimensional and three-dimensional tensegrity spines. Both approaches show noise insensitivity and low tracking error, and can be used for different control goals. The results here demonstrate the first closed-loop control of such structures."
      },
      {
        "node_idx": 145413,
        "score_0_10": 10,
        "title": "leveraging creativity in requirements elicitation within agile software development a systematic literature review",
        "abstract": "Abstract   Agile approaches tend to focus solely on scoping and simplicity rather than on problem solving and discovery. This hampers the development of innovative solutions. Additionally, little has been said about how to capture and represent the real user needs. To fill this gap, some authors argue in favor of the application of \u201cCreative thinking\u201d for requirements elicitation within agile software development. This synergy between creativeness and agility has arisen as a new means of bringing innovation and flexibility to increasingly demanding software.  The aim of the present study is therefore to employ a systematic review to investigate the state-of-the-art of those approaches that leverage creativity in requirements elicitation within Agile Software Development, as well as the benefits, limitations and strength of evidence of these approaches. (Annotation #2)  The review was carried out by following the guidelines proposed by Dr. Kitchenham. The search strategy identified 1451 studies, 17 of which were eventually classified as primary studies. The selected studies contained 13 different and unique proposals. These approaches provide evidence that enhanced creativity in requirements elicitation can be successfully implemented in real software projects. We specifically observed that projects related to user interface development, such as those for mobile or web applications, are good candidates for the use of these approaches. We have also found that agile methodologies such as Scrum, Extreme Programming or methodologies based on rapid modelling are preferred when introducing creativity into requirements elicitation. Despite this being a new research field, there is a mixture of techniques, tools and processes that have already been and are currently being successfully tested in industry. Finally, we have found that, although creativity is an important ingredient with which to bring about innovation, it is not always sufficient to generate new requirements because this needs to be followed by user engagement and a specific context in which proper conditions, such as flexibility, time or resources, have to be met."
      },
      {
        "node_idx": 164718,
        "score_0_10": 10,
        "title": "arm manipulation planning of tethered tools with the help of a tool balancer",
        "abstract": "Robotic manipulation of tethered tools is widely seen in robotic work cells. They may cause excess strain on the tool's cable or undesired entanglements with the robot's arms. This paper presents a manipulation planner with cable orientation constraints for tethered tools suspended by tool balancers. The planner uses orientation constraints to limit the bending of the balancer's cable while the robot manipulates a tool and places it in a desired pose. The constraints reduce entanglements and decrease the torque induced by the cable on the robot joints. Simulation and real-world experiments show that the constrained planner can successfully plan robot motions for the manipulation of suspended tethered tools preventing the robot from damaging the cable or getting its arms entangled, potentially avoiding accidents. The planner is expected to play promising roles in manufacturing cells."
      },
      {
        "node_idx": 26834,
        "score_0_10": 10,
        "title": "to agile or not to agile a comparison of software development methodologies",
        "abstract": "Since the Agile Manifesto, many organizations have explored agile development methods to replace traditional waterfall development. Interestingly, waterfall remains the most widely used practice, suggesting that there is something missing from the many \"flavors\" of agile methodologies. We explore seven of the most common practices to explore this, and evaluate each against a series of criteria centered around product quality and adherence to agile practices. We find that no methodology entirely replaces waterfall and summarize the strengths and weaknesses of each. From this, we conclude that agile methods are, as a whole, unable to cope with the realities of technical debt and large scale systems. Ultimately, no one methodology fits all projects."
      },
      {
        "node_idx": 73322,
        "score_0_10": 10,
        "title": "closed chain manipulation of large objects by multi arm robotic systems",
        "abstract": "Closed kinematic chains are created whenever multiple robot arms concurrently manipulate a single object. The closed-chain constraint, when coupled with robot joint limits, dramatically changes the connectivity of the configuration space. We propose a regrasping move, termed \u201cIK-switch,\u201d which allows efficiently bridging components of the configuration space that are otherwise mutually disconnected. This move, combined with several other developments, such as a method to stabilize the manipulated object using the environment, a new tree structure, and a compliant control scheme, enables us to address complex closed-chain manipulation tasks, such as flipping a chair frame, which is otherwise impossible to realize using existing multi-arm planning methods."
      },
      {
        "node_idx": 161607,
        "score_0_10": 10,
        "title": "an approach for agile soa development using agile principals",
        "abstract": "In dynamic and turbulent business environment, the need for success and survival of any organization is the ability of adapting to changes efficiently and cost-effectively. So, for developing software applications, one of the methods is Service Oriented Architecture (SOA) methodology and other is Agile Methodology. Since embracing changes is the indispensable concept of SOA development as well as Agile Development, using an appropriate SOA methodology able to adapt changes even during system development with the preservation of software quality is necessary. In this paper, a new approach consisted of five steps is presented to add agility to SOA methodologies. This approach, before any SOAbased development, helps architect(s) to determine Core Business Processes (CBPs) by using agile principals for establishing Core Architecture. The most important advantage of this approach according to the results of case study is possibility of embracing changes with the preservation of software quality in SOA developments."
      },
      {
        "node_idx": 154109,
        "score_0_10": 10,
        "title": "design simulation and testing of a flexible actuated spine for quadruped robots",
        "abstract": "Walking quadruped robots face challenges in positioning their feet and lifting their legs during gait cycles over uneven terrain. The robot Laika is under development as a quadruped with a flexible, actuated spine designed to assist with foot movement and balance during these gaits. This paper presents the first set of hardware designs for the spine of Laika, a physical prototype of those designs, and tests in both hardware and simulations that show the prototype's capabilities. Laika's spine is a tensegrity structure, used for its advantages with weight and force distribution, and represents the first working prototype of a tensegrity spine for a quadruped robot. The spine bends by adjusting the lengths of the cables that separate its vertebrae, and twists using an actuated rotating vertebra at its center. The current prototype of Laika has stiff legs attached to the spine, and is used as a test setup for evaluation of the spine itself. This work shows the advantages of Laika's spine by demonstrating the spine lifting each of the robot's four feet, both as a form of balancing and as a precursor for a walking gait. These foot motions, using specific combinations of bending and rotation movements of the spine, are measured in both simulation and hardware experiments. Hardware data are used to calibrate the simulations, such that the simulations can be used for control of balancing or gait cycles in the future. Future work will attach actuated legs to Laika's spine, and examine balancing and gait cycles when combined with leg movements."
      },
      {
        "node_idx": 15373,
        "score_0_10": 10,
        "title": "agile test based modeling",
        "abstract": "Model driven architecture (MDA) concentrates on the use of models during software development. An approach using models as the central development artifact is more abstract, more compact and thus more effective and probably also less error prone. Although the ideas of MDA exist already for years, there is still much to improve in the development process as well as the underlying techniques and tools. Therefore, this paper is a follow up on, reexamining und updating the statements made there. Here two major and strongly related techniques are identified and discussed: Test case modeling and an evolutionary approach to model transformation."
      },
      {
        "node_idx": 125106,
        "score_0_10": 10,
        "title": "sok differential privacies",
        "abstract": "Shortly after its introduction in 2006, differential privacy became the flagship data privacy definition. Since then, numerous variants and extensions were proposed to adapt it to different scenarios and attacker models. In this work, we propose a systematic taxonomy of these variants and extensions. We list all data privacy definitions based on differential privacy, and partition them into seven categories, depending on which aspect of the original definition is modified. These categories act like dimensions: variants belonging to the same category can, in general not be combined, but several categories can be combined to form new definitions. We also establish a partial ordering between variants by summarizing results about their relative strength. Furthermore, we list which of these definitions satisfy some desirable properties, like composition, post-processing, and convexity."
      }
    ]
  },
  "193": {
    "explanation": "Algebraic and constacyclic coding theory over finite rings and fields",
    "topk": [
      {
        "node_idx": 135578,
        "score_0_10": 10,
        "title": "m adic residue codes over mathbb f _q v v s v",
        "abstract": "Due to their rich algebraic structure, cyclic codes have a great deal of significance amongst linear codes. Duadic codes are the generalization of the quadratic residue codes, a special case of cyclic codes. The $m$-adic residue codes are the generalization of the duadic codes. The aim of this paper is to study the structure of the $m$-adic residue codes over the quotient ring $\\mathbb{F}_{q}[v]/(v^s-v).$ We determine the idempotent generators of the $m$-adic residue codes over $\\mathbb{F}_{q}[v]/(v^s-v)$. We obtain some parameters of optimal $m$-adic residue codes over $\\mathbb{F}_{q}[v]/(v^s-v),$ with respect to Griesmer bound for rings."
      },
      {
        "node_idx": 69221,
        "score_0_10": 10,
        "title": "constacyclic codes over finite principal ideal rings",
        "abstract": "In this paper, we give an important isomorphism between contacyclic codes and cyclic codes over finite principal ideal rings. Necessary and sufficient conditions for the existence of non-trivial cyclic self-dual codes over finite principal ideal rings are given."
      },
      {
        "node_idx": 117093,
        "score_0_10": 10,
        "title": "polyadic constacyclic codes over a non chain ring mathbb f _ q u v langle f u g v uv vu rangle f q u v f u g v u v v u",
        "abstract": "Let f(u) and g(v) be two polynomials, not both linear, which split into distinct linear factors over \\(\\mathbb {F}_{q}\\). Let \\(\\mathcal {R}=\\mathbb {F}_{q}[u,v]/ \\langle f(u),g(v),uv-vu\\rangle \\) be a finite commutative non-chain ring. In this paper, we study polyadic \\(\\lambda \\)-constacyclic codes of Type I and Type II over \\(\\mathcal {R}\\) for \\(\\lambda \\in \\mathbb {F}_q^*\\). The Gray images of polyadic negacyclic codes and their extensions lead to construction of self-dual, isodual, self-orthogonal and complementary dual(LCD) codes over \\(\\mathbb {F}_q\\). We also study \\(\\alpha \\)-constacyclic codes over \\(\\mathcal {R}\\) for any unit \\(\\alpha \\) in \\(\\mathcal {R}\\)."
      },
      {
        "node_idx": 37944,
        "score_0_10": 10,
        "title": "computational results of duadic double circulant codes",
        "abstract": "Quadratic residue codes have been one of the most important classes of algebraic codes. They have been generalized into duadic codes and quadratic double circulant codes. In this paper we introduce a new subclass of double circulant codes, called {\\em{duadic double circulant codes}}, which is a generalization of quadratic double circulant codes for prime lengths. This class generates optimal self-dual codes, optimal linear codes, and linear codes with the best known parameters in a systematic way. We describe a method to construct duadic double circulant codes using 4-cyclotomic cosets and give certain duadic double circulant codes over $\\mathbb F_2, \\mathbb F_3, \\mathbb F_4, \\mathbb F_5$, and $\\mathbb F_7$. In particular, we find a new ternary self-dual $[76,38,18]$ code and easily rediscover optimal binary self-dual codes with parameters $[66,33,12]$, $[68,34,12]$, $[86,43,16]$, and $[88,44,16]$ as well as a formally self-dual binary $[82,41,14]$ code."
      },
      {
        "node_idx": 130093,
        "score_0_10": 10,
        "title": "towards efficient coexistence of ieee 802 15 4e tsch and ieee 802 11",
        "abstract": "A major challenge in wide deployment of smart wireless devices, using different technologies and sharing the same 2.4 GHz spectrum, is to achieve coexistence across multiple technologies. The IEEE~802.11 (WLAN) and the IEEE 802.15.4e TSCH (WSN) where designed with different goals in mind and both play important roles for respective applications. However, they cause mutual interference and degraded performance while operating in the same space. To improve this situation we propose an approach to enable a cooperative control which type of network is transmitting at given time, frequency and place. #R##N#We recognize that TSCH based sensor network is expected to occupy only small share of time, and that the nodes are by design tightly synchronized. We develop mechanism enabling over-the-air synchronization of the Wi-Fi network to the TSCH based sensor network. Finally, we show that Wi-Fi network can avoid transmitting in the \"collision periods\". We provide full design and show prototype implementation based on the Commercial off-the-shelf (COTS) devices. Our solution does not require changes in any of the standards."
      },
      {
        "node_idx": 71804,
        "score_0_10": 10,
        "title": "constacyclic codes over finite fields",
        "abstract": "Abstract   An equivalence relation called isometry is introduced to classify constacyclic codes over a finite field; the polynomial generators of constacyclic codes of length      l    t      p    s      are characterized, where  p  is the characteristic of the finite field and  l  is a prime different from  p ."
      },
      {
        "node_idx": 145222,
        "score_0_10": 10,
        "title": "scheduling strategies and throughput optimization for the downlink for ieee 802 11ax and ieee 802 11ac based networks",
        "abstract": "The new IEEE 802.11 standard, IEEE 802.11ax, has the challenging goal of serving more Uplink (UL) traffic and users as compared with his predecessor IEEE 802.11ac, en- abling consistent and reliable streams of data (average throughput) per station. In this paper we explore several new IEEE 802.11ax UL scheduling mechanisms and compare between the maximum throughputs of unidirectional UDP Multi Users (MU) triadic. The evaluation is conducted based on Multiple-Input-Multiple-Output (MIMO) and Orthogonal Frequency Division Multiple Acceess (OFDMA) transmission multiplexing format in IEEE 802.11ax vs. the CSMA/CA MAC in IEEE 802.11ac in the Single User (SU) and MU modes for 1, 4, 8, 16, 32 and 64 stations scenario in reliable and unreliable channels. The comparison is conducted as a function of the Modulation and Coding Schemes (MCS) in use. In IEEE 802.11ax we consider two new flavors of ac- knowledgment operation settings, where the maximum acknowledgment windows are 64 or 256 respectively. In SU scenario IEEE 802.11ax throughputs outperform IEEE 802.11ac by about 64% and 85% in reliable and unreliable channels respectively. In MU-MIMO scenario IEEE 802.11ax throughputs outperform IEEE 802.11ac by up to 263% and 270% in reliable and unreliable channels respectively. Also, as the number of stations increases, the advantage of IEEE 802.11ax in terms of the access delay also increases."
      },
      {
        "node_idx": 157757,
        "score_0_10": 9,
        "title": "galois self dual constacyclic codes",
        "abstract": "Generalizing Euclidean inner product and Hermitian inner product, we introduce Galois inner products, and study the Galois self-dual constacyclic codes in a very general setting by a uniform method. The conditions for existence of Galois self-dual and isometrically Galois self-dual constacyclic codes are obtained. As consequences, the results on self-dual, iso-dual and Hermitian self-dual constacyclic codes are derived."
      },
      {
        "node_idx": 24752,
        "score_0_10": 9,
        "title": "staged evolution with quality gates for model libraries",
        "abstract": "Model evolution is widely considered as a subject under research. Despite its role in research, common purpose concepts, approaches, solutions, and methodologies are missing. Limiting the scope to model libraries makes model evolution and related quality concerns manageable, as we show below. In this paper, we put forward our quality staged model evolution theory for model libraries. It is founded on evolution graphs, which offer a structure for model evolution in model libraries through evolution steps. These evolution steps eventually form a sequence, which can be partitioned into stages by quality gates. Each quality gate is defined by a lightweight quality model and respective characteristics fostering reusability."
      },
      {
        "node_idx": 114320,
        "score_0_10": 9,
        "title": "optimal slice allocation in 5g core networks",
        "abstract": "Network slicing is the key to providing flexible, scalable and on-demand solutions for the vast array of applications in 5G networks. Two key challenges of 5G networks are the network slicing and guaranteeing end-to-end delay for a slice. In this paper, we address the question of optimal allocation of a slice in 5G core networks. We adopt and extend the work by D. Dietrich et al. [1] to create a model that satisfies constraints on end-to-end delay as well as isolation between components of a slice for reliability. Our work takes into consideration Service Level Agreement and performance parameters. Our results show the impact of varying parameters on the utilization of the system."
      }
    ]
  },
  "195": {
    "explanation": "monocular depth estimation and pose prediction from images and videos",
    "topk": [
      {
        "node_idx": 24683,
        "score_0_10": 10,
        "title": "unsupervised monocular depth estimation with left right consistency",
        "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. #R##N#We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth."
      },
      {
        "node_idx": 67267,
        "score_0_10": 10,
        "title": "unsupervised learning of depth and ego motion from video",
        "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings."
      },
      {
        "node_idx": 77194,
        "score_0_10": 10,
        "title": "deeper depth prediction with fully convolutional residual networks",
        "abstract": "This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available."
      },
      {
        "node_idx": 87594,
        "score_0_10": 10,
        "title": "on the k error linear complexity of binary sequences derived from polynomial quotients",
        "abstract": "The k-error linear complexity is an important cryptographic measure of pseudorandom sequences in stream ciphers. In this paper, we investigate the k-error linear complexity of p 2-periodic binary sequences defined from the polynomial quotients modulo p, which are the generalizations of the well-studied Fermat quotients. Indeed, first we determine exact values of the k-error linear complexity over the finite field \\(\\mathbb{F}_2\\) for these binary sequences under the assumption of 2 being a primitive root modulo p 2, and then we determine their k-error linear complexity over the finite field \\(\\mathbb{F}_p\\). Theoretical results obtained indicate that such sequences possess \u2018good\u2019 error linear complexity."
      },
      {
        "node_idx": 25660,
        "score_0_10": 10,
        "title": "on k error linear complexity of pseudorandom binary sequences derived from euler quotients",
        "abstract": "We investigate the   \\begin{document}$  k$\\end{document}  -error linear complexity of pseudorandom binary sequences of period   \\begin{document}$ p^{\\mathfrak{r}} $\\end{document}   derived from the Euler quotients modulo   \\begin{document}$ p^{\\mathfrak{r}-1} $\\end{document}  , a power of an odd prime   \\begin{document}$ p $\\end{document}   for   \\begin{document}$ \\mathfrak{r}\u22652 $\\end{document}  . When   \\begin{document}$ \\mathfrak{r} = 2 $\\end{document}  , this is just the case of polynomial quotients (including Fermat quotients) modulo   \\begin{document}$p  $\\end{document}  , which has been studied in an earlier work of Chen, Niu and Wu. In this work, we establish a recursive relation on the   \\begin{document}$ k $\\end{document}  -error linear complexity of the sequences for the case of   \\begin{document}$ \\mathfrak{r}\u22653 $\\end{document}  . We also state the exact values of the   \\begin{document}$ k $\\end{document}  -error linear complexity for the case of   \\begin{document}$ \\mathfrak{r} = 3 $\\end{document}  . From the results, we can find that the   \\begin{document}$ k $\\end{document}  -error linear complexity of the sequences (of period   \\begin{document}$ p^{\\mathfrak{r}} $\\end{document}  ) does not decrease dramatically for   \\begin{document}$ k  ."
      },
      {
        "node_idx": 149168,
        "score_0_10": 10,
        "title": "trace representation of pseudorandom binary sequences derived from euler quotients",
        "abstract": "We give the trace representation of a family of binary sequences derived from Euler quotients by determining the corresponding defining polynomials. Trace representation can help us producing the sequences efficiently and analyzing their cryptographic properties, such as linear complexity."
      },
      {
        "node_idx": 29194,
        "score_0_10": 10,
        "title": "fuchsian codes for awgn channels",
        "abstract": "We develop a new transmission scheme for additive white Gaussian noisy (AWGN) single-input single-output (SISO) channels without fading based on arithmetic Fuchsian groups. The properly discontinuous character of the action of these groups on the upper half-plane translates into logarithmic decoding complexity."
      },
      {
        "node_idx": 105441,
        "score_0_10": 9,
        "title": "on the structure of generalized toric codes",
        "abstract": "Toric codes are obtained by evaluating rational functions of a nonsingular toric variety at the algebraic torus. One can extend toric codes to the so called generalized toric codes. This extension consists on evaluating elements of an arbitrary polynomial algebra at the algebraic torus instead of a linear combination of monomials whose exponents are rational points of a convex polytope. We study their multicyclic and metric structure, and we use them to express their dual and to estimate their minimum distance."
      },
      {
        "node_idx": 87545,
        "score_0_10": 9,
        "title": "high speed tracking with kernelized correlation filters",
        "abstract": "The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies\u2014any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source."
      },
      {
        "node_idx": 69221,
        "score_0_10": 9,
        "title": "constacyclic codes over finite principal ideal rings",
        "abstract": "In this paper, we give an important isomorphism between contacyclic codes and cyclic codes over finite principal ideal rings. Necessary and sufficient conditions for the existence of non-trivial cyclic self-dual codes over finite principal ideal rings are given."
      }
    ]
  },
  "198": {
    "explanation": "adaptive optimization and spatial transformation mechanisms in deep learning",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 88803,
        "score_0_10": 8,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 151734,
        "score_0_10": 8,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 51364,
        "score_0_10": 8,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 148543,
        "score_0_10": 8,
        "title": "fast and accurate deep network learning by exponential linear units elus",
        "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
      }
    ]
  },
  "199": {
    "explanation": "quantum error correcting codes and communication complexity in cellular automata",
    "topk": [
      {
        "node_idx": 49429,
        "score_0_10": 10,
        "title": "constructions of good entanglement assisted quantum error correcting codes",
        "abstract": "Entanglement-assisted quantum error correcting codes (EAQECCs) are a simple and fundamental class of codes. They allow for the construction of quantum codes from classical codes by relaxing the duality condition and using pre-shared entanglement between the sender and receiver. However, in general it is not easy to determine the number of shared pairs required to construct an EAQECC. In this paper, we show that this number is related to the hull of the classical code. Using this fact, we give methods to construct EAQECCs requiring desirable amount of entanglement. This leads to design families of EAQECCs with good error performance. Moreover, we construct maximal entanglement EAQECCs from LCD codes. Finally, we prove the existence of asymptotically good EAQECCs in the odd characteristic case."
      },
      {
        "node_idx": 56083,
        "score_0_10": 10,
        "title": "various constructions for self dual codes over rings and new binary self dual codes",
        "abstract": "In this work, extension theorems are used for self-dual codes over rings and as applications many new binary self-dual extremal codes are found from self-dual codes over F 2 m + u F 2 m for m = 1 , 2 . The duality and distance preserving Gray maps from F 4 + u F 4 to ( F 2 + u F 2 ) 2 and F 2 4 are used to obtain self-dual codes whose binary Gray images are 64 , 32 , 12 -extremal self-dual. An F 2 + u F 2 -extension is used and as binary images, 178 extremal binary self-dual codes of length 68 with new weight enumerators are obtained. Especially the first examples of codes with \u03b3 = 3 and many codes with the rare \u03b3 = 4 , 6 parameters are obtained. In addition to these, two hundred fifty doubly even self dual 96 , 48 , 16 -codes with new weight enumerators are obtained from four-circulant codes over F 4 + u F 4 . New extremal doubly even binary codes of lengths 80 and 88 are also found by the F 2 + u F 2 -lifts of binary four circulant codes and thus a lower bound on the number of non-isomorphic 3-(80, 16, 665) designs is modified."
      },
      {
        "node_idx": 14612,
        "score_0_10": 9,
        "title": "communications in cellular automata",
        "abstract": "The goal of this paper is to show why the framework of communication complexity seems suitable for the study of cellular automata. Researchers have tackled different algorithmic problems ranging from the complexity of predicting to the decidability of different dynamical properties of cellular automata. But the difference here is that we look for communication protocols arising in the dynamics itself. Our work is guided by the following idea: if we are able to give a protocol describing a cellular automaton, then we can understand its behavior."
      },
      {
        "node_idx": 76428,
        "score_0_10": 9,
        "title": "traced communication complexity of cellular automata",
        "abstract": "We study cellular automata with respect to a new communication complexity problem: each of two players know half of some finite word, and must be able to tell whether the state of the central cell will follow a given evolution, by communicating as little as possible between each other. We present some links with classical dynamical concepts, especially equicontinuity, expansiveness, entropy and give the asymptotic communication complexity of most elementary cellular automata."
      },
      {
        "node_idx": 78665,
        "score_0_10": 9,
        "title": "edu edition spreadsheet competency framework",
        "abstract": "Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences."
      },
      {
        "node_idx": 48329,
        "score_0_10": 9,
        "title": "entanglement assisted quantum mds codes from constacyclic codes with large minimum distance",
        "abstract": "The entanglement-assisted (EA) formalism allows arbitrary classical linear codes to transform into entanglement-assisted quantum error correcting codes (EAQECCs) by using pre-shared entanglement between the sender and the receiver. In this work, we propose a decomposition of the defining set of constacyclic codes. Using this method, we construct four classes of $q$-ary entanglement-assisted quantum MDS (EAQMDS) codes based on classical constacyclic MDS codes by exploiting less pre-shared maximally entangled states. We show that a class of $q$-ary EAQMDS have minimum distance upper limit greater than $3q-1$. Some of them have much larger minimum distance than the known quantum MDS (QMDS) codes of the same length. Most of these $q$-ary EAQMDS codes are new in the sense that their parameters are not covered by the codes available in the literature."
      },
      {
        "node_idx": 169029,
        "score_0_10": 9,
        "title": "a macwilliams identity for convolutional codes the general case",
        "abstract": "A MacWilliams Identity for convolutional codes will be established. It makes use of the weight adjacency matrices of the code and its dual, based on state space realizations (the controller canonical form) of the codes in question. The MacWilliams Identity applies to various notions of duality appearing in the literature on convolutional coding theory."
      },
      {
        "node_idx": 58129,
        "score_0_10": 9,
        "title": "the weight distributions of cyclic codes and elliptic curves",
        "abstract": "Cyclic codes with two zeros and their dual codes as a practically and theoretically interesting class of linear codes, have been studied for many years. However, the weight distributions of cyclic codes are difficult to determine. From elliptic curves, this paper determines the weight distributions of dual codes of cyclic codes with two zeros for a few more cases."
      },
      {
        "node_idx": 121149,
        "score_0_10": 9,
        "title": "on the macwilliams identity for convolutional codes",
        "abstract": "The adjacency matrix associated with a convolutional code collects in a detailed manner information about the weight distribution of the code. A MacWilliams Identity Conjecture, stating that the adjacency matrix of a code fully determines the adjacency matrix of the dual code, will be formulated, and an explicit formula for the transformation will be stated. The formula involves the MacWilliams matrix known from complete weight enumerators of block codes. The conjecture will be proven for the class of convolutional codes where either the code itself or its dual does not have Forney indices bigger than one. For the general case the conjecture is backed up by many examples, and a weaker version will be established."
      },
      {
        "node_idx": 118616,
        "score_0_10": 9,
        "title": "probabilistic reversible automata and quantum automata",
        "abstract": "To study relationship between quantum finite automata and probabilistic finite automata, we introduce a notion of probabilistic reversible automata (PRA, or doubly stochastic automata). We find that there is a strong relationship between different possible models of PRA and corresponding models of quantum finite automata. We also propose a classification of reversible finite 1-way automata."
      }
    ]
  },
  "201": {
    "explanation": "constraint satisfaction problem tractability and complexity analysis",
    "topk": [
      {
        "node_idx": 78527,
        "score_0_10": 10,
        "title": "the proof of csp dichotomy conjecture",
        "abstract": "Many natural combinatorial problems can be expressed as constraint satisfaction problems. This class of problems is known to be NP-complete in general, but certain restrictions on the form of the constraints can ensure tractability. The standard way to parameterize interesting subclasses of the constraint satisfaction problem is via finite constraint languages. The main problem is to classify those subclasses that are solvable in polynomial time and those that are NP-complete. It was conjectured that if a core of a constraint language has a weak near unanimity polymorphism then the corresponding constraint satisfaction problem is tractable, otherwise it is NP-complete. #R##N#In the paper we present an algorithm that solves Constraint Satisfaction Problem in polynomial time for constraint languages having a weak near unanimity polymorphism, which proves the remaining part of the conjecture."
      },
      {
        "node_idx": 122577,
        "score_0_10": 10,
        "title": "the gathering problem for two oblivious robots with unreliable compasses",
        "abstract": "Anonymous mobile robots are often classified into synchronous, semi-synchronous and asynchronous robots when discussing the pattern formation problem. For semi-synchronous robots, all patterns formable with memory are also formable without memory, with the single exception of forming a point (i.e., the gathering) by two robots. However, the gathering problem for two semi-synchronous robots without memory is trivially solvable when their local coordinate systems are consistent, and the impossibility proof essentially uses the inconsistencies in their coordinate systems. Motivated by this, this paper investigates the magnitude of consistency between the local coordinate systems necessary and sufficient to solve the gathering problem for two oblivious robots under semi-synchronous and asynchronous models. To discuss the magnitude of consistency, we assume that each robot is equipped with an unreliable compass, the bearings of which may deviate from an absolute reference direction, and that the local coordinate system of each robot is determined by its compass. We consider two families of unreliable compasses, namely,static compasses with constant bearings, and dynamic compasses the bearings of which can change arbitrarily. #R##N#For each of the combinations of robot and compass models, we establish the condition on deviation \\phi that allows an algorithm to solve the gathering problem, where the deviation is measured by the largest angle formed between the x-axis of a compass and the reference direction of the global coordinate system: \\phi < \\pi/2 for semi-synchronous and asynchronous robots with static compasses, \\phi < \\pi/4 for semi-synchronous robots with dynamic compasses, and \\phi < \\pi/6 for asynchronous robots with dynamic compasses. Except for asynchronous robots with dynamic compasses, these sufficient conditions are also necessary."
      },
      {
        "node_idx": 155778,
        "score_0_10": 10,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 138354,
        "score_0_10": 10,
        "title": "hypothetical answers to continuous queries over data streams",
        "abstract": "Continuous queries over data streams may suffer from blocking operations and/or unbound wait, which may delay answers until some relevant input arrives through the data stream. These delays may turn answers, when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers - \"given the current information, it is possible that X will become true at time t\" - instead of no information at all. #R##N#In this paper we present a semantics for queries and corresponding answers that covers such hypothetical answers, together with an online algorithm for updating the set of facts that are consistent with the currently available information."
      },
      {
        "node_idx": 8773,
        "score_0_10": 10,
        "title": "feasible interpolation for qbf resolution calculi",
        "abstract": "In sharp contrast to classical proof complexity we are currently short of lower bound techniques for QBF proof systems. In this paper we establish the feasible interpolation technique for all resolution-based QBF systems, whether modelling CDCL or expansion-based solving. This both provides the first general lower bound method for QBF proof systems as well as largely extends the scope of classical feasible interpolation. We apply our technique to obtain new exponential lower bounds to all resolution-based QBF systems for a new class of QBF formulas based on the clique problem. Finally, we show how feasible interpolation relates to the recently established lower bound method based on strategy extraction."
      },
      {
        "node_idx": 82276,
        "score_0_10": 10,
        "title": "the complexity of general valued csps",
        "abstract": "An instance of the Valued Constraint Satisfaction Problem (VCSP) is given by a finite set of variables, a finite domain of labels, and a sum of functions, each function depending on a subset of the variables. Each function can take finite values specifying costs of assignments of labels to its variables or the infinite value, which indicates infeasible assignments. The goal is to find an assignment of labels to the variables that minimizes the sum. #R##N#We study (assuming that P $\\ne$ NP) how the complexity of this very general problem depends on the set of functions allowed in the instances, the so-called constraint language. The case when all allowed functions take values in $\\{0,\\infty\\}$ corresponds to ordinary CSPs, where one deals only with the feasibility issue and there is no optimization. This case is the subject of the Algebraic CSP Dichotomy Conjecture predicting for which constraint languages CSPs are tractable and for which NP-hard. The case when all allowed functions take only finite values corresponds to finite-valued CSP, where the feasibility aspect is trivial and one deals only with the optimization issue. The complexity of finite-valued CSPs was fully classified by Thapper and \\v{Z}ivn\\'y. #R##N#An algebraic necessary condition for tractability of a general-valued CSP with a fixed constraint language was recently given by Kozik and Ochremiak. As our main result, we prove that if a constraint language satisfies this algebraic necessary condition, and the feasibility CSP corresponding to the VCSP with this language is tractable, then the VCSP is tractable. The algorithm is a simple combination of the assumed algorithm for the feasibility CSP and the standard LP relaxation. As a corollary, we obtain that a dichotomy for ordinary CSPs would imply a dichotomy for general-valued CSPs."
      },
      {
        "node_idx": 69752,
        "score_0_10": 9,
        "title": "a survey of unsupervised deep domain adaptation",
        "abstract": "Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many unsupervised deep domain adaptation approaches have thus been developed. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions."
      },
      {
        "node_idx": 51426,
        "score_0_10": 9,
        "title": "learning transferable features with deep adaptation networks",
        "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks."
      },
      {
        "node_idx": 54018,
        "score_0_10": 9,
        "title": "popular conjectures imply strong lower bounds for dynamic problems",
        "abstract": "We consider several well-studied problems in dynamic algorithms and prove that sufficient progress on any of them would imply a breakthrough on one of five major open problems in the theory of algorithms: #R##N#1. Is the 3SUM problem on $n$ numbers in $O(n^{2-\\epsilon})$ time for some $\\epsilon>0$? #R##N#2. Can one determine the satisfiability of a CNF formula on $n$ variables in $O((2-\\epsilon)^n poly n)$ time for some $\\epsilon>0$? #R##N#3. Is the All Pairs Shortest Paths problem for graphs on $n$ vertices in $O(n^{3-\\epsilon})$ time for some $\\epsilon>0$? #R##N#4. Is there a linear time algorithm that detects whether a given graph contains a triangle? #R##N#5. Is there an $O(n^{3-\\epsilon})$ time combinatorial algorithm for $n\\times n$ Boolean matrix multiplication? #R##N#The problems we consider include dynamic versions of bipartite perfect matching, bipartite maximum weight matching, single source reachability, single source shortest paths, strong connectivity, subgraph connectivity, diameter approximation and some nongraph problems such as Pagh's problem defined in a recent paper by Patrascu [STOC 2010]."
      },
      {
        "node_idx": 138522,
        "score_0_10": 9,
        "title": "dependence and independence",
        "abstract": "We introduce an atomic formula intuitively saying that given variables are independent from given other variables if a third set of variables is kept constant. We contrast this with dependence logic. We show that our independence atom gives rise to a natural logic capable of formalizing basic intuitions about independence and dependence."
      }
    ]
  },
  "203": {
    "explanation": "malleable and non-malleable coding schemes for data integrity and compression",
    "topk": [
      {
        "node_idx": 14004,
        "score_0_10": 10,
        "title": "malleable coding with fixed segment reuse",
        "abstract": "Storage area networks, remote backup storage systems, and similar information systems frequently modify stored data with updates from new versions. In these systems, it is desirable for the data to not only be compressed but to also be easily modified during updates. A malleable coding scheme considers both compression efficiency and ease of alteration, promoting some form of reuse or recycling of codewords. Malleability cost is the difficulty of synchronizing compressed versions, and malleable codes are of particular interest when representing information and modifying the representation are both expensive. We examine the trade-off between compression efficiency and malleability cost measured with respect to the length of a reused prefix portion. The region of achievable rates and malleability is formulated as an information-theoretic optimization and a single-letter expression is provided. Relationships to coded side information and common information problems are also established."
      },
      {
        "node_idx": 32767,
        "score_0_10": 9,
        "title": "non malleable coding against bit wise and split state tampering",
        "abstract": "Non-malleable coding, introduced by Dziembowski, Pietrzak and Wichs (ICS 2010), aims for protecting the integrity of information against tampering attacks in situations where error-detection is impossible. Intuitively, information encoded by a non-malleable code either decodes to the original message or, in presence of any tampering, to an unrelated message. Dziembowski et al. show existence of non-malleable codes for any class of tampering functions of bounded size. #R##N#We consider constructions of coding schemes against two well-studied classes of tampering functions: bit-wise tampering functions (where the adversary tampers each bit of the encoding independently) and split-state adversaries (where two independent adversaries arbitrarily tamper each half of the encoded sequence). #R##N#1. For bit-tampering, we obtain explicit and efficiently encodable and decodable codes of length $n$ achieving rate $1-o(1)$ and error (security) $\\exp(-\\tilde{\\Omega}(n^{1/7}))$. We improve the error to $\\exp(-\\tilde{\\Omega}(n))$ at the cost of making the construction Monte Carlo with success probability $1-\\exp(-\\Omega(n))$. Previously, the best known construction of bit-tampering codes was the Monte Carlo construction of Dziembowski et al. (ICS 2010) achieving rate ~.1887. #R##N#2. We initiate the study of seedless non-malleable extractors as a variation of non-malleable extractors introduced by Dodis and Wichs (STOC 2009). We show that construction of non-malleable codes for the split-state model reduces to construction of non-malleable two-source extractors. We prove existence of such extractors, which implies that codes obtained from our reduction can achieve rates arbitrarily close to 1/5 and exponentially small error. Currently, the best known explicit construction of split-state coding schemes is due to Aggarwal, Dodis and Lovett (ECCC TR13-081) which only achieves vanishing (polynomially small) rate."
      },
      {
        "node_idx": 96632,
        "score_0_10": 9,
        "title": "malleable coding with fixed reuse",
        "abstract": "In cloud computing, storage area networks, remote backup storage, and similar settings, stored data is modified with updates from new versions. Representing information and modifying the representation are both expensive. Therefore it is desirable for the data to not only be compressed but to also be easily modified during updates. A malleable coding scheme considers both compression efficiency and ease of alteration, promoting codeword reuse. We examine the trade-off between compression efficiency and malleability cost-the difficulty of synchronizing compressed versions-measured as the length of a reused prefix portion. Through a coding theorem, the region of achievable rates and malleability is expressed as a single-letter optimization. Relationships to common information problems are also described."
      },
      {
        "node_idx": 153677,
        "score_0_10": 8,
        "title": "from theory to practice sub nyquist sampling of sparse wideband analog signals",
        "abstract": "Conventional sub-Nyquist sampling methods for analog signals exploit prior information about the spectral support. In this paper, we consider the challenging problem of blind sub-Nyquist sampling of multiband signals, whose unknown frequency support occupies only a small portion of a wide spectrum. Our primary design goals are efficient hardware implementation and low computational load on the supporting digital processing. We propose a system, named the modulated wideband converter, which first multiplies the analog signal by a bank of periodic waveforms. The product is then low-pass filtered and sampled uniformly at a low rate, which is orders of magnitude smaller than Nyquist. Perfect recovery from the proposed samples is achieved under certain necessary and sufficient conditions. We also develop a digital architecture, which allows either reconstruction of the analog input, or processing of any band of interest at a low rate, that is, without interpolating to the high Nyquist rate. Numerical simulations demonstrate many engineering aspects: robustness to noise and mismodeling, potential hardware simplifications, real-time performance for signals with time-varying support and stability to quantization effects. We compare our system with two previous approaches: periodic nonuniform sampling, which is bandwidth limited by existing hardware devices, and the random demodulator, which is restricted to discrete multitone signals and has a high computational load. In the broader context of Nyquist sampling, our scheme has the potential to break through the bandwidth barrier of state-of-the-art analog conversion technologies such as interleaved converters."
      },
      {
        "node_idx": 55272,
        "score_0_10": 8,
        "title": "image super resolution using deep convolutional networks",
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."
      },
      {
        "node_idx": 66570,
        "score_0_10": 8,
        "title": "hyperspectral pansharpening a review",
        "abstract": "Pansharpening aims at fusing a panchromatic image with a multispectral one, to generate an image with the high spatial resolution of the former and the high spectral resolution of the latter. In the last decade, many algorithms have been presented in the literature for pansharpening using multispectral data. With the increasing availability of hyperspectral systems, these methods are now being adapted to hyperspectral images. In this work, we compare new pansharpening techniques designed for hyperspectral data with some of the state of the art methods for multispectral pansharpening, which have been adapted for hyperspectral data. Eleven methods from different classes (component substitution, multiresolution analysis, hybrid, Bayesian and matrix factorization) are analyzed. These methods are applied to three datasets and their effectiveness and robustness are evaluated with widely used performance indicators. In addition, all the pansharpening techniques considered in this paper have been implemented in a MATLAB toolbox that is made available to the community."
      },
      {
        "node_idx": 119218,
        "score_0_10": 8,
        "title": "model based compressive sensing",
        "abstract": "Compressive sensing (CS) is an alternative to Shannon/Nyquist sampling for the acquisition of sparse or compressible signals that can be well approximated by just K ? N elements from an N -dimensional basis. Instead of taking periodic samples, CS measures inner products with M < N random vectors and then recovers the signal via a sparsity-seeking optimization or greedy algorithm. Standard CS dictates that robust signal recovery is possible from M = O(K log(N/K)) measurements. It is possible to substantially decrease M without sacrificing robustness by leveraging more realistic signal models that go beyond simple sparsity and compressibility by including structural dependencies between the values and locations of the signal coefficients. This paper introduces a model-based CS theory that parallels the conventional theory and provides concrete guidelines on how to create model-based recovery algorithms with provable performance guarantees. A highlight is the introduction of a new class of structured compressible signals along with a new sufficient condition for robust structured compressible signal recovery that we dub the restricted amplification property, which is the natural counterpart to the restricted isometry property of conventional CS. Two examples integrate two relevant signal models-wavelet trees and block sparsity-into two state-of-the-art CS recovery algorithms and prove that they offer robust recovery from just M = O(K) measurements. Extensive numerical simulations demonstrate the validity and applicability of our new theory and algorithms."
      },
      {
        "node_idx": 123813,
        "score_0_10": 8,
        "title": "robust 1 bit compressive sensing via binary stable embeddings of sparse vectors",
        "abstract": "The Compressive Sensing (CS) framework aims to ease the burden on analog-to-digital converters (ADCs) by reducing the sampling rate required to acquire and stably recover sparse signals. Practical ADCs not only sample but also quantize each measurement to a finite number of bits; moreover, there is an inverse relationship between the achievable sampling rate and the bit depth. In this paper, we investigate an alternative CS approach that shifts the emphasis from the sampling rate to the number of bits per measurement. In particular, we explore the extreme case of 1-bit CS measurements, which capture just their sign. Our results come in two flavors. First, we consider ideal reconstruction from noiseless 1-bit measurements and provide a lower bound on the best achievable reconstruction error. We also demonstrate that i.i.d. random Gaussian matrices describe measurement mappings achieving, with overwhelming probability, nearly optimal error decay. Next, we consider reconstruction robustness to measurement errors and noise and introduce the Binary $\\epsilon$-Stable Embedding (B$\\epsilon$SE) property, which characterizes the robustness measurement process to sign changes. We show the same class of matrices that provide almost optimal noiseless performance also enable such a robust mapping. On the practical side, we introduce the Binary Iterative Hard Thresholding (BIHT) algorithm for signal reconstruction from 1-bit measurements that offers state-of-the-art performance."
      },
      {
        "node_idx": 77683,
        "score_0_10": 8,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      },
      {
        "node_idx": 71804,
        "score_0_10": 8,
        "title": "constacyclic codes over finite fields",
        "abstract": "Abstract   An equivalence relation called isometry is introduced to classify constacyclic codes over a finite field; the polynomial generators of constacyclic codes of length      l    t      p    s      are characterized, where  p  is the characteristic of the finite field and  l  is a prime different from  p ."
      }
    ]
  },
  "206": {
    "explanation": "computational methods and applications in advanced algebraic structures",
    "topk": [
      {
        "node_idx": 6509,
        "score_0_10": 10,
        "title": "isogenies of elliptic curves a computational approach",
        "abstract": "Isogenies, the mappings of elliptic curves, have become a useful tool in cryptology. These mathematical objects have been proposed for use in computing pairings, constructing hash functions and random number generators, and analyzing the reducibility of the elliptic curve discrete logarithm problem. With such diverse uses, understanding these objects is important for anyone interested in the field of elliptic curve cryptography. This paper, targeted at an audience with a knowledge of the basic theory of elliptic curves, provides an introduction to the necessary theoretical background for understanding what isogenies are and their basic properties. This theoretical background is used to explain some of the basic computational tasks associated with isogenies. Herein, algorithms for computing isogenies are collected and presented with proofs of correctness and complexity analyses. As opposed to the complex analytic approach provided in most texts on the subject, the proofs in this paper are primarily algebraic in nature. This provides alternate explanations that some with a more concrete or computational bias may find more clear."
      },
      {
        "node_idx": 41990,
        "score_0_10": 10,
        "title": "variants of mersenne twister suitable for graphic processors",
        "abstract": "This paper proposes a type of pseudorandom number generator, Mersenne Twister for Graphic Processor (MTGP), for efficient generation on graphic processessing units (GPUs). MTGP supports large state sizes such as 11213 bits, and uses the high parallelism of GPUs in computing many steps of the recursion in parallel. The second proposal is a parameter-set generator for MTGP, named MTGP Dynamic Creator (MTGPDC). MT- GPDC creates up to 2^32 distinct parameter sets which generate sequences with high-dimensional uniformity. This facility is suitable for a large grid of GPUs where each GPU requires separate random number streams. MTGP is based on linear recursion over the two-element field, and has better high-dimensional equidistribution than the Mersenne Twister pseudorandom number generator."
      },
      {
        "node_idx": 104549,
        "score_0_10": 10,
        "title": "zero decomposition with multiplicity of zero dimensional polynomial systems",
        "abstract": "We present a zero decomposition theorem and an algorithm based on Wu's method, which computes a zero decomposition with multiplicity for a given zero-dimensional polynomial system. If the system satisfies some condition, the zero decomposition is of triangular form."
      },
      {
        "node_idx": 34704,
        "score_0_10": 10,
        "title": "multiparty compatibility for concurrent objects",
        "abstract": "Objects and actors are communicating state machines, offering and consuming different services at#R##N#different points in their lifecycle. Two complementary challenges arise when programming such#R##N#systems. When objects interact, their state machines must be \u201ccompatible\u201d, so that services are#R##N#requested only when they are available. Dually, when objects refine other objects, their state machines#R##N#must be \u201ccompliant\u201d, so that services are honoured whenever they are promised.#R##N#In this paper we show how the idea of multiparty compatibility from the session types literature#R##N#can be applied to both of these problems. We present an untyped language in which concurrent objects#R##N#are checked automatically for compatibility and compliance. For simple objects, checking can be#R##N#exhaustive and has the feel of a type system. More complex objects can be partially validated via test#R##N#cases, leading to a methodology closer to continuous testing. Our proof-of-concept implementation#R##N#is limited in some important respects, but demonstrates the potential value of the approach and the#R##N#relationship to existing software development practices."
      },
      {
        "node_idx": 28271,
        "score_0_10": 10,
        "title": "resolving zero divisors using hensel lifting",
        "abstract": "Algorithms which compute modulo triangular sets must respect the presence of zero-divisors. We present Hensel lifting as a tool for dealing with them. We give an application: a modular algorithm for computing GCDs of univariate polynomials with coefficients modulo a radical triangular set over the rationals. Our modular algorithm naturally generalizes previous work from algebraic number theory. We have implemented our algorithm using Maple's RECDEN package. We compare our implementation with the procedure RegularGcd in the RegularChains package."
      },
      {
        "node_idx": 133728,
        "score_0_10": 10,
        "title": "the piranha algebraic manipulator",
        "abstract": "In this paper we present a specialised algebraic manipulation package devoted to Celestial Mechanics. The system, called Piranha, is built on top of a generic and extensible framework, which allows to treat efficiently and in a unified way the algebraic structures most commonly encountered in Celestial Mechanics (such as multivariate polynomials and Poisson series). In this contribution we explain the architecture of the software, with special focus on the implementation of series arithmetics, show its current capabilities, and present benchmarks indicating that Piranha is competitive, performance-wise, with other specialised manipulators."
      },
      {
        "node_idx": 53975,
        "score_0_10": 10,
        "title": "on the boundary between decidability and undecidability of asynchronous session subtyping",
        "abstract": "Session types are behavioural types for guaranteeing that concurrent programs are free from basic communication errors. Recent work has shown that asynchronous session subtyping is undecidable. However, since session types have become popular in mainstream programming languages in which asynchronous communication is the norm rather than the exception, it is crucial to detect significant decidable subtyping relations. Previous work considered extremely restrictive fragments in which limitations were imposed to the size of communication buffer (at most 1) or to the possibility to express multiple choices (disallowing them completely in one of the compared types). In this work, for the first time, we show decidability of a fragment that does not impose any limitation on communication buffers and allows both the compared types to include multiple choices for either input or output, thus yielding a fragment which is more significant from an applicability viewpoint. In general, we study the boundary between decidability and undecidability by considering several fragments of subtyping. Notably, we show that subtyping remains undecidable even if restricted to not using output covariance and input contravariance."
      },
      {
        "node_idx": 145816,
        "score_0_10": 10,
        "title": "on folding a polygon to a polyhedron",
        "abstract": "We show that the open problem presented in Geometric Folding Algorithms: Linkages, Origami, Polyhedra [DO07] is solved by a theorem of Burago and Zalgaller [BZ96] from more than a decade earlier."
      },
      {
        "node_idx": 119111,
        "score_0_10": 10,
        "title": "pythagorean triples and cryptographic coding",
        "abstract": "This paper summarizes basic properties of PPTs and shows that each PPT belongs to one of six different classes. Mapping an ordered sequence of PPTs into a corresponding sequence of these six classes makes it possible to use them in cryptography. We pose problems whose solution would facilitate such cryptographic application."
      },
      {
        "node_idx": 61858,
        "score_0_10": 10,
        "title": "enumerating foldings and unfoldings between polygons and polytopes",
        "abstract": "We pose and answer several questions concerning the number of ways to fold a polygon to a polytope, and how many polytopes can be obtained from one polygon; and the analogous questions for unfolding polytopes to polygons. Our answers are, roughly: exponentially many, or nondenumerably infinite."
      }
    ]
  },
  "207": {
    "explanation": "secure efficient oblivious transfer protocols and homomorphic encryption applications",
    "topk": [
      {
        "node_idx": 56074,
        "score_0_10": 10,
        "title": "wiretapped oblivious transfer",
        "abstract": "In this paper, we study the problem of obtaining $1$-of-$2$ string oblivious transfer (OT) between users Alice and Bob, in the presence of a passive eavesdropper Eve. The resource enabling OT in our setup is a noisy broadcast channel from Alice to Bob and Eve. Apart from the OT requirements between the users, Eve is not allowed to learn anything about the users' inputs. When Alice and Bob are honest-but-curious and the noisy broadcast channel is made up of two independent binary erasure channels (connecting Alice-Bob and Alice-Eve), we derive the $1$-of-$2$ string OT capacity for both $2$-privacy (when Eve can collude with either Alice or Bob) and $1$-privacy (when no such collusion is allowed). We generalize these capacity results to $1$-of-$N$ string OT and study other variants of this problem. When Alice and/or Bob are malicious, we present a different scheme based on interactive hashing. This scheme is shown to be optimal for certain parameter regimes. We present a new formulation of multiple, simultaneous OTs between Alice-Bob and Alice-Cathy. For this new setup, we present schemes and outer bounds that match in all but one regime of parameters. Finally, we consider the setup where the broadcast channel is made up of a cascade of two independent binary erasure channels (connecting Alice-Bob and Bob-Eve) and $1$-of-$2$ string OT is desired between Alice and Bob with $1$-privacy. For this setup, we derive an upper and lower bound on the $1$-of-$2$ string OT capacity which match in one of two possible parameter regimes."
      },
      {
        "node_idx": 21389,
        "score_0_10": 9,
        "title": "a secure database system using homomorphic encryption schemes",
        "abstract": "Cloud computing emerges as an attractive solution that can be delegated to store and process confidential data. However, several security risks are encountered with such a system as the securely encrypted data should be decrypted before processing them. Therefore, the decrypted data is susceptible to reading and alterations. As a result, processing encrypted data has been a research subject since the publication of the RSA encryption scheme in 1978. In this paper we present a relational database system based on homomorphic encryption schemes to preserve the integrity and confidentiality of the data. Our system executes SQL queries over encrypted data. We tested our system with a recently developed homomorphic scheme that enables the execution of arithmetic operations on ciphertexts. We show that the proposed system performs accurate SQL operations, yet its performance discourages a practical implementation of this system."
      },
      {
        "node_idx": 89813,
        "score_0_10": 9,
        "title": "oblivious transfer using elliptic curves",
        "abstract": "This paper proposes an algorithm for oblivious transfer using elliptic curves. Also, we present its application to chosen one-out-of-two oblivious transfer."
      },
      {
        "node_idx": 62455,
        "score_0_10": 9,
        "title": "a new efficient k out of n oblivious transfer protocol",
        "abstract": "This paper presents a new efficient protocol for k-out-of-n oblivious transfer which is a generalization of Parakh's 1-out-of-2 oblivious transfer protocol based on Diffie-Hellman key exchange. In the proposed protocol, the parties involved generate Diffie-Hellman keys obliviously and then use them for oblivious transfer of secrets."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 14891,
        "score_0_10": 8,
        "title": "secured outsourced content based image retrieval based on encrypted signatures extracted from homomorphically encrypted images",
        "abstract": "In this paper, we present a novel Secured Outsourced Content Based Image Retrieval solution, which allows looking for similar images stored into the cloud in a homomorphically encrypted form. Its originality is fourfold. In a first time, it extracts from a Paillier encrypted image a wavelet based global image signature. In second, this signature is extracted in an encrypted form and gives no clues about the image content. In a third time, its calculation does not require the cloud to communicate with a trusted third party as usually proposed by other existing schemes. More clearly, all computations required in order to look for similar images are conducted by the cloud only with no extra-communications. To make possible such a computation, we propose a new fast way to compare encrypted data, these ones being encrypted by the same public key or not, and using a recursive relationship in-between Paillier random values when computing the different resolution levels of the image wavelet transform. Experiments conducted in two distinct frameworks: medical image retrieval with as purpose diagnosis aid support, and face recognition for user authentication; indicate that the proposed SOCBIR does not change image retrieval performance."
      },
      {
        "node_idx": 119950,
        "score_0_10": 8,
        "title": "privacy preserving access of outsourced data via oblivious ram simulation",
        "abstract": "Suppose a client, Alice, has outsourced her data to an external storage provider, Bob, because he has capacity for her massive data set, of size n, whereas her private storage is much smaller--say, of size O(n^{1/r}), for some constant r > 1. Alice trusts Bob to maintain her data, but she would like to keep its contents private. She can encrypt her data, of course, but she also wishes to keep her access patterns hidden from Bob as well. We describe schemes for the oblivious RAM simulation problem with a small logarithmic or polylogarithmic amortized increase in access times, with a very high probability of success, while keeping the external storage to be of size O(n). To achieve this, our algorithmic contributions include a parallel MapReduce cuckoo-hashing algorithm and an external-memory dataoblivious sorting algorithm."
      },
      {
        "node_idx": 57198,
        "score_0_10": 8,
        "title": "oblivious transfer based on key exchange",
        "abstract": "Key-exchange protocols have been overlooked as a possible means for implementing oblivious transfer (OT). In this article, we present protocols for mutual exchange of secrets, 1-out-of-2 OT and coin-flipping similar to the Diffie-Hellman protocol using the idea of obliviously exchanging encryption keys. Since the Diffie-Hellman scheme is widely used, our protocol may provide a useful alternative to the conventional methods for implementation of oblivious transfer and a useful primitive in building larger cryptographic schemes."
      },
      {
        "node_idx": 77825,
        "score_0_10": 8,
        "title": "secure k nearest neighbor query over encrypted data in outsourced environments",
        "abstract": "For the past decade, query processing on relational data has been studied extensively, and many theoretical and practical solutions to query processing have been proposed under various scenarios. With the recent popularity of cloud computing, users now have the opportunity to outsource their data as well as the data management tasks to the cloud. However, due to the rise of various privacy issues, sensitive data (e.g., medical records) need to be encrypted before outsourcing to the cloud. In addition, query processing tasks should be handled by the cloud; otherwise, there would be no point to outsource the data at the first place. To process queries over encrypted data without the cloud ever decrypting the data is a very challenging task. In this paper, we focus on solving the k-nearest neighbor (kNN) query problem over encrypted database outsourced to a cloud: a user issues an encrypted query record to the cloud, and the cloud returns the k closest records to the user. We first present a basic scheme and demonstrate that such a naive solution is not secure. To provide better security, we propose a secure kNN protocol that protects the confidentiality of the data, user's input query, and data access patterns. Also, we empirically analyze the efficiency of our protocols through various experiments. These results indicate that our secure protocol is very efficient on the user end, and this lightweight scheme allows a user to use any mobile device to perform the kNN query."
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      }
    ]
  },
  "208": {
    "explanation": "Security vulnerabilities and integrity issues in browser extensions",
    "topk": [
      {
        "node_idx": 9827,
        "score_0_10": 10,
        "title": "domtegrity ensuring web page integrity against malicious browser extensions",
        "abstract": "In this paper, we address an unsolved problem in the real world: how to ensure the integrity of the web content in a browser in the presence of malicious browser extensions? The problem of exposing confidential user credentials to malicious extensions has been widely understood, which has prompted major banks to deploy two-factor authentication. However, the importance of the `integrity' of the web content has received little attention. We implement two attacks on real-world online banking websites and show that ignoring the `integrity' of the web content can fundamentally defeat two-factor solutions. To address this problem, we propose a cryptographic protocol called DOMtegrity to ensure the end-to-end integrity of the DOM structure of a web page from delivering at a web server to the rendering of the page in the user's browser. DOMtegrity is the first solution that protects DOM integrity without modifying the browser architecture or requiring extra hardware. It works by exploiting subtle yet important differences between browser extensions and in-line JavaScript code. We show how DOMtegrity prevents the earlier attacks and a whole range of man-in-the-browser (MITB) attacks. We conduct extensive experiments on more than 14,000 real-world extensions to evaluate the effectiveness of DOMtegrity."
      },
      {
        "node_idx": 18994,
        "score_0_10": 10,
        "title": "botnet in the browser understanding threats caused by malicious browser extensions",
        "abstract": "Browser extensions have been established as a common feature present in modern browsers. However, some extension systems risk exposing APIs which are too permissive and cohesive with the browser's internal structure, thus leaving a hole for malicious developers to exploit security critical functionality within the browser itself. In this paper, we raise the awareness of the threats caused by browser extensions by presenting a botnet framework based on malicious extensions installed in the user's browser, and an exhaustive range of attacks that can be launched in this framework. We systematically categorize, describe and implement these attacks against Chrome, Firefox and Firefox-for-Android, and verify experiments on Windows, Linux and Android systems. To the best of our knowledge, this paper presents to date the most comprehensive analysis about the threats of botnet in modern browsers due to the over-privileged capabilities possessed by browser extensions. We also discuss countermeasures to the identified problems."
      },
      {
        "node_idx": 69161,
        "score_0_10": 10,
        "title": "context aware computing for the internet of things a survey",
        "abstract": "As we are moving towards the Internet of Things (IoT), the number of sensors deployed around the world is growing at a rapid pace. Market research has shown a significant growth of sensor deployments over the past decade and has predicted a significant increment of the growth rate in the future. These sensors continuously generate enormous amounts of data. However, in order to add value to raw sensor data we need to understand it. Collection, modelling, reasoning, and distribution of context in relation to sensor data plays critical role in this challenge. Context-aware computing has proven to be successful in understanding sensor data. In this paper, we survey context awareness from an IoT perspective. We present the necessary background by introducing the IoT paradigm and context-aware fundamentals at the beginning. Then we provide an in-depth analysis of context life cycle. We evaluate a subset of projects (50) which represent the majority of research and commercial solutions proposed in the field of context-aware computing conducted over the last decade (2001-2011) based on our own taxonomy. Finally, based on our evaluation, we highlight the lessons to be learnt from the past and some possible directions for future research. The survey addresses a broad range of techniques, methods, models, functionalities, systems, applications, and middleware solutions related to context awareness and IoT. Our goal is not only to analyse, compare and consolidate past research work but also to appreciate their findings and discuss their applicability towards the IoT."
      },
      {
        "node_idx": 102043,
        "score_0_10": 10,
        "title": "bigdatabench a big data benchmark suite from internet services",
        "abstract": "As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above."
      },
      {
        "node_idx": 57558,
        "score_0_10": 9,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 28810,
        "score_0_10": 9,
        "title": "digital forensic investigation of cloud storage services",
        "abstract": "The demand for cloud computing is increasing because of the popularity of digital devices and the wide use of the Internet. Among cloud computing services, most consumers use cloud storage services that provide mass storage. This is because these services give them various additional functions as well as storage. It is easy to access cloud storage services using smartphones. With increasing utilization, it is possible for malicious users to abuse cloud storage services. Therefore, a study on digital forensic investigation of cloud storage services is necessary. This paper proposes new procedure for investigating and analyzing the artifacts of all accessible devices, such as Windows, Mac, iPhone, and Android smartphone."
      },
      {
        "node_idx": 55888,
        "score_0_10": 9,
        "title": "ipfs content addressed versioned p2p file system",
        "abstract": "The InterPlanetary File System (IPFS) is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. In some ways, IPFS is similar to the Web, but IPFS could be seen as a single BitTorrent swarm, exchanging objects within one Git repository. In other words, IPFS provides a high throughput content-addressed block storage model, with content-addressed hyper links. This forms a generalized Merkle DAG, a data structure upon which one can build versioned file systems, blockchains, and even a Permanent Web. IPFS combines a distributed hashtable, an incentivized block exchange, and a self-certifying namespace. IPFS has no single point of failure, and nodes do not need to trust each other."
      },
      {
        "node_idx": 6078,
        "score_0_10": 9,
        "title": "quire lightweight provenance for smart phone operating systems",
        "abstract": "Smartphone apps often run with full privileges to access the network and sensitive local resources, making it difficult for remote systems to have any trust in the provenance of network connections they receive. Even within the phone, different apps with different privileges can communicate with one another, allowing one app to trick another into improperly exercising its privileges (a Confused Deputy attack). In Quire, we engineered two new security mechanisms into Android to address these issues. First, we track the call chain of IPCs, allowing an app the choice of operating with the diminished privileges of its callers or to act explicitly on its own behalf. Second, a lightweight signature scheme allows any app to create a signed statement that can be verified anywhere inside the phone. Both of these mechanisms are reflected in network RPCs, allowing remote systems visibility into the state of the phone when an RPC is made. We demonstrate the usefulness of Quire with two example applications. We built an advertising service, running distinctly from the app which wants to display ads, which can validate clicks passed to it from its host. We also built a payment service, allowing an app to issue a request which the payment service validates with the user. An app cannot not forge a payment request by directly connecting to the remote server, nor can the local payment service tamper with the request."
      },
      {
        "node_idx": 21444,
        "score_0_10": 9,
        "title": "windows instant messaging app forensics facebook and skype as case studies",
        "abstract": "Instant messaging (IM) has changed the way people communicate with each other. However, the interactive and instant nature of these applications (apps) made them an attractive choice for malicious cyber activities such as phishing. The forensic examination of IM apps for modern Windows 8.1 (or later) has been largely unexplored, as the platform is relatively new. In this paper, we seek to determine the data remnants from the use of two popular Windows Store application software for instant messaging, namely Facebook and Skype on a Windows 8.1 client machine. This research contributes to an in-depth understanding of the types of terrestrial artefacts that are likely to remain after the use of instant messaging services and application software on a contemporary Windows operating system. Potential artefacts detected during the research include data relating to the installation or uninstallation of the instant messaging application software, log-in and log-off information, contact lists, conversations, and transferred files."
      },
      {
        "node_idx": 2924,
        "score_0_10": 9,
        "title": "microservices yesterday today and tomorrow",
        "abstract": "Microservices is an architectural style inspired by service-oriented computing that has recently started gaining popularity. Before presenting the current state-of-the-art in the field, this chapter reviews the history of software architecture, the reasons that led to the diffusion of objects and services first, and microservices later. Finally, open problems and future challenges are introduced. This survey primarily addresses newcomers to the discipline, while offering an academic viewpoint on the topic. In addition, we investigate some practical issues and point out some potential solutions."
      }
    ]
  },
  "210": {
    "explanation": "algorithms and analysis for communication protocols and synchronization in distributed systems",
    "topk": [
      {
        "node_idx": 107337,
        "score_0_10": 10,
        "title": "coded slotted aloha a graph based method for uncoordinated multiple access",
        "abstract": "In this paper, a random access scheme is introduced, which relies on the combination of packet erasure correcting codes and successive interference cancellation (SIC). The scheme is named coded slotted ALOHA. A bipartite graph representation of the SIC process, resembling iterative decoding of generalized low-density parity-check codes over the erasure channel, is exploited to optimize the selection probabilities of the component erasure correcting codes through a density evolution analysis. The capacity (in packets per slot) of the scheme is then analyzed in the context of the collision channel without feedback. Moreover, a capacity bound is developed, and component code distributions tightly approaching the bound are derived."
      },
      {
        "node_idx": 127096,
        "score_0_10": 10,
        "title": "type inference for deadlock detection in a multithreaded polymorphic typed assembly language",
        "abstract": "We previously developed a polymorphic type system and a type checker for a multithreaded lock-based polymorphic typed assembly language (MIL) that ensures that well-typed programs do not encounter race conditions. This paper extends such work by taking into consideration deadlocks. The extended type system verifies that locks are acquired in the proper order. Towards this end we require a language with annotations that specify the locking order. Rather than asking the programmer (or the compiler's backend) to specifically annotate each newly introduced lock, we present an algorithm to infer the annotations. The result is a type checker whose input language is non-decorated as before, but that further checks that programs are exempt from deadlocks."
      },
      {
        "node_idx": 85703,
        "score_0_10": 10,
        "title": "strong conflict free coloring of intervals",
        "abstract": "We consider the k-strong conflict-free coloring of a set of points on a line with respect to a family of intervals: Each point on the line must be assigned a color so that the coloring has to be conflict-free, in the sense that in every interval I there are at least k colors each appearing exactly once in I. In this paper, we present a polynomial algorithm for the general problem; the algorithm has an approximation factor 5-2/k when k\\geq2 and approximation factor 2 for k=1. In the special case the family contains all the possible intervals on the given set of points, we show that a 2 approximation algorithm exists, for any k\\geq1."
      },
      {
        "node_idx": 13090,
        "score_0_10": 10,
        "title": "deludedly agreeing to agree",
        "abstract": "We study conditions relating to the impossibility of agreeing to disagree in models of interactive KD45 belief (in contrast to models of S5 knowledge, which are used in nearly all the agreements literature). We show that even when the truth axiom is not assumed it turns out that players will find it impossible to agree to disagree under fairly broad conditions."
      },
      {
        "node_idx": 63849,
        "score_0_10": 10,
        "title": "types for x10 clocks",
        "abstract": "X10 is a modern language built from the ground up to handle future parallel systems, from multicore machines to cluster configurations. We take a closer look at a pair of synchronisation mechanisms: finish and clocks. The former waits for the termination of parallel computations, the latter allow multiple concurrent activities to wait for each other at certain points in time. In order to better understand these concepts we study a type system for a stripped down version of X10. The main result assures that well typed programs do not run into the errors identified in the X10 language reference, namely the ClockUseException. The study will open, we hope, doors to a more flexible utilisation of clocks in the X10 language."
      },
      {
        "node_idx": 119832,
        "score_0_10": 9,
        "title": "cooperative slotted aloha for multi base station systems",
        "abstract": "We introduce a framework to study slotted Aloha with cooperative base stations. Assuming a geographic-proximity communication model, we propose several decoding algorithmswith different degrees of base stations' cooperation (non-cooperative, spatial, temporal, and spatio-temporal). With spatial cooperation, neighboring base stations inform each other whenever they collect a user within their coverage overlap; temporal cooperation corresponds to (temporal) successive interference cancellation done locally at each station. We analyze the four decoding algorithms and establish several fundamental results. With all algorithms, the peak throughput (average number of decoded users per slot, across all base stations) increases linearly with the number of base stations. Further, temporal and spatio-temporal cooperations exhibit a threshold behavior with respect to the normalized load (number of users per station, per slot). There exists a positive load $G^\\star$, such that, below $G^\\star$, the decoding probability is asymptotically maximal possible, equal the probability that a user is heard by at least one base station; with non-cooperative decoding and spatial cooperation, we show that $G^\\star$ is zero. Finally, with spatio-temporal cooperation, we optimize the degree distribution according to which users transmit their packet replicas; the optimum is in general very different from the corresponding optimal distribution of the single-base station system."
      },
      {
        "node_idx": 107179,
        "score_0_10": 9,
        "title": "non cooperative games with preplay negotiations",
        "abstract": "We consider an extension of strategic normal form games with a phase of negotiations before the actual play of the game, where players can make binding oers for transfer of utilities to other players after the play of the game, in order to provide additional incentives for each other to play designated strategies. The enforcement of such oers is conditional on the recipients playing the specied strategies and they eect transformations of the payo matrix of the game by accordingly transferring payos between players. Players can exchange series of such oers in a preplay negotiation game in an extensive form. We introduce and analyze solution concepts for normal form games with such preplay oers under various assumptions for the preplay negotiation phase and obtain results for existence of ecient negotiation strategies of the players."
      },
      {
        "node_idx": 92281,
        "score_0_10": 9,
        "title": "cell free massive mimo versus small cells",
        "abstract": "A Cell-Free Massive MIMO (multiple-input multiple-output) system comprises a very large number of distributed access points (APs)which simultaneously serve a much smaller number of users over the same time/frequency resources based on directly measured channel characteristics. The APs and users have only one antenna each. The APs acquire channel state information through time-division duplex operation and the reception of uplink pilot signals transmitted by the users. The APs perform multiplexing/de-multiplexing through conjugate beamforming on the downlink and matched filtering on the uplink. Closed-form expressions for individual user uplink and downlink throughputs lead to max-min power control algorithms. Max-min power control ensures uniformly good service throughout the area of coverage. A pilot assignment algorithm helps to mitigate the effects of pilot contamination, but power control is far more important in that regard. #R##N#Cell-Free Massive MIMO has considerably improved performance with respect to a conventional small-cell scheme, whereby each user is served by a dedicated AP, in terms of both 95%-likely per-user throughput and immunity to shadow fading spatial correlation. Under uncorrelated shadow fading conditions, the cell-free scheme provides nearly 5-fold improvement in 95%-likely per-user throughput over the small-cell scheme, and 10-fold improvement when shadow fading is correlated."
      },
      {
        "node_idx": 38177,
        "score_0_10": 9,
        "title": "rendezvous of two robots with visible bits",
        "abstract": "We study the rendezvous problem for two robots moving in the plane (or on a line). Robots are autonomous, anonymous, oblivious, and carry colored lights that are visible to both. We consider deterministic distributed algorithms in which robots do not use distance information, but try to reduce (or increase) their distance by a constant factor, depending on their lights' colors. #R##N#We give a complete characterization of the number of colors that are necessary to solve the rendezvous problem in every possible model, ranging from fully synchronous to semi-synchronous to asynchronous, rigid and non-rigid, with preset or arbitrary initial configuration. #R##N#In particular, we show that three colors are sufficient in the non-rigid asynchronous model with arbitrary initial configuration. In contrast, two colors are insufficient in the rigid asynchronous model with arbitrary initial configuration and in the non-rigid asynchronous model with preset initial configuration. #R##N#Additionally, if the robots are able to distinguish between zero and non-zero distances, we show how they can solve rendezvous and detect termination using only three colors, even in the non-rigid asynchronous model with arbitrary initial configuration."
      },
      {
        "node_idx": 11209,
        "score_0_10": 9,
        "title": "non preemptive scheduling on machines with setup times",
        "abstract": "Consider the problem in which n jobs that are classified into k types are to be scheduled on m identical machines without preemption. A machine requires a proper setup taking s time units before processing jobs of a given type. The objective is to minimize the makespan of the resulting schedule. We design and analyze an approximation algorithm that runs in time polynomial in n, m and k and computes a solution with an approximation factor that can be made arbitrarily close to 3/2."
      }
    ]
  },
  "211": {
    "explanation": "robot programming using CAD models and motion extraction",
    "topk": [
      {
        "node_idx": 123174,
        "score_0_10": 10,
        "title": "direct off line robot programming via a common cad package",
        "abstract": "a b s t r a c t This paper focuses on intuitive and direct off-line robot programming from a CAD drawing running on a common 3-D CAD package. It explores the most suitable way to represent robot motion in a CAD drawing, how to automatically extract such motion data from the drawing, make the mapping of data from the virtual (CAD model) to the real environment and the process of automatic generation of robot paths/programs. In summary, this study aims to present a novel CAD-based robot programming system accessible to anyone with basic knowledge of CAD and robotics. Experiments on different manipulation tasks show the effectiveness and versatility of the proposed approach."
      },
      {
        "node_idx": 61035,
        "score_0_10": 10,
        "title": "interactive visual exploration of topic models using graphs",
        "abstract": "Probabilistic topic modeling is a popular and powerful family of tools for uncovering thematic structure in large sets of unstructured text documents. While much attention has been directed towards the modeling algorithms and their various extensions, comparatively few studies have concerned how to present or visualize topic models in meaningful ways. In this paper, we present a novel design that uses graphs to visually communicate topic structure and meaning. By connecting topic nodes via descriptive keyterms, the graph representation reveals topic similarities, topic meaning and shared, ambiguous keyterms. At the same time, the graph can be used for information retrieval purposes, to find documents by topic or topic subsets. To exemplify the utility of the design, we illustrate its use for organizing and exploring corpora of financial patents."
      },
      {
        "node_idx": 71799,
        "score_0_10": 10,
        "title": "high level robot programming based on cad dealing with unpredictable environments",
        "abstract": "Purpose \u2013 The purpose of this paper is to present a CAD\u2010based human\u2010robot interface that allows non\u2010expert users to teach a robot in a manner similar to that used by human beings to teach each other.Design/methodology/approach \u2013 Intuitive robot programming is achieved by using CAD drawings to generate robot programs off\u2010line. Sensory feedback allows minimization of the effects of uncertainty, providing information to adjust the robot paths during robot operation.Findings \u2013 It was found that it is possible to generate a robot program from a common CAD drawing and run it without any major concerns about calibration or CAD model accuracy.Research limitations/implications \u2013 A limitation of the proposed system has to do with the fact that it was designed to be used for particular technological applications.Practical implications \u2013 Since most manufacturing companies have CAD packages in their facilities today, CAD\u2010based robot programming may be a good option to program robots without the need for skilled robot ..."
      },
      {
        "node_idx": 109946,
        "score_0_10": 10,
        "title": "left recursion in parsing expression grammars",
        "abstract": "Parsing Expression Grammars (PEGs) are a formalism that can describe all deterministic context-free languages through a set of rules that specify a top-down parser for some language. PEGs are easy to use, and there are efficient implementations of PEG libraries in several programming languages.A frequently missed feature of PEGs is left recursion, which is commonly used in Context-Free Grammars (CFGs) to encode left-associative operations. We present a simple conservative extension to the semantics of PEGs that gives useful meaning to direct and indirect left-recursive rules, and show that our extensions make it easy to express left-recursive idioms from CFGs in PEGs, with similar results. We prove the conservativeness of these extensions, and also prove that they work with any left-recursive PEG.PEGs can also be compiled to programs in a low-level parsing machine. We present an extension to the semantics of the operations of this parsing machine that let it interpret left-recursive PEGs, and prove that this extension is correct with regard to our semantics for left-recursive PEGs. We present a semantics for left-recursive Parsing Expression Grammars.A small extension adds precedence/associativity declarations to operator grammars.We give a semantics for compilation of left-recursive PEGs to a parsing machine.Our semantics are conservative: non-left-recursive PEGs are unaffected."
      },
      {
        "node_idx": 85872,
        "score_0_10": 10,
        "title": "visual abstraction",
        "abstract": "In this article we revisit the concept of abstraction as it is used in visualization and put it on a solid formal footing. While the term \\emph{abstraction} is utilized in many scientific disciplines, arts, as well as everyday life, visualization inherits the notion of data abstraction or class abstraction from computer science, topological abstraction from mathematics, and visual abstraction from arts. All these notions have a lot in common, yet there is a major discrepancy in the terminology and basic understanding about visual abstraction in the context of visualization. We thus root the notion of abstraction in the philosophy of science, clarify the basic terminology, and provide crisp definitions of visual abstraction as a process. Furthermore, we clarify how it relates to similar terms often used interchangeably in the field of visualization. Visual abstraction is characterized by a conceptual space where this process exists, by the purpose it should serve, and by the perceptual and cognitive qualities of the beholder. These characteristics can be used to control the process of visual abstraction to produce effective and informative visual representations."
      },
      {
        "node_idx": 11164,
        "score_0_10": 9,
        "title": "gaze contingent ocular parallax rendering for virtual reality",
        "abstract": "Immersive computer graphics systems strive to generate perceptually realistic user experiences. Current-generation virtual reality (VR) displays are successful in accurately rendering many perceptually important effects, including perspective, disparity, motion parallax, and other depth cues. In this paper we introduce ocular parallax rendering, a technology that accurately renders small amounts of gaze-contingent parallax capable of improving depth perception and realism in VR. Ocular parallax describes the small amounts of depth-dependent image shifts on the retina that are created as the eye rotates. The effect occurs because the centers of rotation and projection of the eye are not the same. We study the perceptual implications of ocular parallax rendering by designing and conducting a series of user experiments. Specifically, we estimate perceptual detection and discrimination thresholds for this effect and demonstrate that it is clearly visible in most VR applications. Additionally, we show that ocular parallax rendering provides an effective ordinal depth cue and it improves the impression of realistic depth in VR."
      },
      {
        "node_idx": 148800,
        "score_0_10": 9,
        "title": "faster range minimum queries",
        "abstract": "Range Minimum Query (RMQ) is an important building brick of many compressed data structures and string matching algorithms. Although this problem is essentially solved in theory, with sophisticated data structures allowing for constant time queries, practical performance and construction time also matter. Additionally, there are offline scenarios in which the number of queries, $q$, is rather small and given beforehand, which encourages to use a simpler approach. In this work, we present a simple data structure, with very fast construction, which allows to handle queries in constant time on average. This algorithm, however, requires access to the input data during queries (which is not the case of sophisticated RMQ solutions). We subsequently refine our technique, combining it with one of the existing succinct solutions with $O(1)$ worst-case time queries and no access to the input array. The resulting hybrid is still a memory frugal data structure, spending usually up to about $3n$ bits, and providing competitive query times, especially for wide ranges. We also show how to make our baseline data structure more compact. Experimental results demonstrate that the proposed BbST (Block-based Sparse Table) variants are competitive to existing solutions, also in the offline scenario."
      },
      {
        "node_idx": 98829,
        "score_0_10": 9,
        "title": "deciding regularity of hairpin completions of regular languages in polynomial time",
        "abstract": "The hairpin completion is an operation on formal languages that has been inspired by the hairpin formation in DNA biochemistry and by DNA computing. In this paper we investigate the hairpin completion of regular languages. #R##N#It is well known that hairpin completions of regular languages are linear context-free and not necessarily regular. As regularity of a (linear) context-free language is not decidable, the question arose whether regularity of a hairpin completion of regular languages is decidable. We prove that this problem is decidable and we provide a polynomial time algorithm. #R##N#Furthermore, we prove that the hairpin completion of regular languages is an unambiguous linear context-free language and, as such, it has an effectively computable growth function. Moreover, we show that the growth of the hairpin completion is exponential if and only if the growth of the underlying languages is exponential and, in case the hairpin completion is regular, then the hairpin completion and the underlying languages have the same growth indicator."
      },
      {
        "node_idx": 149764,
        "score_0_10": 9,
        "title": "an optimal fuzzy pi force motion controller to increase industrial robot autonomy",
        "abstract": "This paper presents a method for robot self-recognition and self-adaptation through the analysis of the contact between the robot end effector and its surrounding environment. Often, in off-line robot programming, the idealized robotic environment (the virtual one) does not reflect accurately the real one. In this situation, we are in the presence of a partially unknown environment (PUE). Thus, robotic systems must have some degree of autonomy to overcome this situation, especially when contact exists. The proposed force/motion control system has an external control loop based on forces and torques exerted on the robot end effector and an internal control loop based on robot motion. The external control loop is tested with an optimal proportional integrative (PI) and a fuzzy-PI controller. The system performance is validated with real-world experiments involving contact in PUEs."
      },
      {
        "node_idx": 80546,
        "score_0_10": 9,
        "title": "tracy a business driven technical debt prioritization framework",
        "abstract": "Technical debt is a pervasive problem in software development. Software development teams have to prioritize debt items and determine whether they should address debt or develop new features at any point in time. This paper presents \"Tracy\", a framework for the prioritization of technical debt using a business-driven approach built on top of business processes. The current stage of the proposed framework is at the beginning of the third phase of Design Science Research, which is usually divided into the phases of exploration, engineering, and evaluation. The exploration and engineering phases involved the participation of 49 professionals from 12 different groups of three companies. The initial evaluation shows that the presented framework is coherent in its structure and that its results contribute to business-driven decision making on technical debt prioritization."
      }
    ]
  },
  "212": {
    "explanation": "resource management and coding theory for cloud and communication systems",
    "topk": [
      {
        "node_idx": 8397,
        "score_0_10": 10,
        "title": "temporal overbooking of lambda functions in the cloud",
        "abstract": "We consider the problem of scheduling \"serverless computing\" instances such as Amazon Lambda functions. Instead of a quota per tenant/customer, we assume demand for Lambda functions is modulated by token-bucket mechanisms per tenant. Based on an upper bound on the stationary number of active \"Lambda servers\" considering the execution-time distribution of Lambda functions, we describe an approach that the cloud could use to overbook Lambda functions for improved utilization of IT resources. An earlier bound for a single service tier is extended to the case of multiple service tiers."
      },
      {
        "node_idx": 120337,
        "score_0_10": 10,
        "title": "serverless in the wild characterizing and optimizing the serverless workload at a large cloud provider",
        "abstract": "Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload from Microsoft Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function coldstarts,while spending fewerresources than state-of-the-practice policies."
      },
      {
        "node_idx": 166346,
        "score_0_10": 10,
        "title": "new constructions of mds codes with complementary duals",
        "abstract": "Linear complementary-dual (LCD for short) codes are linear codes that intersect with their duals trivially. LCD codes have been used in certain communication systems. It is recently found that LCD codes can be applied in cryptography. This application of LCD codes renewed the interest in the construction of LCD codes having a large minimum distance. MDS codes are optimal in the sense that the minimum distance cannot be improved for given length and code size. Constructing LCD MDS codes is thus of significance in theory and practice. Recently, Jin (\\cite{Jin}, IEEE Trans. Inf. Theory, 2016) constructed several classes of LCD MDS codes through generalized Reed-Solomon codes. In this paper, a different approach is proposed to obtain new LCD MDS codes from generalized Reed-Solomon codes. Consequently, new code constructions are provided and certain previously known results in \\cite{Jin} are extended."
      },
      {
        "node_idx": 45753,
        "score_0_10": 10,
        "title": "new mds self dual codes over finite fields of odd characteristic",
        "abstract": "In this paper, we produce new classes of MDS self-dual codes via (extended) generalized Reed-Solomon codes over finite fields of odd characteristic. Among our constructions, there are many MDS self-dual codes with new parameters which have never been reported. For odd prime power $q$ with $q$ square, the total number of lengths for MDS self-dual codes over $\\mathbb{F}_q$ presented in this paper is much more than those in all the previous results."
      },
      {
        "node_idx": 37566,
        "score_0_10": 10,
        "title": "autism children s app using pecs",
        "abstract": "Since autistic children suffers from learning disabilities and communication barriers, this research aim to design, develop and evaluate an Android based mobile application (app) providing better learning environment with inclusion of graphical representation in a cost effective manner. This research evaluate various supporting technologies and finds Picture Exchange Communication System (PECS) to be better choice for integrating with the app. Evaluation results reveal that the inclusion of PECS helped the children suffering from Autistic Spectrum Disorder (ASD) to better communicate with others. The study included autistic children who do not speak, who are unintelligible and who are minimally effective communicators with their present communication system. The evolution results showed encouraging impacts of the Autism App in supporting autistic children to adapt to normal life and improve the standard of their life."
      },
      {
        "node_idx": 109976,
        "score_0_10": 10,
        "title": "an open source research platform for embedded visible light networking",
        "abstract": "Despite the growing interest in visible light communication, a reference networking platform based on commercial off-the-shelf components is not available yet. An open source platform would lower the barriers to entry of VLC network research and help the VLC community gain momentum. We introduce OpenVLC, an open source VLC research platform based on softwaredefined implementation. Built around a creditcard- sized embedded Linux platform with a simple opto-electronic transceiver front-end, OpenVLC offers a basic physical layer, a set of essential medium access primitives, as well as interoperability with Internet protocols. We investigate the performance of OpenVLC and show examples of how it can be used along with standard network diagnostics tools. Our software-defined implementation can currently reach throughput on the order of the basic rate of the IEEE 802.15.7 standard. We discuss several techniques that researchers and engineers could introduce to improve the performance of OpenVLC and envision several directions that can benefit from OpenVLC by adopting it as a reference platform."
      },
      {
        "node_idx": 126960,
        "score_0_10": 10,
        "title": "exploring multimodal data fusion through joint decompositions with flexible couplings",
        "abstract": "A Bayesian framework is proposed to define flexible coupling models for joint tensor decompositions of multiple datasets. Under this framework, a natural formulation of the data fusion problem is to cast it in terms of a joint maximum a posteriori (MAP) estimator. Data-driven scenarios of joint posterior distributions are provided, including general Gaussian priors and non Gaussian coupling priors. We present and discuss implementation issues of algorithms used to obtain the joint MAP estimator. We also show how this framework can be adapted to tackle the problem of joint decompositions of large datasets. In the case of a conditional Gaussian coupling with a linear transformation, we give theoretical bounds on the data fusion performance using the Bayesian Cramer\u2013Rao bound. Simulations are reported for hybrid coupling models ranging from simple additive Gaussian models to Gamma-type models with positive variables and to the coupling of data sets which are inherently of different size due to different resolution of the measurement devices."
      },
      {
        "node_idx": 93669,
        "score_0_10": 9,
        "title": "mobile recommender systems identifying the major concepts",
        "abstract": "This paper identifies the factors that have an impact on mobile recommender systems. Recommender systems have become a technology that has been widely used by various online applications in situations where there is an information overload problem. Numerous applications such as e-Commerce, video platforms and social networks provide personalized recommendations to their users and this has improved the user experience and vendor revenues. The development of recommender systems has been focused mostly on the proposal of new algorithms that provide more accurate recommendations. However, the use of mobile devices and the rapid growth of the internet and networking infrastructure has brought the necessity of using mobile recommender systems. The links between web and mobile recommender systems are described along with how the recommendations in mobile environments can be improved. This work is focused on identifying the links between web and mobile recommender systems and to provide solid future directions that aim to lead in a more integrated mobile recommendation domain."
      },
      {
        "node_idx": 39655,
        "score_0_10": 9,
        "title": "constructions of mds self dual codes from short length",
        "abstract": "Systematic constructions of MDS self-dual codes is widely concerned. In this paper, we consider the constructions of MDS Euclidean self-dual codes from short length. Indeed, the exact constructions of MDS Euclidean self-dual codes from short length ($n=3,4,5,6$) are given. In general, we construct more new of $q$-ary MDS Euclidean self-dual codes from MDS self-dual codes of known length via generalized Reed-Solomon (GRS for short) codes and extended GRS codes."
      },
      {
        "node_idx": 126777,
        "score_0_10": 9,
        "title": "new mds euclidean self orthogonal codes",
        "abstract": "In this paper, a criterion of MDS Euclidean self-orthogonal codes is presented. New MDS Euclidean self-dual codes and self-orthogonal codes are constructed via this criterion. In particular, among our constructions, for large square $q$, about $\\frac{1}{8}\\cdot q$ new MDS Euclidean (almost) self-dual codes over $\\F_q$ can be produced. Moreover, we can construct about $\\frac{1}{4}\\cdot q$ new MDS Euclidean self-orthogonal codes with different even lengths $n$ with dimension $\\frac{n}{2}-1$."
      }
    ]
  },
  "217": {
    "explanation": "neural models integrating linguistic structure and language morphology",
    "topk": [
      {
        "node_idx": 130570,
        "score_0_10": 10,
        "title": "reformulating global grammar constraints",
        "abstract": "An attractive mechanism to specify global constraints in rostering and other domains is via formal languages. For instance, the Regular and Grammar constraints specify constraints in terms of the languages accepted by an automaton and a context-free grammar respectively. Taking advantage of the fixed length of the constraint, we give an algorithm to transform a context-free grammar into an automaton. We then study the use of minimization techniques to reduce the size of such automata and speed up propagation. We show that minimizing such automata after they have been unfolded and domains initially reduced can give automata that are more compact than minimizing before unfolding and reducing. Experimental results show that such transformations can improve the size of rostering problems that we can 'model and run'."
      },
      {
        "node_idx": 29514,
        "score_0_10": 10,
        "title": "mtil17 english to indian langauge statistical machine translation",
        "abstract": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation."
      },
      {
        "node_idx": 15976,
        "score_0_10": 10,
        "title": "remembering chandra kintala",
        "abstract": "With this contribution we would like to remember Chandra M. R. Kintala who passed away in November 2009. We will give short overviews of his CV and his contributions to the field of theoretical and applied computer science and, given the opportunity, will attempt to present the current state of limited nondeterminism and limited resources for machines. Finally, we will briefly touch on some research topics which hopefully will be addressed in the not so distant future."
      },
      {
        "node_idx": 140427,
        "score_0_10": 10,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 88762,
        "score_0_10": 9,
        "title": "computational approach to anaphora resolution in spanish dialogues",
        "abstract": "This paper presents an algorithm for identifying noun-phrase antecedents of pronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora resolution requires numerous sources of information in order to find the correct antecedent of the anaphor. These sources can be of different kinds, e.g., linguistic information, discourse/dialogue structure information, or topic information. For this reason, our algorithm uses various different kinds of information (hybrid information). The algorithm is based on linguistic constraints and preferences and uses an anaphoric accessibility space within which the algorithm finds the noun phrase. We present some experiments related to this algorithm and this space using a corpus of 204 dialogues. The algorithm is implemented in Prolog. According to this study, 95.9% of antecedents were located in the proposed space, a precision of 81.3% was obtained for pronominal anaphora resolution, and 81.5% for adjectival anaphora."
      },
      {
        "node_idx": 11752,
        "score_0_10": 9,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 84403,
        "score_0_10": 9,
        "title": "extending automatic discourse segmentation for texts in spanish to catalan",
        "abstract": "At present, automatic discourse analysis is a relevant research topic in the field of NLP. However, discourse is one of the phenomena most difficult to process. Although discourse parsers have been already developed for several languages, this tool does not exist for Catalan. In order to implement this kind of parser, the first step is to develop a discourse segmenter. In this article we present the first discourse segmenter for texts in Catalan. This segmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses lexical and syntactic information to translate rules valid for Spanish into rules for Catalan. We have evaluated the system by using a gold standard corpus including manually segmented texts and results are promising."
      },
      {
        "node_idx": 137083,
        "score_0_10": 9,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 69967,
        "score_0_10": 9,
        "title": "character aware neural language models",
        "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information."
      }
    ]
  },
  "218": {
    "explanation": "residual connections enhancing deep neural network training and generalization",
    "topk": [
      {
        "node_idx": 53950,
        "score_0_10": 10,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 103461,
        "score_0_10": 8,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      },
      {
        "node_idx": 614,
        "score_0_10": 8,
        "title": "estimating failure in brittle materials using graph theory",
        "abstract": "In brittle fracture applications, failure paths, regions where the failure occurs and damage statistics, are some of the key quantities of interest (QoI). High-fidelity models for brittle failure that accurately predict these QoI exist but are highly computationally intensive, making them infeasible to incorporate in upscaling and uncertainty quantification frameworks. The goal of this paper is to provide a fast heuristic to reasonably estimate quantities such as failure path and damage in the process of brittle failure. Towards this goal, we first present a method to predict failure paths under tensile loading conditions and low-strain rates. The method uses a $k$-nearest neighbors algorithm built on fracture process zone theory, and identifies the set of all possible pre-existing cracks that are likely to join early to form a large crack. The method then identifies zone of failure and failure paths using weighted graphs algorithms. We compare these failure paths to those computed with a high-fidelity model called the Hybrid Optimization Software Simulation Suite (HOSS). A probabilistic evolution model for average damage in a system is also developed that is trained using 150 HOSS simulations and tested on 40 simulations. A non-parametric approach based on confidence intervals is used to determine the damage evolution over time along the dominant failure path. For upscaling, damage is the key QoI needed as an input by the continuum models. This needs to be informed accurately by the surrogate models for calculating effective modulii at continuum-scale. We show that for the proposed average damage evolution model, the prediction accuracy on the test data is more than 90\\%. In terms of the computational time, the proposed models are $\\approx \\mathcal{O}(10^6)$ times faster compared to high-fidelity HOSS."
      },
      {
        "node_idx": 141138,
        "score_0_10": 8,
        "title": "training deep spiking neural networks using backpropagation",
        "abstract": "Deep spiking neural networks (SNNs) hold great potential for improving the latency and energy efficiency of deep neural networks through event-based computation. However, training such networks is difficult due to the non-differentiable nature of asynchronous spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are only considered as noise. This enables an error backpropagation mechanism for deep SNNs, which works directly on spike signals and membrane potentials. Thus, compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statics of spikes more precisely. Our novel framework outperforms all previously reported results for SNNs on the permutation invariant MNIST benchmark, as well as the N-MNIST benchmark recorded with event-based vision sensors."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 22502,
        "score_0_10": 8,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 111390,
        "score_0_10": 8,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 80414,
        "score_0_10": 8,
        "title": "3d u net learning dense volumetric segmentation from sparse annotation",
        "abstract": "This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases."
      }
    ]
  },
  "219": {
    "explanation": "end-to-end differentiable neural network architectures with memory and attention",
    "topk": [
      {
        "node_idx": 43354,
        "score_0_10": 10,
        "title": "neural turing machines",
        "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."
      },
      {
        "node_idx": 45381,
        "score_0_10": 10,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 45355,
        "score_0_10": 10,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 29312,
        "score_0_10": 9,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 27774,
        "score_0_10": 9,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 157548,
        "score_0_10": 9,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 43376,
        "score_0_10": 9,
        "title": "skip thought vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
      },
      {
        "node_idx": 9964,
        "score_0_10": 9,
        "title": "end to end memory networks",
        "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results."
      }
    ]
  },
  "221": {
    "explanation": "detailed 3D face reconstruction from single images",
    "topk": [
      {
        "node_idx": 125243,
        "score_0_10": 10,
        "title": "learning detailed face reconstruction from a single image",
        "abstract": "Reconstructing the detailed geometric structure of a face from a given image is a key to many computer vision and graphics applications, such as motion capture and reenactment. The reconstruction task is challenging as human faces vary extensively when considering expressions, poses, textures, and intrinsic geometries. While many approaches tackle this complexity by using additional data to reconstruct the face of a single subject, extracting facial surface from a single image remains a difficult problem. As a result, single-image based methods can usually provide only a rough estimate of the facial geometry. In contrast, we propose to leverage the power of convolutional neural networks to produce a highly detailed face reconstruction from a single image. For this purpose, we introduce an end-to-end CNN framework which derives the shape in a coarse-to-fine fashion. The proposed architecture is composed of two main blocks, a network that recovers the coarse facial geometry (CoarseNet), followed by a CNN that refines the facial features of that geometry (FineNet). The proposed networks are connected by a novel layer which renders a depth image given a mesh in 3D. Unlike object recognition and detection problems, there are no suitable datasets for training CNNs to perform face geometry reconstruction. Therefore, our training regime begins with a supervised phase, based on synthetic images, followed by an unsupervised phase that uses only unconstrained facial images. The accuracy and robustness of the proposed model is demonstrated by both qualitative and quantitative evaluation tests."
      },
      {
        "node_idx": 18040,
        "score_0_10": 10,
        "title": "effective face frontalization in unconstrained images",
        "abstract": "Frontalization is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation."
      },
      {
        "node_idx": 65880,
        "score_0_10": 10,
        "title": "tnt a statistical part of speech tagger",
        "abstract": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora."
      },
      {
        "node_idx": 117037,
        "score_0_10": 10,
        "title": "voxceleb2 deep speaker recognition",
        "abstract": "The objective of this paper is speaker recognition under noisy and unconstrained conditions. #R##N#We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. #R##N#Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin."
      },
      {
        "node_idx": 74662,
        "score_0_10": 10,
        "title": "a light cnn for deep face representation with noisy labels",
        "abstract": "Convolution neural network (CNN) has significantly pushed forward the development of face recognition and analysis techniques. Current CNN models tend to be deeper and larger to better fit large amounts of training data. When training data are from internet, their labels are often ambiguous and inaccurate. This paper presents a light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce the concept of maxout activation into each convolutional layer of CNN, which results in a Max-Feature-Map (MFM). Different from Rectified Linear Unit that suppresses a neuron by a threshold (or bias), MFM suppresses a neuron by a competitive relationship. MFM can not only separate noisy signals and informative signals but also plays a role of feature selection. Second, a network of five convolution layers and four Network in Network (NIN) layers are implemented to reduce the number of parameters and improve performance. Lastly, a semantic bootstrapping method is accordingly designed to make the prediction of the models be better consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a light model in terms of both computational cost and storage space. The learnt single model with a 256-D representation achieves state-of-the-art results on five face benchmarks without fine-tuning. The light CNN model is released on this https URL"
      },
      {
        "node_idx": 30693,
        "score_0_10": 10,
        "title": "single channel multi speaker separation using deep clustering",
        "abstract": "Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem."
      },
      {
        "node_idx": 45276,
        "score_0_10": 10,
        "title": "acoustic scene classification classifying environments from the sounds they produce",
        "abstract": "In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods."
      },
      {
        "node_idx": 92317,
        "score_0_10": 10,
        "title": "mofa model based deep convolutional face autoencoder for unsupervised monocular reconstruction",
        "abstract": "In this work we propose a novel model-based deep convolutional autoencoder that addresses the highly challenging problem of reconstructing a 3D human face from a single in-the-wild color image. To this end, we combine a convolutional encoder network with an expert-designed generative model that serves as decoder. The core innovation is our new differentiable parametric decoder that encapsulates image formation analytically based on a generative model. Our decoder takes as input a code vector with exactly defined semantic meaning that encodes detailed face pose, shape, expression, skin reflectance and scene illumination. Due to this new way of combining CNN-based with model-based face reconstruction, the CNN-based encoder learns to extract semantically meaningful parameters from a single monocular input image. For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible. The obtained reconstructions compare favorably to current state-of-the-art approaches in terms of quality and richness of representation."
      },
      {
        "node_idx": 96422,
        "score_0_10": 10,
        "title": "3d morphable face models past present and future",
        "abstract": "In this paper, we provide a detailed survey of 3D Morphable Face Models over the 20 years since they were first proposed. The challenges in building and applying these models, namely capture, modeling, image formation, and image analysis, are still active research topics, and we review the state-of-the-art in each of these areas. We also look ahead, identifying unsolved challenges, proposing directions for future research and highlighting the broad range of current and future applications."
      },
      {
        "node_idx": 24309,
        "score_0_10": 10,
        "title": "vggface2 a dataset for recognising faces across pose and age",
        "abstract": "In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available."
      }
    ]
  },
  "223": {
    "explanation": "auction mechanism design and efficiency analysis under uncertainty",
    "topk": [
      {
        "node_idx": 92237,
        "score_0_10": 10,
        "title": "interpolating between truthful and non truthful mechanisms for combinatorial auctions",
        "abstract": "We study the communication complexity of combinatorial auctions via interpolation mechanisms that interpolate between non-truthful and truthful protocols. Specifically, an interpolation mechanism has two phases. In the first phase, the bidders participate in some non-truthful protocol whose output is itself a truthful protocol. In the second phase, the bidders participate in the truthful protocol selected during phase one. Note that virtually all existing auctions have either a non-existent first phase (and are therefore truthful mechanisms), or a non-existent second phase (and are therefore just traditional protocols, analyzed via the Price of Anarchy/Stability). #R##N#The goal of this paper is to understand the benefits of interpolation mechanisms versus truthful mechanisms or traditional protocols, and develop the necessary tools to formally study them. Interestingly, we exhibit settings where interpolation mechanisms greatly outperform the optimal traditional and truthful protocols. Yet, we also exhibit settings where interpolation mechanisms are provably no better than truthful ones. Finally, we apply our new machinery to prove that the recent single-bid mechanism of Devanur et. al.~\\cite{DevanurMSW15} (the only pre-existing interpolation mechanism in the literature) achieves the optimal price of anarchy among a wide class of protocols, a claim that simply can't be addressed by appealing just to machinery from communication complexity or the study of truthful mechanisms."
      },
      {
        "node_idx": 67704,
        "score_0_10": 10,
        "title": "knightian robustness of the vickrey mechanism",
        "abstract": "We investigate the resilience of some classical mechanisms to alternative specifications of preferences and information structures. Specifically, we analyze the Vickrey mechanism for auctions of multiple identical goods when the only information a player $i$ has about the profile of true valuations, $\\theta^*$, consists of a set of distributions, from one of which $\\theta_i^*$ has been drawn. #R##N#In this setting, the players no longer have complete preferences, and the Vickrey mechanism is no longer dominant-strategy. However, we prove that its efficiency performance is excellent, and essentially optimal, in undominated strategies."
      },
      {
        "node_idx": 163732,
        "score_0_10": 10,
        "title": "simultaneous auctions are almost efficient",
        "abstract": "Simultaneous item auctions are simple procedures for allocating items to bidders with potentially complex preferences over different item sets. In a simultaneous auction, every bidder submits bids on all items simultaneously. The allocation and prices are then resolved for each item separately, based solely on the bids submitted on that item. Such procedures occur in practice (e.g. eBay) but are not truthful. We study the efficiency of Bayesian Nash equilibrium (BNE) outcomes of simultaneous first- and second-price auctions when bidders have complement-free (a.k.a. subadditive) valuations. We show that the expected social welfare of any BNE is at least 1/2 of the optimal social welfare in the case of first-price auctions, and at least 1/4 in the case of second-price auctions. These results improve upon the previously-known logarithmic bounds, which were established by [Hassidim, Kaplan, Mansour and Nisan '11] for first-price auctions and by [Bhawalkar and Roughgarden '11] for second-price auctions."
      },
      {
        "node_idx": 85703,
        "score_0_10": 10,
        "title": "strong conflict free coloring of intervals",
        "abstract": "We consider the k-strong conflict-free coloring of a set of points on a line with respect to a family of intervals: Each point on the line must be assigned a color so that the coloring has to be conflict-free, in the sense that in every interval I there are at least k colors each appearing exactly once in I. In this paper, we present a polynomial algorithm for the general problem; the algorithm has an approximation factor 5-2/k when k\\geq2 and approximation factor 2 for k=1. In the special case the family contains all the possible intervals on the given set of points, we show that a 2 approximation algorithm exists, for any k\\geq1."
      },
      {
        "node_idx": 166390,
        "score_0_10": 10,
        "title": "general auction mechanism for search advertising",
        "abstract": "In sponsored search, a number of advertising slots is available on a search results page, and have to be allocated among a set of advertisers competing to display an ad on the page. This gives rise to a bipartite matching market that is typically cleared by the way of an automated auction. Several auction mechanisms have been proposed, with variants of the Generalized Second Price (GSP) being widely used in practice. #R##N#A rich body of work on bipartite matching markets builds upon the stable marriage model of Gale and Shapley and the assignment model of Shapley and Shubik. We apply insights from this line of research into the structure of stable outcomes and their incentive properties to advertising auctions. #R##N#We model advertising auctions in terms of an assignment model with linear utilities, extended with bidder and item specific maximum and minimum prices. Auction mechanisms like the commonly used GSP or the well-known Vickrey-Clarke-Groves (VCG) are interpreted as simply computing a \\emph{bidder-optimal stable matching} in this model, for a suitably defined set of bidder preferences. In our model, the existence of a stable matching is guaranteed, and under a non-degeneracy assumption a bidder-optimal stable matching exists as well. We give an algorithm to find such matching in polynomial time, and use it to design truthful mechanism that generalizes GSP, is truthful for profit-maximizing bidders, implements features like bidder-specific minimum prices and position-specific bids, and works for rich mixtures of bidders and preferences."
      },
      {
        "node_idx": 143772,
        "score_0_10": 10,
        "title": "improved approximation algorithms for geometric set cover",
        "abstract": "Given a collection S of subsets of some set U, and M a subset of U, the set cover problem is to find the smallest subcollection C of S such that M is a subset of the union of the sets in C. While the general problem is NP-hard to solve, even approximately, here we consider some geometric special cases, where usually U = R^d. Extending prior results, we show that approximation algorithms with provable performance exist, under a certain general condition: that for a random subset R of S and function f(), there is a decomposition of the portion of U not covered by R into an expected f(|R|) regions, each region of a particular simple form. We show that under this condition, a cover of size O(f(|C|)) can be found. Our proof involves the generalization of shallow cuttings to more general geometric situations. We obtain constant-factor approximation algorithms for covering by unit cubes in R^3, for guarding a one-dimensional terrain, and for covering by similar-sized fat triangles in R^2. We also obtain improved approximation guarantees for fat triangles, of arbitrary size, and for a class of fat objects."
      },
      {
        "node_idx": 151710,
        "score_0_10": 10,
        "title": "an efficient algorithm for enumerating chordless cycles and chordless paths",
        "abstract": "A chordless cycle (induced cycle) $C$ of a graph is a cycle without any chord, meaning that there is no edge outside the cycle connecting two vertices of the cycle. A chordless path is defined similarly. In this paper, we consider the problems of enumerating chordless cycles/paths of a given graph $G=(V,E),$ and propose algorithms taking $O(|E|)$ time for each chordless cycle/path. In the existing studies, the problems had not been deeply studied in the theoretical computer science area, and no output polynomial time algorithm has been proposed. Our experiments showed that the computation time of our algorithms is constant per chordless cycle/path for non-dense random graphs and real-world graphs. They also show that the number of chordless cycles is much smaller than the number of cycles. We applied the algorithm to prediction of NMR (Nuclear Magnetic Resonance) spectra, and increased the accuracy of the prediction."
      },
      {
        "node_idx": 164935,
        "score_0_10": 10,
        "title": "combinatorial auctions via posted prices",
        "abstract": "We study anonymous posted price mechanisms for combinatorial auctions in a Bayesian framework. In a posted price mechanism, item prices are posted, then the consumers approach the seller sequentially in an arbitrary order, each purchasing her favorite bundle from among the unsold items at the posted prices. These mechanisms are simple, transparent and trivially dominant strategy incentive compatible (DSIC). #R##N#We show that when agent preferences are fractionally subadditive (which includes all submodular functions), there always exist prices that, in expectation, obtain at least half of the optimal welfare. Our result is constructive: given black-box access to a combinatorial auction algorithm A, sample access to the prior distribution, and appropriate query access to the sampled valuations, one can compute, in polytime, prices that guarantee at least half of the expected welfare of A. As a corollary, we obtain the first polytime (in n and m) constant-factor DSIC mechanism for Bayesian submodular combinatorial auctions, given access to demand query oracles. Our results also extend to valuations with complements, where the approximation factor degrades linearly with the level of complementarity."
      },
      {
        "node_idx": 52664,
        "score_0_10": 10,
        "title": "covering partial cubes with zones",
        "abstract": "A partial cube is a graph having an isometric embedding in a hypercube. Partial cubes are characterized by a natural equivalence relation on the edges, whose classes are called zones. The number of zones determines the minimal dimension of a hypercube in which the graph can be embedded. We consider the problem of covering the vertices of a partial cube with the minimum number of zones. The problem admits several special cases, among which are the problem of covering the cells of a line arrangement with a minimum number of lines, and the problem of finding a minimum-size fibre in a bipartite poset. For several such special cases, we give upper and lower bounds on the minimum size of a covering by zones. We also consider the computational complexity of those problems, and establish some hardness results."
      },
      {
        "node_idx": 85494,
        "score_0_10": 10,
        "title": "knightian analysis of the vickrey mechanism",
        "abstract": "We analyze the Vickrey mechanism for auctions of multiple identical goods when the players have both Knightian uncertainty over their own valuations and incomplete preferences. In this model, the Vickrey mechanism is no longer dominant-strategy, and we prove that all dominant-strategy mechanisms are inadequate. However, we also prove that, in undominated strategies, the social welfare produced by the Vickrey mechanism in the worst case is not only very good, but also essentially optimal."
      }
    ]
  },
  "227": {
    "explanation": "neural network spatial transformation and multi-scale pooling techniques",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 66578,
        "score_0_10": 10,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 26874,
        "score_0_10": 9,
        "title": "learning transferable architectures for scalable image recognition",
        "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
      },
      {
        "node_idx": 73053,
        "score_0_10": 9,
        "title": "semantic image segmentation with deep convolutional nets and fully connected crfs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 159068,
        "score_0_10": 9,
        "title": "intrusion detection system for applications using linux containers",
        "abstract": "Linux containers are gaining increasing traction in both individual and industrial use, and as these containers get integrated into mission-critical systems, real-time detection of malicious cyber attacks becomes a critical operational requirement. This paper introduces a real-time host-based intrusion detection system that can be used to passively detect malfeasance against applications within Linux containers running in a standalone or in a cloud multi-tenancy environment. The demonstrated intrusion detection system uses bags of system calls monitored from the host kernel for learning the behavior of an application running within a Linux container and determining anomalous container behavior. Performance of the approach using a database application was measured and results are discussed."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 111842,
        "score_0_10": 8,
        "title": "android malware detection using autoencoder",
        "abstract": "Smartphones have become an intrinsic part of human's life. The smartphone unifies diverse advanced characteristics. It enables users to store various data such as photos, health data, credential bank data, and personal information. The Android operating system is the prevalent mobile operating system and, in the meantime, the most targeted operating system by malware developers. Recently the unparalleled development of Android malware put pressure on researchers to propose effective methods to suppress the spread of the malware. In this paper, we propose a deep learning approach for Android malware detection. The proposed approach investigates five different feature sets and applies Autoencoder to identify malware. The experimental results show that the proposed approach can identify malware with high accuracy."
      }
    ]
  },
  "228": {
    "explanation": "network communication algorithms and interference management techniques",
    "topk": [
      {
        "node_idx": 156364,
        "score_0_10": 10,
        "title": "a comprehensive survey of recent advancements in molecular communication",
        "abstract": "With much advancement in the field of nanotechnology, bioengineering, and synthetic biology over the past decade, microscales and nanoscales devices are becoming a reality. Yet the problem of engineering a reliable communication system between tiny devices is still an open problem. At the same time, despite the prevalence of radio communication, there are still areas where traditional electromagnetic waves find it difficult or expensive to reach. Points of interest in industry, cities, and medical applications often lie in embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or they are located in such a way that directional high frequency systems are ineffective. Inspired by nature, one solution to these problems is molecular communication (MC), where chemical signals are used to transfer information. Although biologists have studied MC for decades, it has only been researched for roughly 10 year from a communication engineering lens. Significant number of papers have been published to date, but owing to the need for interdisciplinary work, much of the results are preliminary. In this survey, the recent advancements in the field of MC engineering are highlighted. First, the biological, chemical, and physical processes used by an MC system are discussed. This includes different components of the MC transmitter and receiver, as well as the propagation and transport mechanisms. Then, a comprehensive survey of some of the recent works on MC through a communication engineering lens is provided. The survey ends with a technology readiness analysis of MC and future research directions."
      },
      {
        "node_idx": 21360,
        "score_0_10": 10,
        "title": "arq for network coding",
        "abstract": "A new coding and queue management algorithm is proposed for communication networks that employ linear network coding. The algorithm has the feature that the encoding process is truly online, as opposed to a block-by-block approach. The setup assumes a packet erasure broadcast channel with stochastic arrivals and full feedback, but the proposed scheme is potentially applicable to more general lossy networks with link-by-link feedback. The algorithm guarantees that the physical queue size at the sender tracks the backlog in degrees of freedom (also called the virtual queue size). The new notion of a node ldquoseeingrdquo a packet is introduced. In terms of this idea, our algorithm may be viewed as a natural extension of ARQ schemes to coded networks. Our approach, known as the drop-when-seen algorithm, is compared with a baseline queuing approach called drop-when-decoded. It is shown that the expected queue size for our approach is O[(1)/(1-rho)] as opposed to Omega[(1)/(1-rho)2] for the baseline approach, where rho is the load factor."
      },
      {
        "node_idx": 17441,
        "score_0_10": 9,
        "title": "power control in two tier femtocell networks",
        "abstract": "In a two tier cellular network - comprised of a central macrocell underlaid with shorter range femtocell hotspots - cross-tier interference limits overall capacity with universal frequency reuse. To quantify near-far effects with universal frequency reuse, this paper derives a fundamental relation providing the largest feasible cellular Signal-to-Interference-Plus-Noise Ratio (SINR), given any set of feasible femtocell SINRs. We provide a link budget analysis which enables simple and accurate performance insights in a two-tier network. A distributed utility- based SINR adaptation at femtocells is proposed in order to alleviate cross-tier interference at the macrocell from cochannel femtocells. The Foschini-Miljanic (FM) algorithm is a special case of the adaptation. Each femtocell maximizes their individual utility consisting of a SINR based reward less an incurred cost (interference to the macrocell). Numerical results show greater than 30% improvement in mean femtocell SINRs relative to FM. In the event that cross-tier interference prevents a cellular user from obtaining its SINR target, an algorithm is proposed that reduces transmission powers of the strongest femtocell interferers. The algorithm ensures that a cellular user achieves its SINR target even with 100 femtocells/cell-site (with typical cellular parameters) and requires a worst case SINR reduction of only 16% at femtocells. These results motivate design of power control schemes requiring minimal network overhead in two-tier networks with shared spectrum."
      },
      {
        "node_idx": 156289,
        "score_0_10": 9,
        "title": "offloading in heterogeneous networks modeling analysis and design insights",
        "abstract": "Pushing data traffic from cellular to WiFi is an example of inter radio access technology (RAT) offloading. While this clearly alleviates congestion on the over-loaded cellular network, the ultimate potential of such offloading and its effect on overall system performance is not well understood. To address this, we develop a general and tractable model that consists of M different RATs, each deploying up to K different tiers of access points (APs), where each tier differs in transmit power, path loss exponent, deployment density and bandwidth. Each class of APs is modeled as an independent Poisson point process (PPP), with mobile user locations modeled as another independent PPP, all channels further consisting of i.i.d. Rayleigh fading. The distribution of rate over the entire network is then derived for a weighted association strategy, where such weights can be tuned to optimize a particular objective. We show that the optimum fraction of traffic offloaded to maximize SINR coverage is not in general the same as the one that maximizes rate coverage, defined as the fraction of users achieving a given rate."
      },
      {
        "node_idx": 158981,
        "score_0_10": 9,
        "title": "probe and adapt rate adaptation for http video streaming at scale",
        "abstract": "Today, the technology for video streaming over the Internet is converging towards a paradigm named HTTP-based adaptive streaming (HAS), which brings two new features. First, by using HTTP/TCP, it leverages network-friendly TCP to achieve both firewall/NAT traversal and bandwidth sharing. Second, by pre-encoding and storing the video in a number of discrete rate levels, it introduces video bitrate adaptivity in a scalable way so that the video encoding is excluded from the closed-loop adaptation. A conventional wisdom in HAS design is that since the TCP throughput observed by a client would indicate the available network bandwidth, it could be used as a reliable reference for video bitrate selection. We argue that this is no longer true when HAS becomes a substantial fraction of the total network traffic. We show that when multiple HAS clients compete at a network bottleneck, the discrete nature of the video bitrates results in difficulty for a client to correctly perceive its fair-share bandwidth. Through analysis and test bed experiments, we demonstrate that this fundamental limitation leads to video bitrate oscillation and other undesirable behaviors that negatively impact the video viewing experience. We therefore argue that it is necessary to design at the application layer using a \"probe and adapt\" principle for video bitrate adaptation (where \"probe\" refers to trial increment of the data rate, instead of sending auxiliary piggybacking traffic), which is akin, but also orthogonal to the transport-layer TCP congestion control. We present PANDA - a client-side rate adaptation algorithm for HAS - as a practical embodiment of this principle. Our test bed results show that compared to conventional algorithms, PANDA is able to reduce the instability of video bitrate selection by over 75% without increasing the risk of buffer underrun."
      },
      {
        "node_idx": 46136,
        "score_0_10": 9,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      },
      {
        "node_idx": 28821,
        "score_0_10": 9,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 135100,
        "score_0_10": 9,
        "title": "specifying and placing chains of virtual network functions",
        "abstract": "Network appliances perform different functions on network flows and constitute an important part of an operator\u2019s network. Normally, a set of chained network functions process network flows. Following the trend of virtualization of networks, virtualization of the network functions has also become a topic of interest. We define a model for formalizing the chaining of network functions using a context-free language. We process deployment requests and construct virtual network function graphs that can be mapped to the network. We describe the mapping as a Mixed Integer Quadratically Constrained Program (MIQCP) for finding the placement of the network functions and chaining them together considering the limited network resources and requirements of the functions. We have performed a Pareto set analysis to investigate the possible trade-offs between different optimization objectives."
      },
      {
        "node_idx": 42659,
        "score_0_10": 9,
        "title": "enhanced intercell interference coordination challenges in heterogeneous networks",
        "abstract": "3GPP LTE-Advanced has recently been investigating heterogeneous network (HetNet) deployments as a cost effective way to deal with the unrelenting traffic demand. HetNets consist of a mix of macrocells, remote radio heads, and low-power nodes such as picocells, femtocells, and relays. Leveraging network topology, increasing the proximity between the access network and the end users, has the potential to provide the next significant performance leap in wireless networks, improving spatial spectrum reuse and enhancing indoor coverage. Nevertheless, deployment of a large number of small cells overlaying the macrocells is not without new technical challenges. In this article, we present the concept of heterogeneous networks and also describe the major technical challenges associated with such network architecture. We focus in particular on the standardization activities within the 3GPP related to enhanced intercell interference coordination."
      },
      {
        "node_idx": 154444,
        "score_0_10": 9,
        "title": "uplink capacity and interference avoidance for two tier femtocell networks",
        "abstract": "Two-tier femtocell networks-- comprising a conventional macrocellular network plus embedded femtocell hotspots-- offer an economically viable solution to achieving high cellular user capacity and improved coverage. With universal frequency reuse and DS-CDMA transmission however, the ensuing cross-tier cochannel interference (CCI) causes unacceptable outage probability. This paper develops an uplink capacity analysis and interference avoidance strategy in such a two-tier CDMA network. We evaluate a network-wide area spectral efficiency metric called the \\emph{operating contour (OC)} defined as the feasible combinations of the average number of active macrocell users and femtocell base stations (BS) per cell-site that satisfy a target outage constraint. The capacity analysis provides an accurate characterization of the uplink outage probability, accounting for power control, path-loss and shadowing effects. Considering worst case CCI at a corner femtocell, results reveal that interference avoidance through a time-hopped CDMA physical layer and sectorized antennas allows about a 7x higher femtocell density, relative to a split spectrum two-tier network with omnidirectional femtocell antennas. A femtocell exclusion region and a tier selection based handoff policy offers modest improvements in the OCs. These results provide guidelines for the design of robust shared spectrum two-tier networks."
      }
    ]
  },
  "229": {
    "explanation": "product-form solutions and efficient neural network pruning techniques",
    "topk": [
      {
        "node_idx": 84264,
        "score_0_10": 10,
        "title": "operational semantics for product form solution",
        "abstract": "In this paper we present product-form solutions from the point of view of stochastic process algebra. In previous work we have shown how to derive product-form solutions for a formalism called Labelled Markov Automata (LMA). LMA are very useful as their relation with the Continuous Time Markov Chains is very direct. The disadvantage of using LMA is that the proofs of properties are cumbersome. In fact, in LMA it is not possible to use the inductive structure of the language in a proof. In this paper we consider a simple stochastic process algebra that has the great advantage of simplifying the proofs. This simple language has been inspired by PEPA, however, detailed analysis of the semantics of cooperation will show the differences between the two formalisms. It will also be shown that the semantics of the cooperation in process algebra influences the correctness of the derivation of the product-form solutions."
      },
      {
        "node_idx": 153811,
        "score_0_10": 10,
        "title": "pruning filters for efficient convnets",
        "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
      },
      {
        "node_idx": 20772,
        "score_0_10": 10,
        "title": "synthesis and analysis of product form petri nets",
        "abstract": "For a large Markovian model, a \"product form\" is an explicit description of the steady-state behaviour which is otherwise generally untractable. Being first introduced in queueing networks, it has been adapted to Markovian Petri nets. Here we address three relevant issues for product-form Petri nets which were left fully or partially open: (1) we provide a sound and complete set of rules for the synthesis; (2) we characterise the exact complexity of classical problems like reachability; (3) we introduce a new subclass for which the normalising constant (a crucial value for product-form expression) can be efficiently computed."
      },
      {
        "node_idx": 56621,
        "score_0_10": 10,
        "title": "business processes integration and performance indicators in a plm",
        "abstract": "In an economic environment more and more competitive, the effective management of information and knowledge is a strategic issue for industrial enterprises. In the global marketplace, companies must use reactive strategies and reduce their products development cycle. In this context, the PLM (Product Lifecycle Management) is considered as a key component of the information system. The aim of this paper is to present an approach to integrate Business Processes in a PLM system. This approach is implemented in automotive sector with second-tier subcontractor"
      },
      {
        "node_idx": 112674,
        "score_0_10": 10,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 145256,
        "score_0_10": 10,
        "title": "large scale reservoir simulations on ibm blue gene q",
        "abstract": "This paper presents our work on simulation of large-scale reservoir models on IBM Blue Gene/Q and studying the scalability of our parallel reservoir simulators. An in-house black oil simulator has been implemented. It uses MPI for communication and is capable of simulating reservoir models with hundreds of millions of grid cells. Benchmarks show that our parallel simulator are thousands of times faster than sequential simulators that designed for workstations and personal computers, and the simulator has excellent scalability."
      },
      {
        "node_idx": 136985,
        "score_0_10": 10,
        "title": "from db nets to coloured petri nets with priorities extended version",
        "abstract": "The recently introduced formalism of DB-nets has brought in a new conceptual way of modelling complex dynamic systems that equally account for the process and data dimensions, considering local data as well as persistent, transactional data. DB-nets combine a coloured variant of Petri nets with name creation and management (which we call nu-CPN), with a relational database. The integration of these two components is realized by equipping the net with special ``view'' places that query the database and expose the resulting answers to the net, with actions that allow transitions to update the content of the database, and with special arcs capturing compensation in case of transaction failure. In this work, we study whether this sophisticated model can be encoded back into nu-CPNs. In particular, we show that the meaningful fragment of DB-nets where database queries are expressed using unions of conjunctive queries with inequalities can be faithfully encoded into $\\nu$-CPNs with transition priorities. This allows us to directly exploit state-of-the-art technologies such as CPN Tools to simulate and analyse this relevant class of DB-nets."
      },
      {
        "node_idx": 165790,
        "score_0_10": 10,
        "title": "channel pruning for accelerating very deep neural networks",
        "abstract": "In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant. Code has been made publicly available."
      },
      {
        "node_idx": 119322,
        "score_0_10": 10,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 125927,
        "score_0_10": 9,
        "title": "compositionality decompositionality and refinement in input output conformance testing",
        "abstract": "We propose an input/output conformance testing theory utilizing Modal Interface Automata with Input Refusals (IR-MIA) as novel behavioral formalism for both the specification and the implementation under test. A modal refinement relation on IR-MIA allows distinguishing between obligatory and allowed output behaviors, as well as between implicitly underspecified and explicitly forbidden input behaviors. The theory therefore supports positive and negative conformance testing with optimistic and pessimistic environmental assumptions. We further show that the resulting conformance relation on IR-MIA, called modal-irioco, enjoys many desirable properties concerning component-based behaviors. First, modal-irioco is preserved under modal refinement and constitutes a preorder under certain restrictions which can be ensured by a canonical input completion for IR-MIA. Second, under the same restrictions, modal-irioco is compositional with respect to parallel composition of IR-MIA with multi-cast and hiding. Finally, the quotient operator on IR-MIA, as the inverse to parallel composition, facilitates decompositionality in conformance testing to solve the unknown-component problem."
      }
    ]
  },
  "230": {
    "explanation": "domain adaptation and transfer learning in deep neural networks",
    "topk": [
      {
        "node_idx": 51426,
        "score_0_10": 10,
        "title": "learning transferable features with deep adaptation networks",
        "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 12113,
        "score_0_10": 9,
        "title": "gans trained by a two time scale update rule converge to a nash equilibrium",
        "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Frechet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark."
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 85171,
        "score_0_10": 9,
        "title": "spectral normalization for generative adversarial networks",
        "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques."
      },
      {
        "node_idx": 77197,
        "score_0_10": 8,
        "title": "adversarial feature learning",
        "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning."
      },
      {
        "node_idx": 58439,
        "score_0_10": 8,
        "title": "deep domain confusion maximizing for domain invariance",
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task."
      },
      {
        "node_idx": 88168,
        "score_0_10": 8,
        "title": "semi supervised learning with deep generative models",
        "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning."
      },
      {
        "node_idx": 86850,
        "score_0_10": 8,
        "title": "least squares generative adversarial networks",
        "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs."
      },
      {
        "node_idx": 30850,
        "score_0_10": 8,
        "title": "cycada cycle consistent adversarial domain adaptation",
        "abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains."
      }
    ]
  },
  "234": {
    "explanation": "multi-scale context aggregation for semantic segmentation",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 77683,
        "score_0_10": 9,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 55272,
        "score_0_10": 9,
        "title": "image super resolution using deep convolutional networks",
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."
      },
      {
        "node_idx": 66578,
        "score_0_10": 8,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 69942,
        "score_0_10": 8,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 120825,
        "score_0_10": 8,
        "title": "encoder decoder with atrous separable convolution for semantic image segmentation",
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
      }
    ]
  },
  "238": {
    "explanation": "spatial transformation and spatial invariance in convolutional features",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 112726,
        "score_0_10": 8,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 77683,
        "score_0_10": 8,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      },
      {
        "node_idx": 66578,
        "score_0_10": 7,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 52018,
        "score_0_10": 7,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      }
    ]
  },
  "239": {
    "explanation": "neural network computation and relational learning methods",
    "topk": [
      {
        "node_idx": 164506,
        "score_0_10": 10,
        "title": "programming from metaphorisms",
        "abstract": "This paper presents a study of the metaphorism pattern of relational specification, showing how it can be refined into recursive programs. Metaphorisms express input-output relationships which preserve relevant information while at the same time some intended optimization takes place. Text processing, sorting, representation changers, etc., are examples of metaphorisms. The kind of metaphorism refinement studied in this paper is a strategy known as change of virtual data structure. By framing metaphorisms in the class of (inductive) regular relations, sufficient conditions are given for such implementations to be calculated using relation algebra. The strategy is illustrated with examples including the derivation of the quicksort and mergesort algorithms, showing what they have in common and what makes them different from the very start of development."
      },
      {
        "node_idx": 153102,
        "score_0_10": 10,
        "title": "embedding entities and relations for learning and inference in knowledge bases",
        "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning."
      },
      {
        "node_idx": 50631,
        "score_0_10": 10,
        "title": "pddl2 1 an extension to pddl for expressing temporal planning domains",
        "abstract": "In recent years research in the planning community has moved increasingly towards application of planners to realistic problems involving both time and many types of resources. For example, interest in planning demonstrated by the space research community has inspired work in observation scheduling, planetary rover exploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application.#R##N##R##N#The International Planning Competitions have acted as an important motivating force behind the progress that has been made in planning since 1998. The third competition (held in 2002) set the planning community the challenge of handling time and numeric resources. This necessitated the development of a modelling language capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition. We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power -- exceeding the capabilities of current planning technology -- and presents a number of important challenges to the research community."
      },
      {
        "node_idx": 168237,
        "score_0_10": 10,
        "title": "the symbol grounding problem",
        "abstract": "There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the \"symbol grounding problem\": How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) \"iconic representations\" , which are analogs of the proximal sensory projections of distal objects and events, and (2) \"categorical representations\" , which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) \"symbolic representations\" , grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., \"An X is a Y that is Z\"). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic \"module,\" however; the symbolic functions would emerge as an intrinsically \"dedicated\" symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded."
      },
      {
        "node_idx": 119385,
        "score_0_10": 9,
        "title": "bisimulations for delimited control operators",
        "abstract": "We propose a survey of the behavioral theory of an untyped lambda-calculus extended with the delimited-control operators shift and reset. We define a contextual equivalence for this calculus, that we then aim to characterize with coinductively defined relations, called bisimilarities. We study different styles of bisimilarities (namely applicative, normal-form, and environmental), and we give several examples to illustrate their respective strengths and weaknesses. We also discuss how to extend this work to other delimited-control operators."
      },
      {
        "node_idx": 128449,
        "score_0_10": 9,
        "title": "confluent hasse diagrams",
        "abstract": "We show that a transitively reduced digraph has a confluent upward drawing if and only if its reachability relation has order dimension at most two. In this case, we construct a confluent upward drawing with $O(n^2)$ features, in an $O(n) \\times O(n)$ grid in $O(n^2)$ time. For the digraphs representing series-parallel partial orders we show how to construct a drawing with $O(n)$ features in an $O(n) \\times O(n)$ grid in $O(n)$ time from a series-parallel decomposition of the partial order. Our drawings are optimal in the number of confluent junctions they use."
      },
      {
        "node_idx": 134568,
        "score_0_10": 9,
        "title": "an operational foundation for delimited continuations in the cps hierarchy",
        "abstract": "We present an abstract machine and a reduction semantics for the#N#lambda-calculus extended with control operators that give access to delimited#N#continuations in the CPS hierarchy. The abstract machine is derived from an#N#evaluator in continuation-passing style (CPS); the reduction semantics (i.e., a#N#small-step operational semantics with an explicit representation of evaluation#N#contexts) is constructed from the abstract machine; and the control operators#N#are the shift and reset family. We also present new applications of delimited#N#continuations in the CPS hierarchy: finding list prefixes and normalization by#N#evaluation for a hierarchical language of units and products."
      },
      {
        "node_idx": 45381,
        "score_0_10": 9,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 9193,
        "score_0_10": 9,
        "title": "confluent layered drawings",
        "abstract": "We combine the idea of confluent drawings with Sugiyama-style drawings in order to reduce the edge crossings in the resultant drawings. Furthermore, it is easier to understand the structures of graphs from the mixed-style drawings. The basic idea is to cover a layered graph by complete bipartite subgraphs (bicliques), then replace bicliques with tree-like structures. The biclique cover problem is reduced to a special edge-coloring problem and solved by heuristic coloring algorithms. Our method can be extended to obtain multi-depth confluent layered drawings."
      },
      {
        "node_idx": 43354,
        "score_0_10": 9,
        "title": "neural turing machines",
        "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."
      }
    ]
  },
  "240": {
    "explanation": "automated planning algorithms and heuristics in AI systems",
    "topk": [
      {
        "node_idx": 164205,
        "score_0_10": 10,
        "title": "decision theoretic planning structural assumptions and computational leverage",
        "abstract": "Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory.#R##N##R##N#This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations.#R##N##R##N#Specialized representations, and algorithms employing these representations, can achieve computational leverage by exploiting these various forms of structure. Certain AI techniques-- in particular those based on the use of structured, intensional representations--can be viewed in this way. This paper surveys several types of representations for both classical and decision-theoretic planning problems, and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing policies or plans. It focuses primarily on abstraction, aggregation and decomposition techniques based on AI-style representations."
      },
      {
        "node_idx": 45381,
        "score_0_10": 10,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 24655,
        "score_0_10": 10,
        "title": "safety of deferred update in transactional memory",
        "abstract": "Transactional memory allows the user to declare sequences of instructions as speculative \\emph{transactions} that can either \\emph{commit} or \\emph{abort}. If a transaction commits, it appears to be executed sequentially, so that the committed transactions constitute a correct sequential execution. If a transaction aborts, none of its instructions can affect other transactions. #R##N#The popular criterion of \\emph{opacity} requires that the views of aborted transactions must also be consistent with the global sequential order constituted by committed ones. This is believed to be important, since inconsistencies observed by an aborted transaction may cause a fatal irrecoverable error or waste of the system in an infinite loop. Intuitively, an opaque implementation must ensure that no intermediate view a transaction obtains before it commits or aborts can be affected by a transaction that has not started committing yet, so called \\emph{deferred-update} semantics. #R##N#In this paper, we intend to grasp this intuition formally. We propose a variant of opacity that explicitly requires the sequential order to respect the deferred-update semantics. We show that our criterion is a safety property, i.e., it is prefix- and limit-closed. Unlike opacity, our property also ensures that a serialization of a history implies serializations of its prefixes. Finally, we show that our property is equivalent to opacity if we assume that no two transactions commit identical values on the same variable, and present a counter-example for scenarios when the \"unique-write\" assumption does not hold."
      },
      {
        "node_idx": 144816,
        "score_0_10": 10,
        "title": "work stealing with latency",
        "abstract": "We study in this paper the impact of communication latency on the classical Work Stealing load balancing algorithm. Our approach considers existing performance models and the underlying algorithms. We introduce a latency parameter in the model and study its overall impact by careful observations of simulation results. Using this method we are able to derive a new expression of the expected running time of divisible load applications. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors one should use for a given work platform combination. We also consider the impact of several algorithmic variants like simultaneous transfers of work or thresholds for avoiding useless transfers. All our results are validated through simulation on a wide range of parameters."
      },
      {
        "node_idx": 53982,
        "score_0_10": 10,
        "title": "program synthesis from polymorphic refinement types",
        "abstract": "We present a method for synthesizing recursive functions that provably satisfy a given specification in the form of a polymorphic refinement type. We observe that such specifications are particularly suitable for program synthesis for two reasons. First, they offer a unique combination of expressive power and decidability, which enables automatic verification---and hence synthesis---of nontrivial programs. Second, a type-based specification for a program can often be effectively decomposed into independent specifications for its components, causing the synthesizer to consider fewer component combinations and leading to a combinatorial reduction in the size of the search space. At the core of our synthesis procedure is a new algorithm for refinement type checking, which supports specification decomposition. #R##N#We have evaluated our prototype implementation on a large set of synthesis problems and found that it exceeds the state of the art in terms of both scalability and usability. The tool was able to synthesize more complex programs than those reported in prior work (several sorting algorithms and operations on balanced search trees), as well as most of the benchmarks tackled by existing synthesizers, often starting from a more concise and intuitive user input."
      },
      {
        "node_idx": 42662,
        "score_0_10": 9,
        "title": "the fast downward planning system",
        "abstract": "Fast Downward is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. However, unlike other PDDL planning systems, Fast Downward does not use the propositional PDDL representation of a planning task directly. Instead, the input is first translated into an alternative representation called multivalued planning tasks, which makes many of the implicit constraints of a propositional planning task explicit. Exploiting this alternative representation, Fast Downward uses hierarchical decompositions of planning tasks for computing its heuristic function, called the causal graph heuristic, which is very different from traditional HSP-like heuristics based on ignoring negative interactions of operators.#R##N##R##N#In this article, we give a full account of Fast Downward's approach to solving multivalued planning tasks. We extend our earlier discussion of the causal graph heuristic to tasks involving axioms and conditional effects and present some novel techniques for search control that are used within Fast Downward's best-first search algorithm: preferred operators transfer the idea of helpful actions from local search to global best-first search, deferred evaluation of heuristic functions mitigates the negative effect of large branching factors on search performance, and multiheuristic best-first search combines several heuristic evaluation functions within a single search algorithm in an orthogonal way. We also describe efficient data structures for fast state expansion (successor generators and axiom evaluators) and present a new non-heuristic search algorithm called focused iterative-broadening search, which utilizes the information encoded in causal graphs in a novel way.#R##N##R##N#Fast Downward has proven remarkably successful: It won the \"classical\" (i. e., propositional, non-optimising) track of the 4th International Planning Competition at ICAPS 2004, following in the footsteps of planners such as FF and LPG. Our experiments show that it also performs very well on the benchmarks of the earlier planning competitions and provide some insights about the usefulness of the new search enhancements."
      },
      {
        "node_idx": 42476,
        "score_0_10": 9,
        "title": "the ff planning system fast plan generation through heuristic search",
        "abstract": "We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP's heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP."
      },
      {
        "node_idx": 162969,
        "score_0_10": 9,
        "title": "on the cost of concurrency in transactional memory",
        "abstract": "Traditional techniques for synchronization are based on \\emph{locking} that provides threads with exclusive access to shared data. \\emph{Coarse-grained} locking typically forces threads to access large amounts of data sequentially and, thus, does not fully exploit hardware concurrency. Program-specific \\emph{fine-grained} locking or \\emph{non-blocking} (\\emph{i.e.}, not using locks) synchronization, on the other hand, is a dark art to most programmers and trusted to the wisdom of a few computing experts. Thus, it is appealing to seek a middle ground between these two extremes: a synchronization mechanism that relieves the programmer of the overhead of reasoning about data conflicts that may arise from concurrent operations without severely limiting the program's performance. The \\emph{Transactional Memory (TM)} abstraction is proposed as such a mechanism: it intends to combine an easy-to-use programming interface with an efficient utilization of the concurrent-computing abilities provided by multicore architectures. TM allows the programmer to \\emph{speculatively} execute sequences of shared-memory operations as \\emph{atomic transactions} with \\emph{all-or-nothing} semantics: the transaction can either \\emph{commit}, in which case it appears as executed sequentially, or \\emph{abort}, in which case its update operations do not take effect. Thus, the programmer can design software having only sequential semantics in mind and let TM take care, at run-time, of resolving the conflicts in concurrent executions. #R##N#Intuitively, we want TMs to allow for as much \\emph{concurrency} as possible: in the absence of severe data conflicts, transactions should be able to progress in parallel. But what are the inherent costs associated with providing high degrees of concurrency in TMs? This is the central question of the thesis."
      },
      {
        "node_idx": 116237,
        "score_0_10": 9,
        "title": "on verifying causal consistency",
        "abstract": "Causal consistency is one of the most adopted consistency criteria for distributed implementations of data structures. It ensures that operations are executed at all sites according to their causal precedence. We address the issue of verifying automatically whether the executions of an implementation of a data structure are causally consistent. We consider two problems: (1) checking whether one single execution is causally consistent, which is relevant for developing testing and bug finding algorithms, and (2) verifying whether all the executions of an implementation are causally consistent. #R##N#We show that the first problem is NP-complete. This holds even for the read-write memory abstraction, which is a building block of many modern distributed systems. Indeed, such systems often store data in key-value stores, which are instances of the read-write memory abstraction. Moreover, we prove that, surprisingly, the second problem is undecidable, and again this holds even for the read-write memory abstraction. However, we show that for the read-write memory abstraction, these negative results can be circumvented if the implementations are data independent, i.e., their behaviors do not depend on the data values that are written or read at each moment, which is a realistic assumption."
      },
      {
        "node_idx": 76162,
        "score_0_10": 9,
        "title": "strict linearizability and abstract atomicity",
        "abstract": "Linearizability is a commonly accepted consistency condition for concurrent objects. Filipovic et al. show that linearizability is equivalent to observational refinement. However, linearizability does not permit concurrent objects to share memory spaces with their client programs. We show that linearizability (or observational refinement) can be broken even though a client program of an object accesses the shared memory spaces without interference from the methods of the object. In this paper, we present strict linearizability which lifts this limitation and can ensure client-side traces and final-states equivalence even in a relaxed program model allowing clients to directly access the states of concurrent objects. We also investigate several important properties of strict linearizability. #R##N#At a high level of abstraction, a concurrent object can be viewed as a concurrent implementation of an abstract data type (ADT). We also present a correctness criterion for relating an ADT and its concurrent implementation, which is the combination of linearizability and data abstraction and can ensure observational equivalence. We also investigate its relationship with strict linearizability."
      }
    ]
  },
  "242": {
    "explanation": "graph planarity and clustering algorithms",
    "topk": [
      {
        "node_idx": 34249,
        "score_0_10": 10,
        "title": "c planarity testing of embedded clustered graphs with bounded dual carving width",
        "abstract": "For a clustered graph, i.e, a graph whose vertex set is recursively partitioned into clusters, the C-Planarity Testing problem asks whether it is possible to find a planar embedding of the graph and a representation of each cluster as a region homeomorphic to a closed disk such that 1. the subgraph induced by each cluster is drawn in the interior of the corresponding disk, 2. each edge intersects any disk at most once, and 3. the nesting between clusters is reflected by the representation, i.e., child clusters are properly contained in their parent cluster. The computational complexity of this problem, whose study has been central to the theory of graph visualization since its introduction in 1995 [Qing-Wen Feng, Robert F. Cohen, and Peter Eades. Planarity for clustered graphs. ESA'95], has only been recently settled [Radoslav Fulek and Csaba D. Toth. Atomic Embeddability, Clustered Planarity, and Thickenability. To appear at SODA'20]. Before such a breakthrough, the complexity question was still unsolved even when the graph has a prescribed planar embedding, i.e, for embedded clustered graphs. #R##N#We show that the C-Planarity Testing problem admits a single-exponential single-parameter FPT algorithm for embedded clustered graphs, when parameterized by the carving-width of the dual graph of the input. This is the first FPT algorithm for this long-standing open problem with respect to a single notable graph-width parameter. Moreover, in the general case, the polynomial dependency of our FPT algorithm is smaller than the one of the algorithm by Fulek and Toth. To further strengthen the relevance of this result, we show that the C-Planarity Testing problem retains its computational complexity when parameterized by several other graph-width parameters, which may potentially lead to faster algorithms."
      },
      {
        "node_idx": 0,
        "score_0_10": 10,
        "title": "evasion attacks against machine learning at test time",
        "abstract": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis."
      },
      {
        "node_idx": 163726,
        "score_0_10": 10,
        "title": "2 connecting outerplanar graphs without blowing up the pathwidth",
        "abstract": "Given a connected outerplanar graph G of pathwidth p, we give an algorithm to add edges to G to get a supergraph of G, which is 2-vertex-connected, outerplanar and of pathwidth O(p). This settles an open problem raised by Biedl, in the context of computing minimum height planar straight line drawings of outerplanar graphs, with their vertices placed on a two dimensional grid. In conjunction with the result of this paper, the constant factor approximation algorithm for this problem obtained by Biedl for 2-vertex-connected outerplanar graphs will work for all outer planar graphs."
      },
      {
        "node_idx": 90915,
        "score_0_10": 9,
        "title": "automated test input generation for android are we there yet",
        "abstract": "Mobile applications, often simply called \"apps\", are increasingly widespread, and we use them daily to perform a number of activities. Like all software, apps must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: code coverage, ability to detect faults, ability to work on multiple platforms, and ease of use. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android."
      },
      {
        "node_idx": 155003,
        "score_0_10": 9,
        "title": "happy software developers solve problems better psychological measurements in empirical software engineering",
        "abstract": "For more than thirty years, it has been claimed that a way to improve software developers\u2019 productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states\u2014emotions and moods\u2014deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint."
      },
      {
        "node_idx": 35379,
        "score_0_10": 9,
        "title": "multiparty testing preorders",
        "abstract": "Variants of the must testing approach have been successfully applied in Service Oriented Computing for analysing the compliance between (contracts exposed by) clients and servers or, more generally, between two peers. It has however been argued that multiparty scenarios call for more permissive notions of compliance because partners usually do not have full coordination capabilities. We propose two new testing preorders, which are obtained by restricting the set of potential observers. For the first preorder, called uncoordinated, we allow only sets of parallel observers that use different parts of the interface of a given service and have no possibility of intercommunication. For the second preorder, that we call individualistic, we instead rely on parallel observers that perceive as silent all the actions that are not in the interface of interest. We have that the uncoordinated preorder is coarser than the classical must testing preorder and finer than the individualistic one. We also provide a characterisation in terms of decorated traces for both preorders: the uncoordinated preorder is defined in terms of must-sets and Mazurkiewicz traces while the individualistic one is described in terms of classes of filtered traces that only contain designated visible actions and must-sets."
      },
      {
        "node_idx": 145176,
        "score_0_10": 9,
        "title": "optimal locally repairable codes and connections to matroid theory",
        "abstract": "Petabyte-scale distributed storage systems are currently transitioning to erasure codes to achieve higher storage efficiency. Classical codes like Reed-Solomon are highly sub-optimal for distributed environments due to their high overhead in single-failure events. Locally Repairable Codes (LRCs) form a new family of codes that are repair efficient. In particular, LRCs minimize the number of nodes participating in single node repairs during which they generate small network traffic. Two large-scale distributed storage systems have already implemented different types of LRCs: Windows Azure Storage and the Hadoop Distributed File System RAID used by Facebook. The fundamental bounds for LRCs, namely the best possible distance for a given code locality, were recently discovered, but few explicit constructions exist. In this work, we present an explicit and optimal LRCs that are simple to construct. Our construction is based on grouping Reed-Solomon (RS) coded symbols to obtain RS coded symbols over a larger finite field. We then partition these RS symbols in small groups, and re-encode them using a simple local code that offers low repair locality. For the analysis of the optimality of the code, we derive a new result on the matroid represented by the code generator matrix."
      },
      {
        "node_idx": 81796,
        "score_0_10": 9,
        "title": "universal adversarial perturbations",
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
      },
      {
        "node_idx": 20053,
        "score_0_10": 9,
        "title": "clustered planarity testing revisited",
        "abstract": "The Hanani--Tutte theorem is a classical result proved for the first time in the 1930s that characterizes planar graphs as graphs that admit a drawing in the plane in which every pair of edges not sharing a vertex cross an even number of times. We generalize this result to clustered graphs with two disjoint clusters, and show that a straightforward extension to flat clustered graphs with three or more disjoint clusters is not possible. For general clustered graphs we show a variant of the Hanani--Tutte theorem in the case when each cluster induces a connected subgraph. Di Battista and Frati proved that clustered planarity of embedded clustered graphs whose every face is incident with at most five vertices can be tested in polynomial time. We give a new and short proof of this result, using the matroid intersection algorithm."
      },
      {
        "node_idx": 63722,
        "score_0_10": 9,
        "title": "measures of edge uncolorability",
        "abstract": "The resistance r(G) of a graph G is the minimum number of edges that have to be removed from G to obtain a graph which is @D(G)-edge-colorable. This paper relates the resistance to other parameters that measure how far a graph is from being @D-edge-colorable. Let r\"v(G) be the minimum number of vertices that have to be removed from G to obtain a class 1 graph. We show that r(G)r\"v(G)@?@?@D(G)2@?, and that this bound is best possible."
      }
    ]
  },
  "243": {
    "explanation": "multi-scale context aggregation and spatial pyramid pooling for semantic segmentation",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 73053,
        "score_0_10": 8,
        "title": "semantic image segmentation with deep convolutional nets and fully connected crfs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
      },
      {
        "node_idx": 14201,
        "score_0_10": 8,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      }
    ]
  },
  "246": {
    "explanation": "efficient matrix completion and low-rank approximation algorithms",
    "topk": [
      {
        "node_idx": 35891,
        "score_0_10": 10,
        "title": "session types intersection types union types",
        "abstract": "We propose a semantically grounded theory of session types which relies on intersection and union types. We argue that intersection and union types are natural candidates for modeling branching points in session types and we show that the resulting theory overcomes some important defects of related behavioral theories. In particular, intersections and unions provide a native solution to the problem of computing joins and meets of session types. Also, the subtyping relation turns out to be a pre-congruence, while this is not always the case in related behavioral theories."
      },
      {
        "node_idx": 100147,
        "score_0_10": 10,
        "title": "communication efficient learning of deep networks from decentralized data",
        "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. #R##N#We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
      },
      {
        "node_idx": 145369,
        "score_0_10": 10,
        "title": "typing copyless message passing",
        "abstract": "We present a calculus that models a form of process interaction based on#N#copyless message passing, in the style of Singularity OS. The calculus is#N#equipped with a type system ensuring that well-typed processes are free from#N#memory faults, memory leaks, and communication errors. The type system is#N#essentially linear, but we show that linearity alone is inadequate, because it#N#leaves room for scenarios where well-typed processes leak significant amounts#N#of memory. We address these problems basing the type system upon an original#N#variant of session types."
      },
      {
        "node_idx": 101499,
        "score_0_10": 9,
        "title": "a simpler approach to matrix completion",
        "abstract": "This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low rank matrix. These results improve on prior work by Candes and Recht, Candes and Tao, and Keshavan, Montanari, and Oh. The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory."
      },
      {
        "node_idx": 138634,
        "score_0_10": 9,
        "title": "matrix completion with noise",
        "abstract": "On the heels of compressed sensing, a remarkable new field has very recently emerged. This field addresses a broad range of problems of significant practical interest, namely, the recovery of a data matrix from what appears to be incomplete, and perhaps even corrupted, information. In its simplest form, the problem is to recover a matrix from a small sample of its entries, and comes up in many areas of science and engineering including collaborative filtering, machine learning, control, remote sensing, and computer vision to name a few. #R##N#This paper surveys the novel literature on matrix completion, which shows that under some suitable conditions, one can recover an unknown low-rank matrix from a nearly minimal set of entries by solving a simple convex optimization problem, namely, nuclear-norm minimization subject to data constraints. Further, this paper introduces novel results showing that matrix completion is provably accurate even when the few observed entries are corrupted with a small amount of noise. A typical result is that one can recover an unknown n x n matrix of low rank r from just about nr log^2 n noisy samples with an error which is proportional to the noise level. We present numerical results which complement our quantitative analysis and show that, in practice, nuclear norm minimization accurately fills in the many missing entries of large low-rank matrices from just a few noisy samples. Some analogies between matrix completion and compressed sensing are discussed throughout."
      },
      {
        "node_idx": 149294,
        "score_0_10": 9,
        "title": "randomized algorithms for matrices and data",
        "abstract": "Randomized algorithms for very large matrix problems have received a great deal of attention in recent years. Much of this work was motivated by problems in large-scale data analysis, and this work was performed by individuals from many different research communities. This monograph will provide a detailed overview of recent work on the theory of randomized matrix algorithms as well as the application of those ideas to the solution of practical problems in large-scale data analysis. An emphasis will be placed on a few simple core ideas that underlie not only recent theoretical advances but also the usefulness of these tools in large-scale data applications. Crucial in this context is the connection with the concept of statistical leverage. This concept has long been used in statistical regression diagnostics to identify outliers; and it has recently proved crucial in the development of improved worst-case matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists. Randomized methods solve problems such as the linear least-squares problem and the low-rank matrix approximation problem by constructing and operating on a randomized sketch of the input matrix. Depending on the specifics of the situation, when compared with the best previously-existing deterministic algorithms, the resulting randomized algorithms have worst-case running time that is asymptotically faster; their numerical implementations are faster in terms of clock-time; or they can be implemented in parallel computing environments where existing numerical algorithms fail to run at all. Numerous examples illustrating these observations will be described in detail."
      },
      {
        "node_idx": 83024,
        "score_0_10": 9,
        "title": "matrix completion from noisy entries",
        "abstract": "Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the `Netflix problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced by Keshavan et al.(2009), based on a combination of spectral techniques and manifold optimization, that we call here OptSpace. We prove performance guarantees that are order-optimal in a number of circumstances."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 19458,
        "score_0_10": 9,
        "title": "low rank approximation and regression in input sparsity time",
        "abstract": "We design a new distribution over $\\poly(r \\eps^{-1}) \\times n$ matrices $S$ so that for any fixed $n \\times d$ matrix $A$ of rank $r$, with probability at least 9/10, $\\norm{SAx}_2 = (1 \\pm \\eps)\\norm{Ax}_2$ simultaneously for all $x \\in \\mathbb{R}^d$. Such a matrix $S$ is called a \\emph{subspace embedding}. Furthermore, $SA$ can be computed in $\\nnz(A) + \\poly(d \\eps^{-1})$ time, where $\\nnz(A)$ is the number of non-zero entries of $A$. This improves over all previous subspace embeddings, which required at least $\\Omega(nd \\log d)$ time to achieve this property. We call our matrices $S$ \\emph{sparse embedding matrices}. #R##N#Using our sparse embedding matrices, we obtain the fastest known algorithms for $(1+\\eps)$-approximation for overconstrained least-squares regression, low-rank approximation, approximating all leverage scores, and $\\ell_p$-regression. The leading order term in the time complexity of our algorithms is $O(\\nnz(A))$ or $O(\\nnz(A)\\log n)$. #R##N#We optimize the low-order $\\poly(d/\\eps)$ terms in our running times (or for rank-$k$ approximation, the $n*\\poly(k/eps)$ term), and show various tradeoffs. For instance, we also use our methods to design new preconditioners that improve the dependence on $\\eps$ in least squares regression to $\\log 1/\\eps$. Finally, we provide preliminary experimental results which suggest that our algorithms are competitive in practice."
      },
      {
        "node_idx": 141819,
        "score_0_10": 9,
        "title": "academic torrents scalable data distribution",
        "abstract": "As competitions get more popular, transferring ever-larger data sets becomes infeasible and costly. For example, downloading the 157.3 GB 2012 ImageNet data set incurs about $4.33 in bandwidth costs per download. Downloading the full ImageNet data set takes 33 days. ImageNet has since become popular beyond the competition, and many papers and models now revolve around this data set. For sharing such an important resource to the machine learning community, the sharers of ImageNet must shoulder a large bandwidth burden. Academic Torrents reduces this burden for disseminating competition data, and also increases download speeds for end users. Academic Torrents is run by a pending nonprofit.. By augmenting an existing HTTP server with a peer-to-peer swarm, requests get re-routed to get data from downloaders. While existing systems slow down with more users, the benefits of Academic Torrents grow, with noticeable effects even when only one other person is downloading."
      }
    ]
  },
  "247": {
    "explanation": "advanced algorithms and theory in automata and formal language processing",
    "topk": [
      {
        "node_idx": 42046,
        "score_0_10": 10,
        "title": "advanced automata minimization",
        "abstract": "We present an efficient algorithm to reduce the size of nondeterministic Buchi word automata, while retaining their language. Additionally, we describe methods to solve PSPACE-complete automata problems like universality, equivalence and inclusion for much larger instances (1-3 orders of magnitude) than before. This can be used to scale up applications of automata in formal verification tools and decision procedures for logical theories. The algorithm is based on new transition pruning techniques. These use criteria based on combinations of backward and forward trace inclusions. Since these relations are themselves PSPACE-complete, we describe methods to compute good approximations of them in polynomial time. Extensive experiments show that the average-case complexity of our algorithm scales quadratically. The size reduction of the automata depends very much on the class of instances, but our algorithm consistently outperforms all previous techniques by a wide margin. We tested our algorithm on Buchi automata derived from LTL-formulae, many classes of random automata and automata derived from mutual exclusion protocols, and compared its performance to the well-known automata tool GOAL."
      },
      {
        "node_idx": 14164,
        "score_0_10": 10,
        "title": "from nondeterministic b uchi and streett automata to deterministic parity automata",
        "abstract": "In this paper we revisit Safra's determinization constructions for automata on infinite words. We show how to construct deterministic automata with fewer states and, most importantly, parity acceptance conditions. Determinization is used in numerous applications, such as reasoning about tree automata, satisfiability of CTL*, and realizability and synthesis of logical specifications. The upper bounds for all these applications are reduced by using the smaller deterministic automata produced by our construction. In addition, the parity acceptance conditions allows to use more efficient algorithms (when compared to handling Rabin or Streett acceptance conditions)."
      },
      {
        "node_idx": 122601,
        "score_0_10": 10,
        "title": "sofic dyck shifts",
        "abstract": "We define the class of sofic-Dyck shifts which extends the class of Markov-Dyck shifts introduced by Inoue, Krieger and Matsumoto. Sofic-Dyck shifts are shifts of sequences whose finite factors form unambiguous context-free languages. We show that they correspond exactly to the class of shifts of sequences whose sets of factors are visibly pushdown languages. We give an expression of the zeta function of a sofic-Dyck shift."
      },
      {
        "node_idx": 113712,
        "score_0_10": 10,
        "title": "nondeterministic automatic complexity of overlap free and almost square free words",
        "abstract": "Shallit and Wang studied deterministic automatic complexity of words. They showed that the automatic Hausdorff dimension $I(\\mathbf t)$ of the infinite Thue word satisfies $1/3\\le I(\\mathbf t)\\le 2/3$. We improve that result by showing that $I(\\mathbf t)\\ge 1/2$. For nondeterministic automatic complexity we show $I(\\mathbf t)=1/2$. We prove that such complexity $A_N$ of a word $x$ of length $n$ satisfies $A_N(x)\\le b(n):=\\lfloor n/2\\rfloor + 1$. This enables us to define the complexity deficiency $D(x)=b(n)-A_N(x)$. If $x$ is square-free then $D(x)=0$. If $x$ almost square-free in the sense of Fraenkel and Simpson, or if $x$ is a strongly cube-free binary word such as the infinite Thue word, then $D(x)\\le 1$. On the other hand, there is no constant upper bound on $D$ for strongly cube-free words in a ternary alphabet, nor for cube-free words in a binary alphabet. The decision problem whether $D(x)\\ge d$ for given $x$, $d$ belongs to $NP\\cap E$."
      },
      {
        "node_idx": 14583,
        "score_0_10": 10,
        "title": "p time completeness of light linear logic and its nondeterministic extension",
        "abstract": "In CSL'99 Roversi pointed out that the Turing machine encoding of Girard's seminal paper \"Light Linear Logic\" has a flaw. Moreover he presented a working version of the encoding in Light Affine Logic, but not in Light Linear Logic. In this paper we present a working version of the encoding in Light Linear Logic. The idea of the encoding is based on a remark of Girard's tutorial paper on Linear Logic. The encoding is also an example which shows usefulness of additive connectives. Moreover we also consider a nondeterministic extension of Light Linear Logic. We show that the extended system is NP-complete in the same meaning as P-completeness of Light Linear Logic."
      },
      {
        "node_idx": 106870,
        "score_0_10": 10,
        "title": "mso definable string transductions and two way finite state transducers",
        "abstract": "String transductions that are definable in monadic second-order (mso) logic (without the use of parameters) are exactly those realized by deterministic two-way finite state transducers. Nondeterministic mso definable string transductions (i.e., those definable with the use of parameters) correspond to compositions of two nondeterministic two-way finite state transducers that have the finite visit property. Both families of mso definable string transductions are characterized in terms of Hennie machines, i.e., two-way finite state transducers with the finite visit property that are allowed to rewrite their input tape."
      },
      {
        "node_idx": 77756,
        "score_0_10": 9,
        "title": "on the construction of skew quasi cyclic codes",
        "abstract": "In this paper we study a special type of quasi-cyclic (QC) codes called skew QC codes. This set of codes is constructed using a non-commutative ring called the skew polynomial rings $F[x;\\theta ]$. After a brief description of the skew polynomial ring $F[x;\\theta ]$ it is shown that skew QC codes are left submodules of the ring $R_{s}^{l}=(F[x;\\theta ]/(x^{s}-1))^{l}.$ The notions of generator and parity-check polynomials are given. We also introduce the notion of similar polynomials in the ring $F[x;\\theta ]$ and show that parity-check polynomials for skew QC codes are unique up to similarity. Our search results lead to the construction of several new codes with Hamming distances exceeding the Hamming distances of the previously best known linear codes with comparable parameters."
      },
      {
        "node_idx": 59862,
        "score_0_10": 9,
        "title": "efficient design of reversible sequential circuit",
        "abstract": "Reversible logic has come to the forefront of theoretical and applied research today. Although many researchers are investigating techniques to synthesize reversible combinational logic, there is little work in the area of sequential reversible logic. Latches and flip-flops are the most significant memory elements for the forthcoming sequential memory elements. In this paper, we proposed two new reversible logic gates MG-1 and MG-2. We then proposed new design techniques for latches and flip-flops with the help of the new proposed gates. The proposed designs are better than the existing ones in terms of number of gates, garbage outputs and delay."
      },
      {
        "node_idx": 66543,
        "score_0_10": 9,
        "title": "energy efficient scheduling for homogeneous multiprocessor systems",
        "abstract": "We present a number of novel algorithms, based on mathematical optimization formulations, in order to solve a homogeneous multiprocessor scheduling problem, while minimizing the total energy consumption. In particular, for a system with a discrete speed set, we propose solving a tractable linear program. Our formulations are based on a fluid model and a global scheduling scheme, i.e. tasks are allowed to migrate between processors. The new methods are compared with three global energy/feasibility optimal workload allocation formulations. Simulation results illustrate that our methods achieve both feasibility and energy optimality and outperform existing methods for constrained deadline tasksets. Specifically, the results provided by our algorithm can achieve up to an 80% saving compared to an algorithm without a frequency scaling scheme and up to 70% saving compared to a constant frequency scaling scheme for some simulated tasksets. Another benefit is that our algorithms can solve the scheduling problem in one step instead of using a recursive scheme. Moreover, our formulations can solve a more general class of scheduling problems, i.e. any periodic real-time taskset with arbitrary deadline. Lastly, our algorithms can be applied to both online and offline scheduling schemes."
      },
      {
        "node_idx": 68710,
        "score_0_10": 9,
        "title": "the largest cognitive systems will be optoelectronic",
        "abstract": "Electrons and photons offer complementary strengths for information processing. Photons are excellent for communication, while electrons are superior for computation and memory. Cognition requires distributed computation to be communicated across the system for information integration. We present reasoning from neuroscience, network theory, and device physics supporting the conjecture that large-scale cognitive systems will benefit from electronic devices performing synaptic, dendritic, and neuronal information processing operating in conjunction with photonic communication. On the chip scale, integrated dielectric waveguides enable fan-out to thousands of connections. On the system scale, fiber and free-space optics can be employed. The largest cognitive systems will be limited by the distance light can travel during the period of a network oscillation. We calculate that optoelectronic networks the area of a large data center ($10^5$\\,m$^2$) will be capable of system-wide information integration at $1$\\,MHz. At frequencies of cortex-wide integration in the human brain ($4$\\,Hz, theta band), optoelectronic systems could integrate information across the surface of the earth."
      }
    ]
  },
  "248": {
    "explanation": "program synthesis and verification in concurrent and recursive systems",
    "topk": [
      {
        "node_idx": 106163,
        "score_0_10": 10,
        "title": "the paths to choreography extraction",
        "abstract": "Choreographies are global descriptions of interactions among concurrent components, most notably used in the settings of verification and synthesis of correct-by-construction software. They require a top-down approach: programmers first write choreographies, and then use them to verify or synthesize their programs. However, most software does not come with choreographies yet, which prevents their application. To attack this problem, previous work investigated choreography extraction, which automatically constructs a choreography that describes the behavior of a given set of programs or protocol specifications.#R##N##R##N#We propose a new extraction methodology that improves on the state of the art: we can deal with programs that are equipped with state and internal computation; time complexity is dramatically better; and we capture programs that work by exploiting asynchronous communication."
      },
      {
        "node_idx": 60109,
        "score_0_10": 10,
        "title": "a core model for choreographic programming",
        "abstract": "Choreographic Programming is a paradigm for developing concurrent programs that are deadlock-free by construction, by programming communications declaratively and then synthesising process implementations automatically. Despite strong interest on choreographies, a foundational model that explains which computations can be performed with the hallmark constructs of choreographies is still missing."
      },
      {
        "node_idx": 53982,
        "score_0_10": 9,
        "title": "program synthesis from polymorphic refinement types",
        "abstract": "We present a method for synthesizing recursive functions that provably satisfy a given specification in the form of a polymorphic refinement type. We observe that such specifications are particularly suitable for program synthesis for two reasons. First, they offer a unique combination of expressive power and decidability, which enables automatic verification---and hence synthesis---of nontrivial programs. Second, a type-based specification for a program can often be effectively decomposed into independent specifications for its components, causing the synthesizer to consider fewer component combinations and leading to a combinatorial reduction in the size of the search space. At the core of our synthesis procedure is a new algorithm for refinement type checking, which supports specification decomposition. #R##N#We have evaluated our prototype implementation on a large set of synthesis problems and found that it exceeds the state of the art in terms of both scalability and usability. The tool was able to synthesize more complex programs than those reported in prior work (several sorting algorithms and operations on balanced search trees), as well as most of the benchmarks tackled by existing synthesizers, often starting from a more concise and intuitive user input."
      },
      {
        "node_idx": 66544,
        "score_0_10": 9,
        "title": "general recursion via coinductive types",
        "abstract": "A fertile field of research in theoretical computer science investigates the rep- resentation of general recursive functions in intensional type theories. Among the most successful approaches are: the use of wellfounded relations, implementation of operational semantics, formalization of domain theory, and inductive definition of domain predicates. Here, a different solution is proposed: exploiting coinductive types to model infinite com- putations. To every type A we associate a type of partial elements A \ufffd , coinductively generated by two constructors: the first, p aq just returns an element a: A; the second, \u22b2 x, adds a computation step to a recursive element x: A \ufffd . We show how this simple device is sufficient to formalize all recursive functions between two given types. It allows the definition of fixed points of finitary, that is, continuous, operators. We will compare this approach to different ones from the literature. Finally, we mention that the formalization, with appropriate structural maps, defines a strong monad."
      },
      {
        "node_idx": 4025,
        "score_0_10": 9,
        "title": "first steps in synthetic guarded domain theory step indexing in the topos of trees",
        "abstract": "We present the topos S of trees as a model of guarded recursion. We study the#N#internal dependently-typed higher-order logic of S and show that S models two#N#modal operators, on predicates and types, which serve as guards in recursive#N#definitions of terms, predicates, and types. In particular, we show how to#N#solve recursive type equations involving dependent types. We propose that the#N#internal logic of S provides the right setting for the synthetic construction#N#of abstract versions of step-indexed models of programming languages and#N#program logics. As an example, we show how to construct a model of a#N#programming language with higher-order store and recursive types entirely#N#inside the internal logic of S. Moreover, we give an axiomatic categorical#N#treatment of models of synthetic guarded domain theory and prove that, for any#N#complete Heyting algebra A with a well-founded basis, the topos of sheaves over#N#A forms a model of synthetic guarded domain theory, generalizing the results#N#for S."
      },
      {
        "node_idx": 135780,
        "score_0_10": 9,
        "title": "practical graph isomorphism ii",
        "abstract": "We report the current state of the graph isomorphism problem from the practical point of view. After describing the general principles of the refinement-individualization paradigm and proving its validity, we explain how it is implemented in several of the key programs. In particular, we bring the description of the best known program nauty up to date and describe an innovative approach called Traces that outperforms the competitors for many difficult graph classes. Detailed comparisons against saucy, Bliss and conauto are presented."
      },
      {
        "node_idx": 167796,
        "score_0_10": 9,
        "title": "kleene algebra with domain",
        "abstract": "We propose Kleene algebra with domain (KAD), an extension of Kleene algebra with two equational axioms for a domain and a codomain operation, respectively. KAD considerably augments the expressiveness of Kleene algebra, in particular for the specification and analysis of state transition systems. We develop the basic calculus, discuss some related theories and present the most important models of KAD. We demonstrate applicability by two examples: First, an algebraic reconstruction of Noethericity and well-foundedness; second, an algebraic reconstruction of propositional Hoare logic."
      },
      {
        "node_idx": 59001,
        "score_0_10": 9,
        "title": "partial replanning for decentralized dynamic task allocation",
        "abstract": "In time-sensitive and dynamic missions, multi-UAV teams must respond quickly to new information and objectives. This paper presents a dynamic decentralized task allocation algorithm for allocating new tasks that appear online during the solving of the task allocation problem. Our algorithm extends the Consensus-Based Bundle Algorithm (CBBA), a decentralized task allocation algorithm, allowing for the fast allocation of new tasks without a full reallocation of existing tasks. CBBA with Partial Replanning (CBBA-PR) enables the team to trade-off between convergence time and increased coordination by resetting a portion of their previous allocation at every round of bidding on tasks. By resetting the last tasks allocated by each agent, we are able to ensure the convergence of the team to a conflict-free solution. CBBA-PR can be further improved by reducing the team size involved in the replanning, further reducing the communication burden of the team and runtime of CBBA-PR. Finally, we validate the faster convergence and improved solution quality of CBBA-PR in multi-UAV simulations."
      },
      {
        "node_idx": 47795,
        "score_0_10": 9,
        "title": "logics for xml",
        "abstract": "This work describes the theoretical and practical foundations of a system for the static analysis of XML processing languages. The system relies on a fixpoint modal logic with converse where models are finite trees. This calculus is expressive enough to capture regular tree types along with multi-directional navigation in trees. The decidability of the logic is proved in time 2^O(n) where n is the size of the input formula. XPath expressions and XML schemas are linearly translated into the logic. Based on these embeddings, several problems of major importance in XML applications are reduced to logical satisfiability. The focus is then given to a sound and complete algorithm for deciding the logic, along with crucial implementation techniques for building an effective solver. Practical experiments using a full system implementation are presented. The system appears efficient in practice for several realistic scenarios. The main application of this work is a new class of static analyzers for programs manipulating XML data. Such analyzers allow to ensure at compile-time valuable properties such as type-safety and optimizations, for safer and more efficient XML processing."
      },
      {
        "node_idx": 42662,
        "score_0_10": 9,
        "title": "the fast downward planning system",
        "abstract": "Fast Downward is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. However, unlike other PDDL planning systems, Fast Downward does not use the propositional PDDL representation of a planning task directly. Instead, the input is first translated into an alternative representation called multivalued planning tasks, which makes many of the implicit constraints of a propositional planning task explicit. Exploiting this alternative representation, Fast Downward uses hierarchical decompositions of planning tasks for computing its heuristic function, called the causal graph heuristic, which is very different from traditional HSP-like heuristics based on ignoring negative interactions of operators.#R##N##R##N#In this article, we give a full account of Fast Downward's approach to solving multivalued planning tasks. We extend our earlier discussion of the causal graph heuristic to tasks involving axioms and conditional effects and present some novel techniques for search control that are used within Fast Downward's best-first search algorithm: preferred operators transfer the idea of helpful actions from local search to global best-first search, deferred evaluation of heuristic functions mitigates the negative effect of large branching factors on search performance, and multiheuristic best-first search combines several heuristic evaluation functions within a single search algorithm in an orthogonal way. We also describe efficient data structures for fast state expansion (successor generators and axiom evaluators) and present a new non-heuristic search algorithm called focused iterative-broadening search, which utilizes the information encoded in causal graphs in a novel way.#R##N##R##N#Fast Downward has proven remarkably successful: It won the \"classical\" (i. e., propositional, non-optimising) track of the 4th International Planning Competition at ICAPS 2004, following in the footsteps of planners such as FF and LPG. Our experiments show that it also performs very well on the benchmarks of the earlier planning competitions and provide some insights about the usefulness of the new search enhancements."
      }
    ]
  },
  "249": {
    "explanation": "advanced secure information flow and data storage verification methods",
    "topk": [
      {
        "node_idx": 76464,
        "score_0_10": 10,
        "title": "a rewritable random access dna based storage system",
        "abstract": "We describe the first DNA-based storage architecture that enables random access to data blocks and rewriting of information stored at arbitrary locations within the blocks. The newly developed architecture overcomes drawbacks of existing read-only methods that require decoding the whole file in order to read one data fragment. Our system is based on new constrained coding techniques and accompanying DNA editing methods that ensure data reliability, specificity and sensitivity of access, and at the same time provide exceptionally high data storage capacity. As a proof of concept, we encoded parts of the Wikipedia pages of six universities in the USA, and selected and edited parts of the text written in DNA corresponding to three of these schools. The results suggest that DNA is a versatile media suitable for both ultrahigh density archival and rewritable storage applications."
      },
      {
        "node_idx": 76162,
        "score_0_10": 10,
        "title": "strict linearizability and abstract atomicity",
        "abstract": "Linearizability is a commonly accepted consistency condition for concurrent objects. Filipovic et al. show that linearizability is equivalent to observational refinement. However, linearizability does not permit concurrent objects to share memory spaces with their client programs. We show that linearizability (or observational refinement) can be broken even though a client program of an object accesses the shared memory spaces without interference from the methods of the object. In this paper, we present strict linearizability which lifts this limitation and can ensure client-side traces and final-states equivalence even in a relaxed program model allowing clients to directly access the states of concurrent objects. We also investigate several important properties of strict linearizability. #R##N#At a high level of abstraction, a concurrent object can be viewed as a concurrent implementation of an abstract data type (ADT). We also present a correctness criterion for relating an ADT and its concurrent implementation, which is the combination of linearizability and data abstraction and can ensure observational equivalence. We also investigate its relationship with strict linearizability."
      },
      {
        "node_idx": 62779,
        "score_0_10": 10,
        "title": "a permission dependent type system for secure information flow analysis",
        "abstract": "We introduce a novel type system for enforcing secure information flow in an imperative language. Our work is motivated by the problem of statically checking potential information leakage in Android applications. To this end, we design a lightweight type system featuring Android permission model, where the permissions are statically assigned to applications and are used to enforce access control in the applications. We take inspiration from a type system by Banerjee and Naumann (BN) to allow security types to be dependent on the permissions of the applications. A novel feature of our type system is a typing rule for conditional branching induced by permission testing, which introduces a merging operator on security types, allowing more precise security policies to be enforced. The soundness of our type system is proved with respect to a notion of noninterference. In addition, a type inference algorithm is presented for the underlying security type system, by reducing the inference problem to a constraint solving problem in the lattice of security types."
      },
      {
        "node_idx": 93662,
        "score_0_10": 10,
        "title": "codes for dna sequence profiles",
        "abstract": "We consider the problem of storing and retrieving information from synthetic DNA media. The mathematical basis of the problem is the construction and design of sequences that may be discriminated based on their collection of substrings observed through a noisy channel. This problem of reconstructing sequences from traces was first investigated in the noiseless setting under the name of \"Markov type\" analysis. Here, we explain the connection between the reconstruction problem and the problem of DNA synthesis and sequencing, and introduce the notion of a DNA storage channel. We analyze the number of sequence equivalence classes under the channel mapping and propose new asymmetric coding techniques to combat the effects of synthesis and sequencing noise. In our analysis, we make use of restricted de Bruijn graphs and Ehrhart theory for rational polytopes."
      },
      {
        "node_idx": 139015,
        "score_0_10": 9,
        "title": "session type isomorphisms",
        "abstract": "There has been a considerable amount of work on retrieving functions in function libraries using their type as search key. The availability of rich component specifications, in the form of behavioral types, enables similar queries where one can search a component library using the behavioral type of a component as the search key. Just like for function libraries, however, component libraries will contain components whose type differs from the searched one in the order of messages or in the position of the branching points. Thus, it makes sense to also look for those components whose type is different from, but isomorphic to, the searched one. #R##N#In this article we give semantic and axiomatic characterizations of isomorphic session types. The theory of session type isomorphisms turns out to be subtle. In part this is due to the fact that it relies on a non-standard notion of equivalence between processes. In addition, we do not know whether the axiomatization is complete. It is known that the isomorphisms for arrow, product and sum types are not finitely axiomatisable, but it is not clear yet whether this negative results holds also for the family of types we consider in this work."
      },
      {
        "node_idx": 145236,
        "score_0_10": 9,
        "title": "a verified information flow architecture",
        "abstract": "SAFE is a clean-slate design for a highly secure computer system, with pervasive mechanisms for tracking and limiting information flows. At the lowest level, the SAFE hardware supports fine-grained programmable tags, with efficient and flexible propagation and combination of tags as instructions are executed. The operating system virtualizes these generic facilities to present an information-flow abstract machine that allows user programs to label sensitive data with rich confidentiality policies. We present a formal, machine-checked model of the key hardware and software mechanisms used to dynamically control information flow in SAFE and an end-to-end proof of noninterference for this model. #R##N#We use a refinement proof methodology to propagate the noninterference property of the abstract machine down to the concrete machine level. We use an intermediate layer in the refinement chain that factors out the details of the information-flow control policy and devise a code generator for compiling such information-flow policies into low-level monitor code. Finally, we verify the correctness of this generator using a dedicated Hoare logic that abstracts from low-level machine instructions into a reusable set of verified structured code generators."
      },
      {
        "node_idx": 48739,
        "score_0_10": 9,
        "title": "the fractal structure of cellular automata on abelian groups",
        "abstract": "It is well-known that the spacetime diagrams of some cellular automata have a fractal structure: for instance Pascal's triangle modulo 2 generates a Sierpinski triangle. Explaining the fractal structure of the spacetime diagrams of cellular automata is a much explored topic, but virtually all of the results revolve around a special class of automata, whose typical features include irreversibility, an alphabet with a ring structure, a global evolution that is a ring homomorphism, and a property known as (weakly) p-Fermat. The class of automata that we study in this article has none of these properties. Their cell structure is weaker, as it does not come with a multiplication, and they are far from being p-Fermat, even weakly. However, they do produce fractal spacetime diagrams, and we explain why and how."
      },
      {
        "node_idx": 13422,
        "score_0_10": 9,
        "title": "reducing opacity to linearizability a sound and complete method",
        "abstract": "Transactional memory is a mechanism that manages thread synchronisation on behalf of a programmer so that blocks of code execute with an illusion of atomicity. The main safety criterion for transactional memory is opacity, which defines conditions for serialising concurrent transactions. #R##N#Proving opacity is complicated because it allows concurrent transactions to observe distinct memory states, while TM implementations are typically based on one single shared store. This paper presents a sound and complete method, based on coarse-grained abstraction, for reducing proofs of opacity to the relatively simpler correctness condition: linearizability. We use our methods to verify TML and NORec from the literature and show our techniques extend to relaxed memory models by showing that both are opaque under TSO without requiring additional fences. Our methods also elucidate TM designs at higher level of abstraction; as an application, we develop a variation of NORec with fast-path reads transactions. All our proofs have been mechanised, either in the Isabelle theorem prover or the PAT model checker."
      },
      {
        "node_idx": 83954,
        "score_0_10": 9,
        "title": "comparing type systems for deadlock freedom",
        "abstract": "Communication-centric software systems exhibit non trivial forms of concurrency and distribution, they are expected to respect intended protocols among interacting services, but also to never \"get stuck\". This intuitive requirement has been expressed by liveness properties such as progress or (dead)lock freedom, various type systems ensure these properties for concurrent processes. Unfortunately, very little is known about the precise relationship between these type systems and the typed processes they induce. #R##N#This paper puts forward the first comparative study of different type systems for enforcing deadlock-freedom in message-passing concurrent processes. We compare two representative classes of deadlock-free typed processes, here denoted L and K. The class L stands out for its canonicity: it results naturally from Curry-Howard interpretations of linear logic propositions as session types. The class K, obtained by encoding session types into Kobayashi's usage types, includes processes not typable in other type systems. #R##N#We show that L is strictly included in K. We also identify the precise condition under which L and K coincide. One key observation is that the degree of sharing between parallel processes determines a new expressiveness hierarchy for typed concurrent processes. Furthermore, we provide two type-preserving procedures for rewriting processes in K into processes in L. Our two procedures suggest that, while effective, the degree of sharing is a rather subtle criterion for distinguishing typed concurrent processes"
      },
      {
        "node_idx": 82210,
        "score_0_10": 9,
        "title": "observability of strapdown ins alignment a global perspective",
        "abstract": "Alignment of the strapdown inertial navigation system (INS) has strong nonlinearity, even worse when maneuvers, e.g., tumbling techniques, are employed to improve the alignment. There is no general rule to attack the observability of a nonlinear system, so most previous works addressed the observability of the corresponding linearized system by implicitly assuming that the original nonlinear system and the linearized one have identical observability characteristics. Strapdown INS alignment is a nonlinear system that has its own characteristics. Using the inherent properties of strapdown INS, e.g., the attitude evolution on the SO(3) manifold, we start from the basic definition and develop a global and constructive approach to investigate the observability of strapdown INS static and tumbling alignment, highlighting the effects of the attitude maneuver on observability. We prove that strapdown INS alignment, considering the unknown constant sensor biases, will be completely observable if the strapdown INS is rotated successively about two different axes and will be nearly observable for finite known unobservable states (no more than two) if it is rotated about a single axis. Observability from a global perspective provides us with insights into and a clearer picture of the problem, shedding light on previous theoretical results on strapdown INS alignment that were not comprehensive or consistent. The reporting of inconsistencies calls for a review of all linearization-based observability studies in the vast literature. Extensive simulations with constructed ideal observers and an extended Kalman filter are carried out, and the numerical results accord with the analysis. The conclusions can also assist in designing the optimal tumbling strategy and the appropriate state observer in practice to maximize the alignment performance."
      }
    ]
  },
  "252": {
    "explanation": "video action recognition and temporal modeling with deep convolutional networks",
    "topk": [
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 39369,
        "score_0_10": 8,
        "title": "person re identification by local maximal occurrence representation and metric learning",
        "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively."
      },
      {
        "node_idx": 121343,
        "score_0_10": 8,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 123034,
        "score_0_10": 8,
        "title": "tagging multimedia stimuli with ontologies",
        "abstract": "Successful management of emotional stimuli is a pivotal issue concerning Affective Computing (AC) and the related research. As a subfield of Artificial Intelligence, AC is concerned not only with the design of computer systems and the accompanying hardware that can recognize, interpret, and process human emotions, but also with the development of systems that can trigger human emotional response in an ordered and controlled manner. This requires the maximum attainable precision and efficiency in the extraction of data from emotionally annotated databases While these databases do use keywords or tags for description of the semantic content, they do not provide either the necessary flexibility or leverage needed to efficiently extract the pertinent emotional content. Therefore, to this extent we propose an introduction of ontologies as a new paradigm for description of emotionally annotated data. The ability to select and sequence data based on their semantic attributes is vital for any study involving metadata, semantics and ontological sorting like the Semantic Web or the Social Semantic Desktop, and the approach described in the paper facilitates reuse in these areas as well."
      },
      {
        "node_idx": 67928,
        "score_0_10": 7,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 108093,
        "score_0_10": 7,
        "title": "temporal segment networks towards good practices for deep action recognition",
        "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices."
      },
      {
        "node_idx": 57930,
        "score_0_10": 7,
        "title": "show attend and tell neural image caption generation with visual attention",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO."
      }
    ]
  },
  "254": {
    "explanation": "Gaussian interference channel capacity and interference alignment techniques",
    "topk": [
      {
        "node_idx": 33272,
        "score_0_10": 10,
        "title": "the approximate capacity of the many to one and one to many gaussian interference channels",
        "abstract": "Recently, Etkin, Tse, and Wang found the capacity region of the two-user Gaussian interference channel to within 1 bit/s/Hz. A natural goal is to apply this approach to the Gaussian interference channel with an arbitrary number of users. We make progress towards this goal by finding the capacity region of the many-to-one and one-to-many Gaussian interference channels to within a constant number of bits. The result makes use of a deterministic model to provide insight into the Gaussian channel. The deterministic model makes explicit the dimension of signal level. A central theme emerges: the use of lattice codes for alignment of interfering signals on the signal level."
      },
      {
        "node_idx": 71257,
        "score_0_10": 10,
        "title": "the two user gaussian interference channel a deterministic view",
        "abstract": "This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel."
      },
      {
        "node_idx": 109425,
        "score_0_10": 10,
        "title": "ergodic interference alignment",
        "abstract": "This paper develops a new communication strategy, ergodic interference alignment, for the K-user interference channel with time-varying fading. At any particular time, each receiver will see a superposition of the transmitted signals plus noise. The standard approach to such a scenario results in each transmitter-receiver pair achieving a rate proportional to 1/K its interference-free ergodic capacity. However, given two well-chosen time indices, the channel coefficients from interfering users can be made to exactly cancel. By adding up these two observations, each receiver can obtain its desired signal without any interference. If the channel gains have independent, uniform phases, this technique allows each user to achieve at least 1/2 its interference-free ergodic capacity at any signal-to-noise ratio. Prior interference alignment techniques were only able to attain this performance as the signal-to-noise ratio tended to infinity. Extensions are given for the case where each receiver wants a message from more than one transmitter as well as the \"X channel\" case (with two receivers) where each transmitter has an independent message for each receiver. Finally, it is shown how to generalize this strategy beyond Gaussian channel models. For a class of finite field interference channels, this approach yields the ergodic capacity region."
      },
      {
        "node_idx": 105442,
        "score_0_10": 9,
        "title": "index coding an interference alignment perspective",
        "abstract": "The index coding problem is studied from an interference alignment perspective, providing new results as well as new insights into, and generalizations of, previously known results. An equivalence is established between multiple unicast index coding where each message is desired by exactly one receiver, and multiple groupcast index coding where a message can be desired by multiple receivers, which settles the heretofore open question of insufficiency of linear codes for the multiple unicast index coding problem by equivalence with multiple groupcast settings where this question has previously been answered. Necessary and sufficient conditions for the achievability of rate half per message are shown to be a natural consequence of interference alignment constraints, and generalizations to feasibility of rate $\\frac{1}{L+1}$ per message when each destination desires at least $L$ messages, are similarly obtained. Finally, capacity optimal solutions are presented to a series of symmetric index coding problems inspired by the local connectivity and local interference characteristics of wireless networks. The solutions are based on vector linear coding."
      },
      {
        "node_idx": 58129,
        "score_0_10": 9,
        "title": "the weight distributions of cyclic codes and elliptic curves",
        "abstract": "Cyclic codes with two zeros and their dual codes as a practically and theoretically interesting class of linear codes, have been studied for many years. However, the weight distributions of cyclic codes are difficult to determine. From elliptic curves, this paper determines the weight distributions of dual codes of cyclic codes with two zeros for a few more cases."
      },
      {
        "node_idx": 67794,
        "score_0_10": 9,
        "title": "topological interference management through index coding",
        "abstract": "This paper studies linear interference networks, both wired and wireless, with no channel state information at the transmitters except a coarse knowledge of the end-to-end one-hop topology of the network that only allows a distinction between weak (zero) and significant (nonzero) channels and no further knowledge of the channel coefficients' realizations. The network capacity (wired) and degrees of freedom (DoF) (wireless) are found to be bounded above by the capacity of an index coding problem for which the antidote graph is the complement of the given interference graph. The problems are shown to be equivalent under linear solutions. An interference alignment perspective is then used to translate the existing index coding solutions into the wired network capacity and wireless network DoF solutions, as well as to find new and unified solutions to different classes of all three problems."
      },
      {
        "node_idx": 47821,
        "score_0_10": 9,
        "title": "pseudo random phase precoded spatial modulation",
        "abstract": "Spatial modulation (SM) is a transmission scheme that uses multiple transmit antennas but only one transmit RF chain. At each time instant, only one among the transmit antennas will be active and the others remain silent. The index of the active transmit antenna will also convey information bits in addition to the information bits conveyed through modulation symbols (e.g.,QAM). Pseudo-random phase precoding (PRPP) is a technique that can achieve high diversity orders even in single antenna systems without the need for channel state information at the transmitter (CSIT) and transmit power control (TPC). In this paper, we exploit the advantages of both SM and PRPP simultaneously. We propose a pseudo-random phase precoded SM (PRPP-SM) scheme, where both the modulation bits and the antenna index bits are precoded by pseudo-random phases. The proposed PRPP-SM system gives significant performance gains over SM system without PRPP and PRPP system without SM. Since maximum likelihood (ML) detection becomes exponentially complex in large dimensions, we propose low complexity local search based detection (LSD) algorithm suited for PRPP-SM systems with large precoder sizes. Our simulation results show that with 4 transmit antennas, 1 receive antenna, $5\\times 20$ pseudo-random phase precoder matrix and BPSK modulation, the performance of PRPP-SM using ML detection is better than SM without PRPP with ML detection by about 9 dB at $10^{-2}$ BER. This performance advantage gets even better for large precoding sizes."
      },
      {
        "node_idx": 121725,
        "score_0_10": 9,
        "title": "precoder index modulation",
        "abstract": "Index modulation, where information bits are conveyed through antenna indices (spatial modulation) and subcarrier indices (subcarrier index modulation) in addition to information bits conveyed through conventional modulation symbols, is getting increased research attention. In this paper, we introduce {\\em precoder index modulation}, where information bits are conveyed through the choice of a precoder matrix at the transmitter from a set of pre-determined pseudo-random phase precoder (PRPP) matrices. Combining precoder index modulation (PIM) and spatial modulation (SM), we introduce a PIM-SM scheme which conveys information bits through both antenna index as well as precoder index. Spectral efficiency (in bits per channel use) and bit error performance of these index modulation schemes are presented."
      },
      {
        "node_idx": 147905,
        "score_0_10": 8,
        "title": "interference alignment with asymmetric complex signaling settling the host madsen nosratinia conjecture",
        "abstract": "It has been conjectured by Host-Madsen and Nosratinia that complex Gaussian interference channels with constant channel coefficients have only one degree-of-freedom regardless of the number of users. While several examples are known of constant channels that achieve more than 1 degree of freedom, these special cases only span a subset of measure zero. In other words, for almost all channel coefficient values, it is not known if more than 1 degree-of-freedom is achievable. In this paper, we settle the Host-Madsen-Nosratinia conjecture in the negative. We show that at least 1.2 degrees-of-freedom are achievable for all values of complex channel coefficients except for a subset of measure zero. For the class of linear beamforming and interference alignment schemes considered in this paper, it is also shown that 1.2 is the maximum number of degrees of freedom achievable on the complex Gaussian 3 user interference channel with constant channel coefficients, for almost all values of channel coefficients. To establish the achievability of 1.2 degrees of freedom we introduce the novel idea of asymmetric complex signaling - i.e., the inputs are chosen to be complex but not circularly symmetric. It is shown that unlike Gaussian point-to-point, multiple-access and broadcast channels where circularly symmetric complex Gaussian inputs are optimal, for interference channels optimal inputs are in general asymmetric. With asymmetric complex signaling, we also show that the 2 user complex Gaussian X channel with constant channel coefficients achieves the outer bound of 4/3 degrees-of-freedom, i.e., the assumption of time-variations/frequency-selectivity used in prior work to establish the same result, is not needed."
      },
      {
        "node_idx": 100650,
        "score_0_10": 8,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      }
    ]
  },
  "255": {
    "explanation": "theory and construction of advanced algebraic and computational structures",
    "topk": [
      {
        "node_idx": 77756,
        "score_0_10": 10,
        "title": "on the construction of skew quasi cyclic codes",
        "abstract": "In this paper we study a special type of quasi-cyclic (QC) codes called skew QC codes. This set of codes is constructed using a non-commutative ring called the skew polynomial rings $F[x;\\theta ]$. After a brief description of the skew polynomial ring $F[x;\\theta ]$ it is shown that skew QC codes are left submodules of the ring $R_{s}^{l}=(F[x;\\theta ]/(x^{s}-1))^{l}.$ The notions of generator and parity-check polynomials are given. We also introduce the notion of similar polynomials in the ring $F[x;\\theta ]$ and show that parity-check polynomials for skew QC codes are unique up to similarity. Our search results lead to the construction of several new codes with Hamming distances exceeding the Hamming distances of the previously best known linear codes with comparable parameters."
      },
      {
        "node_idx": 67861,
        "score_0_10": 10,
        "title": "handling algebraic effects",
        "abstract": "Algebraic effects are computational effects that can be represented by an#N#equational theory whose operations produce the effects at hand. The free model#N#of this theory induces the expected computational monad for the corresponding#N#effect. Algebraic effects include exceptions, state, nondeterminism,#N#interactive input/output, and time, and their combinations. Exception handling,#N#however, has so far received no algebraic treatment. We present such a#N#treatment, in which each handler yields a model of the theory for exceptions,#N#and each handling construct yields the homomorphism induced by the universal#N#property of the free model. We further generalise exception handlers to#N#arbitrary algebraic effects. The resulting programming construct includes many#N#previously unrelated examples from both theory and practice, including#N#relabelling and restriction in Milner's CCS, timeout, rollback, and stream#N#redirection."
      },
      {
        "node_idx": 160260,
        "score_0_10": 10,
        "title": "new classes of permutation binomials and permutation trinomials over finite fields",
        "abstract": "Permutation polynomials over finite fields play important roles in finite fields theory. They also have wide applications in many areas of science and engineering such as coding theory, cryptography, combinatorial design, communication theory and so on. Permutation binomials and trinomials attract people's interest due to their simple algebraic form and additional extraordinary properties. In this paper, several new classes of permutation binomials and permutation trinomials are constructed. Some of these permutation polynomials are generalizations of known ones."
      },
      {
        "node_idx": 14848,
        "score_0_10": 10,
        "title": "syntactic structures of regular languages",
        "abstract": "We introduce here the notion of syntactic lattice algebra which is an analogy of the syntactic monoid and of the syntactic semiring. We present a unified approach to get those three structures."
      },
      {
        "node_idx": 59933,
        "score_0_10": 10,
        "title": "permutation trinomials over finite fields with even characteristic",
        "abstract": "Permutation polynomials have been a subject of study for a long time and have applications in many areas of science and engineering. However, only a small number of specific classes of permutation polynomials are described in the literature so far. In this paper we present a number of permutation trinomials over finite fields, which are of different forms."
      },
      {
        "node_idx": 30607,
        "score_0_10": 10,
        "title": "a tight lower bound for streett complementation",
        "abstract": "Finite automata on infinite words ($\\omega$-automata) proved to be a powerful weapon for modeling and reasoning infinite behaviors of reactive systems. Complementation of $\\omega$-automata is crucial in many of these applications. But the problem is non-trivial; even after extensive study during the past four decades, we still have an important type of $\\omega$-automata, namely Streett automata, for which the gap between the current best lower bound $2^{\\Omega(n \\lg nk)}$ and upper bound $2^{\\Omega(nk \\lg nk)}$ is substantial, for the Streett index size $k$ can be exponential in the number of states $n$. In arXiv:1102.2960 we showed a construction for complementing Streett automata with the upper bound $2^{O(n \\lg n+nk \\lg k)}$ for $k = O(n)$ and $2^{O(n^{2} \\lg n)}$ for $k=\\omega(n)$. In this paper we establish a matching lower bound $2^{\\Omega(n \\lg n+nk \\lg k)}$ for $k = O(n)$ and $2^{\\Omega(n^{2} \\lg n)}$ for $k = \\omega(n)$, and therefore showing that the construction is asymptotically optimal with respect to the $2^{\\Theta(\\cdot)}$ notation."
      },
      {
        "node_idx": 4025,
        "score_0_10": 10,
        "title": "first steps in synthetic guarded domain theory step indexing in the topos of trees",
        "abstract": "We present the topos S of trees as a model of guarded recursion. We study the#N#internal dependently-typed higher-order logic of S and show that S models two#N#modal operators, on predicates and types, which serve as guards in recursive#N#definitions of terms, predicates, and types. In particular, we show how to#N#solve recursive type equations involving dependent types. We propose that the#N#internal logic of S provides the right setting for the synthetic construction#N#of abstract versions of step-indexed models of programming languages and#N#program logics. As an example, we show how to construct a model of a#N#programming language with higher-order store and recursive types entirely#N#inside the internal logic of S. Moreover, we give an axiomatic categorical#N#treatment of models of synthetic guarded domain theory and prove that, for any#N#complete Heyting algebra A with a well-founded basis, the topos of sheaves over#N#A forms a model of synthetic guarded domain theory, generalizing the results#N#for S."
      },
      {
        "node_idx": 166321,
        "score_0_10": 10,
        "title": "second order algebraic theories",
        "abstract": "Fiore and Hur recently introduced a conservative extension of universal algebra and equational logic from first to second order. Second-order universal algebra and second-order equational logic respectively provide a model theory and a formal deductive system for languages with variable binding and parameterised metavariables. This work completes the foundations of the subject from the viewpoint of categorical algebra. Specifically, the paper introduces the notion of second-order algebraic theory and develops its basic theory. Two categorical equivalences are established: at the syntactic level, that of second-order equational presentations and second-order algebraic theories; at the semantic level, that of second-order algebras and second-order functorial models. Our development includes a mathematical definition of syntactic translation between second-order equational presentations. This gives the first formalisation of notions such as encodings and transforms in the context of languages with variable binding."
      },
      {
        "node_idx": 76985,
        "score_0_10": 10,
        "title": "monadic datalog containment on trees",
        "abstract": "We show that the query containment problem for monadic datalog on finite unranked labeled trees can be solved in 2-fold exponential time when (a) considering unordered trees using the axes child and descendant, and when (b) considering ordered trees using the axes firstchild, nextsibling, child, and descendant. When omitting the descendant-axis, we obtain that in both cases the problem is EXPTIME-complete."
      },
      {
        "node_idx": 66544,
        "score_0_10": 10,
        "title": "general recursion via coinductive types",
        "abstract": "A fertile field of research in theoretical computer science investigates the rep- resentation of general recursive functions in intensional type theories. Among the most successful approaches are: the use of wellfounded relations, implementation of operational semantics, formalization of domain theory, and inductive definition of domain predicates. Here, a different solution is proposed: exploiting coinductive types to model infinite com- putations. To every type A we associate a type of partial elements A \ufffd , coinductively generated by two constructors: the first, p aq just returns an element a: A; the second, \u22b2 x, adds a computation step to a recursive element x: A \ufffd . We show how this simple device is sufficient to formalize all recursive functions between two given types. It allows the definition of fixed points of finitary, that is, continuous, operators. We will compare this approach to different ones from the literature. Finally, we mention that the formalization, with appropriate structural maps, defines a strong monad."
      }
    ]
  },
  "257": {
    "explanation": "erasure codes optimization for distributed storage repair and reliability",
    "topk": [
      {
        "node_idx": 105448,
        "score_0_10": 10,
        "title": "network coding for distributed storage systems",
        "abstract": "Distributed storage systems provide reliable access to data through redundancy spread over individually unreliable nodes. Application scenarios include data centers, peer-to-peer storage systems, and storage in wireless networks. Storing data using an erasure code, in fragments spread across nodes, requires less redundancy than simple replication for the same level of reliability. However, since fragments must be periodically replaced as nodes fail, a key question is how to generate encoded fragments in a distributed way while transferring as little data as possible across the network. #R##N#For an erasure coded system, a common practice to repair from a node failure is for a new node to download subsets of data stored at a number of surviving nodes, reconstruct a lost coded block using the downloaded data, and store it at the new node. We show that this procedure is sub-optimal. We introduce the notion of regenerating codes, which allow a new node to download \\emph{functions} of the stored data from the surviving nodes. We show that regenerating codes can significantly reduce the repair bandwidth. Further, we show that there is a fundamental tradeoff between storage and repair bandwidth which we theoretically characterize using flow arguments on an appropriately constructed graph. By invoking constructive results in network coding, we introduce regenerating codes that can achieve any point in this optimal tradeoff."
      },
      {
        "node_idx": 116697,
        "score_0_10": 10,
        "title": "extended unary coding",
        "abstract": "Extended variants of the recently introduced spread unary coding are described. These schemes, in which the length of the code word is fixed, allow representation of approximately n^2 numbers for n bits, rather than the n numbers of the standard unary coding. In the first of two proposed schemes the spread increases, whereas in the second scheme the spread remains constant."
      },
      {
        "node_idx": 58412,
        "score_0_10": 10,
        "title": "semantic security and indistinguishability in the quantum world",
        "abstract": "At CRYPTO 2013, Boneh and Zhandry initiated the study of quantum-secure encryption. They proposed first indistinguishability definitions for the quantum world where the actual indistinguishability only holds for classical messages, and they provide arguments why it might be hard to achieve a stronger notion. In this work, we show that stronger notions are achievable, where the indistinguishability holds for quantum superpositions of messages. We investigate exhaustively the possibilities and subtle differences in defining such a quantum indistinguishability notion for symmetric-key encryption schemes. We justify our stronger definition by showing its equivalence to novel quantum semantic-security notions that we introduce. Furthermore, we show that our new security definitions cannot be achieved by a large class of ciphers --- those which are quasi-preserving the message length. On the other hand, we provide a secure construction based on quantum-resistant pseudorandom permutations; this construction can be used as a generic transformation for turning a large class of encryption schemes into quantum indistinguishable and hence quantum semantically secure ones. Moreover, our construction is the first completely classical encryption scheme shown to be secure against an even stronger notion of indistinguishability, which was previously known to be achievable only by using quantum messages and arbitrary quantum encryption circuits."
      },
      {
        "node_idx": 39332,
        "score_0_10": 10,
        "title": "xoring elephants novel erasure codes for big data",
        "abstract": "Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. #R##N#This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. #R##N#We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication."
      },
      {
        "node_idx": 44329,
        "score_0_10": 10,
        "title": "delegated pseudo secret random qubit generator",
        "abstract": "We define the functionality of delegated pseudo-secret random qubit generator (PSRQG), where a classical client can instruct the preparation of a sequence of random qubits at some distant party. Their classical description is (computationally) unknown to any other party (including the distant party preparing them) but known to the client. We emphasize the unique feature that no quantum communication is required to implement PSRQG. This enables classical clients to perform a class of quantum communication protocols with only a public classical channel with a quantum server. A key such example is the delegated universal quantum computing. Using our functionality one could achieve for the first time a purely classical-client computational secure verifiable delegated universal quantum computing (also referred to as verifiable blind quantum computation). We give a concrete protocol (QFactory) implementing PSRQG, using the Learning-With-Errors problem to construct a trapdoor one-way function with certain desired properties (quantum-safe, two-regular, collision-resistant). We then prove the security in the Quantum-Honest-But-Curious setting and briefly discuss the extension to the malicious case."
      },
      {
        "node_idx": 52827,
        "score_0_10": 10,
        "title": "ubr improving performance of tcp over atm ubr service",
        "abstract": "ATM-UBR switches respond to congestion by dropping cells when their buffers become full. TCP connections running over UBR experience low throughput and high unfairness. For 100% TCP throughput each switch needs buffers equal to the sum of the window sizes of all the TCP connections. Intelligent drop policies can improve the performance of TCP over UBR with limited buffers. The UBR+ service proposes enhancements to UBR for intelligent drop. Early Packet Discard improves throughput but does not attempt to improve fairness. Selective packet drop based on per-connection buffer occupancy improves fairness. The Fair Buffer Allocation scheme further improves both throughput and fairness."
      },
      {
        "node_idx": 48830,
        "score_0_10": 10,
        "title": "tcp selective acknowledgments and ubr drop policies to improve atm ubr performance over terrestrial and satellite networks",
        "abstract": "We study the performance of Selective Acknowledgments with TCP over the ATM-UBR service category. We examine various UBR drop policies, TCP mechanisms and network configurations to recommend optimal parameters for TCP over UBR. We discuss various TCP congestion control mechanisms compare their performance for LAN and WAN networks. We describe the effect of satellite delays on TCP performance over UBR and present simulation results for LAN, WAN and satellite networks. SACK TCP improves the performance of TCP over UBR, especially for large delay networks. Intelligent drop policies at the switches are an important factor for good performance in local area networks."
      },
      {
        "node_idx": 60656,
        "score_0_10": 10,
        "title": "hash functions using chaotic iterations",
        "abstract": "In this paper, a novel formulation of discrete chaotic iterations in the field of dynamical systems is given. Their topological properties are studied: it is mathematically proved that, under some conditions, these iterations have a chaotic behavior in the meaning of Devaney. This chaotic behavior allows us to propose a way to generate new hash functions. An illustration example is detailed in order to show how to use our theoretical study in practice."
      },
      {
        "node_idx": 118548,
        "score_0_10": 10,
        "title": "the random oracle methodology revisited",
        "abstract": "We take a critical look at the relationship between the security of cryptographic schemes in the Random Oracle Model, and the security of the schemes that result from implementing the random oracle by so called \"cryptographic hash functions\". The main result of this paper is a negative one: There exist signature and encryption schemes that are secure in the Random Oracle Model, but for which any implementation of the random oracle results in insecure schemes. #R##N#In the process of devising the above schemes, we consider possible definitions for the notion of a \"good implementation\" of a random oracle, pointing out limitations and challenges."
      },
      {
        "node_idx": 42851,
        "score_0_10": 9,
        "title": "an epistemic foundation for authentication logics extended abstract",
        "abstract": "While there have been many attempts, going back to BAN logic, to base reasoning about security protocols on epistemic notions, they have not been all that successful. Arguably, this has been due to the particular logics chosen. We present a simple logic based on the well-understood modal operators of knowledge, time, and probability, and show that it is able to handle issues that have often been swept under the rug by other approaches, while being flexible enough to capture all the higher- level security notions that appear in BAN logic. Moreover, while still assuming that the knowledge operator allows for unbounded computation, it can handle the fact that a computationally bounded agent cannot decrypt messages in a natural way, by distinguishing strings and message terms. We demonstrate that our logic can capture BAN logic notions by providing a translation of the BAN operators into our logic, capturing belief by a form of probabilistic knowledge."
      }
    ]
  },
  "259": {
    "explanation": "methods and challenges in advanced data analysis and software systems",
    "topk": [
      {
        "node_idx": 42662,
        "score_0_10": 10,
        "title": "the fast downward planning system",
        "abstract": "Fast Downward is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. However, unlike other PDDL planning systems, Fast Downward does not use the propositional PDDL representation of a planning task directly. Instead, the input is first translated into an alternative representation called multivalued planning tasks, which makes many of the implicit constraints of a propositional planning task explicit. Exploiting this alternative representation, Fast Downward uses hierarchical decompositions of planning tasks for computing its heuristic function, called the causal graph heuristic, which is very different from traditional HSP-like heuristics based on ignoring negative interactions of operators.#R##N##R##N#In this article, we give a full account of Fast Downward's approach to solving multivalued planning tasks. We extend our earlier discussion of the causal graph heuristic to tasks involving axioms and conditional effects and present some novel techniques for search control that are used within Fast Downward's best-first search algorithm: preferred operators transfer the idea of helpful actions from local search to global best-first search, deferred evaluation of heuristic functions mitigates the negative effect of large branching factors on search performance, and multiheuristic best-first search combines several heuristic evaluation functions within a single search algorithm in an orthogonal way. We also describe efficient data structures for fast state expansion (successor generators and axiom evaluators) and present a new non-heuristic search algorithm called focused iterative-broadening search, which utilizes the information encoded in causal graphs in a novel way.#R##N##R##N#Fast Downward has proven remarkably successful: It won the \"classical\" (i. e., propositional, non-optimising) track of the 4th International Planning Competition at ICAPS 2004, following in the footsteps of planners such as FF and LPG. Our experiments show that it also performs very well on the benchmarks of the earlier planning competitions and provide some insights about the usefulness of the new search enhancements."
      },
      {
        "node_idx": 100588,
        "score_0_10": 10,
        "title": "in pursuit of spreadsheet excellence",
        "abstract": "The first fully-documented study into the quantitative impact of errors in operational spreadsheets identified an interesting anomaly. One of the five participating organisations involved in the study contributed a set of five spreadsheets of such quality that they set the organisation apart in a statistical sense. This virtuoso performance gave rise to a simple sampling test - The Clean Sheet Test - which can be used to objectively evaluate if an organisation is in control of the spreadsheets it is using in important processes such as financial reporting."
      },
      {
        "node_idx": 102043,
        "score_0_10": 10,
        "title": "bigdatabench a big data benchmark suite from internet services",
        "abstract": "As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above."
      },
      {
        "node_idx": 161685,
        "score_0_10": 10,
        "title": "big data computing and clouds",
        "abstract": "This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions. Survey of solutions for carrying out analytics and Big Data on Clouds.Identification of gaps in technology for Cloud-based analytics.Recommendations of research directions for Cloud-based analytics and Big Data."
      },
      {
        "node_idx": 117688,
        "score_0_10": 10,
        "title": "towards an integrative educational recommender for lifelong learners",
        "abstract": "One of the most ambitious use cases of computer-assisted learning is to build a recommendation system for lifelong learning. Most recommender algorithms exploit similarities between content and users, overseeing the necessity to leverage sensible learning trajectories for the learner. Lifelong learning thus presents unique challenges, requiring scalable and transparent models that can account for learner knowledge and content novelty simultaneously, while also retaining accurate learners representations for long periods of time. We attempt to build a novel educational recommender, that relies on an integrative approach combining multiple drivers of learners engagement. Our first step towards this goal is TrueLearn, which models content novelty and background knowledge of learners and achieves promising performance while retaining a human interpretable learner model."
      },
      {
        "node_idx": 93181,
        "score_0_10": 10,
        "title": "a quality model for actionable analytics in rapid software development",
        "abstract": "Background: Accessing relevant data on the software product, process, and usage as well as integrating and analysing it is crucial to get reliable and timely actionable insights for continuously managing software quality in Rapid Software Development (RSD). In this context, several software analytics tools have been developed in recent years. However, there is a lack of explainable software analytics that software practitioners trust. Aims: We aimed at creating a quality model -the Q-Rapids quality model- for actionable analytics in RSD, implementing it, and evaluating its understandability and relevance. Method: We performed workshops at four companies for determining relevant metrics as well as product and process factors. We also elicited how these metrics and factors are used and interpreted by practitioners when making decisions in RSD. We specified the Q-Rapids quality model by comparing and integrating the four workshops' results. Then, we implemented the Q-Rapids tool for supporting the usage of the Q-Rapids quality model as well as the gathering, integration, and analysis of the required data. Afterwards we installed the Q-Rapids tool in the four companies and performed semi-structured interviews with eight product owners to evaluate the understandability and relevance of the Q-Rapids quality model. Results: The participants of the evaluation perceive the metrics as well as product and process factors of the Q-Rapids quality model as understandable. Also, they consider the Q-Rapids quality model relevant for identifying product and process deficiencies (e.g., blocking code situations). Conclusions: By means of heterogeneous data sources, the Q-Rapids quality model is an enabler for detecting manually time-consuming problems and adding transparency among the system, process, and usage perspectives."
      },
      {
        "node_idx": 10420,
        "score_0_10": 10,
        "title": "a spreadsheet auditing tool evaluated in an industrial context",
        "abstract": "Amongst the large number of write-and-throw-away spreadsheets developed for one-time use there is a rather neglected proportion of spreadsheets that are huge, periodically used, and submitted to regular update-cycles like any conventionally evolving valuable legacy application software. However, due to the very nature of spreadsheets, their evolution is particularly tricky and therefore error-prone. In our strive to develop tools and methodologies to improve spreadsheet quality, we analysed consolidation spreadsheets of an internationally operating company for the errors they contain. The paper presents the results of the field audit, involving 78 spreadsheets with 60,446 non-empty cells. As a by-product, the study performed was also to validate our analysis tools in an industrial context. The evaluated auditing tool offers the auditor a new view on the formula structure of the spreadsheet by grouping similar formulas into equivalence classes. Our auditing approach defines three similarity criteria between formulae, namely copy, logical and structural equivalence. To improve the visualization of large spreadsheets, equivalences and data dependencies are displayed in separated windows that are interlinked with the spreadsheet. The auditing approach helps to find irregularities in the geometrical pattern of similar formulas."
      },
      {
        "node_idx": 83227,
        "score_0_10": 10,
        "title": "truelearn a family of bayesian algorithms to match lifelong learners to open educational resources",
        "abstract": "The recent advances in computer-assisted learning systems and the availability of open educational resources today promise a pathway to providing cost-efficient, high-quality education to large masses of learners. One of the most ambitious use cases of computer-assisted learning is to build a lifelong learning recommendation system. Unlike short-term courses, lifelong learning presents unique challenges, requiring sophisticated recommendation models that account for a wide range of factors such as background knowledge of learners or novelty of the material while effectively maintaining knowledge states of masses of learners for significantly longer periods of time (ideally, a lifetime). This work presents the foundations towards building a dynamic, scalable and transparent recommendation system for education, modelling learner's knowledge from implicit data in the form of engagement with open educational resources. We i) use a text ontology based on Wikipedia to automatically extract knowledge components of educational resources and, ii) propose a set of online Bayesian strategies inspired by the well-known areas of item response theory and knowledge tracing. Our proposal, TrueLearn, focuses on recommendations for which the learner has enough background knowledge (so they are able to understand and learn from the material), and the material has enough novelty that would help the learner improve their knowledge about the subject and keep them engaged. We further construct a large open educational video lectures dataset and test the performance of the proposed algorithms, which show clear promise towards building an effective educational recommendation system."
      },
      {
        "node_idx": 162327,
        "score_0_10": 10,
        "title": "characterizing data analysis workloads in data centers",
        "abstract": "As the amount of data explodes rapidly, more and more corporations are using data centers to make effective decisions and gain a competitive edge. Data analysis applications play a significant role in data centers, and hence it has became increasingly important to understand their behaviors in order to further improve the performance of data center computer systems. In this paper, after investigating three most important application domains in terms of page views and daily visitors, we choose eleven representative data analysis workloads and characterize their micro-architectural characteristics by using hardware performance counters, in order to understand the impacts and implications of data analysis workloads on the systems equipped with modern superscalar out-of-order processors. Our study on the workloads reveals that data analysis applications share many inherent characteristics, which place them in a different class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads, including traditional server workloads (SPECweb2005) and scale-out service workloads (four among six benchmarks in CloudSuite), and accordingly we give several recommendations for architecture and system optimizations. On the basis of our workload characterization work, we released a benchmark suite named DCBench for typical datacenter workloads, including data analysis and service workloads, with an open-source license on our project home page on this http URL We hope that DCBench is helpful for performing architecture and small-to-medium scale system researches for datacenter computing."
      },
      {
        "node_idx": 156775,
        "score_0_10": 10,
        "title": "towards automated data integration in software analytics",
        "abstract": "Software organizations want to be able to base their decisions on the latest set of available data and the real-time analytics derived from them. In order to support \"real-time enterprise\" for software organizations and provide information transparency for diverse stakeholders, we integrate heterogeneous data sources about software analytics, such as static code analysis, testing results, issue tracking systems, network monitoring systems, etc. To deal with the heterogeneity of the underlying data sources, we follow an ontology-based data integration approach in this paper and define an ontology that captures the semantics of relevant data for software analytics. Furthermore, we focus on the integration of such data sources by proposing two approaches: a static and a dynamic one. We first discuss the current static approach with a predefined set of analytic views representing software quality factors and further envision how this process could be automated in order to dynamically build custom user analysis using a semi-automatic platform for managing the lifecycle of analytics infrastructures."
      }
    ]
  },
  "260": {
    "explanation": "graph algorithms for tree spanners and spanner diameter constraints",
    "topk": [
      {
        "node_idx": 141610,
        "score_0_10": 10,
        "title": "tree spanners of small diameter",
        "abstract": "A graph that contains a spanning tree of diameter at most $t$ clearly admits a tree $t$-spanner, since a tree $t$-spanner of a graph $G$ is a sub tree of $G$ such that the distance between pairs of vertices in the tree is at most $t$ times their distance in $G$. In this paper, graphs that admit a tree $t$-spanner of diameter at most $t+1$ are studied. For $t$ equal to 1 or 2 the problem has been solved. For $t=3$ we present an algorithm that determines if a graph admits a tree 3-spanner of diameter at most 4. For $t\\geq4$ it is proved that it is an NP-complete problem to decide whether a graph admits a tree $t$-spanner of diameter at most $t+1$."
      },
      {
        "node_idx": 168271,
        "score_0_10": 10,
        "title": "yacc is dead",
        "abstract": "We present two novel approaches to parsing context-free languages. The first approach is based on an extension of Brzozowski's derivative from regular expressions to context-free grammars. The second approach is based on a generalization of the derivative to parser combinators. The payoff of these techniques is a small (less than 250 lines of code), easy-to-implement parsing library capable of parsing arbitrary context-free grammars into lazy parse forests. Implementations for both Scala and Haskell are provided. Preliminary experiments with S-Expressions parsed millions of tokens per second, which suggests this technique is efficient enough for use in practice."
      },
      {
        "node_idx": 98022,
        "score_0_10": 10,
        "title": "tree spanners of bounded degree graphs",
        "abstract": "A tree   t     t       -spanner of a graph   G     G        is a spanning tree of   G     G        such that the distance between pairs of vertices in the tree is at most   t     t        times their distance in   G     G       . Deciding tree   t     t       -spanner admissible graphs has been proved to be tractable for   t     t    3        and NP-complete for   t>3     t  >  3       , while the complexity status of this problem is unresolved when   t=3     t  =  3       . For every   t>2     t  >  2        and   b>0     b  >  0       , an efficient dynamic programming algorithm to decide tree   t     t       -spanner admissibility of graphs with vertex degrees less than   b     b        is presented. Only for   t=3     t  =  3       , the algorithm remains efficient, when graphs   G     G        with degrees less than   blog|V(G)|     b  log   |  V   (  G  )   |         are examined."
      },
      {
        "node_idx": 39332,
        "score_0_10": 9,
        "title": "xoring elephants novel erasure codes for big data",
        "abstract": "Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. #R##N#This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. #R##N#We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2x on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication."
      },
      {
        "node_idx": 3136,
        "score_0_10": 9,
        "title": "tree drawings revisited",
        "abstract": "We make progress on a number of open problems concerning the area requirement for drawing trees on a grid. We prove that #R##N#1. every tree of size $n$ (with arbitrarily large degree) has a straight-line drawing with area $n2^{O(\\sqrt{\\log\\log n\\log\\log\\log n})}$, improving the longstanding $O(n\\log n)$ bound; #R##N#2. every tree of size $n$ (with arbitrarily large degree) has a straight-line upward drawing with area $n\\sqrt{\\log n}(\\log\\log n)^{O(1)}$, improving the longstanding $O(n\\log n)$ bound; #R##N#3. every binary tree of size $n$ has a straight-line orthogonal drawing with area $n2^{O(\\log^*n)}$, improving the previous $O(n\\log\\log n)$ bound by Shin, Kim, and Chwa (1996) and Chan, Goodrich, Kosaraju, and Tamassia (1996); #R##N#4. every binary tree of size $n$ has a straight-line order-preserving drawing with area $n2^{O(\\log^*n)}$, improving the previous $O(n\\log\\log n)$ bound by Garg and Rusu (2003); #R##N#5. every binary tree of size $n$ has a straight-line orthogonal order-preserving drawing with area $n2^{O(\\sqrt{\\log n})}$, improving the $O(n^{3/2})$ previous bound by Frati (2007)."
      },
      {
        "node_idx": 56074,
        "score_0_10": 9,
        "title": "wiretapped oblivious transfer",
        "abstract": "In this paper, we study the problem of obtaining $1$-of-$2$ string oblivious transfer (OT) between users Alice and Bob, in the presence of a passive eavesdropper Eve. The resource enabling OT in our setup is a noisy broadcast channel from Alice to Bob and Eve. Apart from the OT requirements between the users, Eve is not allowed to learn anything about the users' inputs. When Alice and Bob are honest-but-curious and the noisy broadcast channel is made up of two independent binary erasure channels (connecting Alice-Bob and Alice-Eve), we derive the $1$-of-$2$ string OT capacity for both $2$-privacy (when Eve can collude with either Alice or Bob) and $1$-privacy (when no such collusion is allowed). We generalize these capacity results to $1$-of-$N$ string OT and study other variants of this problem. When Alice and/or Bob are malicious, we present a different scheme based on interactive hashing. This scheme is shown to be optimal for certain parameter regimes. We present a new formulation of multiple, simultaneous OTs between Alice-Bob and Alice-Cathy. For this new setup, we present schemes and outer bounds that match in all but one regime of parameters. Finally, we consider the setup where the broadcast channel is made up of a cascade of two independent binary erasure channels (connecting Alice-Bob and Bob-Eve) and $1$-of-$2$ string OT is desired between Alice and Bob with $1$-privacy. For this setup, we derive an upper and lower bound on the $1$-of-$2$ string OT capacity which match in one of two possible parameter regimes."
      },
      {
        "node_idx": 59581,
        "score_0_10": 9,
        "title": "the dune framework basic concepts and recent developments",
        "abstract": "This paper presents the basic concepts and the module structure of the Distributed and Unified Numerics Environment and reflects on recent developments and general changes that happened since the release of the first Dune version in 2007 and the main papers describing that state [1, 2]. This discussion is accompanied with a description of various advanced features, such as coupling of domains and cut cells, grid modifications such as adaptation and moving domains, high order discretizations and node level performance, non-smooth multigrid methods, and multiscale methods. A brief discussion on current and future development directions of the framework concludes the paper."
      },
      {
        "node_idx": 57558,
        "score_0_10": 9,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 95469,
        "score_0_10": 9,
        "title": "a dynamic id based remote user authentication scheme",
        "abstract": "Password-based authentication schemes are the most widely used techniques for remote user authentication. Many static ID-based remote user authentication schemes both with and without smart cards have been proposed. Most of the schemes do not allow the users to choose and change their passwords, and maintain a verifier table to verify the validity of the user login. In this paper we present a dynamic ID-based remote user authentication scheme using smart cards. Our scheme allows the users to choose and change their passwords freely, and do not maintain any verifier table. The scheme is secure against ID-theft, and can resist the reply attacks, forgery attacks, guessing attacks, insider attacks and stolen verifier attacks."
      },
      {
        "node_idx": 819,
        "score_0_10": 9,
        "title": "lombardi drawings of graphs",
        "abstract": "We introduce the notion of Lombardi graph drawings, named after the American abstract artist Mark Lombardi. In these drawings, edges are represented as circular arcs rather than as line segments or polylines, and the vertices have perfect angular resolution: the edges are equally spaced around each vertex. We describe algorithms for finding Lombardi drawings of regular graphs, graphs of bounded degeneracy, and certain families of planar graphs."
      }
    ]
  },
  "262": {
    "explanation": "quantum-resistant encryption and scalable blockchain protocols",
    "topk": [
      {
        "node_idx": 58412,
        "score_0_10": 10,
        "title": "semantic security and indistinguishability in the quantum world",
        "abstract": "At CRYPTO 2013, Boneh and Zhandry initiated the study of quantum-secure encryption. They proposed first indistinguishability definitions for the quantum world where the actual indistinguishability only holds for classical messages, and they provide arguments why it might be hard to achieve a stronger notion. In this work, we show that stronger notions are achievable, where the indistinguishability holds for quantum superpositions of messages. We investigate exhaustively the possibilities and subtle differences in defining such a quantum indistinguishability notion for symmetric-key encryption schemes. We justify our stronger definition by showing its equivalence to novel quantum semantic-security notions that we introduce. Furthermore, we show that our new security definitions cannot be achieved by a large class of ciphers --- those which are quasi-preserving the message length. On the other hand, we provide a secure construction based on quantum-resistant pseudorandom permutations; this construction can be used as a generic transformation for turning a large class of encryption schemes into quantum indistinguishable and hence quantum semantically secure ones. Moreover, our construction is the first completely classical encryption scheme shown to be secure against an even stronger notion of indistinguishability, which was previously known to be achievable only by using quantum messages and arbitrary quantum encryption circuits."
      },
      {
        "node_idx": 46715,
        "score_0_10": 9,
        "title": "bitcoin ng a scalable blockchain protocol",
        "abstract": "Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential. #R##N#This paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem. #R##N#In addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network."
      },
      {
        "node_idx": 96849,
        "score_0_10": 9,
        "title": "enhancing bitcoin security and performance with strong consistency via collective signing",
        "abstract": "While showing great promise, Bitcoin requires users to wait tens of minutes for transactions to commit, and even then, offering only probabilistic guarantees. This paper introduces ByzCoin, a novel Byzantine consensus protocol that leverages scalable collective signing to commit Bitcoin transactions irreversibly within seconds. ByzCoin achieves Byzantine consensus while preserving Bitcoin's open membership by dynamically forming hash power-proportionate consensus groups that represent recently-successful block miners. ByzCoin employs communication trees to optimize transaction commitment and verification under normal operation while guaranteeing safety and liveness under Byzantine faults, up to a near-optimal tolerance of f faulty group members among 3f + 2 total. ByzCoin mitigates double spending and selfish mining attacks by producing collectively signed transaction blocks within one minute of transaction submission. Tree-structured communication further reduces this latency to less than 30 seconds. Due to these optimizations, ByzCoin achieves a throughput higher than PayPal currently handles, with a confirmation latency of 15-20 seconds."
      },
      {
        "node_idx": 20262,
        "score_0_10": 9,
        "title": "a quantum classical scheme towards quantum functional encryption",
        "abstract": "Quantum encryption is a well studied problem for both classical and quantum information. However, little is known about quantum encryption schemes which enable the user, under different keys, to learn different functions of the plaintext, given the ciphertext. In this paper, we give a novel one-bit secret-key quantum encryption scheme, a classical extension of which allows different key holders to learn different length subsequences of the plaintext from the ciphertext. We prove our quantum-classical scheme secure under the notions of quantum semantic security, quantum entropic indistinguishability, and recent security definitions from the field of functional encryption."
      },
      {
        "node_idx": 162969,
        "score_0_10": 9,
        "title": "on the cost of concurrency in transactional memory",
        "abstract": "Traditional techniques for synchronization are based on \\emph{locking} that provides threads with exclusive access to shared data. \\emph{Coarse-grained} locking typically forces threads to access large amounts of data sequentially and, thus, does not fully exploit hardware concurrency. Program-specific \\emph{fine-grained} locking or \\emph{non-blocking} (\\emph{i.e.}, not using locks) synchronization, on the other hand, is a dark art to most programmers and trusted to the wisdom of a few computing experts. Thus, it is appealing to seek a middle ground between these two extremes: a synchronization mechanism that relieves the programmer of the overhead of reasoning about data conflicts that may arise from concurrent operations without severely limiting the program's performance. The \\emph{Transactional Memory (TM)} abstraction is proposed as such a mechanism: it intends to combine an easy-to-use programming interface with an efficient utilization of the concurrent-computing abilities provided by multicore architectures. TM allows the programmer to \\emph{speculatively} execute sequences of shared-memory operations as \\emph{atomic transactions} with \\emph{all-or-nothing} semantics: the transaction can either \\emph{commit}, in which case it appears as executed sequentially, or \\emph{abort}, in which case its update operations do not take effect. Thus, the programmer can design software having only sequential semantics in mind and let TM take care, at run-time, of resolving the conflicts in concurrent executions. #R##N#Intuitively, we want TMs to allow for as much \\emph{concurrency} as possible: in the absence of severe data conflicts, transactions should be able to progress in parallel. But what are the inherent costs associated with providing high degrees of concurrency in TMs? This is the central question of the thesis."
      },
      {
        "node_idx": 84106,
        "score_0_10": 9,
        "title": "exploring query results",
        "abstract": "Users typically interact with a database by asking queries and examining results provided by the database system. We refer to the user examining the results of a query and asking follow-up questions as query result exploration. Our work builds on two decades of database community research on provenance. One of the contributions of this paper is that we identify constraints in the context of query result exploration that have previously been unexplored in provenance research. These constraints are useful in optimizing performance and thus improving the user experience during query result exploration. #R##N#Further, three approaches for computing provenance have been described in the literature: (a) in the lazy approach, no additional data is materialized and provenance is computed from the base tables as needed; (b) in the eager approach, additional data is materialized and provenance is computed from this additional materialized data; (c) in the hybrid approach, some additional data is materialized, and provenance is computed using this additional materialized data as well as the base tables. In this paper, we investigate lazy and eager approaches that utilize constraints in the context of query result exploration, as well as novel hybrid approaches, where the keys for some of the base tables are materialized as selected by a cost model. For the TPC-H benchmark, we find that the constraints that we identify are applicable to 19 out of the 22 queries, and result in a better performance of the lazy approach for 7 out of these 19 queries. With materialization, our hybrid approach resulted in a better performance for all but one single table TPC-H query with no joins (where existing approach performed as good as our approach). Further, the performance benefit from our approaches are significant, sometimes several orders of magnitude, compared to previous research."
      },
      {
        "node_idx": 100147,
        "score_0_10": 9,
        "title": "communication efficient learning of deep networks from decentralized data",
        "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. #R##N#We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
      },
      {
        "node_idx": 153206,
        "score_0_10": 9,
        "title": "a case for peering of content delivery networks",
        "abstract": "The proliferation of Content Delivery Networks (CDN) reveals that existing content networks are owned and operated by individual companies. As a consequence, closed delivery networks are evolved which do not cooperate with other CDNs and in practice, islands of CDNs are formed. Moreover, the logical separation between contents and services in this context results in two content networking domains. But present trends in content networks and content networking capabilities give rise to the interest in interconnecting content networks. Finding ways for distinct content networks to coordinate and cooperate with other content networks is necessary for better overall service. In addition to that, meeting the QoS requirements of users according to the negotiated Service Level Agreements between the user and the content network is a burning issue in this perspective. In this article, we present an open, scalable and Service-Oriented Architecture based system to assist the creation of open Content and Service Delivery Networks (CSDN) that scale and support sharing of resources with other CSDNs."
      },
      {
        "node_idx": 165403,
        "score_0_10": 9,
        "title": "scaliendb designing and implementing a distributed database using paxos",
        "abstract": "ScalienDB is a scalable, replicated database built on top of the Paxos algorithm. It was developed from 2010 to 2012, when the startup backing it failed. This paper discusses the design decisions of the distributed database, describes interesting parts of the C++ codebase and enumerates lessons learned putting ScalienDB into production at a handful of clients. The source code is available on Github under the AGPL license, but it is no longer developed or maintained."
      },
      {
        "node_idx": 66867,
        "score_0_10": 9,
        "title": "block encryption of quantum messages",
        "abstract": "In modern cryptography, block encryption is a fundamental cryptographic primitive. However, it is impossible for block encryption to achieve the same security as one-time pad. Quantum mechanics has changed the modern cryptography, and lots of researches have shown that quantum cryptography can outperform the limitation of traditional cryptography. #R##N#This article proposes a new constructive mode for private quantum encryption, named $\\mathcal{EHE}$, which is a very simple method to construct quantum encryption from classical primitive. Based on $\\mathcal{EHE}$ mode, we construct a quantum block encryption (QBE) scheme from pseudorandom functions. If the pseudorandom functions are standard secure, our scheme is indistinguishable encryption under chosen plaintext attack. If the pseudorandom functions are permutation on the key space, our scheme can achieve perfect security. In our scheme, the key can be reused and the randomness cannot, so a $2n$-bit key can be used in an exponential number of encryptions, where the randomness will be refreshed in each time of encryption. Thus $2n$-bit key can perfectly encrypt $O(n2^n)$ qubits, and the perfect secrecy would not be broken if the $2n$-bit key is reused for only exponential times. #R##N#Comparing with quantum one-time pad (QOTP), our scheme can be the same secure as QOTP, and the secret key can be reused (no matter whether the eavesdropping exists or not). Thus, the limitation of perfectly secure encryption (Shannon's theory) is broken in the quantum setting. Moreover, our scheme can be viewed as a positive answer to the open problem in quantum cryptography \"how to unconditionally reuse or recycle the whole key of private-key quantum encryption\". In order to physically implement the QBE scheme, we only need to implement two kinds of single-qubit gates (Pauli $X$ gate and Hadamard gate), so it is within reach of current quantum technology."
      }
    ]
  },
  "263": {
    "explanation": "human factors and affective states in software engineering productivity",
    "topk": [
      {
        "node_idx": 104733,
        "score_0_10": 10,
        "title": "effects of sensemaking translucence on distributed collaborative analysis",
        "abstract": "Collaborative sensemaking requires that analysts share their information and insights with each other, but this process of sharing runs the risks of prematurely focusing the investigation on specific suspects. To address this tension, we propose and test an interface for collaborative crime analysis that aims to make analysts more aware of their sensemaking processes. We compare our sensemaking translucence interface to a standard interface without special sensemaking features in a controlled laboratory study. We found that the sensemaking translucence interface significantly improved clue finding and crime solving performance, but that analysts rated the interface lower on subjective measures than the standard interface. We conclude that designing for distributed sensemaking requires balancing task performance vs. user experience and real-time information sharing vs. data accuracy."
      },
      {
        "node_idx": 157548,
        "score_0_10": 10,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 35756,
        "score_0_10": 9,
        "title": "building end to end dialogue systems using generative hierarchical neural network models",
        "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings."
      },
      {
        "node_idx": 127867,
        "score_0_10": 9,
        "title": "group development and group maturity when building agile teams a qualitative and quantitative investigation at eight large companies",
        "abstract": "The agile approach to projects focuses more on close-knit teams than traditional waterfall projects, which means that aspects of group maturity become even more important. This psychological aspect is not much researched in connection to the building of an \"agile team.\" The purpose of this study is to investigate how building agile teams is connected to a group development model taken from social psychology. We conducted ten semi-structured interviews with coaches, Scrum Masters, and managers responsible for the agile process from seven different companies, and collected survey data from 66 group-members from four companies (a total of eight different companies). The survey included an agile measurement tool and the one part of the Group Development Questionnaire. The results show that the practitioners define group developmental aspects as key factors to a successful agile transition. Also, the quantitative measurement of agility was significantly correlated to the group maturity measurement. We conclude that adding these psychological aspects to the description of the \"agile team\" could increase the understanding of agility and partly help define an \"agile team.\" We propose that future work should develop specific guidelines for how software development teams at different maturity levels might adopt agile principles and practices differently."
      },
      {
        "node_idx": 157756,
        "score_0_10": 9,
        "title": "designing for collaborative sensemaking leveraging human cognition for complex tasks",
        "abstract": "My research aims to design systems for complex sensemaking by remotely located non-expert collaborators (crowds), to solve computationally hard problems like crimes."
      },
      {
        "node_idx": 39298,
        "score_0_10": 9,
        "title": "isabelle jedit a prover ide within the pide framework",
        "abstract": "PIDE is a general framework for document-oriented prover interaction and integration, based on a bilingual architecture that combines ML and Scala. The overall aim is to connect LCF-style provers like Isabelle (or Coq or HOL) with sophisticated front-end technology on the JVM platform, overcoming command-line interaction at last. #R##N#The present system description specifically covers Isabelle/jEdit as part of the official release of Isabelle2011-1 (October 2011). It is a concrete Prover IDE implementation based on Isabelle/PIDE library modules (implemented in Scala) on the one hand, and the well-known text editor framework of jEdit (implemented in Java) on the other hand. #R##N#The interaction model of our Prover IDE follows the idea of continuous proof checking: the theory source text is annotated by semantic information by the prover as it becomes available incrementally. This works via an asynchronous protocol that neither blocks the editor nor stops the prover from exploiting parallelism on multi-core hardware. The jEdit GUI provides standard metaphors for augmented text editing (highlighting, squiggles, tooltips, hyperlinks etc.) that we have instrumented to render the formal content from the prover context. Further refinement of the jEdit display engine via suitable plugins and fonts approximates mathematical rendering in the text buffer, including symbols from the TeX repertoire, and sub-/superscripts. #R##N#Isabelle/jEdit is presented here both as a usable interface for current Isabelle, and as a reference application to inspire further projects based on PIDE."
      },
      {
        "node_idx": 24655,
        "score_0_10": 8,
        "title": "safety of deferred update in transactional memory",
        "abstract": "Transactional memory allows the user to declare sequences of instructions as speculative \\emph{transactions} that can either \\emph{commit} or \\emph{abort}. If a transaction commits, it appears to be executed sequentially, so that the committed transactions constitute a correct sequential execution. If a transaction aborts, none of its instructions can affect other transactions. #R##N#The popular criterion of \\emph{opacity} requires that the views of aborted transactions must also be consistent with the global sequential order constituted by committed ones. This is believed to be important, since inconsistencies observed by an aborted transaction may cause a fatal irrecoverable error or waste of the system in an infinite loop. Intuitively, an opaque implementation must ensure that no intermediate view a transaction obtains before it commits or aborts can be affected by a transaction that has not started committing yet, so called \\emph{deferred-update} semantics. #R##N#In this paper, we intend to grasp this intuition formally. We propose a variant of opacity that explicitly requires the sequential order to respect the deferred-update semantics. We show that our criterion is a safety property, i.e., it is prefix- and limit-closed. Unlike opacity, our property also ensures that a serialization of a history implies serializations of its prefixes. Finally, we show that our property is equivalent to opacity if we assume that no two transactions commit identical values on the same variable, and present a counter-example for scenarios when the \"unique-write\" assumption does not hold."
      },
      {
        "node_idx": 155003,
        "score_0_10": 8,
        "title": "happy software developers solve problems better psychological measurements in empirical software engineering",
        "abstract": "For more than thirty years, it has been claimed that a way to improve software developers\u2019 productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states\u2014emotions and moods\u2014deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint."
      },
      {
        "node_idx": 69795,
        "score_0_10": 8,
        "title": "designing as construction of representations a dynamic viewpoint in cognitive design research",
        "abstract": "This article presents a cognitively oriented viewpoint on design. It focuses on cognitive, dynamic aspects of real design, i.e., the actual cognitive activity implemented by designers during their work on professional design projects. Rather than conceiving de-signing as problem solving - Simon's symbolic information processing (SIP) approach - or as a reflective practice or some other form of situated activity - the situativity (SIT) approach - we consider that, from a cognitive viewpoint, designing is most appropriately characterised as a construction of representations. After a critical discussion of the SIP and SIT approaches to design, we present our view-point. This presentation concerns the evolving nature of representations regarding levels of abstraction and degrees of precision, the function of external representations, and specific qualities of representation in collective design. Designing is described at three levels: the organisation of the activity, its strategies, and its design-representation construction activities (different ways to generate, trans-form, and evaluate representations). Even if we adopt a \"generic design\" stance, we claim that design can take different forms depending on the nature of the artefact, and we propose some candidates for dimensions that allow a distinction to be made between these forms of design. We discuss the potential specificity of HCI design, and the lack of cognitive design research occupied with the quality of design. We close our discussion of representational structures and activities by an outline of some directions regarding their functional linkages."
      },
      {
        "node_idx": 17415,
        "score_0_10": 8,
        "title": "understanding the affect of developers theoretical background and guidelines for psychoempirical software engineering",
        "abstract": "Affects--emotions and moods--have an impact on cognitive processing activities and the working performance of individuals. It has been established that software development tasks are undertaken through cognitive processing activities. Therefore, we have proposed to employ psychology theory and measurements in software engineering (SE) research. We have called it \"psychoempirical software engineering\". However, we found out that existing SE research has often fallen into misconceptions about the affect of developers, lacking in background theory and how to successfully employ psychological measurements in studies. The contribution of this paper is threefold. (1) It highlights the challenges to conduct proper affect-related studies with psychology; (2) it provides a comprehensive literature review in affect theory; and (3) it proposes guidelines for conducting psychoempirical software engineering."
      }
    ]
  },
  "265": {
    "explanation": "applications of social robots and virtual environments in therapy and wellbeing",
    "topk": [
      {
        "node_idx": 104596,
        "score_0_10": 10,
        "title": "why robots a survey on the roles and benefits of social robots in the therapy of children with autism",
        "abstract": "This paper reviews the use of socially interactive robots to assist in the therapy of children with autism. The extent to which the robots were successful in helping the children in their social, emotional and communication deficits was investigated. Child\u2013robot interactions were scrutinized with respect to the different target behaviors that are to be elicited from a child during therapy. These behaviors were thoroughly examined with respect to a child\u2019s development needs. Most importantly, experimental data from the surveyed works were extracted and analysed in terms of the target behaviors and of how each robot was used during a therapy session to achieve these behaviors. The study concludes by categorizing the different therapeutic roles that these robots were observed to play, and highlights the important design features that enable them to achieve high levels of effectiveness in autism therapy."
      },
      {
        "node_idx": 146031,
        "score_0_10": 10,
        "title": "diversity of preferences can increase collective welfare in sequential exploration problems",
        "abstract": "In search engines, online marketplaces and other human-computer interfaces large collectives of individuals sequentially interact with numerous alternatives of varying quality. In these contexts, trial and error (exploration) is crucial for uncovering novel high-quality items or solutions, but entails a high cost for individual users. Self-interested decision makers, are often better off imitating the choices of individuals who have already incurred the costs of exploration. Although imitation makes sense at the individual level, it deprives the group of additional information that could have been gleaned by individual explorers. In this paper we show that in such problems, preference diversity can function as a welfare enhancing mechanism. It leads to a consistent increase in the quality of the consumed alternatives that outweighs the increased cost of search for the users."
      },
      {
        "node_idx": 40079,
        "score_0_10": 10,
        "title": "vif virtual interactive fiction with a twist",
        "abstract": "Nowadays computer science can create digital worlds that deeply immerse users; it can also process in real time brain activity to infer their inner states. What marvels can we achieve with such technologies? Go back to displaying text. And unfold a story that follows and molds users as never before."
      },
      {
        "node_idx": 117495,
        "score_0_10": 10,
        "title": "ways of listening and modes of being electroacoustic auditory display",
        "abstract": "Auditory display is concerned with the use of non-speech sound to communicate information. If the term seems at first oxymoronic, then consider auditory display as an activity of perceptualization, that is, the process of making perceptible to humans aspects or features of a given data set or system. Most commonly this is done using visual representations (which process we call visualization) but it is not limited to the visual channel and recent years have witnessed the increased use of auditory representations in the production of tools for exploring data. By way of semiotics and an aesthetic perspective shift this article posits that auditory display may be considered a form of organized sound and explores the listening experience in this context."
      },
      {
        "node_idx": 25139,
        "score_0_10": 10,
        "title": "sonification aesthetics and listening for network situational awareness",
        "abstract": "This paper looks at the problem of using sonification to enable network administrators to maintaining situational awareness about their network environment. Network environments generate a lot of data and the need for continuous monitoring means that sonification systems must be designed in such a way as to maximise acceptance while minimising annoyance and listener fatigue. It will be argued that solutions based on the concept of the soundscape offer an ecological advantage over other sonification designs."
      },
      {
        "node_idx": 90384,
        "score_0_10": 10,
        "title": "online group exercises for older adults of different physical abilities",
        "abstract": "In this paper we describe the design and validation of a virtual fitness environment aiming at keeping older adults physically and socially active. We target particularly older adults who are socially more isolated, physically less active, and with less chances of training in a gym. The virtual fitness environment, namely Gymcentral, was designed to enable and motivate older adults to follow personalised exercises from home, with a (heterogeneous) group of remote friends and under the remote supervision of a Coach. We take the training activity as an opportunity to create social interactions, by complementing training features with social instruments. Finally, we report on the feasibility and effectiveness of the virtual environment, as well as its effects on the usage and social interactions, from an intervention study in Trento, Italy."
      },
      {
        "node_idx": 60587,
        "score_0_10": 9,
        "title": "effects of online group exercises for older adults on physical psychological and social wellbeing a pilot trial",
        "abstract": "Background. There are many factors that can make of group exercises a challenging setting for older adults. A major one in the elderly population is the difference in the level of skills. In this paper we report on the physical, psychological and social wellbeing outcomes of a novel virtual gym that enables online group-exercises in older adults with different levels of skills. #R##N#Methods. A total of 37 older adults (65-87 years old) followed a personalized exercise program based on the OTAGO program for fall prevention, for a period of eight weeks. Participants could join online group exercises using a tablet-based application. Participants were assigned either to a Control group (individual training) or Social group (online group-exercising). Pre- and post- measurements were taken to analyze the physical, psychological and social wellbeing outcomes. The study received ethical approval from the CREATE-NET Ethics Committee on ICT Research Involving Human Beings (Application N. 2014-001). #R##N#Results. There were improvements in both the Social and Control groups in terms of physical outcomes. Interestingly though, while in the Control group fitter individuals tended to adhere more to the training, this was not the case for the Social group, where the initial level had no effect on adherence. For psychological and social wellbeing outcomes there were improvements on both groups, regardless of the application used. #R##N#Conclusion. Group exercising in a virtual gym can be effective in motivating and enabling individuals who are less fit to train as much as fitter individuals. This not only indicates the feasibility of training together despite differences in physical skills but also suggests that online exercise can reduce the effect of skills on adherence in a social context. Longer term interventions with more participants are instead recommended to assess impacts on wellbeing."
      },
      {
        "node_idx": 147305,
        "score_0_10": 9,
        "title": "relaxed binaural lcmv beamforming",
        "abstract": "In this paper, we propose a new binaural beamforming technique, which can be seen as a relaxation of the linearly constrained minimum variance LCMV framework. The proposed method can achieve simultaneous noise reduction and exact binaural cue preservation of the target source, similar to the binaural minimum variance distortionless response BMVDR method. However, unlike BMVDR, the proposed method is also able to preserve the binaural cues of multiple interferers to a certain predefined accuracy. Specifically, it is able to control the trade-off between noise reduction and binaural cue preservation of the interferers by using a separate trade-off parameter per-interferer. Moreover, we provide a robust way of selecting these trade-off parameters in such a way that the preservation accuracy for the binaural cues of the interferers is always better than the corresponding ones of the BMVDR. The relaxation of the constraints in the proposed method achieves approximate binaural cue preservation of more interferers than other previously presented LCMV-based binaural beamforming methods that use strict equality constraints."
      },
      {
        "node_idx": 84982,
        "score_0_10": 9,
        "title": "is spoken language all or nothing implications for future speech based human machine interaction",
        "abstract": "Recent years have seen significant market penetration for voice-based personal assistants such as Apple's Siri. However, despite this success, user take-up is frustratingly low. This position paper argues that there is a habitability gap caused by the inevitable mismatch between the capabilities and expectations of human users and the features and benefits provided by contemporary technology. Suggestions are made as to how such problems might be mitigated, but a more worrisome question emerges: \"is spoken language all-or-nothing\"? The answer, based on contemporary views on the special nature of (spoken) language, is that there may indeed be a fundamental limit to the interaction that can take place between mismatched interlocutors (such as humans and machines). However, it is concluded that interactions between native and non-native speakers, or between adults and children, or even between humans and dogs, might provide critical inspiration for the design of future speech-based human-machine interaction."
      },
      {
        "node_idx": 104099,
        "score_0_10": 9,
        "title": "review of the use of electroencephalography as an evaluation method for human computer interaction",
        "abstract": "Evaluating human-computer interaction is essential as a broadening population uses machines, sometimes in sensitive contexts. However, traditional evaluation methods may fail to combine real-time measures, an \"objective\" approach and data contextualization. In this review we look at how adding neuroimaging techniques can respond to such needs. We focus on electroencephalography (EEG), as it could be handled effectively during a dedicated evaluation phase. We identify workload, attention, vigilance, fatigue, error recognition, emotions, engagement, flow and immersion as being recognizable by EEG. We find that workload, attention and emotions assessments would benefit the most from EEG. Moreover, we advocate to study further error recognition through neuroimaging to enhance usability and increase user experience."
      }
    ]
  },
  "266": {
    "explanation": "privacy-preserving biometric encryption and quantum-resistant cryptographic schemes",
    "topk": [
      {
        "node_idx": 12655,
        "score_0_10": 10,
        "title": "negative databases for biometric data",
        "abstract": "Negative databases - negative representations of a set of data - have been introduced in 2004 to protect the data they contain. Today, no solution is known to constitute biometric negative databases. This is surprising as biometric applications are very demanding of such protection for privacy reasons. The main difficulty comes from the fact that biometric captures of the same trait give different results and comparisons of the stored reference with the fresh captured biometric data has to take into account this variability. In this paper, we give a first answer to this problem by exhibiting a way to create and exploit biometric negative databases."
      },
      {
        "node_idx": 58412,
        "score_0_10": 10,
        "title": "semantic security and indistinguishability in the quantum world",
        "abstract": "At CRYPTO 2013, Boneh and Zhandry initiated the study of quantum-secure encryption. They proposed first indistinguishability definitions for the quantum world where the actual indistinguishability only holds for classical messages, and they provide arguments why it might be hard to achieve a stronger notion. In this work, we show that stronger notions are achievable, where the indistinguishability holds for quantum superpositions of messages. We investigate exhaustively the possibilities and subtle differences in defining such a quantum indistinguishability notion for symmetric-key encryption schemes. We justify our stronger definition by showing its equivalence to novel quantum semantic-security notions that we introduce. Furthermore, we show that our new security definitions cannot be achieved by a large class of ciphers --- those which are quasi-preserving the message length. On the other hand, we provide a secure construction based on quantum-resistant pseudorandom permutations; this construction can be used as a generic transformation for turning a large class of encryption schemes into quantum indistinguishable and hence quantum semantically secure ones. Moreover, our construction is the first completely classical encryption scheme shown to be secure against an even stronger notion of indistinguishability, which was previously known to be achievable only by using quantum messages and arbitrary quantum encryption circuits."
      },
      {
        "node_idx": 3591,
        "score_0_10": 10,
        "title": "group secret key generation algorithms",
        "abstract": "We consider a pair-wise independent network where every pair of terminals in the network observes a common pair-wise source that is independent of all the sources accessible to the other pairs. We propose a method for secret key agreement in such a network that is based on well-established point-to-point techniques and repeated application of the one-time pad. Three specific problems are investigated. 1) Each terminal's observations are correlated only with the observations of a central terminal. All these terminals wish to generate a common secret key. 2) In a pair-wise independent network, two designated terminals wish to generate a secret key with the help of other terminals. 3) All the terminals in a pair-wise independent network wish to generate a common secret key. A separate protocol for each of these problems is proposed. Furthermore, we show that the protocols for the first two problems are optimal and the protocol for the third problem is efficient, in terms of the resulting secret key rates."
      },
      {
        "node_idx": 73855,
        "score_0_10": 10,
        "title": "identification with encrypted biometric data",
        "abstract": "Biometrics make human identification possible with a sample of a biometric trait and an associated database. Classical identification techniques lead to privacy concerns. This paper introduces a new method to identify someone using his biometrics in an encrypted way. Our construction combines Bloom Filters with Storage and Locality-Sensitive Hashing. We apply this error-tolerant scheme, in a Hamming space, to achieve biometric identification in an efficient way. This is the first non-trivial identification scheme dealing with fuzziness and encrypted data."
      },
      {
        "node_idx": 31334,
        "score_0_10": 9,
        "title": "feedback capacity of stationary gaussian channels",
        "abstract": "The feedback capacity of additive stationary Gaussian noise channels is characterized as the solution to a variational problem. Toward this end, it is proved that the optimal feedback coding scheme is stationary. When specialized to the first-order autoregressive moving average noise spectrum, this variational characterization yields a closed-form expression for the feedback capacity. In particular, this result shows that the celebrated Schalkwijk-Kailath coding scheme achieves the feedback capacity for the first-order autoregressive moving average Gaussian channel, positively answering a long-standing open problem studied by Butman, Schalkwijk-Tiernan, Wolfowitz, Ozarow, Ordentlich, Yang-Kavcic-Tatikonda, and others. More generally, it is shown that a k-dimensional generalization of the Schalkwijk-Kailath coding scheme achieves the feedback capacity for any autoregressive moving average noise spectrum of order k. Simply put, the optimal transmitter iteratively refines the receiver's knowledge of the intended message."
      },
      {
        "node_idx": 150975,
        "score_0_10": 9,
        "title": "a secret sharing scheme using groups",
        "abstract": "In this paper a secret sharing scheme based on the word problem in groups is introduced. The security of the scheme and possible variations are discussed in section 2. The article concludes with the suggestion of two categories of platform groups for the implementation of the scheme."
      },
      {
        "node_idx": 27319,
        "score_0_10": 9,
        "title": "distinguisher based attacks on public key cryptosystems using reed solomon codes",
        "abstract": "Because of their interesting algebraic properties, several authors promote the use of generalized Reed-Solomon codes in cryptography. Niederreiter was the first to suggest an instantiation of his cryptosystem with them but Sidelnikov and Shestakov showed that this choice is insecure. Wieschebrink proposed a variant of the McEliece cryptosystem which consists in concatenating a few random columns to a generator matrix of a secretly chosen generalized Reed-Solomon code. More recently, new schemes appeared which are the homomorphic encryption scheme proposed by Bogdanov and Lee, and a variation of the McEliece cryptosystem proposed by Baldi et \\textit{al.} which hides the generalized Reed-Solomon code by means of matrices of very low rank. #R##N#In this work, we show how to mount key-recovery attacks against these public-key encryption schemes. We use the concept of distinguisher which aims at detecting a behavior different from the one that one would expect from a random code. All the distinguishers we have built are based on the notion of component-wise product of codes. It results in a powerful tool that is able to recover the secret structure of codes when they are derived from generalized Reed-Solomon codes. Lastly, we give an alternative to Sidelnikov and Shestakov attack by building a filtration which enables to completely recover the support and the non-zero scalars defining the secret generalized Reed-Solomon code."
      },
      {
        "node_idx": 125377,
        "score_0_10": 9,
        "title": "cryptanalysis of the cfvz cryptosystem",
        "abstract": "The paper analyzes a new public key cryptosystem whose security is based on a matrix version of the discrete logarithm problem over an elliptic curve. It is shown that the complexity of solving the underlying problem for the proposed system is dominated by the complexity of solving a fixed number of discrete logarithm problems in the group of an elliptic curve. Using an adapted Pollard rho algorithm it is shown that this problem is essentially as hard as solving one discrete logarithm problem in the group of an elliptic curve."
      },
      {
        "node_idx": 68571,
        "score_0_10": 9,
        "title": "enhanced public key security for the mceliece cryptosystem",
        "abstract": "This paper studies a variant of the McEliece cryptosystem able to ensure that the code used as the public key is no longer permutation-equivalent to the secret code. This increases the security level of the public key, thus opening the way for reconsidering the adoption of classical families of codes, like Reed-Solomon codes, that have been longly excluded from the McEliece cryptosystem for security reasons. It is well known that codes of these classes are able to yield a reduction in the key size or, equivalently, an increased level of security against information set decoding; so, these are the main advantages of the proposed solution. We also describe possible vulnerabilities and attacks related to the considered system, and show what design choices are best suited to avoid them."
      },
      {
        "node_idx": 73024,
        "score_0_10": 9,
        "title": "variations on the sensitivity conjecture",
        "abstract": "We present a selection of known as well as new variants of the Sensitivity Conjecture and point out some weaker versions that are also open."
      }
    ]
  },
  "268": {
    "explanation": "robotic grasp detection and 6D object pose estimation",
    "topk": [
      {
        "node_idx": 67267,
        "score_0_10": 10,
        "title": "unsupervised learning of depth and ego motion from video",
        "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings."
      },
      {
        "node_idx": 123412,
        "score_0_10": 10,
        "title": "posecnn a convolutional neural network for 6d object pose estimation in cluttered scenes",
        "abstract": "Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at this https URL."
      },
      {
        "node_idx": 23457,
        "score_0_10": 9,
        "title": "data driven grasp synthesis a survey",
        "abstract": "We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps. We divide the approaches into three groups based on whether they synthesize grasps for known, familiar or unknown objects. This structure allows us to identify common object representations and perceptual processes that facilitate the employed data-driven grasp synthesis technique. In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation. In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects. Finally for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps. Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping. We also draw a parallel to the classical approaches that rely on analytic formulations."
      },
      {
        "node_idx": 24683,
        "score_0_10": 9,
        "title": "unsupervised monocular depth estimation with left right consistency",
        "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. #R##N#We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth."
      },
      {
        "node_idx": 28717,
        "score_0_10": 9,
        "title": "real time grasp detection using convolutional neural networks",
        "abstract": "We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways."
      },
      {
        "node_idx": 94920,
        "score_0_10": 9,
        "title": "posenet a convolutional network for real time 6 dof camera relocalization",
        "abstract": "We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at this http URL"
      },
      {
        "node_idx": 35167,
        "score_0_10": 9,
        "title": "optimization model for planning precision grasps with multi fingered hands",
        "abstract": "Precision grasps with multi-fingered hands are important for precise placement and in-hand manipulation tasks. Searching precision grasps on the object represented by point cloud, is challenging due to the complex object shape, high-dimensionality, collision and undesired properties of the sensing and positioning. This paper proposes an optimization model to search for precision grasps with multi-fingered hands. The model takes noisy point cloud of the object as input and optimizes the grasp quality by iteratively searching for the palm pose and finger joints positions. The collision between the hand and the object is approximated and penalized by a series of least-squares. The collision approximation is able to handle the point cloud representation of the objects with complex shapes. The proposed optimization model is able to locate collision-free optimal precision grasps efficiently. The average computation time is 0.50 sec/grasp. The searching is robust to the incompleteness and noise of the point cloud. The effectiveness of the algorithm is demonstrated by experiments."
      },
      {
        "node_idx": 45381,
        "score_0_10": 9,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 12715,
        "score_0_10": 9,
        "title": "invisible pushdown languages",
        "abstract": "Context free languages allow one to express data with hierarchical structure, at the cost of losing some of the useful properties of languages recognized by finite automata on words. However, it is possible to restore some of these properties by making the structure of the tree visible, such as is done by visibly pushdown languages, or finite automata on trees. In this paper, we show that the structure given by such approaches remains invisible when it is read by a finite automaton (on word). In particular, we show that separability with a regular language is undecidable for visibly pushdown languages, just as it is undecidable for general context free languages."
      },
      {
        "node_idx": 154569,
        "score_0_10": 9,
        "title": "it is undecidable if two regular tree languages can be separated by a deterministic tree walking automaton",
        "abstract": "The following problem is shown undecidable: given regular languages L,K of finite trees, decide if there exists a deterministic tree-walking automaton which accepts all trees in L and rejects all trees in K. The proof uses a technique of Kopczy\\'nski from LICS 2016."
      }
    ]
  },
  "270": {
    "explanation": "efficient distributed checkpoint-restart for high-performance computing systems",
    "topk": [
      {
        "node_idx": 57612,
        "score_0_10": 10,
        "title": "arxiv and the ref open access policy",
        "abstract": "HEFCE's Policy for open access in the post-2014 Research Excellence Framework states \"authors' outputs must have been deposited in an institutional or subject repository\". There is no definition of a subject repository in the policy: however, there is a footnote stating: \"Individuals depositing their outputs in a subject repository are advised to ensure that their chosen repository meets the requirements set out in this policy.\" The longest standing subject repository (or repository of any kind) is arXiv.org, established in 1991. arXiv is an open access repository of scientific research available to authors and researchers worldwide and acts as a scholarly communications forum informed and guided by scientists. Content held on arXiv is free to the end user and researchers can deposit their content freely. As of April 2018, arXiv held over 1,377,000 eprints. In some disciplines arXiv is considered essential to the sharing and publication of research. The HEFCE requirements on repositories are defined in the Information and Audit Requirements which lists the \"Accepted date\", the \"Version of deposited file\" and \"available open access immediately after the publisher embargo\" are expected as part of the REF submission. However, while many records in arXiv have multiple versions of work, the Author's Accepted Manuscript is not identified and there is no field to record the acceptance date of the work. Because arXiv does not capture these two specific information points it does not meet the technical requirements to be a compliant subject repository for the purposes of REF. This paper is presenting the case that articles deposited to arXiv are, in general, compliant with the requirements of the HEFCE policy. The paper summarises some work undertaken by Jisc to establish if there are other factors that can indicate the likelihood of formal compliance to the HEFCE policy."
      },
      {
        "node_idx": 159822,
        "score_0_10": 10,
        "title": "oltp on hardware islands",
        "abstract": "Modern hardware is abundantly parallel and increasingly heterogeneous. The numerous processing cores have non-uniform access latencies to the main memory and to the processor caches, which causes variability in the communication costs. Unfortunately, database systems mostly assume that all processing cores are the same and that microarchitecture differences are not significant enough to appear in critical database execution paths. As we demonstrate in this paper, however, hardware heterogeneity does appear in the critical path and conventional database architectures achieve suboptimal and even worse, unpredictable performance. We perform a detailed performance analysis of OLTP deployments in servers with multiple cores per CPU (multicore) and multiple CPUs per server (multisocket). We compare different database deployment strategies where we vary the number and size of independent database instances running on a single server, from a single shared-everything instance to fine-grained shared-nothing configurations. We quantify the impact of non-uniform hardware on various deployments by (a) examining how efficiently each deployment uses the available hardware resources and (b) measuring the impact of distributed transactions and skewed requests on different workloads. Finally, we argue in favor of shared-nothing deployments that are topology- and workload-aware and take advantage of fast on-chip communication between islands of cores on the same socket."
      },
      {
        "node_idx": 104130,
        "score_0_10": 10,
        "title": "transparent checkpoint restart over infiniband",
        "abstract": "InfiniBand is widely used for low-latency, high-throughput cluster computing. Saving the state of the InfiniBand network as part of distributed checkpointing has been a long-standing challenge for researchers. Because of a lack of a solution, typical MPI implementations have included custom checkpoint-restart services that \"tear down\" the network, checkpoint each node as if the node were a standalone computer, and then re-connect the network again. We present the first example of transparent, system-initiated checkpoint-restart that directly supports InfiniBand. The new approach is independent of any particular Linux kernel, thus simplifying the current practice of using a kernel-based module, such as BLCR. This direct approach results in checkpoints that are found to be faster than with the use of a checkpoint-restart service. The generality of this approach is shown not only by checkpointing an MPI computation, but also a native UPC computation (Berkeley Unified Parallel C), which does not use MPI. Scalability is shown by checkpointing 2,048 MPI processes across 128 nodes (with 16 cores per node). In addition, a cost-effective debugging approach is also enabled, in which a checkpoint image from an InfiniBand-based production cluster is copied to a local Ethernet-based cluster, where it can be restarted and an interactive debugger can be attached to it. This work is based on a plugin that extends the DMTCP (Distributed MultiThreaded CheckPointing) checkpoint-restart package."
      },
      {
        "node_idx": 120596,
        "score_0_10": 10,
        "title": "using cache coloring to mitigate inter set write variation in non volatile caches",
        "abstract": "In recent years, researchers have explored use of non-volatile devices such as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they provide high density and consume low leakage power. A common limitation of all non-volatile devices is their limited write endurance. Further, since existing cache management policies are write-variation unaware, excessive writes to a few blocks may lead to a quick failure of the whole cache. We propose an architectural technique for wear-leveling of non-volatile last level caches (LLCs). Our technique uses cache-coloring approach which adds a software-controlled mapping layer between groups of physical pages and cache sets. Periodically the mapping is altered to ensure that write-traffic can be spread uniformly to different sets of the cache to achieve wear-leveling. Simulations performed with an x86-64 simulator and SPEC2006 benchmarks show that our technique reduces the worst-case writes to cache blocks and thus improves the cache lifetime by 4.07X."
      },
      {
        "node_idx": 75900,
        "score_0_10": 10,
        "title": "system level scalable checkpoint restart for petascale computing",
        "abstract": "Fault tolerance for the upcoming exascale generation has long been an area of active research. One of the components of a fault tolerance strategy is checkpointing. Petascale-level checkpointing is demonstrated through a new mechanism for virtualization of the InfiniBand UD (unreliable datagram) mode, and for updating the remote address on each UD-based send, due to lack of a fixed peer. Note that InfiniBand UD is required to support modern MPI implementations. An extrapolation from the current results to future SSD-based storage systems provides evidence that the current approach will remain practical in the exascale generation. This transparent checkpointing approach is evaluated using a framework of the DMTCP checkpointing package. Results are shown for HPCG (linear algebra), NAMD (molecular dynamics), and the NAS NPB benchmarks. In tests up to 32,752 MPI processes on 32,752 CPU cores, checkpointing of a computation with a 38 TB memory footprint in 11 minutes is demonstrated. Runtime overhead is reduced to less than 1%. The approach is also evaluated across three widely used MPI implementations."
      },
      {
        "node_idx": 76663,
        "score_0_10": 10,
        "title": "reproducible execution of posix programs with dios",
        "abstract": "In this paper, we describe DiOS, a lightweight model operating system which can be used to execute programs that make use of POSIX APIs. Such executions are fully reproducible: running the same program with the same inputs twice will result in two exactly identical instruction traces, even if the program uses threads for parallelism. #R##N#DiOS is implemented almost entirely in portable C and C++: although its primary platform is DiVM, a verification-oriented virtual machine, it can be configured to also run in KLEE, a symbolic executor. Finally, it can be compiled into machine code to serve as a user-mode kernel. #R##N#Additionally, DiOS is modular and extensible. Its various components can be combined to match both the capabilities of the underlying platform and to provide services required by a particular program. New components can be added to cover additional system calls or APIs. #R##N#The experimental evaluation has two parts. DiOS is first evaluated as a component of a program verification platform based on DiVM. In the second part, we consider its portability and modularity by combining it with the symbolic executor KLEE."
      },
      {
        "node_idx": 28213,
        "score_0_10": 10,
        "title": "from model checking to runtime verification and back",
        "abstract": "We describe a novel approach for adapting an existing software model checker to perform precise runtime verification. The software under test is allowed to communicate with the wider environment (including the file system and network). The modifications to the model checker are small and self-contained, making this a viable strategy for re-using existing model checking tools in a new context."
      },
      {
        "node_idx": 155786,
        "score_0_10": 10,
        "title": "a platform for teaching applied distributed software development the ongoing journey of the helsinki software factory",
        "abstract": "Teaching distributed software development (DSD) in project courses where student teams are geographically distributed promises several benefits. One main benefit is that in contrast to traditional classroom courses, students can experience the effects of distribution and the mechanisms for coping with distribution by themselves, therefore understanding their relevance for software development. They can thus learn to take more care of distribution challenges and risks when starting to develop software in industry. However, providing a sustainable environment for such project courses is difficult. A development environment is needed that can connect to different distributed teams and an ongoing routine to conduct such courses needs to be established. This article sketches a picture of the Software Factory, a platform that supports teaching distributed student projects and that has now been operational for more than three years. We describe the basic steps of conducting Software Factory projects, and portray experiences from past factory projects. In addition, we provide a short overview of related approaches and future activities."
      },
      {
        "node_idx": 125229,
        "score_0_10": 10,
        "title": "divm model checking with llvm and graph memory",
        "abstract": "In this paper, we introduce the concept of a virtual machine with graph-organised memory as a versatile backend for both explicit-state and abstraction-driven verification of software. Our virtual machine uses the LLVM IR as its instruction set, enriched with a small set of hypercalls. We show that the provided hypercalls are sufficient to implement a small operating system, which can then be linked with applications to provide a POSIX-compatible verification environment. Finally, we demonstrate the viability of the approach through a comparison with a more traditionally-designed LLVM model checker."
      },
      {
        "node_idx": 69111,
        "score_0_10": 9,
        "title": "jetracer a framework for java gui event tracing",
        "abstract": "The present paper introduces the open-source Java Event Tracer (JETracer) framework for real-time tracing of GUI events within applications based on the AWT, Swing or SWT graphical toolkits. Our framework provides a common event model for supported toolkits, the possibility of receiving GUI events in real-time, good performance in the case of complex target applications and the possibility of deployment over a network. The present paper provides the rationale for JETracer, presents related research and details its technical implementation. An empirical evaluation where JETracer is used to trace GUI events within five popular, open-source applications is also presented."
      }
    ]
  },
  "272": {
    "explanation": "multi-scale spatial pooling and 3D object detection techniques",
    "topk": [
      {
        "node_idx": 66578,
        "score_0_10": 10,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 161063,
        "score_0_10": 10,
        "title": "multi view 3d object detection network for autonomous driving",
        "abstract": "This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods."
      },
      {
        "node_idx": 94920,
        "score_0_10": 10,
        "title": "posenet a convolutional network for real time 6 dof camera relocalization",
        "abstract": "We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at this http URL"
      },
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 87545,
        "score_0_10": 9,
        "title": "high speed tracking with kernelized correlation filters",
        "abstract": "The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies\u2014any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source."
      },
      {
        "node_idx": 58780,
        "score_0_10": 9,
        "title": "collaborative visual area coverage",
        "abstract": "Abstract   This article examines the problem of visual area coverage by a network of Mobile Aerial Agents (MAAs). Each MAA is assumed to be equipped with a downwards facing camera with a conical field of view which covers all points within a circle on the ground. The diameter of that circle is proportional to the altitude of the MAA, whereas the quality of the covered area decreases with the altitude. A distributed control law that maximizes a joint coverage-quality criterion by adjusting the MAAs\u2019 spatial coordinates is developed. The effectiveness of the proposed control scheme is evaluated through simulation studies."
      },
      {
        "node_idx": 59347,
        "score_0_10": 9,
        "title": "netvlad cnn architecture for weakly supervised place recognition",
        "abstract": "We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the \"Vector of Locally Aggregated Descriptors\" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 162377,
        "score_0_10": 9,
        "title": "r fcn object detection via region based fully convolutional networks",
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: this https URL"
      },
      {
        "node_idx": 54934,
        "score_0_10": 9,
        "title": "frustum pointnets for 3d object detection from rgb d data",
        "abstract": "In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability."
      }
    ]
  },
  "274": {
    "explanation": "visualization and localization of CNN-based deep feature representations",
    "topk": [
      {
        "node_idx": 49351,
        "score_0_10": 10,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 151734,
        "score_0_10": 10,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 77301,
        "score_0_10": 9,
        "title": "grad cam visual explanations from deep networks via gradient based localization",
        "abstract": "We propose a technique for producing \u2018visual explanations\u2019 for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach\u2014Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say \u2018dog\u2019 in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265\u2013290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      }
    ]
  },
  "275": {
    "explanation": "advanced network information theory and low-resolution MIMO communication techniques",
    "topk": [
      {
        "node_idx": 145848,
        "score_0_10": 10,
        "title": "lecture notes on network information theory",
        "abstract": "These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at this http URL The previous (and obsolete) version of the lecture notes can be found at this http URL"
      },
      {
        "node_idx": 105528,
        "score_0_10": 10,
        "title": "optimal inverter var control in distribution systems with high pv penetration",
        "abstract": "The intent of the study detailed in this paper is to demonstrate the benefits of inverter var control on a fast timescale to mitigate rapid and large voltage fluctuations due to the high penetration of photovoltaic generation and the resulting reverse power flow. Our approach is to formulate the volt/var control as a radial optimal power flow (OPF) problem to minimize line losses and energy consumption, subject to constraints on voltage magnitudes. An efficient solution to the radial OPF problem is presented and used to study the structure of optimal inverter var injection and the net benefits, taking into account the additional cost of inverter losses when operating at non-unity power factor. This paper will illustrate how, depending on the circuit topology and its loading condition, the inverter's optimal reactive power injection is not necessarily monotone with respect to their real power output. The results are demonstrated on a distribution feeder on the Southern California Edison system that has a very light load and a 5 MW photovoltaic (PV) system installed away from the substation."
      },
      {
        "node_idx": 163667,
        "score_0_10": 10,
        "title": "rate region of the quadratic gaussian two encoder source coding problem",
        "abstract": "We determine the rate region of the quadratic Gaussian two-encoder source-coding problem. This rate region is achieved by a simple architecture that separates the analog and digital aspects of the compression. Furthermore, this architecture requires higher rates to send a Gaussian source than it does to send any other source with the same covariance. Our techniques can also be used to determine the sum rate of some generalizations of this classical problem. Our approach involves coupling the problem to a quadratic Gaussian ``CEO problem.''"
      },
      {
        "node_idx": 61110,
        "score_0_10": 10,
        "title": "capacity analysis of one bit quantized mimo systems with transmitter channel state information",
        "abstract": "With bandwidths on the order of a gigahertz in emerging wireless systems, high-resolution analog-to-digital convertors (ADCs) become a power consumption bottleneck. One solution is to employ low resolution one-bit ADCs. In this paper, we analyze the flat fading multiple-input multiple-output (MIMO) channel with one-bit ADCs. Channel state information is assumed to be known at both the transmitter and receiver. For the multiple-input single-output channel, we derive the exact channel capacity. For the single-input multiple-output and MIMO channel, the capacity at infinite signal-to-noise ratio (SNR) is found. We also derive upper bound at finite SNR, which is tight when the channel has full row rank. In addition, we propose an efficient method to design the input symbols to approach the capacity achieving solution. We incorporate millimeter wave channel characteristics and find the bounds on the infinite SNR capacity. The results show how the number of paths and number of receive antennas impact the capacity."
      },
      {
        "node_idx": 153677,
        "score_0_10": 10,
        "title": "from theory to practice sub nyquist sampling of sparse wideband analog signals",
        "abstract": "Conventional sub-Nyquist sampling methods for analog signals exploit prior information about the spectral support. In this paper, we consider the challenging problem of blind sub-Nyquist sampling of multiband signals, whose unknown frequency support occupies only a small portion of a wide spectrum. Our primary design goals are efficient hardware implementation and low computational load on the supporting digital processing. We propose a system, named the modulated wideband converter, which first multiplies the analog signal by a bank of periodic waveforms. The product is then low-pass filtered and sampled uniformly at a low rate, which is orders of magnitude smaller than Nyquist. Perfect recovery from the proposed samples is achieved under certain necessary and sufficient conditions. We also develop a digital architecture, which allows either reconstruction of the analog input, or processing of any band of interest at a low rate, that is, without interpolating to the high Nyquist rate. Numerical simulations demonstrate many engineering aspects: robustness to noise and mismodeling, potential hardware simplifications, real-time performance for signals with time-varying support and stability to quantization effects. We compare our system with two previous approaches: periodic nonuniform sampling, which is bandwidth limited by existing hardware devices, and the random demodulator, which is restricted to discrete multitone signals and has a high computational load. In the broader context of Nyquist sampling, our scheme has the potential to break through the bandwidth barrier of state-of-the-art analog conversion technologies such as interleaved converters."
      },
      {
        "node_idx": 132080,
        "score_0_10": 9,
        "title": "near maximum likelihood detector and channel estimator for uplink multiuser massive mimo systems with one bit adcs",
        "abstract": "In massive multiple-input multiple-output (MIMO) systems, it may not be power efficient to have a high-resolution analog-to-digital converter (ADC) for each antenna element. In this paper, a near maximum likelihood (nML) detector for uplink multiuser massive MIMO systems is proposed where each antenna is connected to a pair of one-bit ADCs, i.e., one for each real and imaginary component of the baseband signal. The exhaustive search over all the possible transmitted vectors required in the original maximum likelihood (ML) detection problem is relaxed to formulate an ML estimation problem. Then, the ML estimation problem is converted into a convex optimization problem which can be efficiently solved. Using the solution, the base station can perform simple symbol-by-symbol detection for the transmitted signals from multiple users. To further improve detection performance, we also develop a two-stage nML detector that exploits the structures of both the original ML and the proposed (one-stage) nML detectors. Numerical results show that the proposed nML detectors are efficient enough to simultaneously support multiple uplink users adopting higher-order constellations, e.g., 16 quadrature amplitude modulation. Since our detectors exploit the channel state information as part of the detection, an ML channel estimation technique with one-bit ADCs that shares the same structure with our proposed nML detector is also developed. The proposed detectors and channel estimator provide a complete low power solution for the uplink of a massive MIMO system."
      },
      {
        "node_idx": 35040,
        "score_0_10": 9,
        "title": "transient frequency control with regional cooperation for power networks",
        "abstract": "This paper proposes a centralized and a distributed sub-optimal control strategy to maintain in safe regions the real-time transient frequencies of a given collection of buses, and simultaneously preserve asymptotic stability of the entire network. In a receding horizon fashion, the centralized control input is obtained by iteratively solving an open-loop optimization aiming to minimize the aggregate control effort over controllers regulated on individual buses with transient frequency and stability constraints. Due to the non-convexity of the optimization, we propose a convexification technique by identifying a reference control input trajectory. We then extend the centralized control to a distributed scheme, where each subcontroller can only access the state information within a local region. Simulations on a IEEE-39 network illustrate our results."
      },
      {
        "node_idx": 100650,
        "score_0_10": 9,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 55398,
        "score_0_10": 9,
        "title": "the capacity of channels with feedback",
        "abstract": "We introduce a general framework for treating channels with memory and feedback. First, we generalize Massey's concept of directed information and use it to characterize the feedback capacity of general channels. Second, we present coding results for Markov channels. This requires determining appropriate sufficient statistics at the encoder and decoder. Third, a dynamic programming framework for computing the capacity of Markov channels is presented. Fourth, it is shown that the average cost optimality equation (ACOE) can be viewed as an implicit single-letter characterization of the capacity. Fifth, scenarios with simple sufficient statistics are described."
      },
      {
        "node_idx": 31334,
        "score_0_10": 9,
        "title": "feedback capacity of stationary gaussian channels",
        "abstract": "The feedback capacity of additive stationary Gaussian noise channels is characterized as the solution to a variational problem. Toward this end, it is proved that the optimal feedback coding scheme is stationary. When specialized to the first-order autoregressive moving average noise spectrum, this variational characterization yields a closed-form expression for the feedback capacity. In particular, this result shows that the celebrated Schalkwijk-Kailath coding scheme achieves the feedback capacity for the first-order autoregressive moving average Gaussian channel, positively answering a long-standing open problem studied by Butman, Schalkwijk-Tiernan, Wolfowitz, Ozarow, Ordentlich, Yang-Kavcic-Tatikonda, and others. More generally, it is shown that a k-dimensional generalization of the Schalkwijk-Kailath coding scheme achieves the feedback capacity for any autoregressive moving average noise spectrum of order k. Simply put, the optimal transmitter iteratively refines the receiver's knowledge of the intended message."
      }
    ]
  },
  "277": {
    "explanation": "formal verification and automation in quantum programming",
    "topk": [
      {
        "node_idx": 93947,
        "score_0_10": 10,
        "title": "toward certified quantum programming",
        "abstract": "While recent progress in quantum hardware open the door for significant speedup in certain key areas, quantum algorithms are still hard to implement right, and the validation of such quantum programs is a challenge. Early attempts either suffer from the lack of automation or parametrized reasoning, or require the user to write specifications and algorithms far from those presented in research articles or textbooks, and as a consequence, no significant quantum algorithm implementation has been currently verified in a scale-invariant manner. We propose QBRICKS, the first development environment for certified quantum programs featuring clear separation between code and proof, scale-invariance specification and proof, high degree of proof automation and allowing to encode quantum programs in a natural way, i.e. close to textbook style. This environment features a new domain-specific language for quantum programs, namely QBRICKS-DSL, together with a new logical specification language QBRICKS-SPEC . Especially, we introduce and intensively build upon HOPS, a higher-order extension of the recent path-sum semantics, used for both specification (parametrized, versatile) and automation (closure properties). QBRICKS builds on best practice of formal verification for the classic case and tailor them to the quantum case. To illustrate the opportunity of QBRICKS , we implement the first scale-invariant verified implementations of non-trivial quantum algorithms, namely phase estimation (QPE) -- the purely quantum part of Shor algorithm for integer factoring -- and Grover search. It proves by fact that applying formal verification to real quantum programs is possible and should be further developed."
      },
      {
        "node_idx": 11142,
        "score_0_10": 10,
        "title": "fast integer multiplication using modular arithmetic",
        "abstract": "We give an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm for multiplying two $N$-bit integers that improves the $O(N\\cdot \\log N\\cdot \\log\\log N)$ algorithm by Sch\\\"{o}nhage-Strassen. Both these algorithms use modular arithmetic. Recently, F\\\"{u}rer gave an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm which however uses arithmetic over complex numbers as opposed to modular arithmetic. In this paper, we use multivariate polynomial multiplication along with ideas from F\\\"{u}rer's algorithm to achieve this improvement in the modular setting. Our algorithm can also be viewed as a $p$-adic version of F\\\"{u}rer's algorithm. Thus, we show that the two seemingly different approaches to integer multiplication, modular and complex arithmetic, are similar."
      },
      {
        "node_idx": 75621,
        "score_0_10": 10,
        "title": "proving linearisability via coarse grained abstraction",
        "abstract": "Linearisability has become the standard safety criterion for concurrent data structures ensuring that the effect of a concrete operation takes place after the execution some atomic statement (often referred to as the linearisation point). Identification of linearisation points is a non-trivial task and it is even possible for an operation to be linearised by the execution of other concurrent operations. This paper presents a method for verifying linearisability that does not require identification of linearisation points in the concrete code. Instead, we show that the concrete program is a refinement of some coarse-grained abstraction. The linearisation points in the abstraction are straightforward to identify and the linearisability proof itself is simpler due to the coarse granularity of its atomic statements. The concrete fine-grained program is a refinement of the coarse-grained program, and hence is also linearisable because every behaviour of the concrete program is a possible behaviour its abstraction."
      },
      {
        "node_idx": 40874,
        "score_0_10": 10,
        "title": "n ary fuzzy logic and neutrosophic logic operators",
        "abstract": "We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy logic and neutrosophic logic operators using the smarandache codification of the Venn diagram and a defined vector neutrosophic law. In such way, new operators in neutrosophic logic/set/probability are built."
      },
      {
        "node_idx": 14690,
        "score_0_10": 10,
        "title": "boolean hedonic games",
        "abstract": "We study hedonic games with dichotomous preferences. Hedonic games are cooperative games in which players desire to form coalitions, but only care about the makeup of the coalitions of which they are members; they are indifferent about the makeup of other coalitions. The assumption of dichotomous preferences means that, additionally, each player's preference relation partitions the set of coalitions of which that player is a member into just two equivalence classes: satisfactory and unsatisfactory. A player is indifferent between satisfactory coalitions, and is indifferent between unsatisfactory coalitions, but strictly prefers any satisfactory coalition over any unsatisfactory coalition. We develop a succinct representation for such games, in which each player's preference relation is represented by a propositional formula. We show how solution concepts for hedonic games with dichotomous preferences are characterised by propositional formulas."
      },
      {
        "node_idx": 20262,
        "score_0_10": 10,
        "title": "a quantum classical scheme towards quantum functional encryption",
        "abstract": "Quantum encryption is a well studied problem for both classical and quantum information. However, little is known about quantum encryption schemes which enable the user, under different keys, to learn different functions of the plaintext, given the ciphertext. In this paper, we give a novel one-bit secret-key quantum encryption scheme, a classical extension of which allows different key holders to learn different length subsequences of the plaintext from the ciphertext. We prove our quantum-classical scheme secure under the notions of quantum semantic security, quantum entropic indistinguishability, and recent security definitions from the field of functional encryption."
      },
      {
        "node_idx": 145121,
        "score_0_10": 10,
        "title": "simple causes of complexity in hedonic games",
        "abstract": "Hedonic games provide a natural model of coalition formation among self-interested agents. The associated problem of finding stable outcomes in such games has been extensively studied. In this paper, we identify simple conditions on expressivity of hedonic games that are sufficient for the problem of checking whether a given game admits a stable outcome to be computationally hard. Somewhat surprisingly, these conditions are very mild and intuitive. Our results apply to a wide range of stability concepts (core stability, individual stability, Nash stability, etc.) and to many known formalisms for hedonic games (additively separable games, games with W-preferences, fractional hedonic games, etc.), and unify and extend known results for these formalisms. They also have broader applicability: for several classes of hedonic games whose computational complexity has not been explored in prior work, we show that our framework immediately implies a number of hardness results for them."
      },
      {
        "node_idx": 70244,
        "score_0_10": 10,
        "title": "robust pose invariant shape and texture based hand recognition",
        "abstract": "This paper presents a novel personal identification and verification system using information extracted from the hand shape and texture. The system has two major constituent modules: a fully automatic and robust peg free segmentation and pose normalisation module, and a recognition module. In the first module, the hand is segmented from its background using a thresholding technique based on Otsu`s method combined with a skin colour detector. A set of fully automatic algorithms are then proposed to segment the palm and fingers. In these algorithms, the skeleton and the contour of the hand and fingers are estimated and used to determine the global pose of the hand and the pose of each individual finger. Finally the palm and fingers are cropped, pose corrected and normalised. In the recognition module, various shape and texture based features are extracted and used for matching purposes. The modified Hausdorff distance, the Iterative Closest Point (ICP) and Independent Component Analysis (ICA) algorithms are used for shape and texture features of the fingers. For the palmprints, we use the Discrete Cosine Transform (DCT), directional line features and ICA. Recognition (identification and verification) tests were performed using fusion strategies based on the similarity scores of the fingers and the palm. Experimental results show that the proposed system exhibits a superior performance over existing systems with an accuracy of over 98\\% for hand identification and verification (at equal error rate) in a database of 560 different subjects."
      },
      {
        "node_idx": 142640,
        "score_0_10": 9,
        "title": "backdoors to tractable answer set programming",
        "abstract": "Answer Set Programming (ASP) is an increasingly popular framework for declarative programming that admits the description of problems by means of rules and constraints that form a disjunctive logic program. In particular, many AI problems such as reasoning in a nonmonotonic setting can be directly formulated in ASP. Although the main problems of ASP are of high computational complexity, located at the second level of the Polynomial Hierarchy, several restrictions of ASP have been identified in the literature, under which ASP problems become tractable. #R##N#In this paper we use the concept of backdoors to identify new restrictions that make ASP problems tractable. Small backdoors are sets of atoms that represent \"clever reasoning shortcuts\" through the search space and represent a hidden structure in the problem input. The concept of backdoors is widely used in the areas of propositional satisfiability and constraint satisfaction. We show that it can be fruitfully adapted to ASP. We demonstrate how backdoors can serve as a unifying framework that accommodates several tractable restrictions of ASP known from the literature. Furthermore, we show how backdoors allow us to deploy recent algorithmic results from parameterized complexity theory to the domain of answer set programming."
      },
      {
        "node_idx": 119204,
        "score_0_10": 9,
        "title": "a knowledge compilation map",
        "abstract": "We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages."
      }
    ]
  },
  "278": {
    "explanation": "sequence-to-sequence conversational and generative neural modeling",
    "topk": [
      {
        "node_idx": 157548,
        "score_0_10": 10,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 35756,
        "score_0_10": 9,
        "title": "building end to end dialogue systems using generative hierarchical neural network models",
        "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings."
      },
      {
        "node_idx": 135057,
        "score_0_10": 9,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 91693,
        "score_0_10": 9,
        "title": "tacotron towards end to end speech synthesis",
        "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given   pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 54610,
        "score_0_10": 9,
        "title": "digital watermarking in the singular vector domain",
        "abstract": "Many current watermarking algorithms insert data in the spatial or transform domains like the discrete cosine, the discrete Fourier, and the discrete wavelet transforms. In this paper, we present a data-hiding algorithm that exploits the singular value decomposition (SVD) representation of the data. We compute the SVD of the host image and the watermark and embed the watermark in the singular vectors of the host image. The proposed method leads to an imperceptible scheme for digital images, both in grey scale and color and is quite robust against attacks like noise and JPEG compression."
      },
      {
        "node_idx": 29312,
        "score_0_10": 8,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 162900,
        "score_0_10": 8,
        "title": "a diversity promoting objective function for neural conversation models",
        "abstract": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \"I don't know\") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations."
      },
      {
        "node_idx": 51364,
        "score_0_10": 8,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      }
    ]
  },
  "279": {
    "explanation": "learning spatially invariant feature transformations in CNNs",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 139975,
        "score_0_10": 8,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 141503,
        "score_0_10": 8,
        "title": "holographic image sensing",
        "abstract": "Holographic representations of data enable distributed storage with progressive refinement when the stored packets of data are made available in any arbitrary order. In this paper, we propose and test patch-based transform coding holographic sensing of image data. Our proposal is optimized for progressive recovery under random order of retrieval of the stored data. The coding of the image patches relies on the design of distributed projections ensuring best image recovery, in terms of the $\\ell_2$ norm, at each retrieval stage. The performance depends only on the number of data packets that has been retrieved thus far. #R##N#Several possible options to enhance the quality of the recovery while changing the size and number of data packets are discussed and tested. This leads us to examine several interesting bit-allocation and rate-distortion trade offs, highlighted for a set of natural images with ensemble estimated statistical properties."
      },
      {
        "node_idx": 146266,
        "score_0_10": 8,
        "title": "holographic sensing",
        "abstract": "Holographic representations of data encode information in packets of equal importance that enable progressive recovery. The quality of recovered data improves as more and more packets become available. This progressive recovery of the information is independent of the order in which packets become available. Such representations are ideally suited for distributed storage and for the transmission of data packets over networks with unpredictable delays and or erasures.  Several methods for holographic representations of signals and images have been proposed over the years and multiple description information theory also deals with such representations. Surprisingly, however, these methods had not been considered in the classical framework of optimal least-squares estimation theory, until very recently. We develop a least-squares approach to the design of holographic representation for stochastic data vectors, relying on the framework widely used in modeling signals and images."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 22502,
        "score_0_10": 8,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 112726,
        "score_0_10": 8,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 151734,
        "score_0_10": 8,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 14201,
        "score_0_10": 8,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      }
    ]
  },
  "280": {
    "explanation": "energy cost optimization in shared renewable energy storage systems",
    "topk": [
      {
        "node_idx": 94358,
        "score_0_10": 10,
        "title": "real time shared energy storage management for renewable energy integration in smart grid",
        "abstract": "Energy storage systems (ESSs) are essential components of the future smart grids with high penetration of renewable energy sources. However, deploying individual ESSs for all energy consumers, especially in large systems, may not be practically feasible mainly due to high upfront cost of purchasing many ESSs and space limitation. As a result, the concept of shared ESS enabling all users charge/discharge to/from a common ESS has become appealing. In this paper, we study the energy management problem of a group of users with renewable energy sources and controllable (i.e., demand responsive) loads that all share a common ESS so as to minimize their sum weighted energy cost. Specifically, we propose a distributed algorithm to solve the formulated problem, which iteratively derives the optimal values of charging/discharging to/from the shared ESS, while only limited information is exchanged between users and a central controller; hence, the privacy of users is preserved. With the optimal charging and discharging values obtained, each user needs to independently solve a simple linear programming (LP) problem to derive the optimal energy consumption of its controllable loads over time as well as that of purchased from the grid. Using simulations, we show that the shared ESS can achieve lower energy cost compared to the case of distributed ESSs, where each user owns its ESS and does not share it with others. Next, we propose online algorithms for the real-time energy management, under non-zero prediction errors of load and renewable energy. The proposed algorithms differ in complexity and the information required to be shared between the users and central controller, where their performance is also compared via simulations."
      },
      {
        "node_idx": 26874,
        "score_0_10": 10,
        "title": "learning transferable architectures for scalable image recognition",
        "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
      },
      {
        "node_idx": 21714,
        "score_0_10": 9,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 167062,
        "score_0_10": 9,
        "title": "energy management for demand responsive users with shared energy storage system",
        "abstract": "This paper investigates the energy management problem for multiple self-interested users, each with renewable energy generation as well as both the fixed and controllable loads, that all share a common energy storage system (ESS). The self-interested users are willing to sell/buy energy to/from the shared ESS if they can achieve lower energy costs compared to the case of no energy trading while preserving their privacy e.g. sharing only limited information with a central controller. Under this setup, we propose an iterative algorithm by which the central controller coordinates the charging/discharging values to/from the shared ESS by all users such that their individual energy costs reduce at the same time. For performance benchmark, the case of cooperative users that all belong to the same entity is considered, where they share all the required information with the central controller so as to minimize their total energy cost. Finally, the effectiveness of our proposed algorithm in simultaneously reducing users' energy costs is shown via simulations based on realistic system data of California, US."
      },
      {
        "node_idx": 104043,
        "score_0_10": 9,
        "title": "darts differentiable architecture search",
        "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms."
      },
      {
        "node_idx": 16420,
        "score_0_10": 9,
        "title": "a survey on graph drawing beyond planarity",
        "abstract": "Graph Drawing Beyond Planarity is a rapidly growing research area that classifies and studies geometric representations of non-planar graphs in terms of forbidden crossing configurations. Aim of this survey is to describe the main research directions in this area, the most prominent known results, and some of the most challenging open problems."
      },
      {
        "node_idx": 158015,
        "score_0_10": 8,
        "title": "efficient neural architecture search via parameter sharing",
        "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%."
      },
      {
        "node_idx": 75250,
        "score_0_10": 8,
        "title": "evaluating the retrieval effectiveness of web search engines using a representative query sample",
        "abstract": "Search engine retrieval effectiveness studies are usually small-scale, using only limited query samples. Furthermore, queries are selected by the researchers. We address these issues by taking a random representative sample of 1,000 informational and 1,000 navigational queries from a major German search engine and comparing Google's and Bing's results based on this sample. Jurors were found through crowdsourcing, data was collected using specialised software, the Relevance Assessment Tool (RAT). We found that while Google outperforms Bing in both query types, the difference in the performance for informational queries was rather low. However, for navigational queries, Google found the correct answer in 95.3 per cent of cases whereas Bing only found the correct answer 76.6 per cent of the time. We conclude that search engine performance on navigational queries is of great importance, as users in this case can clearly identify queries that have returned correct results. So, performance on this query type may contribute to explaining user satisfaction with search engines."
      },
      {
        "node_idx": 59347,
        "score_0_10": 8,
        "title": "netvlad cnn architecture for weakly supervised place recognition",
        "abstract": "We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the \"Vector of Locally Aggregated Descriptors\" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks."
      },
      {
        "node_idx": 111390,
        "score_0_10": 8,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      }
    ]
  },
  "281": {
    "explanation": "distributed monitoring and calibration techniques for sensors and systems",
    "topk": [
      {
        "node_idx": 72378,
        "score_0_10": 10,
        "title": "an omnidirectional aerial manipulation platform for contact based inspection",
        "abstract": "This paper presents an omnidirectional aerial manipulation platform for robust and responsive interaction with unstructured environments, toward the goal of contact-based inspection. The fully actuated tilt-rotor aerial system is equipped with a rigidly mounted end-effector, and is able to exert a 6 degree of freedom force and torque, decoupling the system's translational and rotational dynamics, and enabling precise interaction with the environment while maintaining stability. An impedance controller with selective apparent inertia is formulated to permit compliance in certain degrees of freedom while achieving precise trajectory tracking and disturbance rejection in others. Experiments demonstrate disturbance rejection, push-and-slide interaction, and on-board state estimation with depth servoing to interact with local surfaces. The system is also validated as a tool for contact-based non-destructive testing of concrete infrastructure."
      },
      {
        "node_idx": 138571,
        "score_0_10": 10,
        "title": "coalgebraic logic programming from semantics to implementation",
        "abstract": "Coinductive definitions, such as that of an infinite stream, may often be described by elegant logic programs, but ones for which SLD-refutation is of no value as SLD-derivations fall into infinite loops. Such definitions give rise to questions of lazy corecursive derivations and parallelism, as execution of such logic programs can have both recursive and corecursive features at once. Observational and coalgebraic semantics have been used to study them abstractly. The programming developments have often occurred separately and have usually been implementation-led. Here, we give a coherent semantics-led account of the issues, starting with abstract category theoretic semantics, developing coalgebra to characterise naturally arising trees, and proceeding towards implementation of a new dialect, CoALP, of logic programming, characterised by guarded lazy corecursion and parallelism."
      },
      {
        "node_idx": 128431,
        "score_0_10": 10,
        "title": "a type system for a stochastic cls",
        "abstract": "The Stochastic Calculus of Looping Sequences is suitable to describe the evolution of microbiological systems, taking into account the speed of the described activities. We propose a type system for this calculus that models how the presence of positive and negative catalysers can modify these speeds. We claim that types are the right abstraction in order to represent the interaction between elements without specifying exactly the element positions. Our claim is supported through an example modelling the lactose operon. The Calculus of Looping Sequences (CLS for short) [4, 5, 19], is a formalism for describing biological systems and their evolution. CLS is based on term rewriting, given a set of predefined rules modelling the activities one would like to describe. The model has been extended with several features, such as a commutative parallel composition operator, and some semantic means, such as bisimulations [5, 7], which are common in process calculi. This permits to combine the simplicity of notation of rewrite systems with the advantage of a form of compositionality. A Stochastic version of CLS (SCLS for short) is proposed in [6]. Rates are associated with rewrite rules in order to model the speed of the described activities. Therefore, transitions derived in S CLS are driven by a rate that models the parameter of an exponential distribution and characterizes the stoch astic behaviour of the transition. The choice of the next rule to be applied and of the time of its applicatio n is based on the classical Gillespie\u2019s algorithm [15]. Defining a stochastic semantics for CLS requires a correct en umeration of all the possible and distinct ways to apply each rewrite rule within a term. A single pattern may have several, though isomorphic, matches within a CLS term. In this paper, we simplify the counting mechanism used in [6] by imposing some restrictions on the patterns modelling the rewrite rul es. Each rewrite rule states explicitly the types of the elements whose occurrence are able to speed-up or slow-down a reaction. The occurrences of the elements of these types are then processed by a rate function (instead of a rate constant) which is used to compute the actual rate of a transition. We show how we can define patterns in our typed stochastic framework to model some common biological activities, and, in particular, we underline the possibility to combine the modelling of positive and negative catalysers within a single rule by reproducing a general case of osmosis. Finally, as a complete modelling application, we show the expressiveness of our formalism by describing the lactose operon in Escherichia Coli."
      },
      {
        "node_idx": 95061,
        "score_0_10": 10,
        "title": "decentralised ltl monitoring",
        "abstract": "Users wanting to monitor distributed or component-based systems often perceive them as monolithic systems which, seen from the outside, exhibit a uniform behaviour as opposed to many components displaying many local behaviours that together constitute the system's global behaviour. This level of abstraction is often reasonable, hiding implementation details from users who may want to specify the system's global behaviour in terms of an LTL formula. However, the problem that arises then is how such a specification can actually be monitored in a distributed system that has no central data collection point, where all the components' local behaviours are observable. In this case, the LTL specification needs to be decomposed into sub-formulae which, in turn, need to be distributed amongst the components' locally attached monitors, each of which sees only a distinct part of the global behaviour. The main contribution of this paper is an algorithm for distributing and monitoring LTL formulae, such that satisfac- tion or violation of specifications can be detected by local monitors alone. We present an implementation and show that our algorithm introduces only a minimum delay in detecting satisfaction/violation of a specification. Moreover, our practical results show that the communication overhead introduced by the local monitors is considerably lower than the number of messages that would need to be sent to a central data collection point."
      },
      {
        "node_idx": 11855,
        "score_0_10": 9,
        "title": "a minimal oo calculus for modelling biological systems",
        "abstract": "In this paper we present a minimal object oriented core calculus for modelling the biological notion of type that arises from biological ontologies in formalisms based on term rewriting. This calculus implements encapsulation, method invocation, subtyping and a simple formof overriding inheritance, and it is applicable to models designed in the most popular term-rewriting formalisms. The classes implemented in a formalism can be used in several models, like programming libraries."
      },
      {
        "node_idx": 16751,
        "score_0_10": 9,
        "title": "dynamic magnetometer calibration and alignment to inertial sensors by kalman filtering",
        "abstract": "Magnetometer and inertial sensors are widely used for orientation estimation. Magnetometer usage is often troublesome, as it is prone to be interfered by onboard or ambient magnetic disturbance. The onboard soft-iron material distorts not only the magnetic field, but the magnetometer sensor frame coordinate and the cross-sensor misalignment relative to inertial sensors. It is desirable to conveniently put magnetic and inertial sensors information in a common frame. Existing methods either split the problem into successive intrinsic and cross-sensor calibrations, or rely on stationary accelerometer measurements which is infeasible in dynamic conditions. This paper formulates the magnetometer calibration and alignment to inertial sensors as a state estimation problem, and collectively solves the magnetometer intrinsic and cross-sensor calibrations, as well as the gyroscope bias estimation. Sufficient conditions are derived for the problem to be globally observable, even when no accelerometer information is used at all. An extended Kalman filter is designed to implement the state estimation and comprehensive test data results show the superior performance of the proposed approach. It is immune to acceleration disturbance and applicable potentially in any dynamic conditions."
      },
      {
        "node_idx": 66452,
        "score_0_10": 9,
        "title": "automatic synthesis of switching controllers for linear hybrid automata",
        "abstract": "In this paper we study the problem of automatically generating switching controllers for the class of Linear Hybrid Automata, with respect to safety objectives. We identify and solve inaccuracies contained in previous characterizations of the problem, providing a sound and complete symbolic fixpoint procedure, based on polyhedral abstractions of the state space. We also prove the termination of each iteration of the procedure. Some promising experimental results are presented, based on an implementation of the fixpoint procedure on top of the tool PHAVer."
      },
      {
        "node_idx": 127370,
        "score_0_10": 9,
        "title": "gyroscope calibration via magnetometer",
        "abstract": "Magnetometers, gyroscopes and accelerometers are commonly used sensors in a variety of applications. The paper proposes a novel gyroscope calibration method in the homogeneous magnetic field by the help of magnetometer. It is shown that, with sufficient rotation excitation, the homogeneous magnetic field vector can be exploited to serve as a good reference for calibrating low-cost gyroscopes. The calibration parameters include the gyroscope scale factor, non-orthogonal coefficient and bias for three axes, as well as its misalignment to the magnetometer frame. Simulation and field test results demonstrate the method's effectiveness."
      },
      {
        "node_idx": 154461,
        "score_0_10": 9,
        "title": "coinductive big step operational semantics",
        "abstract": "Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results."
      },
      {
        "node_idx": 166932,
        "score_0_10": 9,
        "title": "on calibration of three axis magnetometer",
        "abstract": "Magnetometer has received wide applications in attitude determination and scientific measurements. Calibration is an important step for any practical magnetometer use. The most popular three-axis magnetometer calibration methods are attitude-independent and have been founded on an approximate maximum likelihood (ML) estimation with a quartic subjective function, derived from the fact that the magnitude of the calibrated measurements should be constant in a homogeneous magnetic field. This paper highlights the shortcomings of those popular methods and proposes to use the quadratic optimal ML estimation instead for magnetometer calibration. The simulation and test results show that the optimal ML calibration is superior to the approximate ML methods for magnetometer calibration in both accuracy and stability, especially for those situations without sufficient attitude excitation. The significant benefits deserve the moderately increased computation burden. The main conclusion obtained in the context of magnetometer in this paper is potentially applicable to various kinds of three-axis sensors."
      }
    ]
  },
  "286": {
    "explanation": "bi-directional LSTM and CNN architectures for sequence labeling tasks",
    "topk": [
      {
        "node_idx": 112674,
        "score_0_10": 10,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 63929,
        "score_0_10": 10,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 78341,
        "score_0_10": 10,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 11752,
        "score_0_10": 10,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 26180,
        "score_0_10": 9,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 160949,
        "score_0_10": 9,
        "title": "a convolutional neural network for modelling sentences",
        "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."
      },
      {
        "node_idx": 153102,
        "score_0_10": 9,
        "title": "embedding entities and relations for learning and inference in knowledge bases",
        "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning."
      },
      {
        "node_idx": 137083,
        "score_0_10": 9,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 116697,
        "score_0_10": 9,
        "title": "extended unary coding",
        "abstract": "Extended variants of the recently introduced spread unary coding are described. These schemes, in which the length of the code word is fixed, allow representation of approximately n^2 numbers for n bits, rather than the n numbers of the standard unary coding. In the first of two proposed schemes the spread increases, whereas in the second scheme the spread remains constant."
      },
      {
        "node_idx": 29312,
        "score_0_10": 9,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      }
    ]
  },
  "288": {
    "explanation": "energy harvesting and wireless communication optimization",
    "topk": [
      {
        "node_idx": 28821,
        "score_0_10": 10,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 46603,
        "score_0_10": 10,
        "title": "energy harvesting wireless communications a review of recent advances",
        "abstract": "This paper summarizes recent contributions in the broad area of energy harvesting wireless communications. In particular, we provide the current state of the art for wireless networks composed of energy harvesting nodes, starting from the information-theoretic performance limits to transmission scheduling policies and resource allocation, medium access, and networking issues. The emerging related area of energy transfer for self-sustaining energy harvesting wireless networks is considered in detail covering both energy cooperation aspects and simultaneous energy and information transfer. Various potential models with energy harvesting nodes at different network scales are reviewed, as well as models for energy consumption at the nodes."
      },
      {
        "node_idx": 116111,
        "score_0_10": 10,
        "title": "optimal packet scheduling in an energy harvesting communication system",
        "abstract": "We consider the optimal packet scheduling problem in a single-user energy harvesting wireless communication system. In this system, both the data packets and the harvested energy are modeled to arrive at the source node randomly. Our goal is to adaptively change the transmission rate according to the traffic load and available energy, such that the time by which all packets are delivered is minimized. Under a deterministic system setting, we assume that the energy harvesting times and harvested energy amounts are known before the transmission starts. For the data traffic arrivals, we consider two different scenarios. In the first scenario, we assume that all bits have arrived and are ready at the transmitter before the transmission starts. In the second scenario, we consider the case where packets arrive during the transmissions, with known arrival times and sizes. We develop optimal off-line scheduling policies which minimize the time by which all packets are delivered to the destination, under causality constraints on both data and energy arrivals."
      },
      {
        "node_idx": 100857,
        "score_0_10": 9,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 124513,
        "score_0_10": 9,
        "title": "optimal energy management policies for energy harvesting sensor nodes",
        "abstract": "We study a sensor node with an energy harvesting source. The generated energy can be stored in a buffer. The sensor node periodically senses a random field and generates a packet. These packets are stored in a queue and transmitted using the energy available at that time. We obtain energy management policies that are throughput optimal, i.e., the data queue stays stable for the largest possible data rate. Next we obtain energy management policies which minimize the mean delay in the queue. We also compare performance of several easily implementable sub-optimal energy management policies. A greedy policy is identified which, in low SNR regime, is throughput optimal and also minimizes mean delay."
      },
      {
        "node_idx": 50012,
        "score_0_10": 9,
        "title": "optimum transmission policies for battery limited energy harvesting nodes",
        "abstract": "Wireless networks with energy harvesting battery powered nodes are quickly emerging as a viable option for future wireless networks with extended lifetime. Equally important to their counterpart in the design of energy harvesting radios are the design principles that this new networking paradigm calls for. In particular, unlike wireless networks considered up to date, the energy replenishment process and the storage constraints of the rechargeable batteries need to be taken into account in designing efficient transmission strategies. In this work, we consider such transmission policies for rechargeable nodes, and identify the optimum solution for two related problems. Specifically, the transmission policy that maximizes the short term throughput, i.e., the amount of data transmitted in a finite time horizon is found. In addition, we show the relation of this optimization problem to another, namely, the minimization of the transmission completion time for a given amount of data, and solve that as well. The transmission policies are identified under the constraints on energy causality, i.e., energy replenishment process, as well as the energy storage, i.e., battery capacity. The power-rate relationship for this problem is assumed to be an increasing concave function, as dictated by information theory. For battery replenishment, a model with discrete packets of energy arrivals is considered. We derive the necessary conditions that the throughput-optimal allocation satisfies, and then provide the algorithm that finds the optimal transmission policy with respect to the short-term throughput and the minimum transmission completion time. Numerical results are presented to confirm the analytical findings."
      },
      {
        "node_idx": 72524,
        "score_0_10": 9,
        "title": "optimal energy allocation for wireless communications with energy harvesting constraints",
        "abstract": "We consider the use of energy harvesters, in place of conventional batteries with fixed energy storage, for point-to-point wireless communications. In addition to the challenge of transmitting in a channel with time selective fading, energy harvesters provide a perpetual but unreliable energy source. In this paper, we consider the problem of energy allocation over a finite horizon, taking into account channel conditions and energy sources that are time varying, so as to maximize the throughput. Two types of side information (SI) on the channel conditions and harvested energy are assumed to be available: causal SI (of the past and present slots) or full SI (of the past, present and future slots). We obtain structural results for the optimal energy allocation, via the use of dynamic programming and convex optimization techniques. In particular, if unlimited energy can be stored in the battery with harvested energy and the full SI is available, we prove the optimality of a water-filling energy allocation solution where the so-called water levels follow a staircase function."
      },
      {
        "node_idx": 14004,
        "score_0_10": 9,
        "title": "malleable coding with fixed segment reuse",
        "abstract": "Storage area networks, remote backup storage systems, and similar information systems frequently modify stored data with updates from new versions. In these systems, it is desirable for the data to not only be compressed but to also be easily modified during updates. A malleable coding scheme considers both compression efficiency and ease of alteration, promoting some form of reuse or recycling of codewords. Malleability cost is the difficulty of synchronizing compressed versions, and malleable codes are of particular interest when representing information and modifying the representation are both expensive. We examine the trade-off between compression efficiency and malleability cost measured with respect to the length of a reused prefix portion. The region of achievable rates and malleability is formulated as an information-theoretic optimization and a single-letter expression is provided. Relationships to coded side information and common information problems are also established."
      },
      {
        "node_idx": 14646,
        "score_0_10": 8,
        "title": "wireless powered communication opportunities and challenges",
        "abstract": "The performance of wireless communication is fundamentally constrained by the limited battery life of wireless devices, the operations of which are frequently disrupted due to the need of manual battery replacement/recharging. The recent advance in RF-enabled wireless energy transfer (WET) technology provides an attractive solution named wireless powered communication (WPC), where the wireless devices are powered by dedicated wireless power transmitters to provide continuous and stable microwave energy over the air. As a key enabling technology for truly perpetual communications, WPC opens up the potential to build a network with larger throughput, higher robustness, and increased flexibility compared to its battery-powered counterpart. However, the combination of wireless energy and information transmissions also raises many new research problems and implementation issues that need to be addressed. In this article, we provide an overview of stateof- the-art RF-enabled WET technologies and their applications to wireless communications, highlighting the key design challenges, solutions, and opportunities ahead."
      },
      {
        "node_idx": 96632,
        "score_0_10": 8,
        "title": "malleable coding with fixed reuse",
        "abstract": "In cloud computing, storage area networks, remote backup storage, and similar settings, stored data is modified with updates from new versions. Representing information and modifying the representation are both expensive. Therefore it is desirable for the data to not only be compressed but to also be easily modified during updates. A malleable coding scheme considers both compression efficiency and ease of alteration, promoting codeword reuse. We examine the trade-off between compression efficiency and malleability cost-the difficulty of synchronizing compressed versions-measured as the length of a reused prefix portion. Through a coding theorem, the region of achievable rates and malleability is expressed as a single-letter optimization. Relationships to common information problems are also described."
      }
    ]
  },
  "289": {
    "explanation": "joint stance detection and named entity recognition in tweets",
    "topk": [
      {
        "node_idx": 111569,
        "score_0_10": 10,
        "title": "joint named entity recognition and stance detection in tweets",
        "abstract": "Named entity recognition (NER) is a well-established task of information extraction which has been studied for decades. More recently, studies reporting NER experiments on social media texts have emerged. On the other hand, stance detection is a considerably new research topic usually considered within the scope of sentiment analysis. Stance detection studies are mostly applied to texts of online debates where the stance of the text owner for a particular target, either explicitly or implicitly mentioned in text, is explored. In this study, we investigate the possible contribution of named entities to the stance detection task in tweets. We report the evaluation results of NER experiments as well as that of the subsequent stance detection experiments using named entities, on a publicly-available stance-annotated data set of tweets. Our results indicate that named entities obtained with a high-performance NER system can contribute to stance detection performance on tweets."
      },
      {
        "node_idx": 11752,
        "score_0_10": 10,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 92181,
        "score_0_10": 10,
        "title": "usfd2 annotating temporal expresions and tlinks for tempeval 2",
        "abstract": "We describe the University of Sheffield system used in the TempEval-2 challenge, USFD2. The challenge requires the automatic identification of temporal entities and relations in text. USFD2 identifies and anchors temporal expressions, and also attempts two of the four temporal relation assignment tasks. A rule-based system picks out and anchors temporal expressions, and a maximum entropy classifier assigns temporal link labels, based on features that include descriptions of associated temporal signal words. USFD2 identified temporal expressions successfully, and correctly classified their type in 90% of cases. Determining the relation between an event and time expression in the same sentence was performed at 63% accuracy, the second highest score in this part of the challenge."
      },
      {
        "node_idx": 48659,
        "score_0_10": 10,
        "title": "nrc canada building the state of the art in sentiment analysis of tweets",
        "abstract": "In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated us available resources."
      },
      {
        "node_idx": 168878,
        "score_0_10": 10,
        "title": "predicting abnormal returns from news using text classification",
        "abstract": "We show how text from news articles can be used to predict intraday price movements of financial assets using support vector machines. Multiple kernel learning is used to combine equity returns with text as predictive features to increase classification performance and we develop an analytic center cutting plane method to solve the kernel learning problem efficiently. We observe that while the direction of returns is not predictable using either text or returns, their size is, with text features producing significantly better performance than historical returns alone."
      },
      {
        "node_idx": 137083,
        "score_0_10": 10,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 113431,
        "score_0_10": 9,
        "title": "an approach to 2d signals recovering in compressive sensing context",
        "abstract": "In this paper we study the compressive sensing effects on 2D signals exhibiting sparsity in 2D DFT domain. A simple algorithm for reconstruction of randomly under-sampled data is proposed. It is based on the analytically determined threshold that precisely separates signal and non-signal components in the 2D DFT domain. The algorithm operates fast in a single iteration providing the accurate signal reconstruction. In the situations that are not comprised by the analytic derivation and constrains, the algorithm is still efficient and need just a couple of iterations. The proposed solution shows promising results in ISAR imaging (simulated data are used), where the reconstruction is achieved even in the case when less than 10% of data is available."
      },
      {
        "node_idx": 10080,
        "score_0_10": 9,
        "title": "prosody based automatic segmentation of speech into sentences and topics",
        "abstract": "A crucial step in processing speech audio data for information extraction, topic detection, or browsing/playback is to segment the input into sentence and topic units. Speech segmentation is challenging, since the cues typically present for segmenting text (headers, paragraphs, punctuation) are absent in spoken language. We investigate the use of prosody (information gleaned from the timing and melody of speech) for these tasks. Using decision tree and hidden Markov modeling techniques, we combine prosodic cues with word-based approaches, and evaluate performance on two speech corpora, Broadcast News and Switchboard. Results show that the prosodic model alone performs on par with, or better than, word-based statistical language models -- for both true and automatically recognized words in news speech. The prosodic model achieves comparable performance with significantly less training data, and requires no hand-labeling of prosodic events. Across tasks and corpora, we obtain a significant improvement over word-only models using a probabilistic combination of prosodic and lexical information. Inspection reveals that the prosodic models capture language-independent boundary indicators described in the literature. Finally, cue usage is task and corpus dependent. For example, pause and pitch features are highly informative for segmenting news speech, whereas pause, duration and word-based cues dominate for natural conversation."
      },
      {
        "node_idx": 150982,
        "score_0_10": 9,
        "title": "normalization of relative and incomplete temporal expressions in clinical narratives",
        "abstract": "Objective To improve the normalization of relative and incomplete temporal expressions (RI-TIMEXes) in clinical narratives.#N##N#Methods We analyzed the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotated the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier, and a rule-based RI-TIMEX text span parser. We experimented with different feature sets and performed an error analysis for each system component.#N##N#Results The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification, and rule-based parsing accuracy of 74.68%, 87.71%, and 57.2% (82.09% under relaxed matching criteria), respectively, on the held-out test set of the 2012 i2b2 temporal relation challenge.#N##N#Discussion Experiments with feature sets reveal some interesting findings, such as: the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis showed that underrepresented anchor point and anchor relation classes are difficult to detect.#N##N#Conclusions We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge."
      },
      {
        "node_idx": 162484,
        "score_0_10": 9,
        "title": "stance and sentiment in tweets",
        "abstract": "We can often detect from a person's utterances whether he/she is in favor of or against a given target entity -- their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet--target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification."
      }
    ]
  },
  "290": {
    "explanation": "sequence modeling and neural network architectures for NLP tasks",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 29312,
        "score_0_10": 10,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 135057,
        "score_0_10": 9,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 63929,
        "score_0_10": 9,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 112674,
        "score_0_10": 9,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 116521,
        "score_0_10": 9,
        "title": "recurrent neural network regularization",
        "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation."
      },
      {
        "node_idx": 78341,
        "score_0_10": 8,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 124702,
        "score_0_10": 8,
        "title": "convolutional sequence to sequence learning",
        "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      }
    ]
  },
  "291": {
    "explanation": "meta-learning and few-shot adaptation techniques",
    "topk": [
      {
        "node_idx": 155778,
        "score_0_10": 10,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 108826,
        "score_0_10": 10,
        "title": "delving deeper into mooc student dropout prediction",
        "abstract": "In order to obtain reliable accuracy estimates for automatic MOOC dropout predictors, it is important to train and test them in a manner consistent with how they will be used in practice. Yet most prior research on MOOC dropout prediction has measured test accuracy on the same course used for training the classifier, which can lead to overly optimistic accuracy estimates. In order to understand better how accuracy is affected by the training+testing regime, we compared the accuracy of a standard dropout prediction architecture (clickstream features + logistic regression) across 4 different training paradigms. Results suggest that (1) training and testing on the same course (\"post-hoc\") can overestimate accuracy by several percentage points; (2) dropout classifiers trained on proxy labels based on students' persistence are surprisingly competitive with post-hoc training (87.33% versus 90.20% AUC averaged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performance does not vary significantly with the academic discipline. Finally, we also research new dropout prediction architectures based on deep, fully-connected, feed-forward neural networks and find that (4) networks with as many as 5 hidden layers can statistically significantly increase test accuracy over that of logistic regression."
      },
      {
        "node_idx": 28770,
        "score_0_10": 9,
        "title": "church a language for generative models",
        "abstract": "We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques."
      },
      {
        "node_idx": 83425,
        "score_0_10": 9,
        "title": "likely to stop predicting stopout in massive open online courses",
        "abstract": "Understanding why students stopout will help in understanding how students learn in MOOCs. In this report, part of a 3 unit compendium, we describe how we build accurate predictive models of MOOC student stopout. We document a scalable, stopout prediction methodology, end to end, from raw source data to model analysis. We attempted to predict stopout for the Fall 2012 offering of 6.002x. This involved the meticulous and crowd-sourced engineering of over 25 predictive features extracted for thousands of students, the creation of temporal and non-temporal data representations for use in predictive modeling, the derivation of over 10 thousand models with a variety of state-of-the-art machine learning techniques and the analysis of feature importance by examining over 70000 models. We found that stop out prediction is a tractable problem. Our models achieved an AUC (receiver operating characteristic area-under-the-curve) as high as 0.95 (and generally 0.88) when predicting one week in advance. Even with more difficult prediction problems, such as predicting stop out at the end of the course with only one weeks' data, the models attained AUCs of 0.7."
      },
      {
        "node_idx": 95061,
        "score_0_10": 9,
        "title": "decentralised ltl monitoring",
        "abstract": "Users wanting to monitor distributed or component-based systems often perceive them as monolithic systems which, seen from the outside, exhibit a uniform behaviour as opposed to many components displaying many local behaviours that together constitute the system's global behaviour. This level of abstraction is often reasonable, hiding implementation details from users who may want to specify the system's global behaviour in terms of an LTL formula. However, the problem that arises then is how such a specification can actually be monitored in a distributed system that has no central data collection point, where all the components' local behaviours are observable. In this case, the LTL specification needs to be decomposed into sub-formulae which, in turn, need to be distributed amongst the components' locally attached monitors, each of which sees only a distinct part of the global behaviour. The main contribution of this paper is an algorithm for distributing and monitoring LTL formulae, such that satisfac- tion or violation of specifications can be detected by local monitors alone. We present an implementation and show that our algorithm introduces only a minimum delay in detecting satisfaction/violation of a specification. Moreover, our practical results show that the communication overhead introduced by the local monitors is considerably lower than the number of messages that would need to be sent to a central data collection point."
      },
      {
        "node_idx": 84316,
        "score_0_10": 9,
        "title": "dueling network architectures for deep reinforcement learning",
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
      },
      {
        "node_idx": 70166,
        "score_0_10": 8,
        "title": "introduction to online convex optimization",
        "abstract": "This manuscript portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives."
      },
      {
        "node_idx": 102507,
        "score_0_10": 8,
        "title": "developing cyber buffer zones",
        "abstract": "The United Nations conducts peace operations around the world, aiming tomaintain peace and security in conflict torn areas. Whilst early operations werelargely successful, the changing nature of warfare and conflict has often left peaceoperations strugglingto adapt. In this article, we make a contribution towardsefforts to plan for the next evolution in both intra and inter-state conflict: cyberwarfare. It is now widely accepted that cyber warfare will be a component offuture conflicts, and much researchhas been devoted to how governments andmilitaries can prepare for and fight in this new domain [1]. Despite the vastamount of research relating to cyber warfare, there has been less discussion onits impact towards successful peace operations. This is agap in knowledge thatis important to address, since the restoration of peace following conflict of anykind is of global importance. It is however a complex topic requiring discussionacross multiple domains. Input from the technical, political, governmental andsocietal domains are critical in forming the concept of cyber peacekeeping.Previous work on this topic has sought to define the concept of cyber peacekeeping[2, 3, 4]. We build upon this work by exploring the practicalities ofstarting up a cyber peacekeeping component and setting up a Cyber Buffer Zone (CBZ)."
      },
      {
        "node_idx": 116147,
        "score_0_10": 8,
        "title": "the bisimulation problem for equational graphs of finite out degree",
        "abstract": "The \"bisimulation problem\" for equational graphs of finite out-degree is shown to be decidable. We reduce this problem to the bisimulation problem for deterministic rational (vectors of) boolean series on the alphabet of a dpda M. We then exhibit a complete formal system for deducing equivalent pairs of such vectors."
      },
      {
        "node_idx": 32637,
        "score_0_10": 8,
        "title": "symbolic reachability analysis of higher order context free processes",
        "abstract": "We consider the problem of symbolic reachability analysis of higher-order context-free processes. These models are generalizations of the context-free processes (also called BPA processes) where each process manipulates a data structure which can be seen as a nested stack of stacks. Our main result is that, for any higher-order context-free process, the set of all predecessors of a given regular set of configurations is regular and effectively constructible. This result generalizes the analogous result which is known for level 1 context-free processes. We show that this result holds also in the case of backward reachability analysis under a regular constraint on configurations. As a corollary, we obtain a symbolic model checking algorithm for the temporal logic E(U,X) with regular atomic predicates, i.e., the fragment of CTL restricted to the EU and EX modalities."
      }
    ]
  },
  "294": {
    "explanation": "polar code construction and decoding algorithms for communication channels",
    "topk": [
      {
        "node_idx": 127804,
        "score_0_10": 10,
        "title": "polarization for arbitrary discrete memoryless channels",
        "abstract": "Channel polarization, originally proposed for binary-input channels, is generalized to arbitrary discrete memoryless channels. Specifically, it is shown that when the input alphabet size is a prime number, a similar construction to that for the binary case leads to polarization. This method can be extended to channels of composite input alphabet sizes by decomposing such channels into a set of channels with prime input alphabet sizes. It is also shown that all discrete memoryless channels can be polarized by randomized constructions. The introduction of randomness does not change the order of complexity of polar code construction, encoding, and decoding. A previous result on the error probability behavior of polar codes is also extended to the case of arbitrary discrete memoryless channels. The generalization of polarization to channels with arbitrary finite input alphabet sizes leads to polar-coding methods for approaching the true (as opposed to symmetric) channel capacity of arbitrary channels with discrete or continuous input alphabets."
      },
      {
        "node_idx": 79738,
        "score_0_10": 9,
        "title": "llr based successive cancellation list decoding of polar codes",
        "abstract": "We show that successive cancellation list decoding can be formulated exclusively using log-likelihood ratios. In addition to numerical stability, the log-likelihood ratio based formulation has useful properties that simplify the sorting step involved in successive cancellation list decoding. We propose a hardware architecture of the successive cancellation list decoder in the log-likelihood ratio domain which, compared with a log-likelihood domain implementation, requires less irregular and smaller memories. This simplification, together with the gains in the metric sorter, lead to    $ 56\\%$   to   $137\\%$   higher throughput per unit area than other recently proposed architectures. We then evaluate the empirical performance of the CRC-aided successive cancellation list decoder at different list sizes using different CRCs and conclude that it is important to adapt the CRC length to the list size in order to achieve the best error-rate performance of concatenated polar codes. Finally, we synthesize conventional successive cancellation decoders at large block-lengths with the same block-error probability as our proposed CRC-aided successive cancellation list decoders to demonstrate that, while our decoders have slightly lower throughput and larger area, they have a significantly smaller decoding latency."
      },
      {
        "node_idx": 92235,
        "score_0_10": 9,
        "title": "performance of polar codes for channel and source coding",
        "abstract": "Polar codes, introduced recently by Ar\\i kan, are the first family of codes known to achieve capacity of symmetric channels using a low complexity successive cancellation decoder. Although these codes, combined with successive cancellation, are optimal in this respect, their finite-length performance is not record breaking. We discuss several techniques through which their finite-length performance can be improved. We also study the performance of these codes in the context of source coding, both lossless and lossy, in the single-user context as well as for distributed applications."
      },
      {
        "node_idx": 69868,
        "score_0_10": 8,
        "title": "an adaptive successive cancellation list decoder for polar codes with cyclic redundancy check",
        "abstract": "In this letter, we propose an adaptive SC (Successive Cancellation)-List decoder for polar codes with CRC. This adaptive SC-List decoder iteratively increases the list size until the decoder outputs contain at least one survival path which can pass CRC. Simulation shows that the adaptive SC-List decoder provides significant complexity reduction. We also demonstrate that polar code (2048, 1024) with 24-bit CRC decoded by our proposed adaptive SC-List decoder with very large list size can achieve a frame error rate FER=0.001 at Eb/No=1.1dB, which is about 0.2dB from the information theoretic limit at this block length."
      },
      {
        "node_idx": 51364,
        "score_0_10": 8,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 23523,
        "score_0_10": 8,
        "title": "polar codes are optimal for lossy source coding",
        "abstract": "We consider lossy source compression of a binary symmetric source using polar codes and the low-complexity successive encoding algorithm. It was recently shown by Arikan that polar codes achieve the capacity of arbitrary symmetric binary-input discrete memoryless channels under a successive decoding strategy. We show the equivalent result for lossy source compression, i.e., we show that this combination achieves the rate-distortion bound for a binary symmetric source. We further show the optimality of polar codes for various problems including the binary Wyner-Ziv and the binary Gelfand-Pinsker problem"
      },
      {
        "node_idx": 15986,
        "score_0_10": 8,
        "title": "fast polar decoders algorithm and implementation",
        "abstract": "Polar codes provably achieve the symmetric capacity of a memoryless channel while having an explicit construction. The adoption of polar codes however, has been hampered by the low throughput of their decoding algorithm. This work aims to increase the throughput of polar decoding hardware by an order of magnitude relative to successive-cancellation decoders and is more than 8 times faster than the current fastest polar decoder. We present an algorithm, architecture, and FPGA implementation of a flexible, gigabit-per-second polar decoder."
      },
      {
        "node_idx": 119322,
        "score_0_10": 8,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 111390,
        "score_0_10": 7,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 41252,
        "score_0_10": 7,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      }
    ]
  },
  "295": {
    "explanation": "programming frameworks and algorithms for high-performance computing",
    "topk": [
      {
        "node_idx": 68625,
        "score_0_10": 10,
        "title": "modules over monads and linearity",
        "abstract": "Inspired by the classical theory of modules over a monoid, we give a first account of the natural notion of module over a monad. The associated notion of morphism of left modules (\"Linear\" natural transformations) captures an important property of compatibility with substitution, in the heterogeneous case where \"terms\" and variables therein could be of different types as well as in the homogeneous case. In this paper, we present basic constructions of modules and we show examples concerning in particular abstract syntax and lambda-calculus."
      },
      {
        "node_idx": 24673,
        "score_0_10": 10,
        "title": "likwid a lightweight performance oriented tool suite for x86 multicore environments",
        "abstract": "Exploiting the performance of today's processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command-line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and toggling hardware prefetchers. An API for using the performance counting features from user code is also included. We clearly state the differences to the widely used PAPI interface. To demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well-known OpenMP STREAM triad benchmark, and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips."
      },
      {
        "node_idx": 74433,
        "score_0_10": 10,
        "title": "likwid a lightweight performance oriented tool suite for x86 multicore environments",
        "abstract": "Exploiting the performance of today's processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command-line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and toggling hardware prefetchers. An API for using the performance counting features from user code is also included. We clearly state the differences to the widely used PAPI interface. To demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well-known OpenMP STREAM triad benchmark, and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips."
      },
      {
        "node_idx": 145926,
        "score_0_10": 9,
        "title": "the friendship paradox in scale free networks",
        "abstract": "Our friends have more friends than we do. That is the basis of the friendship paradox. In mathematical terms, the mean number of friends of friends is higher than the mean number of friends. In the present study, we analyzed the relationship between the mean degree of vertices (individuals),  , and the mean number of friends of friends,  , in scale-free networks with degrees ranging from a minimum degree (k_min) to a maximum degree (k_max). We deduced an expression for   -   for scale-free networks following a power-law distribution with a given scaling parameter (alpha). Based on this expression, we can quantify how the degree distribution of a scale-free network affects the mean number of friends of friends."
      },
      {
        "node_idx": 133719,
        "score_0_10": 9,
        "title": "resilience for exascale enabled multigrid methods",
        "abstract": "With the increasing number of components and further miniaturization the mean time between faults in supercomputers will decrease. System level fault tolerance techniques are expensive and cost energy, since they are often based on redundancy. Also classical check-point-restart techniques reach their limits when the time for storing the system state to backup memory becomes excessive. Therefore, algorithm-based fault tolerance mechanisms can become an attractive alternative. This article investigates the solution process for elliptic partial differential equations that are discretized by finite elements. Faults that occur in the parallel geometric multigrid solver are studied in various model scenarios. In a standard domain partitioning approach, the impact of a failure of a core or a node will affect one or several subdomains. Different strategies are developed to compensate the effect of such a failure algorithmically. The recovery is achieved by solving a local subproblem with Dirichlet boundary conditions using local multigrid cycling algorithms. Additionally, we propose a superman strategy where extra compute power is employed to minimize the time of the recovery process."
      },
      {
        "node_idx": 23080,
        "score_0_10": 9,
        "title": "high level signatures and initial semantics",
        "abstract": "We present a device for specifying and reasoning about syntax for datatypes, programming languages, and logic calculi. More precisely, we consider a general notion of \"signature\" for specifying syntactic constructions. Our signatures subsume classical algebraic signatures (i.e., signatures for languages with variable binding, such as the pure lambda calculus) and extend to much more general examples.#N#In the spirit of Initial Semantics, we define the \"syntax generated by a signature\" to be the initial object - if it exists - in a suitable category of models. Our notions of signature and syntax are suited for compositionality and provide, beyond the desired algebra of terms, a well-behaved substitution and the associated inductive/recursive principles.#N#Our signatures are \"general\" in the sense that the existence of an associated syntax is not automatically guaranteed. In this work, we identify a large and simple class of signatures which do generate a syntax.#N#This paper builds upon ideas from a previous attempt by Hirschowitz-Maggesi, which, in turn, was directly inspired by some earlier work of Ghani-Uustalu-Hamana and Matthes-Uustalu.#N#The main results presented in the paper are computer-checked within the UniMath system."
      },
      {
        "node_idx": 161391,
        "score_0_10": 9,
        "title": "decades of jurimetrics",
        "abstract": "Jurimetrics: decades of history, decades to-be auspicious. A Brazilian point of view on the trajectory of this forgotten concept in the quantitative approach of the law, with code and examples in free software."
      },
      {
        "node_idx": 125844,
        "score_0_10": 9,
        "title": "petfmm a dynamically load balancing parallel fast multipole library",
        "abstract": "Fast algorithms for the computation of N-body problems can be broadly classified into mesh-based interpolation methods, and hierarchical or multiresolution methods. To this latter class belongs the well-known fast multipole method (FMM), which offers (N) complexity. The FMM is a complex algorithm, and the programming difficulty associated with it has arguably diminished its impact, being a barrier for adoption. This paper presents an extensible parallel library for N-body interactions utilizing the FMM algorithm. A prominent feature of this library is that it is designed to be extensible, with a view to unifying efforts involving many algorithms based on the same principles as the FMM and enabling easy development of scientific application codes. The paper also details an exhaustive model for the computation of tree-based N-body algorithms in parallel, including both work estimates and communications estimates. With this model, we are able to implement a method to provide automatic, a priori load balancing of the parallel execution, achieving optimal distribution of the computational work among processors and minimal inter-processor communications. Using a client application that performs the calculation of velocity induced by N vortex particles in two dimensions, ample verification and testing of the library was performed. Strong scaling results are presented with 10 million particles on up to 256 processors, including both speedup and parallel efficiency. The largest problem size that has been run with the PetFMM library at this point was 64 million particles in 64 processors. The library is currently able to achieve over 85% parallel efficiency for 64 processes. The performance study, computational model, and application demonstrations presented in this paper are limited to 2D. However, the software architecture was designed to make an extension of this work to 3D straightforward, as the framework is templated over the dimension. The software library is open source under the PETSc license, even less restrictive than the BSD license; this guarantees the maximum impact to the scientific community and encourages peer-based collaboration for the extensions and applications. Copyright \u00a9 2010 John Wiley & Sons, Ltd."
      },
      {
        "node_idx": 92778,
        "score_0_10": 9,
        "title": "parallel multiphysics simulations of charged particles in microfluidic flows",
        "abstract": "The article describes parallel multiphysics simulations of charged particles in microfluidic flows with the waLBerla framework. To this end, three physical effects are coupled: rigid body dynamics, fluid flow modelled by a lattice Boltzmann algorithm, and electric potentials represented by a finite volume discretisation. For solving the finite volume discretisation for the electrostatic forces, a cell-centered multigrid algorithm is developed that conforms to the lattice Boltzmann meshes and the parallel communication structure of waLBerla. The new functionality is validated with suitable benchmark scenarios. Additionally, the parallel scaling and the numerical efficiency of the algorithms are analysed on an advanced supercomputer."
      },
      {
        "node_idx": 133470,
        "score_0_10": 9,
        "title": "walberla a block structured high performance framework for multiphysics simulations",
        "abstract": "Programming current supercomputers efficiently is a challenging task. Multiple levels of parallelism on the core, on the compute node, and between nodes need to be exploited to make full use of the system. Heterogeneous hardware architectures with accelerators further complicate the development process. waLBerla addresses these challenges by providing the user with highly efficient building blocks for developing simulations on block-structured grids. The block-structured domain partitioning is flexible enough to handle complex geometries, while the structured grid within each block allows for highly efficient implementations of stencil-based algorithms. We present several example applications realized with waLBerla, ranging from lattice Boltzmann methods to rigid particle simulations. Most importantly, these methods can be coupled together, enabling multiphysics simulations. The framework uses meta-programming techniques to generate highly efficient code for CPUs and GPUs from a symbolic method formulation. To ensure software quality and performance portability, a continuous integration toolchain automatically runs an extensive test suite encompassing multiple compilers, hardware architectures, and software configurations."
      }
    ]
  },
  "299": {
    "explanation": "multi-scale context aggregation for semantic segmentation",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 162377,
        "score_0_10": 9,
        "title": "r fcn object detection via region based fully convolutional networks",
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: this https URL"
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 130093,
        "score_0_10": 8,
        "title": "towards efficient coexistence of ieee 802 15 4e tsch and ieee 802 11",
        "abstract": "A major challenge in wide deployment of smart wireless devices, using different technologies and sharing the same 2.4 GHz spectrum, is to achieve coexistence across multiple technologies. The IEEE~802.11 (WLAN) and the IEEE 802.15.4e TSCH (WSN) where designed with different goals in mind and both play important roles for respective applications. However, they cause mutual interference and degraded performance while operating in the same space. To improve this situation we propose an approach to enable a cooperative control which type of network is transmitting at given time, frequency and place. #R##N#We recognize that TSCH based sensor network is expected to occupy only small share of time, and that the nodes are by design tightly synchronized. We develop mechanism enabling over-the-air synchronization of the Wi-Fi network to the TSCH based sensor network. Finally, we show that Wi-Fi network can avoid transmitting in the \"collision periods\". We provide full design and show prototype implementation based on the Commercial off-the-shelf (COTS) devices. Our solution does not require changes in any of the standards."
      },
      {
        "node_idx": 163164,
        "score_0_10": 8,
        "title": "rethinking atrous convolution for semantic image segmentation",
        "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
      },
      {
        "node_idx": 120825,
        "score_0_10": 8,
        "title": "encoder decoder with atrous separable convolution for semantic image segmentation",
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
      },
      {
        "node_idx": 26460,
        "score_0_10": 8,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 167244,
        "score_0_10": 8,
        "title": "a concrete example of inclusive design deaf oriented accessibility",
        "abstract": "One of the continuing challenges of Human Computer Interaction research is the full inclusion of people with special needs into the digital world. In particular, this crucial category includes people that experiences some kind of limitation in exploiting traditional information communication channels. One immediately thinks about blind people, and several researches aim at addressing their needs. On the contrary, limitations suffered by deaf people are often underestimated. This often the result of a kind of ignorance or misunderstanding of the real nature of their communication difficulties. This chapter aims at both increasing the awareness of deaf problems in the digital world, and at proposing the project of a comprehensive solution for their better inclusion. As for the former goal, we will provide a bird\u2019s-eye presentation of history and evolution of understanding of deafness issues, and of strategies to address them. As for the latter, we will present the design, implementation and evaluation of the first nucleus of a comprehensive digital framework to facilitate the access of deaf people into the digital world."
      },
      {
        "node_idx": 66578,
        "score_0_10": 7,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      }
    ]
  },
  "301": {
    "explanation": "human action video classification and recognition",
    "topk": [
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 141661,
        "score_0_10": 10,
        "title": "hypergraphic lp relaxations for steiner trees",
        "abstract": "We investigate hypergraphic LP relaxations for the Steiner tree problem, primarily the partition LP relaxation introduced by Konemann et al. [Math. Programming, 2009]. Specifically, we are interested in proving upper bounds on the integrality gap of this LP, and studying its relation to other linear relaxations. Our results are the following.#R##N##R##N#Structural results: We extend the technique of uncrossing, usually applied to families of sets, to families of partitions. As a consequence we show that any basic feasible solution to the partition LP formulation has sparse support. Although the number of variables could be exponential, the number of positive variables is at most the number of terminals.#R##N##R##N#Relations with other relaxations: We show the equivalence of the partition LP relaxation with other known hypergraphic relaxations. We also show that these hypergraphic relaxations are equivalent to the well studied bidirected cut relaxation, if the instance is quasibipartite.#R##N##R##N#Integrality gap upper bounds: We show an upper bound of $\\sqrt{3} \\doteq 1.729$ on the integrality gap of these hypergraph relaxations in general graphs. In the special case of uniformly quasibipartite instances, we show an improved upper bound of 73/60\u22501.216. By our equivalence theorem, the latter result implies an improved upper bound for the bidirected cut relaxation as well."
      },
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 34466,
        "score_0_10": 9,
        "title": "differential invariants under gamma correction",
        "abstract": "This paper presents invariants under gamma correction and similarity transformations. The invariants are local features based on differentials which are implemented using derivatives of the Gaussian. The use of the proposed invariant representation is shown to yield improved correlation results in a template matching scenario."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 88462,
        "score_0_10": 9,
        "title": "a differential invariant for zooming",
        "abstract": "This paper presents an invariant under scaling and linear brightness change. The invariant is based on differentials and therefore is a local feature. Rotationally invariant 2-d differential Gaussian operators up to third order are proposed for the implementation of the invariant. The performance is analyzed by simulating a camera zoom-out."
      },
      {
        "node_idx": 18744,
        "score_0_10": 9,
        "title": "matrix f5 algorithms and tropical grobner bases computation",
        "abstract": "Let K be a field equipped with a valuation. Tropical varieties over K can be defined with a theory of Grobner bases taking into account the valuation of K. Because of the use of the valuation, this theory is promising for stable computations over polynomial rings over a p-adic fields.   We design a strategy to compute such tropical Grobner bases by adapting the Matrix-F5 algorithm. Two variants of the Matrix-F5 algorithm, depending on how the Macaulay matrices are built, are available to tropical computation with respective modifications. The former is more numerically stable while the latter is faster.   Our study is performed both over any exact field with valuation and some inexact fields like Qp or Fq [[t]]. In the latter case, we track the loss in precision, and show that the numerical stability can compare very favorably to the case of classical Grobner bases when the valuation is non-trivial. Numerical examples are provided."
      },
      {
        "node_idx": 37534,
        "score_0_10": 9,
        "title": "on vertex coloring without monochromatic triangles",
        "abstract": "We study a certain relaxation of the classic vertex coloring problem, namely, a coloring of vertices of undirected, simple graphs, such that there are no monochromatic triangles. We give the first classification of the problem in terms of classic and parametrized algorithms. Several computational complexity results are also presented, which improve on the previous results found in the literature. We propose the new structural parameter for undirected, simple graphs -- the triangle-free chromatic number $\\chi_3$. We bound $\\chi_3$ by other known structural parameters. We also present two classes of graphs with interesting coloring properties, that play pivotal role in proving useful observation about our problem. We give/ask several conjectures/questions throughout this paper to encourage new research in the area of graph coloring."
      },
      {
        "node_idx": 23377,
        "score_0_10": 9,
        "title": "on monotone drawings of trees",
        "abstract": "A crossing-free straight-line drawing of a graph is monotone if there is a monotone path between any pair of vertices with respect to some direction. We show how to construct a monotone drawing of a tree with $n$ vertices on an $O(n^{1.5}) \\times O(n^{1.5})$ grid whose angles are close to the best possible angular resolution. Our drawings are convex, that is, if every edge to a leaf is substituted by a ray, the (unbounded) faces form convex regions. It is known that convex drawings are monotone and, in the case of trees, also crossing-free. #R##N#A monotone drawing is strongly monotone if, for every pair of vertices, the direction that witnesses the monotonicity comes from the vector that connects the two vertices. We show that every tree admits a strongly monotone drawing. For biconnected outerplanar graphs, this is easy to see. On the other hand, we present a simply-connected graph that does not have a strongly monotone drawing in any embedding."
      },
      {
        "node_idx": 47387,
        "score_0_10": 9,
        "title": "on self approaching and increasing chord drawings of 3 connected planar graphs",
        "abstract": "An $st$-path in a drawing of a graph is self-approaching if during the traversal of the corresponding curve from $s$ to any point $t'$ on the curve the distance to $t'$ is non-increasing. A path has increasing chords if it is self-approaching in both directions. A drawing is self-approaching (increasing-chord) if any pair of vertices is connected by a self-approaching (increasing-chord) path. #R##N#We study self-approaching and increasing-chord drawings of triangulations and 3-connected planar graphs. We show that in the Euclidean plane, triangulations admit increasing-chord drawings, and for planar 3-trees we can ensure planarity. We prove that strongly monotone (and thus increasing-chord) drawings of trees and binary cactuses require exponential resolution in the worst case, answering an open question by Kindermann et al. [GD'14]. Moreover, we provide a binary cactus that does not admit a self-approaching drawing. Finally, we show that 3-connected planar graphs admit increasing-chord drawings in the hyperbolic plane and characterize the trees that admit such drawings."
      }
    ]
  },
  "303": {
    "explanation": "spatial transformations and pose estimation in vision tasks",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 161063,
        "score_0_10": 10,
        "title": "multi view 3d object detection network for autonomous driving",
        "abstract": "This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods."
      },
      {
        "node_idx": 72516,
        "score_0_10": 9,
        "title": "impact of gate assignment on gate holding departure control strategies",
        "abstract": "Gate holding reduces congestion by reducing the number of aircraft present on the airport surface at any time, while not starving the runway. Because some departing flights are held at gates, there is a possibility that arriving flights cannot access the gates and have to wait until the gates are cleared. This is called a gate conflict. Robust gate assignment is an assignment that minimizes gate conflicts by assigning gates to aircraft to maximize the time gap between two consecutive flights at the same gate; it makes gate assignment robust, but passengers may walk longer to transfer flights. In order to simulate the airport departure process, a queuing model is introduced. The model is calibrated and validated with actual data from New York La Guardia Airport (LGA) and a U.S. hub airport. Then, the model simulates the airport departure process with the current gate assignment and a robust gate assignment to assess the impact of gate assignment on gate-holding departure control. The results show that the robust gate assignment reduces the number of gate conflicts caused by gate holding compared to the current gate assignment. Therefore, robust gate assignment can be combined with gate-holding departure control to improve operations at congested airports with limited gate resources."
      },
      {
        "node_idx": 67267,
        "score_0_10": 9,
        "title": "unsupervised learning of depth and ego motion from video",
        "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings."
      },
      {
        "node_idx": 151058,
        "score_0_10": 9,
        "title": "simulation of pedestrians crossing a street",
        "abstract": "The simulation of vehicular traffic as well as pedestrian dynamics meanwhile both have a decades long history. The success of this conference series, PED and others show that the interest in these topics is still strongly increasing. This contribution deals with a combination of both systems: pedestrians crossing a street. In a VISSIM simulation for varying demand jam sizes of vehicles as well as pedestrians and the travel times of the pedestrians are measured and compared. The study is considered as a study of VISSIM's con ict area functionality as such, as there is no empirical data available to use for calibration issues. Above a vehicle demand threshold the results show a non-monotonic dependence of pedestrians' travel time on pedestrian demand."
      },
      {
        "node_idx": 94920,
        "score_0_10": 9,
        "title": "posenet a convolutional network for real time 6 dof camera relocalization",
        "abstract": "We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples. PoseNet code, dataset and an online demonstration is available on our project webpage, at this http URL"
      },
      {
        "node_idx": 48003,
        "score_0_10": 9,
        "title": "extended range telepresence for evacuation training in pedestrian simulations",
        "abstract": "In this contribution, we propose a new framework to evaluate pedestrian simulations by using Extended Range Telepresence. Telepresence is used as a virtual reality walking simulator, which provides the user with a realistic impression of being present and walking in a virtual environment that is much larger than the real physical environment, in which the user actually walks. The validation of the simulation is performed by comparing motion data of the telepresent user with simulated data at some points of the simulation. The use of haptic feedback from the simulation makes the framework suitable for training in emergency situations."
      },
      {
        "node_idx": 32771,
        "score_0_10": 9,
        "title": "keep it smpl automatic estimation of 3d human pose and shape from a single image",
        "abstract": "We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art."
      },
      {
        "node_idx": 87545,
        "score_0_10": 9,
        "title": "high speed tracking with kernelized correlation filters",
        "abstract": "The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies\u2014any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source."
      },
      {
        "node_idx": 61221,
        "score_0_10": 9,
        "title": "point set registration coherent point drift",
        "abstract": "Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods."
      }
    ]
  },
  "306": {
    "explanation": "adaptive learning rates for robust neural network training",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 45355,
        "score_0_10": 10,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 124702,
        "score_0_10": 8,
        "title": "convolutional sequence to sequence learning",
        "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU."
      },
      {
        "node_idx": 29312,
        "score_0_10": 8,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 116521,
        "score_0_10": 8,
        "title": "recurrent neural network regularization",
        "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation."
      },
      {
        "node_idx": 135057,
        "score_0_10": 8,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 94405,
        "score_0_10": 8,
        "title": "contact force and joint torque estimation using skin",
        "abstract": "In this paper, we present algorithms to estimate external contact forces and joint torques using only skin, i.e. distributed tactile sensors. To deal with gaps between the tactile sensors (taxels), we use interpolation techniques. The application of these interpolation techniques allows us to estimate contact forces and joint torques without the need for expensive force-torque sensors. Validation was performed using the iCub humanoid robot."
      },
      {
        "node_idx": 131036,
        "score_0_10": 8,
        "title": "deep learning with limited numerical precision",
        "abstract": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding."
      }
    ]
  },
  "307": {
    "explanation": "computational complexity and NP-hardness of games and algorithms",
    "topk": [
      {
        "node_idx": 87360,
        "score_0_10": 10,
        "title": "candy crush is np hard",
        "abstract": "We prove that playing Candy Crush to achieve a given score in a fixed number of swaps is NP-hard."
      },
      {
        "node_idx": 154425,
        "score_0_10": 10,
        "title": "complexity limitations on quantum computation",
        "abstract": "We use the powerful tools of counting complexity and generic oracles to help understand the limitations of the complexity of quantum computation. We show several results for the probabilistic quantum class BQP. #R##N#1. BQP is low for PP, i.e., PP^BQP=PP. #R##N#2. There exists a relativized world where P=BQP and the polynomial-time hierarchy is infinite. #R##N#3. There exists a relativized world where BQP does not have complete sets. #R##N#4. There exists a relativized world where P=BQP but P is not equal to UP intersect coUP and one-way functions exist. This gives a relativized answer to an open question of Simon."
      },
      {
        "node_idx": 168168,
        "score_0_10": 10,
        "title": "an elegant argument that p is not np",
        "abstract": "In this note, we present an elegant argument that P is not NP by demonstrating that the Meet-in-the-Middle algorithm must have the fastest running-time of all deterministic and exact algorithms which solve the SUBSET-SUM problem on a classical computer."
      },
      {
        "node_idx": 81796,
        "score_0_10": 10,
        "title": "universal adversarial perturbations",
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
      },
      {
        "node_idx": 143940,
        "score_0_10": 10,
        "title": "phutball endgames are hard",
        "abstract": "We show that, in John Conway's board game Phutball (or Philosopher's Football), it is NP-complete to determine whether the current player has a move that immediately wins the game. In contrast, the similar problems of determining whether there is an immediately winning move in checkers, or a move that kings a man, are both solvable in polynomial time."
      },
      {
        "node_idx": 39461,
        "score_0_10": 10,
        "title": "tetris is hard even to approximate",
        "abstract": "In the popular computer game of Tetris, the player is given a sequence of tetromino pieces and must pack them into a rectangular gameboard initially occupied by a given configuration of filled squares; any completely filled row of the gameboard is cleared and all pieces above it drop by one row. We prove that in the offline version of Tetris, it is NP-complete to maximize the number of cleared rows, maximize the number of tetrises (quadruples of rows simultaneously filled and cleared), minimize the maximum height of an occupied square, or maximize the number of pieces placed before the game ends. We furthermore show the extreme inapproximability of the first and last of these objectives to within a factor of p^(1-epsilon), when given a sequence of p pieces, and the inapproximability of the third objective to within a factor of (2 - epsilon), for any epsilon>0. Our results hold under several variations on the rules of Tetris, including different models of rotation, limitations on player agility, and restricted piece sets."
      },
      {
        "node_idx": 83083,
        "score_0_10": 9,
        "title": "mastermind is np complete",
        "abstract": "In this paper we show that the Mastermind Satisfiability Problem (MSP) is NP-complete. The Mastermind is a popular game which can be turned into a logical puzzle called Mastermind Satisfiability Problem in a similar spirit to the Minesweeper puzzle. By proving that MSP is NP-complete, we reveal its intrinsic computational property that makes it challenging and interesting. This serves as an addition to our knowledge about a host of other puzzles, such as Minesweeper, Mah-Jongg, and the 15-puzzle."
      },
      {
        "node_idx": 71257,
        "score_0_10": 9,
        "title": "the two user gaussian interference channel a deterministic view",
        "abstract": "This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel."
      },
      {
        "node_idx": 67821,
        "score_0_10": 9,
        "title": "kalman meets shannon",
        "abstract": "We consider the problem of communicating the state of a dynamical system via a Shannon Gaussian channel. The receiver, which acts as both a decoder and estimator, observes the noisy measurement of the channel output and makes an optimal estimate of the state of the dynamical system in the minimum mean square sense. The transmitter observes a possibly noisy measurement of the state of the dynamical system. These measurements are then used to encode the message to be transmitted over a noisy Gaussian channel, where a per sample power constraint is imposed on the transmitted message. Thus, we get a mixed problem of Shannon's source-channel coding problem and a sort of Kalman filtering problem. We first consider the problem of communication with full state measurements at the transmitter and show that optimal linear encoders don't need to have memory and the optimal linear decoders have an order of at most that of the state dimension. We also give explicitly the structure of the optimal linear filters. For the case where the transmitter has access to noisy measurements of the state, we derive a separation principle for the optimal communication scheme, where the transmitter needs a filter with an order of at most the dimension of the state of the dynamical system. The results are derived for first order linear dynamical systems, but may be extended to MIMO systems with arbitrary order."
      },
      {
        "node_idx": 67865,
        "score_0_10": 9,
        "title": "key recycling in authentication",
        "abstract": "In their seminal work on authentication, Wegman and Carter propose that to authenticate multiple messages, it is sufficient to reuse the same hash function as long as each tag is encrypted with a one-time pad. They argue that because the one-time pad is perfectly hiding, the hash function used remains completely unknown to the adversary. Since their proof is not composable, we revisit it using a composable security framework. It turns out that the above argument is insufficient: if the adversary learns whether a corrupted message was accepted or rejected, information about the hash function is leaked, and after a bounded finite amount of rounds it is completely known. We show however that this leak is very small: Wegman and Carter's protocol is still    \\( \\varepsilon \\)   -secure, if    \\( \\varepsilon \\)   -almost strongly universal   \\(_2\\)    hash functions are used. This implies that the secret key corresponding to the choice of hash function can be reused in the next round of authentication without any additional error than this    \\( \\varepsilon \\)   . We also show that if the players have a mild form of synchronization, namely that the receiver knows when a message should be received, the key can be recycled for any arbitrary task, not only new rounds of authentication."
      }
    ]
  },
  "309": {
    "explanation": "construction and properties of convolutional and linear error-correcting codes",
    "topk": [
      {
        "node_idx": 131114,
        "score_0_10": 10,
        "title": "convolutional codes techniques of construction",
        "abstract": "In this paper we show how to construct new convolutional codes from old ones by applying the well-known techniques: puncturing, extending, expanding, direct sum, the (u|u + v) construction and the product code construction. By applying these methods, several new families of convolutional codes can be constructed. As an example of code expansion, families of convolutional codes derived from classical Bose- Chaudhuri-Hocquenghem (BCH), character codes and Melas codes are constructed."
      },
      {
        "node_idx": 130575,
        "score_0_10": 10,
        "title": "error trellis state complexity of ldpc convolutional codes based on circulant matrices",
        "abstract": "Let H(D) be the parity-check matrix of an LDPC convolutional code corresponding to the parity-check matrix H of a QC code obtained using the method of Tanner et al. We see that the entries in H(D) are all monomials and several rows (columns) have monomial factors. Let us cyclically shift the rows of H. Then the parity-check matrix H'(D) corresponding to the modified matrix H' defines another convolutional code. However, its free distance is lower-bounded by the minimum distance of the original QC code. Also, each row (column) of H'(D) has a factor different from the one in H(D). We show that the state-space complexity of the error-trellis associated with H'(D) can be significantly reduced by controlling the row shifts applied to H with the error-correction capability being preserved."
      },
      {
        "node_idx": 37717,
        "score_0_10": 10,
        "title": "differential and integral invariants under mobius transformation",
        "abstract": "One of the most challenging problems in the domain of 2-D image or 3-D shape is to handle the non-rigid deformation. From the perspective of transformation groups, the conformal transformation is a key part of the diffeomorphism. According to the Liouville Theorem, an important part of the conformal transformation is the Mobius transformation, so we focus on Mobius transformation and propose two differential expressions that are invariable under 2-D and 3-D Mobius transformation respectively. Next, we analyze the absoluteness and relativity of invariance on them and their components. After that, we propose integral invariants under Mobius transformation based on the two differential expressions. Finally, we propose a conjecture about the structure of differential invariants under conformal transformation according to our observation on the composition of the above two differential invariants."
      },
      {
        "node_idx": 100914,
        "score_0_10": 10,
        "title": "isomorphism between differential and moment invariants under affine transform",
        "abstract": "The invariant is one of central topics in science, technology and engineering. The differential invariant is essential in understanding or describing some important phenomena or procedures in mathematics, physics, chemistry, biology or computer science etc. The derivation of differential invariants is usually difficult or complicated. This paper reports a discovery that under the affine transform, differential invariants have similar structures with moment invariants up to a scalar function of transform parameters. If moment invariants are known, relative differential invariants can be obtained by the substitution of moments by derivatives with the same order. Whereas moment invariants can be calculated by multiple integrals, this method provides a simple way to derive differential invariants without the need to resolve any equation system. Since the definition of moments on different manifolds or in different dimension of spaces is well established, differential invariants on or in them will also be well defined. Considering that moments have a strong background in mathematics and physics, this technique offers a new view angle to the inner structure of invariants. Projective differential invariants can also be found in this way with a screening process."
      },
      {
        "node_idx": 121149,
        "score_0_10": 10,
        "title": "on the macwilliams identity for convolutional codes",
        "abstract": "The adjacency matrix associated with a convolutional code collects in a detailed manner information about the weight distribution of the code. A MacWilliams Identity Conjecture, stating that the adjacency matrix of a code fully determines the adjacency matrix of the dual code, will be formulated, and an explicit formula for the transformation will be stated. The formula involves the MacWilliams matrix known from complete weight enumerators of block codes. The conjecture will be proven for the class of convolutional codes where either the code itself or its dual does not have Forney indices bigger than one. For the general case the conjecture is backed up by many examples, and a weaker version will be established."
      },
      {
        "node_idx": 66398,
        "score_0_10": 10,
        "title": "z4 linear hadamard and extended perfect codes",
        "abstract": "Abstract   If  N  = 2  k  \u2265 16 then there exist exactly \u230a( k  \u2212 1)/2\u230b pairwise nonequivalent  Z  4 -linear Hadamard ( N , 2 N, N /2)-codes and \u230a( k  + 1)/2\u230b pairwise nonequivalent  Z  4 -linear extended perfect ( N , 2  N  /2 N , 4)-codes. Recurrent construction of  Z  4 -linear Hadamard codes is given."
      },
      {
        "node_idx": 41252,
        "score_0_10": 9,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 71804,
        "score_0_10": 9,
        "title": "constacyclic codes over finite fields",
        "abstract": "Abstract   An equivalence relation called isometry is introduced to classify constacyclic codes over a finite field; the polynomial generators of constacyclic codes of length      l    t      p    s      are characterized, where  p  is the characteristic of the finite field and  l  is a prime different from  p ."
      },
      {
        "node_idx": 69221,
        "score_0_10": 9,
        "title": "constacyclic codes over finite principal ideal rings",
        "abstract": "In this paper, we give an important isomorphism between contacyclic codes and cyclic codes over finite principal ideal rings. Necessary and sufficient conditions for the existence of non-trivial cyclic self-dual codes over finite principal ideal rings are given."
      },
      {
        "node_idx": 5225,
        "score_0_10": 9,
        "title": "sixteen new linear codes with plotkin sum",
        "abstract": "Sixteen new linear codes are presented: three of them improve the lower bounds on the minimum distance for a linear code and the rest are an explicit construction of unknown codes attaining the lower bounds on the minimum distance. They are constructed using the Plotkin sum of two linear codes, also called $(u|u+v)$ construction. The computations have been achieved using an exhaustiv search."
      }
    ]
  },
  "311": {
    "explanation": "security vulnerabilities from speculative execution side-channel attacks",
    "topk": [
      {
        "node_idx": 57558,
        "score_0_10": 10,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 4513,
        "score_0_10": 10,
        "title": "beetle swarm optimization algorithm theory and application",
        "abstract": "In this paper, a new meta-heuristic algorithm, called beetle swarm optimization algorithm, is proposed by enhancing the performance of swarm optimization through beetle foraging principles. The performance of 23 benchmark functions is tested and compared with widely used algorithms, including particle swarm optimization algorithm, genetic algorithm (GA) and grasshopper optimization algorithm . Numerical experiments show that the beetle swarm optimization algorithm outperforms its counterparts. Besides, to demonstrate the practical impact of the proposed algorithm, two classic engineering design problems, namely, pressure vessel design problem and himmelblaus optimization problem, are also considered and the proposed beetle swarm optimization algorithm is shown to be competitive in those applications."
      },
      {
        "node_idx": 4112,
        "score_0_10": 9,
        "title": "performance evaluation of impulse radio uwb systems with pulse based polarity randomization",
        "abstract": "In this paper, the performance of a binary phase shift keyed random time-hopping impulse radio system with pulse-based polarity randomization is analyzed. The effects of interframe interference and multiple-access interference on the performance of a generic Rake receiver are investigated for asynchronous systems in frequency-selective environments. A key step is to model the asynchronous system as a chip-synchronous system with uniformly distributed timing jitter for the transmitted pulses of interfering users. This model allows the analytical technique developed for the synchronous case to be extended to the asynchronous case and allows the derivation of closed-form equations for the bit error probability in various Rake receiver architectures. It is shown that a Gaussian approximation can be used for both multiple-access and interframe interference as long as the number of frames per symbols is large (typically, at least 5), whereas there is no minimum requirement for the number of users for the equations to hold. It is observed that under many circumstances, the chip-synchronous case shows a worse bit error probability performance than the asynchronous case; the amount of the difference depends on the autocorrelation function of the ultra-wideband pulse and the signal-to-interference-plus-noise-ratio of the system. Simulations studies support the approximate analysis."
      },
      {
        "node_idx": 106143,
        "score_0_10": 9,
        "title": "dynalog an automated dynamic analysis framework for characterizing android applications",
        "abstract": "Android is becoming ubiquitous and currently has the largest share of the mobile OS market with billions of application downloads from the official app market. It has also become the platform most targeted by mobile malware that are becoming more sophisticated to evade state-of-the-art detection approaches. Many Android malware families employ obfuscation techniques in order to avoid detection and this may defeat static analysis based approaches. Dynamic analysis on the other hand may be used to overcome this limitation. Hence in this paper we propose DynaLog, a dynamic analysis based framework for characterizing Android applications. The framework provides the capability to analyse the behaviour of applications based on an extensive number of dynamic features. It provides an automated platform for mass analysis and characterization of apps that is useful for quickly identifying and isolating malicious applications. The DynaLog framework leverages existing open source tools to extract and log high level behaviours, API calls, and critical events that can be used to explore the characteristics of an application, thus providing an extensible dynamic analysis platform for detecting Android malware. DynaLog is evaluated using real malware samples and clean applications demonstrating its capabilities for effective analysis and detection of malicious applications."
      },
      {
        "node_idx": 94253,
        "score_0_10": 9,
        "title": "estimating the accuracy of the return on investment roi performance evaluations",
        "abstract": "Return on Investment (ROI) is one of the most popular performance measurement and evaluation metrics. ROI analysis (when applied correctly) is a powerful tool in comparing solutions and making informed decisions on the acquisitions of information systems. The ROI sensitivity to error is a natural thought, and common sense suggests that ROI evaluations cannot be absolutely accurate. However, literature review revealed that in most publications and analyst firms reports, this issue is just overlooked. On the one hand, the results of the ROI calculations are implied to be produced with a mathematical rigor, possibility of errors is not mentioned and amount of errors is not estimated. On the contrary, another approach claims ROI evaluations to be absolutely inaccurate because, in view of their authors, future benefits (especially, intangible) cannot be estimated within any reasonable boundaries. The purpose of this study is to provide a systematic research of the accuracy of the ROI evaluations in the context of the information systems implementations. The main contribution of the study is that this is the first systematic effort to evaluate ROI accuracy. Analytical expressions have been derived for estimating errors of the ROI evaluations. Results of the Monte Carlo simulation will help practitioners in making informed decisions based on explicitly stated factors influencing the ROI uncertainties. The results of this research are intended for researchers in information systems, technology solutions and business management, and also for information specialists, project managers, program managers, technology directors, and information systems evaluators. Most results are applicable to ROI evaluations in a wider subject area."
      },
      {
        "node_idx": 17415,
        "score_0_10": 9,
        "title": "understanding the affect of developers theoretical background and guidelines for psychoempirical software engineering",
        "abstract": "Affects--emotions and moods--have an impact on cognitive processing activities and the working performance of individuals. It has been established that software development tasks are undertaken through cognitive processing activities. Therefore, we have proposed to employ psychology theory and measurements in software engineering (SE) research. We have called it \"psychoempirical software engineering\". However, we found out that existing SE research has often fallen into misconceptions about the affect of developers, lacking in background theory and how to successfully employ psychological measurements in studies. The contribution of this paper is threefold. (1) It highlights the challenges to conduct proper affect-related studies with psychology; (2) it provides a comprehensive literature review in affect theory; and (3) it proposes guidelines for conducting psychoempirical software engineering."
      },
      {
        "node_idx": 96990,
        "score_0_10": 9,
        "title": "what happens when software developers are un happy",
        "abstract": "Abstract   The growing literature on affect among software developers mostly reports on the linkage between happiness, software quality, and developer productivity. Understanding happiness and unhappiness in all its components \u2013 positive and negative emotions and moods \u2013 is an attractive and important endeavor. Scholars in industrial and organizational psychology have suggested that understanding happiness and unhappiness could lead to cost-effective ways of enhancing working conditions, job performance, and to limiting the occurrence of psychological disorders. Our comprehension of the consequences of (un)happiness among developers is still too shallow, being mainly expressed in terms of development productivity and software quality. In this paper, we study what happens when developers are happy and unhappy while developing software. Qualitative data analysis of responses given by 317 questionnaire participants identified 42 consequences of unhappiness and 32 of happiness. We found consequences of happiness and unhappiness that are beneficial and detrimental for developers\u2019 mental well-being, the software development process, and the produced artifacts. Our classification scheme, available as open data enables new happiness research opportunities of cause-effect type, and it can act as a guideline for practitioners for identifying damaging effects of unhappiness and for fostering happiness on the job."
      },
      {
        "node_idx": 155003,
        "score_0_10": 9,
        "title": "happy software developers solve problems better psychological measurements in empirical software engineering",
        "abstract": "For more than thirty years, it has been claimed that a way to improve software developers\u2019 productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states\u2014emotions and moods\u2014deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint."
      },
      {
        "node_idx": 64247,
        "score_0_10": 9,
        "title": "bsas beetle swarm antennae search algorithm for optimization problems",
        "abstract": "Beetle antennae search (BAS) is an efficient meta-heuristic algorithm. However, the convergent results of BAS rely heavily on the random beetle direction in every iterations. More specifically, different random seeds may cause different optimized results. Besides, the step-size update algorithm of BAS cannot guarantee objective become smaller in iterative process. In order to solve these problems, this paper proposes Beetle Swarm Antennae Search Algorithm (BSAS) which combines swarm intelligence algorithm with feedback-based step-size update strategy. BSAS employs k beetles to find more optimal position in each moving rather than one beetle. The step-size updates only when k beetles return without better choices. Experiments are carried out on building system identification. The results reveal the efficacy of the BSAS algorithm to avoid influence of random direction of Beetle. In addition, the estimation errors decrease as the beetles number goes up."
      },
      {
        "node_idx": 65448,
        "score_0_10": 9,
        "title": "beetle antennae search without parameter tuning bas wpt for multi objective optimization",
        "abstract": "Beetle antennae search (BAS) is an efficient meta-heuristic algorithm inspired by foraging behaviors of beetles. This algorithm includes several parameters for tuning and the existing results are limited to solve single objective optimization. This work pushes forward the research on BAS by providing one variant that releases the tuning parameters and is able to handle multi-objective optimization. This new approach applies normalization to simplify the original algorithm and uses a penalty function to exploit infeasible solutions with low constraint violation to solve the constraint optimization problem. Extensive experimental studies are carried out and the results reveal efficacy of the proposed approach to constraint handling."
      }
    ]
  },
  "312": {
    "explanation": "wireless communication coding, capacity scaling, and energy harvesting optimization",
    "topk": [
      {
        "node_idx": 19635,
        "score_0_10": 10,
        "title": "approximately universal codes over slow fading channels",
        "abstract": "Performance of reliable communication over a coherent slow fading channel at high SNR is succinctly captured as a fundamental tradeoff between diversity and multiplexing gains. We study the problem of designing codes that optimally tradeoff the diversity and multiplexing gains. Our main contribution is a precise characterization of codes that are universally tradeoff-optimal, i.e., they optimally tradeoff the diversity and multiplexing gains for every statistical characterization of the fading channel. We denote this characterization as one of approximate universality where the approximation is in the connection between error probability and outage capacity with diversity and multiplexing gains, respectively. The characterization of approximate universality is then used to construct new coding schemes as well as to show optimality of several schemes proposed in the space-time coding literature."
      },
      {
        "node_idx": 46136,
        "score_0_10": 9,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      },
      {
        "node_idx": 100857,
        "score_0_10": 9,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 41252,
        "score_0_10": 9,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 16140,
        "score_0_10": 9,
        "title": "modeling and analyzing millimeter wave cellular systems",
        "abstract": "We provide a comprehensive overview of mathematical models and analytical techniques for millimeter wave (mmWave) cellular systems. The two fundamental physical differences from conventional sub-6-GHz cellular systems are: 1) vulnerability to blocking and 2) the need for significant directionality at the transmitter and/or receiver, which is achieved through the use of large antenna arrays of small individual elements. We overview and compare models for both of these factors, and present a baseline analytical approach based on stochastic geometry that allows the computation of the statistical distributions of the downlink signal-to-interference-plus-noise ratio (SINR) and also the per link data rate, which depends on the SINR as well as the average load. There are many implications of the models and analysis: 1) mmWave systems are significantly more noise-limited than at sub-6 GHz for most parameter configurations; 2) initial access is much more difficult in mmWave; 3) self-backhauling is more viable than in sub-6-GHz systems, which makes ultra-dense deployments more viable, but this leads to increasingly interference-limited behavior; and 4) in sharp contrast to sub-6-GHz systems cellular operators can mutually benefit by sharing their spectrum licenses despite the uncontrolled interference that results from doing so. We conclude by outlining several important extensions of the baseline model, many of which are promising avenues for future research."
      },
      {
        "node_idx": 127804,
        "score_0_10": 9,
        "title": "polarization for arbitrary discrete memoryless channels",
        "abstract": "Channel polarization, originally proposed for binary-input channels, is generalized to arbitrary discrete memoryless channels. Specifically, it is shown that when the input alphabet size is a prime number, a similar construction to that for the binary case leads to polarization. This method can be extended to channels of composite input alphabet sizes by decomposing such channels into a set of channels with prime input alphabet sizes. It is also shown that all discrete memoryless channels can be polarized by randomized constructions. The introduction of randomness does not change the order of complexity of polar code construction, encoding, and decoding. A previous result on the error probability behavior of polar codes is also extended to the case of arbitrary discrete memoryless channels. The generalization of polarization to channels with arbitrary finite input alphabet sizes leads to polar-coding methods for approaching the true (as opposed to symmetric) channel capacity of arbitrary channels with discrete or continuous input alphabets."
      },
      {
        "node_idx": 74729,
        "score_0_10": 9,
        "title": "a survey on non orthogonal multiple access for 5g networks research challenges and future trends",
        "abstract": "Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed."
      },
      {
        "node_idx": 105819,
        "score_0_10": 9,
        "title": "efficient low redundancy codes for correcting multiple deletions",
        "abstract": "We consider the problem of constructing binary codes to recover from $k$-bit deletions with efficient encoding/decoding, for a fixed $k$. The single deletion case is well understood, with the Varshamov-Tenengolts-Levenshtein code from 1965 giving an asymptotically optimal construction with $\\approx 2^n/n$ codewords of length $n$, i.e., at most $\\log n$ bits of redundancy. However, even for the case of two deletions, there was no known explicit construction with redundancy less than $n^{\\Omega(1)}$. #R##N#For any fixed $k$, we construct a binary code with $c_k \\log n$ redundancy that can be decoded from $k$ deletions in $O_k(n \\log^4 n)$ time. The coefficient $c_k$ can be taken to be $O(k^2 \\log k)$, which is only quadratically worse than the optimal, non-constructive bound of $O(k)$. We also indicate how to modify this code to allow for a combination of up to $k$ insertions and deletions. #R##N#We also note that among *linear* codes capable of correcting $k$ deletions, the $(k+1)$-fold repetition code is essentially the best possible."
      },
      {
        "node_idx": 79165,
        "score_0_10": 8,
        "title": "optimal beamforming for two way multi antenna relay channel with analogue network coding",
        "abstract": "This paper studies the wireless two-way relay channel (TWRC), where two source nodes, S1 and S2, exchange information through an assisting relay node, R. It is assumed that R receives the sum signal from S1 and S2 in one time-slot, and then amplifies and forwards the received signal to both S1 and S2 in the next time-slot. By applying the principle of analogue network (ANC), each of S1 and S2 cancels the so-called \"self-interference\" in the received signal from R and then decodes the desired message. Assuming that S1 and S2 are each equipped with a single antenna and R with multi-antennas, this paper analyzes the capacity region of an ANC-based TWRC with linear processing (beamforming) at R. The capacity region contains all the achievable bidirectional rate-pairs of S1 and S2 under the given transmit power constraints at S1, S2, and R. We present the optimal relay beamforming structure as well as an efficient algorithm to compute the optimal beamforming matrix based on convex optimization techniques. Low-complexity suboptimal relay beamforming schemes are also presented, and their achievable rates are compared against the capacity with the optimal scheme."
      },
      {
        "node_idx": 28821,
        "score_0_10": 8,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      }
    ]
  },
  "319": {
    "explanation": "improving neural network training and generalization techniques",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 88803,
        "score_0_10": 10,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 43354,
        "score_0_10": 9,
        "title": "neural turing machines",
        "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 155778,
        "score_0_10": 8,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 106114,
        "score_0_10": 8,
        "title": "matching networks for one shot learning",
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
      },
      {
        "node_idx": 148543,
        "score_0_10": 8,
        "title": "fast and accurate deep network learning by exponential linear units elus",
        "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      }
    ]
  },
  "320": {
    "explanation": "efficient and interpretable neural network text classification and entity recognition",
    "topk": [
      {
        "node_idx": 124619,
        "score_0_10": 10,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 153102,
        "score_0_10": 9,
        "title": "embedding entities and relations for learning and inference in knowledge bases",
        "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning."
      },
      {
        "node_idx": 26180,
        "score_0_10": 9,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 112674,
        "score_0_10": 9,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 140427,
        "score_0_10": 8,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 63929,
        "score_0_10": 8,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 78341,
        "score_0_10": 8,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 67704,
        "score_0_10": 8,
        "title": "knightian robustness of the vickrey mechanism",
        "abstract": "We investigate the resilience of some classical mechanisms to alternative specifications of preferences and information structures. Specifically, we analyze the Vickrey mechanism for auctions of multiple identical goods when the only information a player $i$ has about the profile of true valuations, $\\theta^*$, consists of a set of distributions, from one of which $\\theta_i^*$ has been drawn. #R##N#In this setting, the players no longer have complete preferences, and the Vickrey mechanism is no longer dominant-strategy. However, we prove that its efficiency performance is excellent, and essentially optimal, in undominated strategies."
      },
      {
        "node_idx": 98234,
        "score_0_10": 8,
        "title": "the mythos of model interpretability",
        "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not."
      }
    ]
  },
  "322": {
    "explanation": "research trends and metrics in educational technology and bibliometrics",
    "topk": [
      {
        "node_idx": 114324,
        "score_0_10": 10,
        "title": "learning analytics in massive open online courses",
        "abstract": "Educational technology has obtained great importance over the last fifteen years. At present, the umbrella of educational technology incorporates multitudes of engaging online environments and fields. Learning analytics and Massive Open Online Courses (MOOCs) are two of the most relevant emerging topics in this domain. Since they are open to everyone at no cost, MOOCs excel in attracting numerous participants that can reach hundreds and hundreds of thousands. Experts from different disciplines have shown significant interest in MOOCs as the phenomenon has rapidly grown. In fact, MOOCs have been proven to scale education in disparate areas. Their benefits are crystallized in the improvement of educational outcomes, reduction of costs and accessibility expansion. Due to their unusual massiveness, the large datasets of MOOC platforms require advanced tools and methodologies for further examination. The key importance of learning analytics is reflected here. MOOCs offer diverse challenges and practices for learning analytics to tackle. In view of that, this thesis combines both fields in order to investigate further steps in the learning analytics capabilities in MOOCs. The primary research of this dissertation focuses on the integration of learning analytics in MOOCs, and thereafter looks into examining students' behavior on one side and bridging MOOC issues on the other side. The research was done on the Austrian iMooX xMOOC platform. We followed the prototyping and case studies research methodology to carry out the research questions of this dissertation. The main contributions incorporate designing a general learning analytics framework, learning analytics prototype, records of students' behavior in nearly every MOOC's variables (discussion forums, interactions in videos, self-assessment quizzes, login frequency), a cluster of student engagement..."
      },
      {
        "node_idx": 36059,
        "score_0_10": 10,
        "title": "engaging learning analytics in moocs the good the bad and the ugly",
        "abstract": "Learning Analytics is an emerging field in the vast areas of Educational Technology and Technology Enhanced Learning (TEL). It provides tools and techniques that offer researchers the ability to analyze, study, and benchmark institutions, learners and teachers as well as online learning environments such as MOOCs. Massive Open Online Courses (MOOCs) are considered to be a very active and an innovative form of bringing educational content to a broad community. Due to the reasons of being free and accessible to the public, MOOCs attracted a large number of heterogeneous learners who differ in education level, gender, and age. However, there are pressing demands to adjust the quality of the hosted courses, as well as controlling the high dropout ratio and the lack of interaction. With the help of Learning Analytics, it is possible to contain such issues. In this publication, we discuss the principles of engaging Learning Analytics in MOOCs learning environments and review its potential and capabilities (the good), constraints (the bad), and fallacy analytics (the ugly) based on our experience in last years."
      },
      {
        "node_idx": 40714,
        "score_0_10": 10,
        "title": "tweeting biomedicine an analysis of tweets and citations in the biomedical literature",
        "abstract": "Data collected by social media platforms have recently been introduced as a new source for indicators to help measure the impact of scholarly research in ways that are complementary to traditional citation-based indicators. Data generated from social media activities related to scholarly content can be used to reflect broad types of impact. This paper aims to provide systematic evidence regarding how often Twitter is used to diffuse journal articles in the biomedical and life sciences. The analysis is based on a set of 1.4 million documents covered by both PubMed and Web of Science (WoS) and published between 2010 and 2012. The number of tweets containing links to these documents was analyzed to evaluate the degree to which certain journals, disciplines, and specialties were represented on Twitter. It is shown that, with less than 10% of PubMed articles mentioned on Twitter, its uptake is low in general. The relationship between tweets and WoS citations was examined for each document at the level of journals and specialties. The results show that tweeting behavior varies between journals and specialties and correlations between tweets and citations are low, implying that impact metrics based on tweets are different from those based on citations. A framework utilizing the coverage of articles and the correlation between Twitter mentions and citations is proposed to facilitate the evaluation of novel social-media based metrics and to shed light on the question in how far the number of tweets is a valid metric to measure research impact."
      },
      {
        "node_idx": 70846,
        "score_0_10": 9,
        "title": "finding trends in software research",
        "abstract": "This paper explores the structure of research papers in software engineering. Using text mining, we study 35,391 software engineering (SE) papers from 34 leading SE venues over the last 25 years. These venues were divided, nearly evenly, between conferences and journals. An important aspect of this analysis is that it is fully automated and repeatable. To achieve that automation, we used topic modeling (with LDA) to mine 10 topics that represent much of the structure of contemporary SE. The 10 topics presented here should not be \"set in stone\" as the only topics worthy of study in SE. Rather our goal is to report that (a) text mining methods can detect large scale trends within our community; (b) those topic change with time; so (c) it is important to have automatic agents that can update our understanding of our community whenever new data arrives."
      },
      {
        "node_idx": 42623,
        "score_0_10": 9,
        "title": "the emerging field of signal processing on graphs extending high dimensional data analysis to networks and other irregular domains",
        "abstract": "In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogs to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions."
      },
      {
        "node_idx": 166900,
        "score_0_10": 9,
        "title": "exploring the academic invisible web",
        "abstract": "Purpose \u2013 The purpose of this article is to provide a critical review of Bergman's study on the deep web. In addition, this study brings a new concept into the discussion, the academic invisible web (AIW). The paper defines the academic invisible web as consisting of all databases and collections relevant to academia but not searchable by the general\u2010purpose internet search engines. Indexing this part of the invisible web is central to scientific search engines. This paper provides an overview of approaches followed thus far.Design/methodology/approach \u2013 Provides a discussion of measures and calculations, estimation based on informetric laws. Also gives a literature review on approaches for uncovering information from the invisible web.Findings \u2013 Bergman's size estimate of the invisible web is highly questionable. This paper demonstrates some major errors in the conceptual design of the Bergman paper. A new (raw) size estimate is given.Research limitations/implications \u2013 The precision of this estimate is ..."
      },
      {
        "node_idx": 129769,
        "score_0_10": 9,
        "title": "how do you define and measure research productivity",
        "abstract": "Productivity is the quintessential indicator of efficiency in any production system. It seems it has become a norm in bibliometrics to define research productivity as the number of publications per researcher, distinguishing it from impact. In this work we operationalize the economic concept of productivity for the specific context of research activity and show the limits of the commonly accepted definition. We propose then a measurable form of research productivity through the indicator \"Fractional Scientific Strength (FSS)\", in keeping with the microeconomic theory of production. We present the methodology for measure of FSS at various levels of analysis: individual, field, discipline, department, institution, region and nation. Finally, we compare the ranking lists of Italian universities by the two definitions of research productivity."
      },
      {
        "node_idx": 119139,
        "score_0_10": 9,
        "title": "how well developed are altmetrics a cross disciplinary analysis of the presence of alternative metrics in scientific publications",
        "abstract": "In this paper an analysis of the presence and possibilities of altmetrics for bibliometric and performance analysis is carried out. Using the web based tool Impact Story, we collected metrics for 20,000 random publications from the Web of Science. We studied both the presence and distribution of altmetrics in the set of publications, across fields, document types and over publication years, as well as the extent to which altmetrics correlate with citation indicators. The main result of the study is that the altmetrics source that provides the most metrics is Mendeley, with metrics on readerships for 62.6 % of all the publications studied, other sources only provide marginal information. In terms of relation with citations, a moderate spearman correlation (r = 0.49) has been found between Mendeley readership counts and citation indicators. Other possibilities and limitations of these indicators are discussed and future research lines are outlined."
      },
      {
        "node_idx": 79898,
        "score_0_10": 9,
        "title": "representation learning on graphs methods and applications",
        "abstract": "Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work."
      },
      {
        "node_idx": 3504,
        "score_0_10": 9,
        "title": "the use of percentiles and percentile rank classes in the analysis of bibliometric data opportunities and limits",
        "abstract": "Percentiles have been established in bibliometrics as an important alternative to mean-based indicators for obtaining a normalized citation impact of publications. Percentiles have a number of advantages over standard bibliometric indicators used frequently: for example, their calculation is not based on the arithmetic mean which should not be used for skewed bibliometric data. This study describes the opportunities and limits and the advantages and disadvantages of using percentiles in bibliometrics. We also address problems in the calculation of percentiles and percentile rank classes for which there is not (yet) a satisfactory solution. It will be hard to compare the results of different percentile-based studies with each other unless it is clear that the studies were done with the same choices for percentile calculation and rank assignment."
      }
    ]
  },
  "323": {
    "explanation": "coding theory and error correction in Reed-Solomon and network codes",
    "topk": [
      {
        "node_idx": 94809,
        "score_0_10": 10,
        "title": "deep holes and mds extensions of reed solomon codes",
        "abstract": "We study the problem of classifying deep holes of Reed-Solomon codes. We show that this problem is equivalent to the problem of classifying MDS extensions of Reed-Solomon codes by one digit. This equivalence allows us to improve recent results on the former problem. In particular, we classify deep holes of Reed-Solomon codes of dimension greater than half the alphabet size. #R##N#We also give a complete classification of deep holes of Reed Solomon codes with redundancy three in all dimensions."
      },
      {
        "node_idx": 127804,
        "score_0_10": 10,
        "title": "polarization for arbitrary discrete memoryless channels",
        "abstract": "Channel polarization, originally proposed for binary-input channels, is generalized to arbitrary discrete memoryless channels. Specifically, it is shown that when the input alphabet size is a prime number, a similar construction to that for the binary case leads to polarization. This method can be extended to channels of composite input alphabet sizes by decomposing such channels into a set of channels with prime input alphabet sizes. It is also shown that all discrete memoryless channels can be polarized by randomized constructions. The introduction of randomness does not change the order of complexity of polar code construction, encoding, and decoding. A previous result on the error probability behavior of polar codes is also extended to the case of arbitrary discrete memoryless channels. The generalization of polarization to channels with arbitrary finite input alphabet sizes leads to polar-coding methods for approaching the true (as opposed to symmetric) channel capacity of arbitrary channels with discrete or continuous input alphabets."
      },
      {
        "node_idx": 143193,
        "score_0_10": 10,
        "title": "on deep holes of projective reed solomon codes",
        "abstract": "In this paper, we obtain new results on the covering radius and deep holes for projective Reed-Solomon (PRS) codes."
      },
      {
        "node_idx": 20843,
        "score_0_10": 10,
        "title": "computational geometry column 43",
        "abstract": "The concept of pointed pseudo-triangulations is defined and a few of its applications described."
      },
      {
        "node_idx": 29194,
        "score_0_10": 10,
        "title": "fuchsian codes for awgn channels",
        "abstract": "We develop a new transmission scheme for additive white Gaussian noisy (AWGN) single-input single-output (SISO) channels without fading based on arithmetic Fuchsian groups. The properly discontinuous character of the action of these groups on the upper half-plane translates into logarithmic decoding complexity."
      },
      {
        "node_idx": 76464,
        "score_0_10": 9,
        "title": "a rewritable random access dna based storage system",
        "abstract": "We describe the first DNA-based storage architecture that enables random access to data blocks and rewriting of information stored at arbitrary locations within the blocks. The newly developed architecture overcomes drawbacks of existing read-only methods that require decoding the whole file in order to read one data fragment. Our system is based on new constrained coding techniques and accompanying DNA editing methods that ensure data reliability, specificity and sensitivity of access, and at the same time provide exceptionally high data storage capacity. As a proof of concept, we encoded parts of the Wikipedia pages of six universities in the USA, and selected and edited parts of the text written in DNA corresponding to three of these schools. The results suggest that DNA is a versatile media suitable for both ultrahigh density archival and rewritable storage applications."
      },
      {
        "node_idx": 41252,
        "score_0_10": 9,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 77887,
        "score_0_10": 9,
        "title": "maximum likelihood decoding of reed solomon codes is np hard",
        "abstract": "Maximum-likelihood decoding is one of the central algorithmic problems in coding theory. It has been known for over 25 years that maximum-likelihood decoding of general linear codes is NP-hard. Nevertheless, it was so far unknown whether maximum- likelihood decoding remains hard for any specific family of codes with nontrivial algebraic structure. In this paper, we prove that maximum-likelihood decoding is NP-hard for the family of Reed-Solomon codes. We moreover show that maximum-likelihood decoding of Reed-Solomon codes remains hard even with unlimited preprocessing, thereby strengthening a result of Bruck and Naor."
      },
      {
        "node_idx": 56248,
        "score_0_10": 9,
        "title": "explicit deep holes of reed solomon codes",
        "abstract": "In this paper, deep holes of Reed-Solomon (RS) codes are studied. A new class of deep holes for generalized affine RS codes is given if the evaluation set satisfies certain combinatorial structure. Three classes of deep holes for projective Reed-Solomon (PRS) codes are constructed explicitly. In particular, deep holes of PRS codes with redundancy three are completely obtained when the characteristic of the finite field is odd. Most (asymptotically of ratio $1$) of the deep holes of PRS codes with redundancy four are also obtained."
      },
      {
        "node_idx": 78813,
        "score_0_10": 9,
        "title": "maximum width empty square and rectangular annulus",
        "abstract": "An annulus is, informally, a ring-shaped region, often described by two concentric circles. The maximum-width empty annulus problem asks to find an annulus of a certain shape with the maximum possible width that avoids a given set of $n$ points in the plane. This problem can also be interpreted as the problem of finding an optimal location of a ring-shaped obnoxious facility among the input points. In this paper, we study square and rectangular variants of the maximum-width empty anuulus problem, and present first nontrivial algorithms. Specifically, our algorithms run in $O(n^3)$ and $O(n^2 \\log n)$ time for computing a maximum-width empty axis-parallel square and rectangular annulus, respectively. Both algorithms use only $O(n)$ space."
      }
    ]
  },
  "324": {
    "explanation": "sensor calibration and alignment using magnetometer and inertial data",
    "topk": [
      {
        "node_idx": 127370,
        "score_0_10": 10,
        "title": "gyroscope calibration via magnetometer",
        "abstract": "Magnetometers, gyroscopes and accelerometers are commonly used sensors in a variety of applications. The paper proposes a novel gyroscope calibration method in the homogeneous magnetic field by the help of magnetometer. It is shown that, with sufficient rotation excitation, the homogeneous magnetic field vector can be exploited to serve as a good reference for calibrating low-cost gyroscopes. The calibration parameters include the gyroscope scale factor, non-orthogonal coefficient and bias for three axes, as well as its misalignment to the magnetometer frame. Simulation and field test results demonstrate the method's effectiveness."
      },
      {
        "node_idx": 166932,
        "score_0_10": 10,
        "title": "on calibration of three axis magnetometer",
        "abstract": "Magnetometer has received wide applications in attitude determination and scientific measurements. Calibration is an important step for any practical magnetometer use. The most popular three-axis magnetometer calibration methods are attitude-independent and have been founded on an approximate maximum likelihood (ML) estimation with a quartic subjective function, derived from the fact that the magnitude of the calibrated measurements should be constant in a homogeneous magnetic field. This paper highlights the shortcomings of those popular methods and proposes to use the quadratic optimal ML estimation instead for magnetometer calibration. The simulation and test results show that the optimal ML calibration is superior to the approximate ML methods for magnetometer calibration in both accuracy and stability, especially for those situations without sufficient attitude excitation. The significant benefits deserve the moderately increased computation burden. The main conclusion obtained in the context of magnetometer in this paper is potentially applicable to various kinds of three-axis sensors."
      },
      {
        "node_idx": 164205,
        "score_0_10": 10,
        "title": "decision theoretic planning structural assumptions and computational leverage",
        "abstract": "Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory.#R##N##R##N#This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations.#R##N##R##N#Specialized representations, and algorithms employing these representations, can achieve computational leverage by exploiting these various forms of structure. Certain AI techniques-- in particular those based on the use of structured, intensional representations--can be viewed in this way. This paper surveys several types of representations for both classical and decision-theoretic planning problems, and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing policies or plans. It focuses primarily on abstraction, aggregation and decomposition techniques based on AI-style representations."
      },
      {
        "node_idx": 166919,
        "score_0_10": 10,
        "title": "on non monotonic conditional reasoning",
        "abstract": "This note is concerned with a formal analysis of the problem of non-monotonic reasoning in intelligent systems, especially when the uncertainty is taken into account in a quantitative way. A firm connection between logic and probability is established by introducing conditioning notions by means of formal structures that do not rely on quantitative measures. The associated conditional logic, compatible with conditional probability evaluations, is non-monotonic relative to additional evidence. Computational aspects of conditional probability logic are mentioned. The importance of this development lies on its role to provide a conceptual basis for various forms of evidence combination and on its significance to unify multi-valued and non-monotonic logics"
      },
      {
        "node_idx": 38173,
        "score_0_10": 10,
        "title": "clausal temporal resolution",
        "abstract": "In this article, we examine how clausal resolution can be applied to a specific, but widely used, non-classical logic, namely discrete linear temporal logic. Thus, we first define a normal form for temporal formulae and show how arbitrary temporal formulae can be translated into the normal form, while preserving satisfiability. We then introduce novel resolution rules that can be applied to formulae in this normal form, provide a range of examples and examine the correctness and complexity of this approach is examined and. This clausal resolution approach. Finally, we describe related work and future developments concerning this work."
      },
      {
        "node_idx": 87336,
        "score_0_10": 10,
        "title": "2 player nash and nonsymmetric bargaining via flexible budget markets",
        "abstract": "The solution to a Nash or a nonsymmetric bargaining game is obtained by maximizing a concave function over a convex set, i.e., it is the solution to a convex program. We show that each 2-player game whose convex program has linear constraints, admits a rational solution and such a solution can be found in polynomial time using only an LP solver. If in addition, the game is succinct, i.e., the coefficients in its convex program are ``small'', then its solution can be found in strongly polynomial time. We also give a non-succinct linear game whose solution can be found in strongly polynomial time. The notion of flexible budget markets, introduced in \\cite{va.NB}, plays a crucial role in the design of these algorithms."
      },
      {
        "node_idx": 33657,
        "score_0_10": 10,
        "title": "learning dexterous in hand manipulation",
        "abstract": "We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: this https URL"
      },
      {
        "node_idx": 21414,
        "score_0_10": 9,
        "title": "a type directed negation elimination",
        "abstract": "In the modal mu-calculus, a formula is well-formed if each recursive variable occurs underneath an even number of negations. By means of De Morgan's laws, it is easy to transform any well-formed formula into an equivalent formula without negations \u00e2\u0080\u0093 its negation normal form. Moreover, if the formula is of size n, its negation normal form of is of the same size O(n). The full modal mu-calculus and the negation normal form fragment are thus equally expressive and concise. #R##N#In this paper we extend this result to the higher-order modal fixed point logic (HFL), an extension of the modal mu-calculus with higher-order recursive predicate transformers. We present a procedure that converts a formula into an equivalent formula without negations of quadratic size in the worst case and of linear size when the number of variables of the formula is fixed."
      },
      {
        "node_idx": 62354,
        "score_0_10": 9,
        "title": "zero determinant strategies in iterated multi strategy games",
        "abstract": "Self-serving, rational agents sometimes cooperate to their mutual benefit. The two-player iterated prisoner's dilemma game is a model for including the emergence of cooperation. It is generally believed that there is no simple ultimatum strategy which a player can control the return of the other participants. The recent discovery of the powerful class of zero-determinant strategies in the iterated prisoner's dilemma dramatically expands our understanding of the classic game by uncovering strategies that provide a unilateral advantage to sentient players pitted against unwitting opponents. However, strategies in the prisoner's dilemma game are only two strategies. Are there these results for general multi-strategy games? To address this question, the paper develops a theory for zero-determinant strategies for multi-strategy games, with any number of strategies. The analytical results exhibit a similar yet different scenario to the case of two-strategy games. Zero-determinant strategies in iterated prisoner's dilemma can be seen as degenerate case of our results. The results are also applied to the snowdrift game, the hawk-dove game and the chicken game."
      },
      {
        "node_idx": 16751,
        "score_0_10": 9,
        "title": "dynamic magnetometer calibration and alignment to inertial sensors by kalman filtering",
        "abstract": "Magnetometer and inertial sensors are widely used for orientation estimation. Magnetometer usage is often troublesome, as it is prone to be interfered by onboard or ambient magnetic disturbance. The onboard soft-iron material distorts not only the magnetic field, but the magnetometer sensor frame coordinate and the cross-sensor misalignment relative to inertial sensors. It is desirable to conveniently put magnetic and inertial sensors information in a common frame. Existing methods either split the problem into successive intrinsic and cross-sensor calibrations, or rely on stationary accelerometer measurements which is infeasible in dynamic conditions. This paper formulates the magnetometer calibration and alignment to inertial sensors as a state estimation problem, and collectively solves the magnetometer intrinsic and cross-sensor calibrations, as well as the gyroscope bias estimation. Sufficient conditions are derived for the problem to be globally observable, even when no accelerometer information is used at all. An extended Kalman filter is designed to implement the state estimation and comprehensive test data results show the superior performance of the proposed approach. It is immune to acceleration disturbance and applicable potentially in any dynamic conditions."
      }
    ]
  },
  "325": {
    "explanation": "resource allocation and scheduling in distributed cloud and grid systems",
    "topk": [
      {
        "node_idx": 145706,
        "score_0_10": 10,
        "title": "an auction driven self organizing cloud delivery model",
        "abstract": "The three traditional cloud delivery models -- IaaS, PaaS, and SaaS -- constrain access to cloud resources by hiding their raw functionality and forcing us to use them indirectly via a restricted set of actions. Can we introduce a new delivery model, and, at the same time, support improved security, a higher degree of assurance, find relatively simple solutions to the hard cloud resource management problems, eliminate some of the inefficiencies related to resource virtualization, allow the assembly of clouds of clouds, and, last but not least, minimize the number of interoperability standards? #R##N#We sketch a self-organizing architecture for very large compute clouds composed of many-core processors and heterogeneous coprocessors. We discuss how self-organization will address each of the challenges described above. The approach is {\\em bid-centric}. The system of heterogeneous cloud resources is dynamically, and autonomically, configured to bid to meet the needs identified in a high-level task or service specification. When the task is completed, or the service is retired, the resources are released for subsequent reuse. #R##N#Our approach mimics the process followed by individual researchers who, in response to a call for proposals released by a funding agency, organize themselves in groups of various sizes and specialities. If the bid is successful, then the group carries out the proposed work and releases the results. After the work is completed, individual researchers in the group disperse, possibly joining other groups or submitting individual bids in response to other proposals. Similar protocols are common to other human activities such as procurement management."
      },
      {
        "node_idx": 129115,
        "score_0_10": 10,
        "title": "gridsim a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing",
        "abstract": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular paradigms for next generation parallel and distributed computing. The management of resources and scheduling of applications in such large-scale distributed systems is a complex undertaking. In order to prove the effectiveness of resource brokers and associated scheduling algorithms, their performance needs to be evaluated under different scenarios such as varying number of resources and users with different requirements. In a grid environment, it is hard and even impossible to perform scheduler performance evaluation in a repeatable and controllable manner as resources and users are distributed across multiple organizations with their own policies. To overcome this limitation, we have developed a Java-based discrete-event grid simulation toolkit called GridSim. The toolkit supports modeling and simulation of heterogeneous grid resources (both time- and space-shared), users and application models. It provides primitives for creation of application tasks, mapping of tasks to resources, and their management. To demonstrate suitability of the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker and evaluated the performance of deadline and budget constrained cost- and time-minimization scheduling algorithms."
      },
      {
        "node_idx": 103632,
        "score_0_10": 10,
        "title": "win prediction in esports mixed rank match prediction in multi player online battle arena games",
        "abstract": "Esports has emerged as a popular genre for players as well as spectators, supporting a global entertainment industry. Esports analytics has evolved to address the requirement for data-driven feedback, and is focused on cyber-athlete evaluation, strategy and prediction. Towards the latter, previous work has used match data from a variety of player ranks from hobbyist to professional players. However, professional players have been shown to behave differently than lower ranked players. Given the comparatively limited supply of professional data, a key question is thus whether mixed-rank match datasets can be used to create data-driven models which predict winners in professional matches and provide a simple in-game statistic for viewers and broadcasters. Here we show that, although there is a slightly reduced accuracy, mixed-rank datasets can be used to predict the outcome of professional matches, with suitably optimized configurations."
      },
      {
        "node_idx": 103928,
        "score_0_10": 10,
        "title": "time to die death prediction in dota 2 using deep learning",
        "abstract": "Esports have become major international sports with hundreds of millions of spectators. Esports games generate massive amounts of telemetry data. Using these to predict the outcome of esports matches has received considerable attention, but micro-predictions, which seek to predict events inside a match, is as yet unknown territory. Micro-predictions are however of perennial interest across esports commentators and audience, because they provide the ability to observe events that might otherwise be missed: esports games are highly complex with fast-moving action where the balance of a game can change in the span of seconds, and where events can happen in multiple areas of the playing field at the same time. Such events can happen rapidly, and it is easy for commentators and viewers alike to miss an event and only observe the following impact of events. In Dota 2, a player hero being killed by the opposing team is a key event of interest to commentators and audience. We present a deep learning network with shared weights which provides accurate death predictions within a five-second window. The network is trained on a vast selection of Dota 2 gameplay features and professional/semi-professional level match dataset. Even though death events are rare within a game (1\\% of the data), the model achieves 0.377 precision with 0.725 recall on test data when prompted to predict which of any of the 10 players of either team will die within 5 seconds. An example of the system applied to a Dota 2 match is presented. This model enables real-time micro-predictions of kills in Dota 2, one of the most played esports titles in the world, giving commentators and viewers time to move their attention to these key events."
      },
      {
        "node_idx": 92352,
        "score_0_10": 10,
        "title": "optimized execution of business processes on blockchain",
        "abstract": "Blockchain technology enables the execution of collaborative business processes involving untrusted parties without requiring a central authority. Specifically, a process model comprising tasks performed by multiple parties can be coordinated via smart contracts operating on the blockchain. The consensus mechanism governing the blockchain thereby guarantees that the process model is followed by each party. However, the cost required for blockchain use is highly dependent on the volume of data recorded and the frequency of data updates by smart contracts. This paper proposes an optimized method for executing business processes on top of commodity blockchain technology. The paper presents a method for compiling a process model into a smart contract that encodes the preconditions for executing each task in the process using a space-optimized data structure. The method is empirically compared to a previously proposed baseline by replaying execution logs, including one from a real-life business process, and measuring resource consumption."
      },
      {
        "node_idx": 165733,
        "score_0_10": 10,
        "title": "data driven fuzzy modeling using deep learning",
        "abstract": "Fuzzy modeling has many advantages over the non-fuzzy methods, such as robustness against uncertainties and less sensitivity to the varying dynamics of nonlinear systems. Data-driven fuzzy modeling needs to extract fuzzy rules from the input/output data, and train the fuzzy parameters. This paper takes advantages from deep learning, probability theory, fuzzy modeling, and extreme learning machines. We use the restricted Boltzmann machine (RBM) and probability theory to overcome some common problems in data based modeling methods. The RBM is modified such that it can be trained with continuous values. A probability based clustering method is proposed to partition the hidden features from the RBM, and extract fuzzy rules with probability measurement. An extreme learning machine and an optimization method are applied to train the consequent part of the fuzzy rules and the probability parameters. The proposed method is validated with two benchmark problems."
      },
      {
        "node_idx": 134038,
        "score_0_10": 10,
        "title": "analysing the impact of a ddos attack announcement on victim stock prices",
        "abstract": "DDoS attacks are increasingly used by 'hackers' and 'hacktivists' for various purposes. A number of on-line tools are available to launch an attack of significant intensity. These attacks lead to a variety of losses at the victim's end. We analyse the impact of Distributed Denial-of-Service (DDoS) attack announcements over a period of 5 years on the stock prices of the victim firms. We propose a method for event studies that does not assume the cumulative abnormal returns to be normally distributed, instead we use the empirical distribution for testing purposes. In most cases we find no significant impact on the stock returns but in cases where a DDoS attack creates an interruption in the services provided to the customer, we find a significant negative impact."
      },
      {
        "node_idx": 66729,
        "score_0_10": 10,
        "title": "rapidprom mine your processes and not just your data",
        "abstract": "The number of events recorded for operational processes is growing every year. This applies to all domains: from health care and e-government to production and maintenance. Event data are a valuable source of information for organizations that need to meet requirements related to compliance, efficiency, and customer service. Process mining helps to turn these data into real value: by discovering the real processes, by automatically identifying bottlenecks, by analyzing deviations and sources of non-compliance, by revealing the actual behavior of people, etc. Process mining is very different from conventional data mining and machine learning techniques. ProM is a powerful open-source process mining tool supporting hundreds of analysis techniques. However, ProM does not support analysis based on scientific workflows. RapidProM, an extension of RapidMiner based on ProM, combines the best of both worlds. Complex process mining workflows can be modeled and executed easily and subsequently reused for other data sets. Moreover, using RapidProM, one can benefit from combinations of process mining with other types of analysis available through the RapidMiner marketplace."
      },
      {
        "node_idx": 145315,
        "score_0_10": 10,
        "title": "a multi agent simulation of retail management practices",
        "abstract": "We apply Agent-Based Modeling and Simulation (ABMS) to investigate a set of problems in a retail context. Specifically, we are working to understand the relationship between human resource management practices and retail productivity. Despite the fact we are working within a relatively novel and complex domain, it is clear that intelligent agents do offer potential for developing organizational capabilities in the future. Our multi-disciplinary research team has worked with a UK department store to collect data and capture perceptions about operations from actors within departments. Based on this case study work, we have built a simulator that we present in this paper. We then use the simulator to gather empirical evidence regarding two specific management practices: empowerment and employee development."
      },
      {
        "node_idx": 153205,
        "score_0_10": 9,
        "title": "bidding under uncertainty theory and experiments",
        "abstract": "This paper describes a study of agent bidding strategies, assuming combinatorial valuations for complementary and substitutable goods, in three auction environments: sequential auctions, simultaneous auctions, and the Trading Agent Competition (TAC) Classic hotel auction design, a hybrid of sequential and simultaneous auctions. The problem of bidding in sequential auctions is formulated as an MDP, and it is argued that expected marginal utility bidding is the optimal bidding policy. The problem of bidding in simultaneous auctions is formulated as a stochastic program, and it is shown by example that marginal utility bidding is not an optimal bidding policy, even in deterministic settings. Two alternative methods of approximating a solution to this stochastic program are presented: the first method, which relies on expected values, is optimal in deterministic environments; the second method, which samples the nondeterministic environment, is asymptotically optimal as the number of samples tends to infinity. Finally, experiments with these various bidding policies are described in the TAC Classic setting."
      }
    ]
  },
  "328": {
    "explanation": "string repeat detection and parallelized longest repeat queries",
    "topk": [
      {
        "node_idx": 5315,
        "score_0_10": 10,
        "title": "on longest repeat queries",
        "abstract": "Repeat finding in strings has important applications in subfields such as computational biology. Surprisingly, all prior work on repeat finding did not consider the constraint on the locality of repeats. In this paper, we propose and study the problem of finding longest repetitive substrings covering particular string positions. We propose an $O(n)$ time and space algorithm for finding the longest repeat covering every position of a string of size $n$. Our work is optimal since the reading and the storage of an input string of size $n$ takes $O(n)$ time and space. Because any substring of a repeat is also a repeat, our solution to longest repeat queries effectively provides a \"stabbing\" tool for practitioners for finding most of the repeats that cover particular string positions."
      },
      {
        "node_idx": 60562,
        "score_0_10": 10,
        "title": "on longest repeat queries using gpu",
        "abstract": "Repeat finding in strings has important applications in subfields such as computational biology. The challenge of finding the longest repeats covering particular string positions was recently proposed and solved by \\.{I}leri et al., using a total of the optimal $O(n)$ time and space, where $n$ is the string size. However, their solution can only find the \\emph{leftmost} longest repeat for each of the $n$ string position. It is also not known how to parallelize their solution. In this paper, we propose a new solution for longest repeat finding, which although is theoretically suboptimal in time but is conceptually simpler and works faster and uses less memory space in practice than the optimal solution. Further, our solution can find \\emph{all} longest repeats of every string position, while still maintaining a faster processing speed and less memory space usage. Moreover, our solution is \\emph{parallelizable} in the shared memory architecture (SMA), enabling it to take advantage of the modern multi-processor computing platforms such as the general-purpose graphics processing units (GPU). We have implemented both the sequential and parallel versions of our solution. Experiments with both biological and non-biological data show that our sequential and parallel solutions are faster than the optimal solution by a factor of 2--3.5 and 6--14, respectively, and use less memory space."
      },
      {
        "node_idx": 159881,
        "score_0_10": 10,
        "title": "efficient multi user computation offloading for mobile edge cloud computing",
        "abstract": "Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases."
      },
      {
        "node_idx": 141183,
        "score_0_10": 10,
        "title": "algorithms for scheduling deadline sensitive malleable tasks",
        "abstract": "Due to the ubiquitous batch data processing in cloud computing, the fundamental model of scheduling malleable batch tasks and its extensions have received significant attention recently. In this model, a set of n tasks is to be scheduled on C identical machines and each task is specified by a value, a workload, a deadline and a parallelism bound. Within the parallelism bound, the number of the machines allocated to a task can vary over time and its workload will not change accordingly. In this paper, the two core results of this paper are to quantitatively characterize a sufficient and necessary condition such that a set of malleable batch tasks with deadlines can be feasibly scheduled on C machines, and to propose a polynomial time algorithm to produce such a feasible schedule. The core results provide a conceptual tool and an optimal scheduling algorithm to enable proposing new analysis and design of algorithms or improving existing algorithms for extensive scheduling objectives."
      },
      {
        "node_idx": 157174,
        "score_0_10": 10,
        "title": "stackelberg network pricing games",
        "abstract": "We study a multi-player one-round game termed Stackelberg Network Pricing Game, in which a leader can set prices for a subset of $m$ priceable edges in a graph. The other edges have a fixed cost. Based on the leader's decision one or more followers optimize a polynomial-time solvable combinatorial minimization problem and choose a minimum cost solution satisfying their requirements based on the fixed costs and the leader's prices. The leader receives as revenue the total amount of prices paid by the followers for priceable edges in their solutions, and the problem is to find revenue maximizing prices. Our model extends several known pricing problems, including single-minded and unit-demand pricing, as well as Stackelberg pricing for certain follower problems like shortest path or minimum spanning tree. Our first main result is a tight analysis of a single-price algorithm for the single follower game, which provides a $(1+\\epsilon) \\log m$-approximation for any $\\epsilon >0$. This can be extended to provide a $(1+\\epsilon)(\\log k + \\log m)$-approximation for the general problem and $k$ followers. The latter result is essentially best possible, as the problem is shown to be hard to approximate within $\\mathcal{O(\\log^\\epsilon k + \\log^\\epsilon m)$. If followers have demands, the single-price algorithm provides a $(1+\\epsilon)m^2$-approximation, and the problem is hard to approximate within $\\mathcal{O(m^\\epsilon)$ for some $\\epsilon >0$. Our second main result is a polynomial time algorithm for revenue maximization in the special case of Stackelberg bipartite vertex cover, which is based on non-trivial max-flow and LP-duality techniques. Our results can be extended to provide constant-factor approximations for any constant number of followers."
      },
      {
        "node_idx": 137724,
        "score_0_10": 9,
        "title": "market oriented cloud computing vision hype and reality for delivering it services as computing utilities",
        "abstract": "This keynote paper: presents a 21st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21st century vision."
      },
      {
        "node_idx": 41265,
        "score_0_10": 9,
        "title": "decentralized computation offloading game for mobile cloud computing",
        "abstract": "Mobile cloud computing is envisioned as a promising approach to augment computation capabilities of mobile devices for emerging resource-hungry mobile applications. In this paper, we propose a game theoretic approach for achieving efficient computation offloading for mobile cloud computing. We formulate the decentralized computation offloading decision making problem among mobile device users as a decentralized computation offloading game. We analyze the structural property of the game and show that the game always admits a Nash equilibrium. We then design a decentralized computation offloading mechanism that can achieve a Nash equilibrium of the game and quantify its efficiency ratio over the centralized optimal solution. Numerical results demonstrate that the proposed mechanism can achieve efficient computation offloading performance and scale well as the system size increases."
      },
      {
        "node_idx": 83098,
        "score_0_10": 9,
        "title": "energy efficient resource allocation for mobile edge computation offloading",
        "abstract": "Mobile-edge computation offloading (MECO) offloads intensive mobile computation to clouds located at the edges of cellular networks. Thereby, MECO is envisioned as a promising technique for prolonging the battery lives and enhancing the computation capacities of mobiles. In this paper, we study resource allocation for a multiuser MECO system based on time-division multiple access (TDMA) and orthogonal frequency-division multiple access (OFDMA). First, for the TDMA MECO system with infinite or finite computation capacity, the optimal resource allocation is formulated as a convex optimization problem for minimizing the weighted sum mobile energy consumption under the constraint on computation latency. The optimal policy is proved to have a threshold-based structure with respect to a derived offloading priority function, which yields priorities for users according to their channel gains and local computing energy consumption. As a result, users with priorities above and below a given threshold perform complete and minimum offloading, respectively. Moreover, for the cloud with finite capacity, a sub-optimal resource-allocation algorithm is proposed to reduce the computation complexity for computing the threshold. Next, we consider the OFDMA MECO system, for which the optimal resource allocation is formulated as a non-convex mixed-integer problem. To solve this challenging problem and characterize its policy structure, a sub-optimal low-complexity algorithm is proposed by transforming the OFDMA problem to its TDMA counterpart. The corresponding resource allocation is derived by defining an average offloading priority function and shown to have close-to-optimal performance by simulation."
      },
      {
        "node_idx": 100888,
        "score_0_10": 9,
        "title": "mobile edge computing a survey on architecture and computation offloading",
        "abstract": "Technological evolution of mobile user equipment (UEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the UEs is constrained by limited battery capacity and energy consumption of the UEs. A suitable solution extending the battery life-time of the UEs is to offload the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces significant execution delay consisting of delivery of the offloaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the UE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: 1) decision on computation offloading; 2) allocation of computing resource within the MEC; and 3) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC."
      },
      {
        "node_idx": 129115,
        "score_0_10": 9,
        "title": "gridsim a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing",
        "abstract": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular paradigms for next generation parallel and distributed computing. The management of resources and scheduling of applications in such large-scale distributed systems is a complex undertaking. In order to prove the effectiveness of resource brokers and associated scheduling algorithms, their performance needs to be evaluated under different scenarios such as varying number of resources and users with different requirements. In a grid environment, it is hard and even impossible to perform scheduler performance evaluation in a repeatable and controllable manner as resources and users are distributed across multiple organizations with their own policies. To overcome this limitation, we have developed a Java-based discrete-event grid simulation toolkit called GridSim. The toolkit supports modeling and simulation of heterogeneous grid resources (both time- and space-shared), users and application models. It provides primitives for creation of application tasks, mapping of tasks to resources, and their management. To demonstrate suitability of the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker and evaluated the performance of deadline and budget constrained cost- and time-minimization scheduling algorithms."
      }
    ]
  },
  "332": {
    "explanation": "graph coloring algorithms and complexity results",
    "topk": [
      {
        "node_idx": 92885,
        "score_0_10": 10,
        "title": "vertex coloring with star defects",
        "abstract": "Defective coloring is a variant of traditional vertex-coloring, according to which adjacent vertices are allowed to have the same color, as long as the monochromatic components induced by the corresponding edges have a certain structure. Due to its important applications, as for example in the bipartisation of graphs, this type of coloring has been extensively studied, mainly with respect to the size, degree, and acyclicity of the monochromatic components. #R##N#In this paper we focus on defective colorings in which the monochromatic components are acyclic and have small diameter, namely, they form stars. For outerplanar graphs, we give a linear-time algorithm to decide if such a defective coloring exists with two colors and, in the positive case, to construct one. Also, we prove that an outerpath (i.e., an outerplanar graph whose weak-dual is a path) always admits such a two-coloring. Finally, we present NP-completeness results for non-planar and planar graphs of bounded degree for the cases of two and three colors."
      },
      {
        "node_idx": 47858,
        "score_0_10": 10,
        "title": "instantaneously trained neural networks",
        "abstract": "This paper presents a review of instantaneously trained neural networks (ITNNs). These networks trade learning time for size and, in the basic model, a new hidden node is created for each training sample. Various versions of the corner-classification family of ITNNs, which have found applications in artificial intelligence (AI), are described. Implementation issues are also considered."
      },
      {
        "node_idx": 61581,
        "score_0_10": 10,
        "title": "hatching for 3d prints line based halftoning for dual extrusion fused deposition modeling",
        "abstract": "Abstract   This work presents a halftoning technique to manufacture 3D objects with the appearance of continuous grayscale imagery for Fused Deposition Modeling (FDM) printers. While droplet-based dithering is a common halftoning technique, this is not applicable to FDM printing, since FDM builds up objects by extruding material in semi-continuous paths. The line-based halftoning principle called \u2018hatching\u2019 is applied to the line patterns naturally occurring in FDM prints, which are built up in a layer-by-layer fashion. The proposed halftoning technique is not limited by the challenges existing techniques face; existing FDM coloring techniques greatly influence the surface geometry and deteriorate with surface slopes deviating from vertical or greatly influence the basic parameters of the printing process and thereby the structural properties of the resulting product. Furthermore, the proposed technique has little effect on printing time. Experiments on a dual-nozzle FDM printer show promising results. Future work is required to calibrate the perceived tone."
      },
      {
        "node_idx": 19884,
        "score_0_10": 10,
        "title": "une approche csp pour l aide a la localisation d erreurs",
        "abstract": "We introduce in this paper a new CP-based approach to support errors location in a program for which a counter-example is available, i.e. an instantiation of the input variables that violates the post-condition. To provide helpful information for error location, we generate a constraint system for the paths of the CFG (Control Flow Graph) for which at most k conditional statements may be erroneous. Then, we calculate Minimal Correction Sets (MCS) of bounded size for each of these paths. The removal of one of these sets of constraints yields a maximal satisfiable subset, in other words, a maximal subset of constraints satisfying the post condition. We extend the algorithm proposed by Liffiton and Sakallah \\cite{LiS08} to handle programs with numerical statements more efficiently. We present preliminary experimental results that are quite encouraging."
      },
      {
        "node_idx": 75281,
        "score_0_10": 10,
        "title": "hereditary biclique helly graphs recognition and maximal biclique enumeration",
        "abstract": "A biclique is a set of vertices that induce a bipartite complete graph. A graph G is biclique-Helly when its family of maximal bicliques satisfies the Helly property. If every induced subgraph of G is also biclique-Helly, then G is hereditary biclique-Helly. A graph is C_4-dominated when every cycle of length 4 contains a vertex that is dominated by the vertex of the cycle that is not adjacent to it. In this paper we show that the class of hereditary biclique-Helly graphs is formed precisely by those C_4-dominated graphs that contain no triangles and no induced cycles of length either 5, or 6. Using this characterization, we develop an algorithm for recognizing hereditary biclique-Helly graphs in O(n^2+\\alpha m) time and O(m) space. (Here n, m, and \\alpha = O(m^{1/2}) are the number of vertices and edges, and the arboricity of the graph, respectively.) As a subprocedure, we show how to recognize those C_4-dominated graphs that contain no triangles in O(\\alpha m) time and O(m) space. Finally, we show how to enumerate all the maximal bicliques of a C_4-dominated graph with no triangles in O(n^2 + \\alpha m) time and O(\\alpha m) space, and we discuss how some biclique problems can be solved in O(\\alpha m) time and O(n+m) space."
      },
      {
        "node_idx": 161218,
        "score_0_10": 10,
        "title": "firefighting on trees",
        "abstract": "In the Firefighter problem, introduced by Hartnell in 1995, a fire spreads through a graph while a player chooses which vertices to protect in order to contain it. In this paper, we focus on the case of trees and we consider as well the Fractional Firefighter game where the amount of protection allocated to a vertex lies between 0 and 1. While most of the work in this area deals with a constant amount of firefighters available at each turn, we consider three research questions which arise when including the sequence of firefighters as part of the instance. We first introduce the online version of both Firefighter and Fractional Firefighter, in which the number of firefighters available at each turn is revealed over time. We show that a greedy algorithm on finite trees is 1/2-competitive for both online versions, which generalises a result previously known for special cases of Firefighter. We also show that the optimal competitive ratio of online Firefighter ranges between 1/2 and the inverse of the golden ratio. Next, given two firefighter sequences, we discuss sufficient conditions for the existence of an infinite tree that separates them, in the sense that the fire can be contained with one sequence but not with the other. To this aim, we study a new purely numerical game called targeting game. Finally, we give sufficient conditions for the fire to be contained, expressed as the asymptotic comparison of the number of firefighters and the size of the tree levels."
      },
      {
        "node_idx": 52664,
        "score_0_10": 9,
        "title": "covering partial cubes with zones",
        "abstract": "A partial cube is a graph having an isometric embedding in a hypercube. Partial cubes are characterized by a natural equivalence relation on the edges, whose classes are called zones. The number of zones determines the minimal dimension of a hypercube in which the graph can be embedded. We consider the problem of covering the vertices of a partial cube with the minimum number of zones. The problem admits several special cases, among which are the problem of covering the cells of a line arrangement with a minimum number of lines, and the problem of finding a minimum-size fibre in a bipartite poset. For several such special cases, we give upper and lower bounds on the minimum size of a covering by zones. We also consider the computational complexity of those problems, and establish some hardness results."
      },
      {
        "node_idx": 35222,
        "score_0_10": 9,
        "title": "rainbow colouring of split graphs",
        "abstract": "A rainbow path in an edge coloured graph is a path in which no two edges are coloured the same. A rainbow colouring of a connected graph G is a colouring of the edges of G such that every pair of vertices in G is connected by at least one rainbow path. The minimum number of colours required to rainbow colour G is called its rainbow connection number. Between them, Chakraborty et al. [J. Comb. Optim., 2011] and Ananth et al. [FSTTCS, 2012] have shown that for every integer k, k \\geq 2, it is NP-complete to decide whether a given graph can be rainbow coloured using k colours. #R##N#A split graph is a graph whose vertex set can be partitioned into a clique and an independent set. Chandran and Rajendraprasad have shown that the problem of deciding whether a given split graph G can be rainbow coloured using 3 colours is NP-complete and further have described a linear time algorithm to rainbow colour any split graph using at most one colour more than the optimum [COCOON, 2012]. In this article, we settle the computational complexity of the problem on split graphs and thereby discover an interesting dichotomy. Specifically, we show that the problem of deciding whether a given split graph can be rainbow coloured using k colours is NP-complete for k \\in {2,3}, but can be solved in polynomial time for all other values of k."
      },
      {
        "node_idx": 14035,
        "score_0_10": 9,
        "title": "maximum delta edge colorable subgraphs of class ii graphs",
        "abstract": "A graph $G$ is class II, if its chromatic index is at least $\\Delta+1$. Let $H$ be a maximum $\\Delta$-edge-colorable subgraph of $G$. The paper proves best possible lower bounds for $\\frac{|E(H)|}{|E(G)|}$, and structural properties of maximum $\\Delta$-edge-colorable subgraphs. It is shown that every set of vertex-disjoint cycles of a class II graph with $\\Delta\\geq3$ can be extended to a maximum $\\Delta$-edge-colorable subgraph. Simple graphs have a maximum $\\Delta$-edge-colorable subgraph such that the complement is a matching. Furthermore, a maximum $\\Delta$-edge-colorable subgraph of a simple graph is always class I."
      },
      {
        "node_idx": 128297,
        "score_0_10": 9,
        "title": "tools for parsimonious edge colouring of graphs with maximum degree three",
        "abstract": "In a graph $G$ of maximum degree $\\Delta$ let $\\gamma$ denote the largest fraction of edges that can be $\\Delta$ edge-coloured. Albertson and Haas showed that $\\gamma \\geq \\frac{13}{15}$ when $G$ is cubic \\cite{AlbHaa}. The notion of $\\delta-$minimum edge colouring was introduced in \\cite{FouPhd} in order to extend the so called {\\em parcimonious edge-colouring} to graphs with maximum degree $3$. We propose here an english translation of some structural properties already present in \\cite{Fou,FouPhd} (in French) for $\\delta-$minimum edge colourings of graphs with maximum degree $3$."
      }
    ]
  },
  "339": {
    "explanation": "multimodal visual-language alignment and description generation",
    "topk": [
      {
        "node_idx": 130623,
        "score_0_10": 10,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 123935,
        "score_0_10": 9,
        "title": "bottom up and top down attention for image captioning and visual question answering",
        "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 48739,
        "score_0_10": 9,
        "title": "the fractal structure of cellular automata on abelian groups",
        "abstract": "It is well-known that the spacetime diagrams of some cellular automata have a fractal structure: for instance Pascal's triangle modulo 2 generates a Sierpinski triangle. Explaining the fractal structure of the spacetime diagrams of cellular automata is a much explored topic, but virtually all of the results revolve around a special class of automata, whose typical features include irreversibility, an alphabet with a ring structure, a global evolution that is a ring homomorphism, and a property known as (weakly) p-Fermat. The class of automata that we study in this article has none of these properties. Their cell structure is weaker, as it does not come with a multiplication, and they are far from being p-Fermat, even weakly. However, they do produce fractal spacetime diagrams, and we explain why and how."
      },
      {
        "node_idx": 168237,
        "score_0_10": 9,
        "title": "the symbol grounding problem",
        "abstract": "There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the \"symbol grounding problem\": How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) \"iconic representations\" , which are analogs of the proximal sensory projections of distal objects and events, and (2) \"categorical representations\" , which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) \"symbolic representations\" , grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., \"An X is a Y that is Z\"). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic \"module,\" however; the symbolic functions would emerge as an intrinsically \"dedicated\" symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 43376,
        "score_0_10": 9,
        "title": "skip thought vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      }
    ]
  },
  "342": {
    "explanation": "robust stability and control of switched and port-Hamiltonian systems",
    "topk": [
      {
        "node_idx": 136970,
        "score_0_10": 10,
        "title": "robust stability conditions for switched linear systems under restricted switching",
        "abstract": "We propose matrix commutator based stability characterization for discrete-time switched linear systems under restricted switching. Given an admissible minimum dwell time, we identify sufficient conditions on subsystems such that a switched system is stable under all switching signals that obey the given restriction. The primary tool for our analysis is commutation relations between the subsystem matrices. Our stability conditions are robust with respect to small perturbations in the elements of these matrices. In case of arbitrary switching (i.e., given minimum dwell time = 1), we recover the prior result [1,Proposition 1] as a special case of our result."
      },
      {
        "node_idx": 117900,
        "score_0_10": 10,
        "title": "measurable stochastics for brane calculus",
        "abstract": "We give a stochastic extension of the Brane Calculus, along the lines of recent work by Cardelli and Mardare. In this presentation, the semantics of a Brane process is a measure of the stochastic distribution of possible derivations. To this end, we first introduce a labelled transition system for Brane Calculus, proving its adequacy w.r.t. the usual reduction semantics. Then, brane systems are presented as Markov processes over the measurable space generated by terms up-to syntactic congruence, and where the measures are indexed by the actions of this new LTS. Finally, we provide a SOS presentation of this stochastic semantics, which is compositional and syntax-driven."
      },
      {
        "node_idx": 73180,
        "score_0_10": 10,
        "title": "contraction analysis of switched systems via regularization",
        "abstract": "We study incremental stability and convergence of switched (bimodal) Filippov systems via contraction analysis. In particular, by using results on regularization of switched dynamical systems, we derive sufficient conditions for convergence of any two trajectories of the Filippov system between each other within some region of interest. We then apply these conditions to the study of different classes of Filippov systems including piecewise smooth (PWS) systems, piecewise affine (PWA) systems and relay feedback systems. We show that contrary to previous approaches, our conditions allow the system to be studied in metrics other than the Euclidean norm. The theoretical results are illustrated by numerical simulations on a set of representative examples that confirm their effectiveness and ease of application."
      },
      {
        "node_idx": 5610,
        "score_0_10": 10,
        "title": "embedding constrained model predictive control in a continuous time dynamic feedback",
        "abstract": "This paper introduces a continuous-time constrained nonlinear control scheme which implements a model predictive control strategy as a continuous-time dynamic system. The approach is based on the idea that the solution of the optimal control problem can be embedded into the internal states of a dynamic control law which runs in parallel to the system. Using input to state stability arguments, it is shown that if the controller dynamics are sufficiently fast with respect to the plant dynamics, the interconnection between the two systems is asymptotically stable. Additionally, it is shown that, by augmenting the proposed scheme with an add-on unit known as an Explicit Reference Governor, it is possible to drastically increase the set of initial conditions that can be steered to the desired reference without violating the constraints. Numerical examples demonstrate the effectiveness of the proposed scheme."
      },
      {
        "node_idx": 49333,
        "score_0_10": 9,
        "title": "a graph theoretic approach to input to state stability of switched systems",
        "abstract": "This article deals with input-to-state stability (ISS) of discrete-time switched systems. Given a family of nonlinear systems with exogenous inputs, we present a class of switching signals under which the resulting switched system is ISS. We allow non-ISS systems in the family and our analysis involves graph-theoretic arguments. A weighted digraph is associated to the switched system, and a switching signal is expressed as an infinite walk on this digraph, both in a natural way. Our class of stabilizing switching signals (infinite walks) is periodic in nature and affords simple algorithmic construction."
      },
      {
        "node_idx": 107718,
        "score_0_10": 9,
        "title": "novel active disturbance rejection control based on nested linear extended state observers",
        "abstract": "In this paper, a Novel Active Disturbance Rejection Control (N-ADRC) strategy is proposed that replaces the Linear Extended state observer (LESO) used in Conventional ADRC (C-ADRC) with a Nested LESO. In the nested LESO, the inner-loop LESO actively estimates and eliminates the generalized disturbance. Increasing the bandwidth improves the estimation accuracy which may tolerate noise and conflict with H/W limitations and the sampling frequency of the system. Therefore, an alternative scenario is offered without increasing the bandwidth of the inner-loop LESO provided that the rate of change of the generalized disturbance estimation error is upper bounded. This is achieved by the placing an outer-loop LESO in parallel with the inner one, it estimates and eliminates the remaining generalized disturbance that eluded from the inner-loop LESO due to bandwidth limitations. The stability of LESO and nested LESO is investigated using Lyapunov stability analysis. Simulations on uncertain nonlinear SISO system with time-varying exogenous disturbance revealed that the proposed nested LESO can successfully deal with a generalized disturbance in both noisy and noise-free environments, where the Integral Time Absolute Error (ITAE) of the tracking error for the nested LESO is reduced by 69.87% from that of the LESO."
      },
      {
        "node_idx": 61277,
        "score_0_10": 9,
        "title": "constraint tightening and stability in stochastic model predictive control",
        "abstract": "Constraint tightening to non-conservatively guarantee recursive feasibility and stability in Stochastic Model Predictive Control is addressed. Stability and feasibility requirements are considered separately, highlighting the difference between existence of a solution and feasibility of a suitable, a priori known candidate solution. Subsequently, a Stochastic Model Predictive Control algorithm which unifies previous results is derived, leaving the designer the option to balance an increased feasible region against guaranteed bounds on the asymptotic average performance and convergence time. Besides typical performance bounds, under mild assumptions, we prove asymptotic stability in probability of the minimal robust positively invariant set obtained by the unconstrained LQ-optimal controller. A numerical example, demonstrating the efficacy of the proposed approach in comparison with classical, recursively feasible Stochastic MPC and Robust MPC, is provided."
      },
      {
        "node_idx": 493,
        "score_0_10": 9,
        "title": "robust integral action of port hamiltonian systems",
        "abstract": "Interconnection and damping assignment, passivity-based control (IDA-PBC) has proven to be a successful control technique for the stabilisation of many nonlinear systems. In this paper, we propose a method to robustify a system which has been stabilised using IDA-PBC with respect to constant, matched disturbances via the addition of integral action. The proposed controller extends previous work on the topic by being robust against the damping of the system, a quantity which may not be known in many applications."
      },
      {
        "node_idx": 155679,
        "score_0_10": 9,
        "title": "cooperative task planning of multi agent systems under timed temporal specifications",
        "abstract": "In this paper the problem of cooperative task planning of multi-agent systems when timed constraints are imposed to the system is investigated. We consider timed constraints given by Metric Interval Temporal Logic (MITL). We propose a method for automatic control synthesis in a two-stage systematic procedure. With this method we guarantee that all the agents satisfy their own individual task specifications as well as that the team satisfies a team global task specification."
      },
      {
        "node_idx": 127417,
        "score_0_10": 9,
        "title": "new results on disturbance rejection for energy shaping controlled port hamiltonian systems",
        "abstract": "In this paper we present a method to robustify energy-shaping controllers for port-Hamiltonian (pH) systems by adding an integral action that rejects unknown additive disturbances. The proposed controller preserves the pH structure and, by adding to the new energy function a suitable cross term between the plant and the controller coordinates, it avoids the unnatural coordinate transformation used in the past. This paper extends our previous work by relaxing the requirement that the systems Hamiltonian is strictly convex and separable, which allows the controller to be applied to a large class of mechanical systems, including underactuated systems with non-constant mass matrix. Furthermore, it is shown that the proposed integral action control is robust against unknown damping in the case of fully-actuated systems."
      }
    ]
  },
  "345": {
    "explanation": "theoretical computer science and graph algorithms",
    "topk": [
      {
        "node_idx": 15976,
        "score_0_10": 10,
        "title": "remembering chandra kintala",
        "abstract": "With this contribution we would like to remember Chandra M. R. Kintala who passed away in November 2009. We will give short overviews of his CV and his contributions to the field of theoretical and applied computer science and, given the opportunity, will attempt to present the current state of limited nondeterminism and limited resources for machines. Finally, we will briefly touch on some research topics which hopefully will be addressed in the not so distant future."
      },
      {
        "node_idx": 78507,
        "score_0_10": 10,
        "title": "regular separability of one counter automata",
        "abstract": "The regular separability problem asks, for two given languages, if there exists a regular language including one of them but disjoint from the other. Our main result is decidability, and PSpace-completeness, of the regular separability problem for languages of one counter automata without zero tests (also known as one counter nets). This contrasts with undecidability of the regularity problem for one counter nets, and with undecidability of the regular separability problem for one counter automata, which is our second result."
      },
      {
        "node_idx": 102769,
        "score_0_10": 10,
        "title": "some new results on the cross correlation of m sequences",
        "abstract": "The determination of the cross correlation between an $m$-sequence and its decimated sequence has been a long-standing research problem. Considering a ternary $m$-sequence of period $3^{3r}-1$, we determine the cross correlation distribution for decimations $d=3^{r}+2$ and $d=3^{2r}+2$, where $\\gcd(r,3)=1$. Meanwhile, for a binary $m$-sequence of period $2^{2lm}-1$, we make an initial investigation for the decimation $d=\\frac{2^{2lm}-1}{2^{m}+1}+2^{s}$, where $l \\ge 2$ is even and $0 \\le s \\le 2m-1$. It is shown that the cross correlation takes at least four values. Furthermore, we confirm the validity of two famous conjectures due to Sarwate et al. and Helleseth in this case."
      },
      {
        "node_idx": 76222,
        "score_0_10": 9,
        "title": "elsa efficient long term secure storage of large datasets",
        "abstract": "An increasing amount of information today is generated, exchanged, and stored digitally. This also includes long-lived and highly sensitive information (e.g., electronic health records, governmental documents) whose integrity and confidentiality must be protected over decades or even centuries. While there is a vast amount of cryptography-based data protection schemes, only few are designed for long-term protection. Recently, Braun et al. (AsiaCCS'17) proposed the first long-term protection scheme that provides renewable integrity protection and information-theoretic confidentiality protection. However, computation and storage costs of their scheme increase significantly with the number of stored data items. As a result, their scheme appears suitable only for protecting databases with a small number of relatively large data items, but unsuitable for databases that hold a large number of relatively small data items (e.g., medical record databases). In this work, we present a solution for efficient long-term integrity and confidentiality protection of large datasets consisting of relatively small data items. First, we construct a renewable vector commitment scheme that is information-theoretically hiding under selective decommitment. We then combine this scheme with renewable timestamps and information-theoretically secure secret sharing. The resulting solution requires only a single timestamp for protecting a dataset while the state of the art requires a number of timestamps linear in the number of data items. We implemented our solution and measured its performance in a scenario where 12 000 data items are aggregated, stored, protected, and verified over a time span of 100 years. Our measurements show that our new solution completes this evaluation scenario an order of magnitude faster than the state of the art."
      },
      {
        "node_idx": 21251,
        "score_0_10": 9,
        "title": "relaxed disk packing",
        "abstract": "Motivated by biological questions, we study configurations of equal-sized disks in the Euclidean plane that neither pack nor cover. Measuring the quality by the probability that a random point lies in exactly one disk, we show that the regular hexagonal grid gives the maximum among lattice configurations."
      },
      {
        "node_idx": 42623,
        "score_0_10": 9,
        "title": "the emerging field of signal processing on graphs extending high dimensional data analysis to networks and other irregular domains",
        "abstract": "In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogs to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions."
      },
      {
        "node_idx": 155073,
        "score_0_10": 9,
        "title": "multi clique width",
        "abstract": "Multi-clique-width is obtained by a simple modification in the definition of clique-width. It has the advantage of providing a natural extension of tree-width. Unlike clique-width, it does not explode exponentially compared to tree-width. Efficient algorithms based on multi-clique-width are still possible for interesting tasks like computing the independent set polynomial or testing $c$-colorability. In particular, $c$-colorability can be tested in time linear in $n$ and singly exponential in $c$ and the width $k$ of a given multi-$k$-expression. For these tasks, the running time as a function of the multi-clique-width is the same as the running time of the fastest known algorithm as a function of the clique-width. This results in an exponential speed-up for some graphs, if the corresponding graph generating expressions are given. The reason is that the multi-clique-width is never bigger, but is exponentially smaller than the clique-width for many graphs. This gap shows up when the tree-width is basically equal to the multi-clique width as well as when the tree-width is not bounded by any function of the clique-width."
      },
      {
        "node_idx": 18134,
        "score_0_10": 9,
        "title": "phase retrieval via matrix completion",
        "abstract": "This paper develops a novel framework for phase retrieval, a problem which arises in X-ray crystallography, diffraction imaging, astronomical imaging and many other applications. Our approach combines multiple structured illuminations together with ideas from convex programming to recover the phase from intensity measurements, typically from the modulus of the diffracted wave. We demonstrate empirically that any complex-valued object can be recovered from the knowledge of the magnitude of just a few diffracted patterns by solving a simple convex optimization problem inspired by the recent literature on matrix completion. More importantly, we also demonstrate that our noise-aware algorithms are stable in the sense that the reconstruction degrades gracefully as the signal-to-noise ratio decreases. Finally, we introduce some theory showing that one can design very simple structured illumination patterns such that three diffracted figures uniquely determine the phase of the object we wish to recover."
      },
      {
        "node_idx": 117451,
        "score_0_10": 9,
        "title": "learning with submodular functions a convex optimization perspective",
        "abstract": "Submodular functions are relevant to machine learning for at least two reasons: (1) some problems may be expressed directly as the optimization of submodular functions and (2) the lovasz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning. In this monograph, we present the theory of submodular functions from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems. In particular, we show how submodular function minimization is equivalent to solving a wide variety of convex optimization problems. This allows the derivation of new efficient algorithms for approximate and exact submodular function minimization with theoretical guarantees and good practical performance. By listing many examples of submodular functions, we review various applications to machine learning, such as clustering, experimental design, sensor placement, graphical model structure learning or subset selection, as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions."
      },
      {
        "node_idx": 136959,
        "score_0_10": 9,
        "title": "well indumatched trees and graphs of bounded girth",
        "abstract": "A graph G is called well-indumatched if all of its maximal induced matchings have the same size. In this paper we characterize all well-indumatched trees. We provide a linear time algorithm to decide if a tree is well-indumatched or not. Then, we characterize minimal well-indumatched graphs of girth at least 9 and show subsequently that for an odd integer g greater than or equal to 9 and different from 11, there is no well-indumatched graph of girth g. On the other hand, there are infinitely many well-indumatched unicyclic graphs of girth k, where k is in {3, 5, 7} or k is an even integer greater than 2. We also show that, although the recognition of well-indumatched graphs is known to be co-NP-complete in general, one can recognize in polynomial time well-indumatched graphs where the size of maximal induced matchings is fixed."
      }
    ]
  },
  "349": {
    "explanation": "wireless MISO broadcast channel precoding optimization",
    "topk": [
      {
        "node_idx": 100650,
        "score_0_10": 10,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 36570,
        "score_0_10": 10,
        "title": "largest inscribed rectangles in geometric convex sets",
        "abstract": "We consider the problem of finding inscribed boxes and axis-aligned inscribed boxes of maximum volume, inside a compact and solid convex set. Our algorithms are capable of solving these two problems in any such set that can be represented with finite number of convex inequalities. For the axis-aligned case, we formulate the problem for higher dimensions and present an exact optimization algorithm which solves the problem in $\\mathcal{O}(d^3+d^2n)$ time, where $d$ is the dimension and $n$ is the number of inequalities defining the convex set. For the general case, after formulating the problem for higher dimensions we investigate the traditional 2-dimensional problem, which is in the literature merely considered for convex polygons, for a broad range of convex sets. We first present a new exact algorithm that finds the largest inscribed axis-aligned rectangle in such convex sets for any given direction of axes in $\\mathcal{O}(n)$ time. Using this exact algorithm as a subroutine, we present an $\\epsilon$-approximation algorithm that computes $(1-\\epsilon)$-approximation to the largest inscribed rectangle with computational complexity of $\\mathcal{O}(\\epsilon^{-1}n)$. Finally, we show that how this running time can be improved to $\\mathcal{O}(\\epsilon^{-1}\\log n)$ with a $\\mathcal{O}(\\epsilon^{-1}n)$ pre-processing time when the convex set is a polygon."
      },
      {
        "node_idx": 74729,
        "score_0_10": 10,
        "title": "a survey on non orthogonal multiple access for 5g networks research challenges and future trends",
        "abstract": "Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed."
      },
      {
        "node_idx": 7733,
        "score_0_10": 9,
        "title": "low snr capacity of noncoherent fading channels",
        "abstract": "Discrete-time Rayleigh-fading single-input single-output (SISO) and multiple-input multiple-output (MIMO) channels are considered, with no channel state information at the transmitter or the receiver. The fading is assumed to be stationary and correlated in time, but independent from antenna to antenna. Peak-power and average-power constraints are imposed on the transmit antennas. For MIMO channels, these constraints are either imposed on the sum over antennas, or on each individual antenna. For SISO channels and MIMO channels with sum power constraints, the asymptotic capacity as the peak signal-to-noise ratio (SNR) goes to zero is identified; for MIMO channels with individual power constraints, this asymptotic capacity is obtained for a class of channels called transmit separable channels. The results for MIMO channels with individual power constraints are carried over to SISO channels with delay spread (i.e., frequency-selective fading)."
      },
      {
        "node_idx": 70493,
        "score_0_10": 9,
        "title": "exponential sums cyclic codes and sequences the odd characteristic kasami case",
        "abstract": "Let $q=p^n$ with $n=2m$ and $p$ be an odd prime. Let $0\\leq k\\leq n-1$ and $k\\neq m$. In this paper we determine the value distribution of following exponential(character) sums \\[\\sum\\limits_{x\\in \\bF_q}\\zeta_p^{\\Tra_1^m (\\alpha x^{p^{m}+1})+\\Tra_1^n(\\beta x^{p^k+1})}\\quad(\\alpha\\in \\bF_{p^m},\\beta\\in \\bF_{q})\\] and \\[\\sum\\limits_{x\\in \\bF_q}\\zeta_p^{\\Tra_1^m (\\alpha x^{p^{m}+1})+\\Tra_1^n(\\beta x^{p^k+1}+\\ga x)}\\quad(\\alpha\\in \\bF_{p^m},\\beta,\\ga\\in \\bF_{q})\\] where $\\Tra_1^n: \\bF_q\\ra \\bF_p$ and $\\Tra_1^m: \\bF_{p^m}\\ra\\bF_p$ are the canonical trace mappings and $\\zeta_p=e^{\\frac{2\\pi i}{p}}$ is a primitive $p$-th root of unity. As applications: (1). We determine the weight distribution of the cyclic codes $\\cC_1$ and $\\cC_2$ over $\\bF_{p^t}$ with parity-check polynomials $h_2(x)h_3(x)$ and $h_1(x)h_2(x)h_3(x)$ respectively where $t$ is a divisor of $d=\\gcd(m,k)$, and $h_1(x)$, $h_2(x)$ and $h_3(x)$ are the minimal polynomials of $\\pi^{-1}$, $\\pi^{-(p^k+1)}$ and $\\pi^{-(p^m+1)}$ over $\\bF_{p^t}$ respectively for a primitive element $\\pi$ of $\\bF_q$. (2). We determine the correlation distribution among a family of m-sequences. This paper extends the results in \\cite{Zen Li}."
      },
      {
        "node_idx": 32446,
        "score_0_10": 9,
        "title": "efficient computation of minimum area rectilinear convex hull under rotation and generalizations",
        "abstract": "Let $P$ be a set of $n$ points in the plane. We compute the value of $\\theta\\in [0,2\\pi)$ for which the rectilinear convex hull of $P$, denoted by $\\mathcal{RH}_{\\theta}(P)$, has minimum (or maximum) area in optimal $O(n\\log n)$ time and $O(n)$ space, improving the previous $O(n^2)$ bound. Let $\\mathcal{O}$ be a set of $k$ lines through the origin sorted by slope and let $\\alpha_i$ be the aperture angles of the $2k$ sectors defined by every pair of two consecutive lines. Let $\\Theta_{i}=\\pi-\\alpha_i$ and $\\Theta=\\min\\{\\Theta_i:i=1,\\ldots,2k\\}$. We further obtain: (1) Given a set $\\mathcal{O}$ such that $\\Theta\\ge\\frac{\\pi}{2}$, we provide an algorithm to compute the $\\mathcal{O}$-convex hull of $P$ in optimal $O(n\\log n)$ time and $O(n)$ space, while if $\\Theta<\\frac{\\pi}{2}$ the complexities are $O(\\frac{n}{\\Theta}\\log n)$ time and $O(\\frac{n}{\\Theta})$ space. (2) Given a set $\\mathcal{O}$ such that $\\Theta\\ge\\frac{\\pi}{2}$, we compute and maintain the boundary of the ${\\mathcal{O}}_{\\theta}$-convex hull of $P$ for $\\theta\\in [0,2\\pi)$ in $O(kn\\log n)$ time and $O(kn)$ space, or in $O(k\\frac{n}{\\Theta}\\log n)$ time and $O(k\\frac{n}{\\Theta})$ space if $\\Theta<\\frac{\\pi}{2}$. (3) Finally, given a set $\\mathcal{O}$ such that $\\Theta\\ge\\frac{\\pi}{2}$, we compute the ${\\mathcal{O}}_{\\theta}$-convex hull of $P$ of minimum (or maximum) area over all $\\theta\\in [0,2\\pi)$ in $O(kn\\log n)$ time and $O(kn)$ space."
      },
      {
        "node_idx": 17426,
        "score_0_10": 9,
        "title": "noncoherent capacity of underspread fading channels",
        "abstract": "We derive bounds on the noncoherent capacity of wide-sense stationary uncorrelated scattering (WSSUS) channels that are selective both in time and frequency, and are underspread, i.e., the product of the channel's delay spread and Doppler spread is small. The underspread assumption is satisfied by virtually all wireless communication channels. For input signals that are peak constrained in time and frequency, we obtain upper and lower bounds on capacity that are explicit in the channel's scattering function, are accurate for a large range of bandwidth, and allow to coarsely identify the capacity-optimal bandwidth as a function of the peak power and the channel's scattering function. We also obtain a closed-form expression for the first-order Taylor series expansion of capacity in the infinite-bandwidth limit, and show that our bounds are tight in the wideband regime. For input signals that are peak constrained in time only (and, hence, allowed to be peaky in frequency), we provide upper and lower bounds on the infinite-bandwidth capacity. Our lower bound is closely related to a result by Viterbi (1967). We find cases where the bounds coincide and, hence, the infinite-bandwidth capacity is characterized exactly. The analysis in this paper is based on a discrete-time discrete-frequency approximation of WSSUS time- and frequency-selective channels. This discretization takes the underspread property of the channel explicitly into account."
      },
      {
        "node_idx": 116103,
        "score_0_10": 9,
        "title": "delta greedy t spanner",
        "abstract": "We introduce a new geometric spanner, $\\delta$-Greedy, whose construction is based on a generalization of the known Path-Greedy and Gap-Greedy spanners. The $\\delta$-Greedy spanner combines the most desirable properties of geometric spanners both in theory and in practice. More specifically, it has the same theoretical and practical properties as the Path-Greedy spanner: a natural definition, small degree, linear number of edges, low weight, and strong $(1+\\varepsilon)$-spanner for every $\\varepsilon>0$. The $\\delta$-Greedy algorithm is an improvement over the Path-Greedy algorithm with respect to the number of shortest path queries and hence with respect to its construction time. We show how to construct such a spanner for a set of $n$ points in the plane in $O(n^2 \\log n)$ time. #R##N#The $\\delta$-Greedy spanner has an additional parameter, $\\delta$, which indicates how close it is to the Path-Greedy spanner on the account of the number of shortest path queries. For $\\delta = t$ the output spanner is identical to the Path-Greedy spanner, while the number of shortest path queries is, in practice, linear. #R##N#Finally, we show that for a set of $n$ points placed independently at random in a unit square the expected construction time of the $\\delta$-Greedy algorithm is $O(n \\log n)$. Our analysis indicates that the $\\delta$-Greedy spanner gives the best results among the known spanners of expected $O(n \\log n)$ time for random point sets. Moreover, the analysis implies that by setting $\\delta = t$, the $\\delta$-Greedy algorithm provides a spanner identical to the Path-Greedy spanner in expected $O(n \\log n)$ time."
      },
      {
        "node_idx": 20702,
        "score_0_10": 9,
        "title": "computing the greedy spanner in linear space",
        "abstract": "The greedy spanner is a high-quality spanner: its total weight, edge count and maximal degree are asymptotically optimal and in practice significantly better than for any other spanner with reasonable construction time. Unfortunately, all known algorithms that compute the greedy spanner of n points use Omega(n^2) space, which is impractical on large instances. To the best of our knowledge, the largest instance for which the greedy spanner was computed so far has about 13,000 vertices. #R##N#We present a O(n)-space algorithm that computes the same spanner for points in R^d running in O(n^2 log^2 n) time for any fixed stretch factor and dimension. We discuss and evaluate a number of optimizations to its running time, which allowed us to compute the greedy spanner on a graph with a million vertices. To our knowledge, this is also the first algorithm for the greedy spanner with a near-quadratic running time guarantee that has actually been implemented."
      },
      {
        "node_idx": 137708,
        "score_0_10": 9,
        "title": "cooperative interference management with miso beamforming",
        "abstract": "In this correspondence, we study the downlink transmission in a multi-cell system, where multiple base stations (BSs) each with multiple antennas cooperatively design their respective transmit beamforming vectors to optimize the overall system performance. For simplicity, it is assumed that all mobile stations (MSs) are equipped with a single antenna each, and there is one active MS in each cell at one time. Accordingly, the system of interests can be modeled by a multiple-input single-output (MISO) Gaussian interference channel (IC), termed as MISO-IC, with interference treated as noise. We propose a new method to characterize different rate-tuples for active MSs on the Pareto boundary of the achievable rate region for the MISO-IC, by exploring the relationship between the MISO-IC and the cognitive radio (CR) MISO channel. We show that each Pareto-boundary rate-tuple of the MISO-IC can be achieved in a decentralized manner when each of the BSs attains its own channel capacity subject to a certain set of interference-power constraints (also known as interference-temperature constraints in the CR system) at the other MS receivers. Furthermore, we show that this result leads to a new decentralized algorithm for implementing the multi-cell cooperative downlink beamforming."
      }
    ]
  },
  "350": {
    "explanation": "robotic grasping, teleoperation, and shape-changing robotic systems",
    "topk": [
      {
        "node_idx": 70146,
        "score_0_10": 10,
        "title": "shapebots shape changing swarm robots",
        "abstract": "We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces."
      },
      {
        "node_idx": 35167,
        "score_0_10": 10,
        "title": "optimization model for planning precision grasps with multi fingered hands",
        "abstract": "Precision grasps with multi-fingered hands are important for precise placement and in-hand manipulation tasks. Searching precision grasps on the object represented by point cloud, is challenging due to the complex object shape, high-dimensionality, collision and undesired properties of the sensing and positioning. This paper proposes an optimization model to search for precision grasps with multi-fingered hands. The model takes noisy point cloud of the object as input and optimizes the grasp quality by iteratively searching for the palm pose and finger joints positions. The collision between the hand and the object is approximated and penalized by a series of least-squares. The collision approximation is able to handle the point cloud representation of the objects with complex shapes. The proposed optimization model is able to locate collision-free optimal precision grasps efficiently. The average computation time is 0.50 sec/grasp. The searching is robust to the incompleteness and noise of the point cloud. The effectiveness of the algorithm is demonstrated by experiments."
      },
      {
        "node_idx": 39016,
        "score_0_10": 10,
        "title": "more than a feeling learning to grasp and regrasp using vision and touch",
        "abstract": "For humans, the process of grasping an object relies heavily on rich tactile feedback. Most recent robotic grasping work, however, has been based only on visual input, and thus cannot easily benefit from feedback after initiating contact. In this letter, we investigate how a robot can learn to use tactile information to iteratively and efficiently adjust its grasp. To this end, we propose an end-to-end action-conditional model that learns regrasping policies from raw visuo-tactile data. This model - a deep, multimodal convolutional network - predicts the outcome of a candidate grasp adjustment, and then executes a grasp by iteratively selecting the most promising actions. Our approach requires neither calibration of the tactile sensors nor any analytical modeling of contact forces, thus reducing the engineering effort required to obtain efficient grasping policies. We train our model with data from about 6450 grasping trials on a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger. Across extensive experiments, our approach outperforms a variety of baselines at 1) estimating grasp adjustment outcomes, 2) selecting efficient grasp adjustments for quick grasping, and 3) reducing the amount of force applied at the fingers, while maintaining competitive performance. Finally, we study the choices made by our model and show that it has successfully acquired useful and interpretable grasping behaviors."
      },
      {
        "node_idx": 39237,
        "score_0_10": 10,
        "title": "emg controlled non anthropomorphic hand teleoperation using a continuous teleoperation subspace",
        "abstract": "We present a method for EMG-driven teleoperation of non-anthropomorphic robot hands. EMG sensors are appealing as a wearable, inexpensive, and unobtrusive way to gather information about the teleoperator's hand pose. However, mapping from EMG signals to the pose space of a non-anthropomorphic hand presents multiple challenges. We present a method that first projects from forearm EMG into a subspace relevant to teleoperation. To increase robustness, we use a model which combines continuous and discrete predictors along different dimensions of this subspace. We then project from the teleoperation subspace into the pose space of the robot hand. Our method is effective and intuitive, as it enables novice users to teleoperate pick and place tasks faster and more robustly than state-of-the-art EMG teleoperation methods when applied to a non-anthropomorphic, multi-DOF robot hand."
      },
      {
        "node_idx": 110727,
        "score_0_10": 10,
        "title": "embedding defeasible logic into logic programming",
        "abstract": "Defeasible reasoning is a simple but efficient approach to nonmonotonic reasoning that has recently attracted considerable interest and that has found various applications. Defeasible logic and its variants are an important family of defeasible reasoning methods. So far no relationship has been established between defeasible logic and mainstream nonmonotonic reasoning approaches. #R##N#In this paper we establish close links to known semantics of logic programs. In particular, we give a translation of a defeasible theory D into a meta-program P(D). We show that under a condition of decisiveness, the defeasible consequences of D correspond exactly to the sceptical conclusions of P(D) under the stable model semantics. Without decisiveness, the result holds only in one direction (all defeasible consequences of D are included in all stable models of P(D)). If we wish a complete embedding for the general case, we need to use the Kunen semantics of P(D), instead."
      },
      {
        "node_idx": 130094,
        "score_0_10": 9,
        "title": "materials that make robots smart",
        "abstract": "We posit that embodied artificial intelligence is not only a computational, but also a materials problem. While the importance of material and structural properties in the control loop are well understood, materials can take an active role during control by tight integration of sensors, actuators, computation and communication. We envision such materials to abstract functionality, therefore making the construction of intelligent robots more straightforward and robust. For example, robots could be made of bones that measure load, muscles that move, skin that provides the robot with information about the kind and location of tactile sensations ranging from pressure, to texture and damage, eyes that extract high-level information, and brain material that provides computation in a scalable manner. Such materials will not resemble any existing engineered materials, but rather the heterogeneous components out of which their natural counterparts are made. We describe the state-of-the-art in so-called \"robotic materials\", their opportunities for revolutionizing applications ranging from manipulation to autonomous driving, and open challenges the robotics community needs to address in collaboration with allies, such as wireless sensor network researchers and polymer scientists."
      },
      {
        "node_idx": 104596,
        "score_0_10": 9,
        "title": "why robots a survey on the roles and benefits of social robots in the therapy of children with autism",
        "abstract": "This paper reviews the use of socially interactive robots to assist in the therapy of children with autism. The extent to which the robots were successful in helping the children in their social, emotional and communication deficits was investigated. Child\u2013robot interactions were scrutinized with respect to the different target behaviors that are to be elicited from a child during therapy. These behaviors were thoroughly examined with respect to a child\u2019s development needs. Most importantly, experimental data from the surveyed works were extracted and analysed in terms of the target behaviors and of how each robot was used during a therapy session to achieve these behaviors. The study concludes by categorizing the different therapeutic roles that these robots were observed to play, and highlights the important design features that enable them to achieve high levels of effectiveness in autism therapy."
      },
      {
        "node_idx": 168656,
        "score_0_10": 9,
        "title": "robotics and integrated formal methods necessity meets opportunity",
        "abstract": "Robotic systems are multi-dimensional entities, combining both hardware and software, that are heavily dependent on, and influenced by, interactions with the real world. They can be variously categorised as embedded, cyber-physical, real-time, hybrid, adaptive and even autonomous systems, with a typical robotic system being likely to contain all of these aspects. The techniques for developing and verifying each of these system varieties are often quite distinct. This, together with the sheer complexity of robotic systems, leads us to argue that diverse formal techniques must be integrated in order to develop, verify, and provide certification evidence for, robotic systems. Furthermore, we propose the fast evolving field of robotics as an ideal catalyst for the advancement of integrated formal methods research, helping to drive the field in new and exciting directions and shedding light on the development of large-scale, dynamic, complex systems."
      },
      {
        "node_idx": 33854,
        "score_0_10": 9,
        "title": "a whole body software abstraction layer for control design of free floating mechanical systems",
        "abstract": "In this paper, we propose a software abstraction layer to simplify the design and synthesis of whole-body controllers without making any preliminary assumptions on the control law to be implemented. The main advantage of the proposed library is the decoupling of the control software from implementation details, which are related to the robotic platform. Furthermore, the resulting code is more clean and concise than ad-hoc code, as it focuses only on the implementation of the control law. In addition, we present a reference implementation of the abstraction layer together with a Simulink interface to provide support to Model-Driven based development. We also show the implementation of a simple proportional-derivative plus gravity compensation control together with a more complex momentum-based bipedal balance controller."
      },
      {
        "node_idx": 96505,
        "score_0_10": 9,
        "title": "a continuous teleoperation subspace with empirical and algorithmic mapping algorithms for non anthropomorphic hands",
        "abstract": "Teleoperation is a valuable tool for robotic manipulators in highly unstructured environments. However, finding an intuitive mapping between a human hand and a non-anthropomorphic robot hand can be difficult, due to the hands' dissimilar kinematics. In this paper, we seek to create a mapping between the human hand and a fully actuated, non-anthropomorphic robot hand that is intuitive enough to enable effective real-time teleoperation, even for novice users. To accomplish this, we propose a low-dimensional teleoperation subspace which can be used as an intermediary for mapping between hand pose spaces. We present two different methods to define the teleoperation subspace: an empirical definition, which requires a person to define hand motions in an intuitive, hand-specific way, and an algorithmic definition, which is kinematically independent, and uses objects to define the subspace. We use each of these definitions to create a teleoperation mapping for different hands. We validate both the empirical and algorithmic mappings with teleoperation experiments controlled by novices and performed on two kinematically distinct hands. The experiments show that the proposed subspace is relevant to teleoperation, intuitive enough to enable control by novices, and can generalize to non-anthropomorphic hands with different kinematic configurations."
      }
    ]
  },
  "351": {
    "explanation": "deep neural network-based automatic feature extraction and encoding",
    "topk": [
      {
        "node_idx": 39924,
        "score_0_10": 10,
        "title": "simultaneous feature learning and hash coding with deep neural networks",
        "abstract": "Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods."
      },
      {
        "node_idx": 61904,
        "score_0_10": 10,
        "title": "face recognition system",
        "abstract": "The face detection system and method attempts classification of a test image before performing all of the kernel evaluations. Many subimages are not faces and should be relatively easy to identify as such. Thus, the SVM classifier try to discard non-face images using as few kernel evaluations as possible using a cascade SVM classification. In the first stage, a score is computed for the first two support vectors, and the score is compared to a threshold. If the score is below the threshold value, the subimage is classified as not a face. If the score is above the threshold value, the cascade SVM classification function continues to apply more complicated decision rules, each time doubling the number of kernel evaluations, classifying the image as a non-face (and thus terminating the process) as soon as the test image fails to satisfy one of the decision rules. Finally, if the subimage has satisfied all intermediary decision rules, and has now reached the point at which all support vectors must be considered, the original decision function is applied. Satisfying this final rule, and all intermediary rules, is the only way for a test image to garner a positive (face) classification."
      },
      {
        "node_idx": 81796,
        "score_0_10": 10,
        "title": "universal adversarial perturbations",
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
      },
      {
        "node_idx": 17920,
        "score_0_10": 9,
        "title": "deep learning based electroencephalography analysis a systematic review",
        "abstract": "Electroencephalography (EEG) is a complex signal and can require several years of training to be correctly interpreted. Recently, deep learning (DL) has shown great promise in helping make sense of EEG signals due to its capacity to learn good feature representations from raw data. Whether DL truly presents advantages as compared to more traditional EEG processing approaches, however, remains an open question. In this work, we review 156 papers that apply DL to EEG, published between January 2010 and July 2018, and spanning different application domains such as epilepsy, sleep, brain-computer interfacing, and cognitive and affective monitoring. We extract trends and highlight interesting approaches in order to inform future research and formulate recommendations. Various data items were extracted for each study pertaining to 1) the data, 2) the preprocessing methodology, 3) the DL design choices, 4) the results, and 5) the reproducibility of the experiments. Our analysis reveals that the amount of EEG data used across studies varies from less than ten minutes to thousands of hours. As for the model, 40% of the studies used convolutional neural networks (CNNs), while 14% used recurrent neural networks (RNNs), most often with a total of 3 to 10 layers. Moreover, almost one-half of the studies trained their models on raw or preprocessed EEG time series. Finally, the median gain in accuracy of DL approaches over traditional baselines was 5.4% across all relevant studies. More importantly, however, we noticed studies often suffer from poor reproducibility: a majority of papers would be hard or impossible to reproduce given the unavailability of their data and code. To help the field progress, we provide a list of recommendations for future studies and we make our summary table of DL and EEG papers available and invite the community to contribute."
      },
      {
        "node_idx": 156065,
        "score_0_10": 9,
        "title": "the numpy array a structure for efficient numerical computation",
        "abstract": "In the Python world, NumPy arrays are the standard representation for numerical data. Here, we show how these arrays enable efficient implementation of numerical computations in a high-level language. Overall, three techniques are applied to improve performance: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. We first present the NumPy array structure, then show how to use it for efficient computation, and finally how to share array data with other libraries."
      },
      {
        "node_idx": 162705,
        "score_0_10": 9,
        "title": "deep representation learning in speech processing challenges recent advances and future trends",
        "abstract": "Research on speech processing has traditionally considered the task of designing hand-engineered acoustic features (feature engineering) as a separate distinct problem from the task of designing efficient machine learning (ML) models to make prediction and classification decisions. There are two main drawbacks to this approach: firstly, the feature engineering being manual is cumbersome and requires human knowledge; and secondly, the designed features might not be best for the objective at hand. This has motivated the adoption of a recent trend in speech community towards utilisation of representation learning techniques, which can learn an intermediate representation of the input signal automatically that better suits the task at hand and hence lead to improved performance. The significance of representation learning has increased with advances in deep learning (DL), where the representations are more useful and less dependent on human knowledge, making it very conducive for tasks like classification, prediction, etc. The main contribution of this paper is to present an up-to-date and comprehensive survey on different techniques of speech representation learning by bringing together the scattered research across three distinct research areas including Automatic Speech Recognition (ASR), Speaker Recognition (SR), and Speaker Emotion Recognition (SER). Recent reviews in speech have been conducted for ASR, SR, and SER, however, none of these has focused on the representation learning from speech---a gap that our survey aims to bridge."
      },
      {
        "node_idx": 11402,
        "score_0_10": 9,
        "title": "in datacenter performance analysis of a tensor processing unit",
        "abstract": "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."
      },
      {
        "node_idx": 127958,
        "score_0_10": 9,
        "title": "object detection with deep learning a review",
        "abstract": "Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems."
      },
      {
        "node_idx": 44070,
        "score_0_10": 9,
        "title": "the ibm 2016 speaker recognition system",
        "abstract": "In this paper we describe the recent advancements made in the IBM i-vector speaker recognition system for conversational speech. In particular, we identify key techniques that contribute to significant improvements in performance of our system, and quantify their contributions. The techniques include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is formulated to alleviate some of the limitations associated with the conventional linear discriminant analysis (LDA) that assumes Gaussian class-conditional distributions, 2) the application of speaker- and channel-adapted features, which are derived from an automatic speech recognition (ASR) system, for speaker recognition, and 3) the use of a deep neural network (DNN) acoustic model with a large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 speaker recognition evaluation (SRE) extended core conditions involving telephone and microphone trials. Experimental results indicate that: 1) the NDA is more effective (up to 35% relative improvement in terms of EER) than the traditional parametric LDA for speaker recognition, 2) when compared to raw acoustic features (e.g., MFCCs), the ASR speaker-adapted features provide gains in speaker recognition performance, and 3) increasing the number of output units in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k) provides consistent improvements in performance (for example from 37% to 57% relative EER gains over our baseline GMM i-vector system). To our knowledge, results reported in this paper represent the best performances published to date on the NIST SRE 2010 extended core tasks."
      },
      {
        "node_idx": 33955,
        "score_0_10": 9,
        "title": "building high level features using large scale unsupervised learning",
        "abstract": "We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8% accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative im- provement over the previous state-of-the-art."
      }
    ]
  },
  "352": {
    "explanation": "robot motion risk and reward-aware path planning",
    "topk": [
      {
        "node_idx": 167846,
        "score_0_10": 10,
        "title": "aerial robot control in close proximity to ceiling a force estimation based nonlinear mpc",
        "abstract": "Being motivated by ceiling inspection applications via unmanned aerial vehicles (UAVs) which require close proximity flight to surfaces, a systematic control approach enabling safe and accurate close proximity flight is proposed in this work. There are two main challenges for close proximity flights: (i) the trust characteristics varies drastically for the different distance from the ceiling which results in a complex nonlinear dynamics; (ii) the system needs to consider physical and environmental constraints to safely fly in close proximity. To address these challenges, a novel framework consisting of a constrained optimization-based force estimation and an optimization-based nonlinear controller is proposed. Experimental results illustrate that the performance of the proposed control approach can stabilize UAV down to 1 cm distance to the ceiling. Furthermore, we report that the UAV consumes up to 12.5% less power when it is operated 1 cm distance to ceiling, which is promising potential for more battery-efficient inspection flights."
      },
      {
        "node_idx": 114165,
        "score_0_10": 10,
        "title": "robust guided image filtering",
        "abstract": "The process of using one image to guide the filtering process of another one is called Guided Image Filtering (GIF). The main challenge of GIF is the structure inconsistency between the guidance image and the target image. Besides, noise in the target image is also a challenging issue especially when it is heavy. In this paper, we propose a general framework for Robust Guided Image Filtering (RGIF), which contains a data term and a smoothness term, to solve the two issues mentioned above. The data term makes our model simultaneously denoise the target image and perform GIF which is robust against the heavy noise. The smoothness term is able to make use of the property of both the guidance image and the target image which is robust against the structure inconsistency. While the resulting model is highly non-convex, it can be solved through the proposed Iteratively Re-weighted Least Squares (IRLS) in an efficient manner. For challenging applications such as guided depth map upsampling, we further develop a data-driven parameter optimization scheme to properly determine the parameter in our model. This optimization scheme can help to preserve small structures and sharp depth edges even for a large upsampling factor (8x for example). Moreover, the specially designed structure of the data term and the smoothness term makes our model perform well in edge-preserving smoothing for single-image tasks (i.e., the guidance image is the target image itself). This paper is an extension of our previous work [1], [2]."
      },
      {
        "node_idx": 85872,
        "score_0_10": 10,
        "title": "visual abstraction",
        "abstract": "In this article we revisit the concept of abstraction as it is used in visualization and put it on a solid formal footing. While the term \\emph{abstraction} is utilized in many scientific disciplines, arts, as well as everyday life, visualization inherits the notion of data abstraction or class abstraction from computer science, topological abstraction from mathematics, and visual abstraction from arts. All these notions have a lot in common, yet there is a major discrepancy in the terminology and basic understanding about visual abstraction in the context of visualization. We thus root the notion of abstraction in the philosophy of science, clarify the basic terminology, and provide crisp definitions of visual abstraction as a process. Furthermore, we clarify how it relates to similar terms often used interchangeably in the field of visualization. Visual abstraction is characterized by a conceptual space where this process exists, by the purpose it should serve, and by the perceptual and cognitive qualities of the beholder. These characteristics can be used to control the process of visual abstraction to produce effective and informative visual representations."
      },
      {
        "node_idx": 10470,
        "score_0_10": 10,
        "title": "higher order momentum distributions and locally affine lddmm registration",
        "abstract": "To achieve sparse parametrizations that allows intuitive analysis, we aim to represent deformation with a basis containing interpretable elements, and we wish to use elements that have the description capacity to represent the deformation compactly. To accomplish this, we introduce in this paper higher-order momentum distributions in the LDDMM registration framework. While the zeroth order moments previously used in LDDMM only describe local displacement, the first-order momenta that are proposed here represent a basis that allows local description of affine transformations and subsequent compact description of non-translational movement in a globally non-rigid deformation. The resulting representation contains directly interpretable information from both mathematical and modeling perspectives. We develop the mathematical construction of the registration framework with higher-order momenta, we show the implications for sparse image registration and deformation description, and we provide examples of how the parametrization enables registration with a very low number of parameters. The capacity and interpretability of the parametrization using higher-order momenta lead to natural modeling of articulated movement, and the method promises to be useful for quantifying ventricle expansion and progressing atrophy during Alzheimer's disease."
      },
      {
        "node_idx": 76333,
        "score_0_10": 10,
        "title": "explicit motion risk representation",
        "abstract": "This paper presents a formal definition and explicit representation of robot motion risk. Currently, robot motion risk has not been formally defined, but has already been used in motion and path planning. Risk is either implicitly represented as model uncertainty using probabilistic approaches, where the definition of risk is somewhat avoided, or explicitly modeled as a simple function of states, without a formal definition. In this work, we provide formal reasoning behind what risk is for robot motion and propose a formal definition of risk in terms of a sequence of motion, namely path. Mathematical approaches to represent motion risk are also presented, which is in accordance with our risk definition and properties. The definition and representation of risk provide a meaningful way to evaluate or construct robot motion or path plans. The understanding of risk is even of greater interest for the search and rescue community: the deconstructed environments cast extra risk onto the robot, since they are working under extreme conditions. A proper risk representation has the potential to reduce robot failure in the field."
      },
      {
        "node_idx": 119750,
        "score_0_10": 10,
        "title": "explicit risk aware path planning with reward maximization",
        "abstract": "This paper develops a path planner that minimizes risk (e.g. motion execution) while maximizing accumulated reward (e.g., quality of sensor viewpoint) motivated by visual assistance or tracking scenarios in unstructured or confined environments. In these scenarios, the robot should maintain the best viewpoint as it moves to the goal. However, in unstructured or confined environments, some paths may increase the risk of collision; therefore there is a tradeoff between risk and reward. Conventional state-dependent risk or probabilistic uncertainty modeling do not consider path-level risk or is difficult to acquire. This risk-reward planner explicitly represents risk as a function of motion plans, i.e., paths. Without manual assignment of the negative impact to the planner caused by risk, this planner takes in a pre-established viewpoint quality map and plans target location and path leading to it simultaneously, in order to maximize overall reward along the entire path while minimizing risk. Exact and approximate algorithms are presented, whose solution is further demonstrated on a physical tethered aerial vehicle. Other than the visual assistance problem, the proposed framework also provides a new planning paradigm to address minimum-risk planning under dynamical risk and absence of substructure optimality and to balance the trade-off between reward and risk."
      },
      {
        "node_idx": 58780,
        "score_0_10": 10,
        "title": "collaborative visual area coverage",
        "abstract": "Abstract   This article examines the problem of visual area coverage by a network of Mobile Aerial Agents (MAAs). Each MAA is assumed to be equipped with a downwards facing camera with a conical field of view which covers all points within a circle on the ground. The diameter of that circle is proportional to the altitude of the MAA, whereas the quality of the covered area decreases with the altitude. A distributed control law that maximizes a joint coverage-quality criterion by adjusting the MAAs\u2019 spatial coordinates is developed. The effectiveness of the proposed control scheme is evaluated through simulation studies."
      },
      {
        "node_idx": 164841,
        "score_0_10": 10,
        "title": "visual backpropagation",
        "abstract": "We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem."
      },
      {
        "node_idx": 43757,
        "score_0_10": 9,
        "title": "tight localizations of feedback sets",
        "abstract": "The classical NP-hard \\emph{feedback arc set problem} (FASP) and \\emph{feedback vertex set problem} (FVSP) ask for a minimum set of arcs $\\varepsilon \\subseteq E$ or vertices $\\nu \\subseteq V$ whose removal $G\\setminus \\varepsilon$, $G\\setminus \\nu$ makes a given multi-digraph $G=(V,E)$ acyclic, respectively. The corresponding decision problems are part of the $21$ NP-complete problems of R. M. Karp's famous list. Though both problems are known to be APX-hard, approximation algorithms or proofs of inapproximability are unknown. We propose a new $\\mathcal{O}(|V||E|^4)$-heuristic for the directed FASP. While $r \\approx 1.3606$ is known to be a lower bound for the APX-hardness, at least by validation, we show that $r \\leq 2$. Applying the approximation preserving $L$-reduction from the directed FVSP to the FASP thereby allows computing feedback vertex sets with the same accuracy. #R##N#Benchmarking the algorithm with state of the art alternatives yields that, for the first time, the most relevant instance class of large sparse graphs can be solved efficiently within tight error bounds. Our derived insights might provide a path to prove the APX-completeness of both problems."
      },
      {
        "node_idx": 103498,
        "score_0_10": 9,
        "title": "a learning based visual saliency prediction model for stereoscopic 3d video lbvs 3d",
        "abstract": "Over the past decade, many computational saliency prediction models have been proposed for 2D images and videos. Considering that the human visual system has evolved in a natural 3D environment, it is only natural to want to design visual attention models for 3D content. Existing monocular saliency models are not able to accurately predict the attentive regions when applied to 3D image/video content, as they do not incorporate depth information. This paper explores stereoscopic video saliency prediction by exploiting both low-level attributes such as brightness, color, texture, orientation, motion, and depth, as well as high-level cues such as face, person, vehicle, animal, text, and horizon. Our model starts with a rough segmentation and quantifies several intuitive observations such as the effects of visual discomfort level, depth abruptness, motion acceleration, elements of surprise, size and compactness of the salient regions, and emphasizing only a few salient objects in a scene. A new fovea-based model of spatial distance between the image regions is adopted for considering local and global feature calculations. To efficiently fuse the conspicuity maps generated by our method to one single saliency map that is highly correlated with the eye-fixation data, a random forest based algorithm is utilized. The performance of the proposed saliency model is evaluated against the results of an eye-tracking experiment, which involved 24 subjects and an in-house database of 61 captured stereoscopic videos. Our stereo video database as well as the eye-tracking data are publicly available along with this paper. Experiment results show that the proposed saliency prediction method achieves competitive performance compared to the state-of-the-art approaches."
      }
    ]
  },
  "353": {
    "explanation": "human and machine agency interaction effects",
    "topk": [
      {
        "node_idx": 84147,
        "score_0_10": 10,
        "title": "online networks and subjective well being",
        "abstract": "We argue that the use of online networks may threaten subjective well-being in several ways, due to the inherent attributes of Internet-mediated interaction and through its effects on social trust and sociability. We test our hypotheses on a representative sample of the Italian population. We find a significantly negative correlation between online networking and well-being. This result is partially confirmed after accounting for endogeneity. We explore the direct and indirect effects of the use of social networking sites (SNS) on well-being in a SEM analysis. We find that online networking plays a positive role in subjective well-being through its impact on physical interactions, whereas SNS use is associated with lower social trust. The overall effect of networking on individual welfare is significantly negative."
      },
      {
        "node_idx": 42309,
        "score_0_10": 10,
        "title": "molecular and dna artificial neural networks via fractional coding",
        "abstract": "This paper considers implementation of artificial neural networks (ANNs) using molecular computing and DNA based on fractional coding. Prior work had addressed molecular two-layer ANNs with binary inputs and arbitrary weights. In prior work using fractional coding, a simple molecular perceptron that computes sigmoid of scaled weighted sum of the inputs was presented where the inputs and the weights lie between [-1, 1]. Even for computing the perceptron, the prior approach suffers from two major limitations. First, it cannot compute the sigmoid of the weighted sum, but only the sigmoid of the scaled weighted sum. Second, many machine learning applications require the coefficients to be arbitrarily positive and negative numbers that are not bounded between [-1, 1]; such numbers cannot be handled by the prior perceptron using fractional coding. This paper makes four contributions. First molecular perceptrons that can handle arbitrary weights and can compute sigmoid of the weighted sums are presented. Thus, these molecular perceptrons are ideal for regression applications and multi-layer ANNs. A new molecular divider is introduced and is used to compute sigmoid(ax) where a > 1. Second, based on fractional coding, a molecular artificial neural network (ANN) with one hidden layer is presented. Third, a trained ANN classifier with one hidden layer from seizure prediction application from electroencephalogram is mapped to molecular reactions and DNA and their performances are presented. Fourth, molecular activation functions for rectified linear unit (ReLU) and softmax are also presented."
      },
      {
        "node_idx": 112011,
        "score_0_10": 10,
        "title": "automation in human machine networks how increasing machine agency affects human agency",
        "abstract": "Efficient human-machine networks require productive interaction between human and machine actors. In this study, we address how a strengthening of machine agency, for example through increasing levels of automation, affect the human actors of the networks. Findings from case studies within air traffic management, crisis management, and crowd evacuation are presented, exemplifying how automation may strengthen the agency of human actors in the network through responsibility sharing and task allocation, and serve as a needed prerequisite of innovation and change."
      },
      {
        "node_idx": 118581,
        "score_0_10": 10,
        "title": "mediated behavioural change in human machine networks exploring network characteristics trust and motivation",
        "abstract": "Human-machine networks pervade much of contemporary life. Network change is the product of structural modifications along with differences in participant be-havior. If we assume that behavioural change in a human-machine network is the result of changing the attitudes of participants in the network, then the question arises whether network structure can affect participant attitude. Taking citizen par-ticipation as an example, engagement with relevant stakeholders reveals trust and motivation to be the major objectives for the network. Using a typology to de-scribe network state based on multiple characteristic or dimensions, we can pre-dict possible behavioural outcomes in the network. However, this has to be medi-ated via attitude change. Motivation for the citizen participation network can only increase in line with enhanced trust. The focus for changing network dynamics, therefore, shifts to the dimensional changes needed to encourage increased trust. It turns out that the coordinated manipulation of multiple dimensions is needed to bring about the desired shift in attitude."
      },
      {
        "node_idx": 163756,
        "score_0_10": 10,
        "title": "articulation and clarification of the dendritic cell algorithm",
        "abstract": "The Dendritic Cell algorithm (DCA) is inspired by recent work in innate immunity. In this paper a formal description of the DCA is given. The DCA is described in detail, and its use as an anomaly detector is illustrated within the context of computer security. A port scan detection task is performed to substantiate the influence of signal selection on the behaviour of the algorithm. Experimental results provide a comparison of differing input signal mappings."
      },
      {
        "node_idx": 167109,
        "score_0_10": 10,
        "title": "online human bot interactions detection estimation and characterization",
        "abstract": "Increasing evidence suggests that a growing amount of social media content is generated by autonomous entities known as social bots. In this work we present a framework to detect such entities on Twitter. We leverage more than a thousand features extracted from public data and meta-data about users: friends, tweet content and sentiment, network patterns, and activity time series. We benchmark the classification framework by using a publicly available dataset of Twitter bots. This training data is enriched by a manually annotated collection of active Twitter users that include both humans and bots of varying sophistication. Our models yield high accuracy and agreement with each other and can detect bots of different nature. Our estimates suggest that between 9% and 15% of active Twitter accounts are bots. Characterizing ties among accounts, we observe that simple bots tend to interact with bots that exhibit more human-like behaviors. Analysis of content flows reveals retweet and mention strategies adopted by bots to interact with different target groups. Using clustering analysis, we characterize several subclasses of accounts, including spammers, self promoters, and accounts that post content from connected applications."
      },
      {
        "node_idx": 3995,
        "score_0_10": 10,
        "title": "an information theoretic measure of judea pearl s identifiability and causal influence",
        "abstract": "In this paper, we define a new information theoretic measure that we call the \"uprooted information\". We show that a necessary and sufficient condition for a probability $P(s|do(t))$ to be \"identifiable\" (in the sense of Pearl) in a graph $G$ is that its uprooted information be non-negative for all models of the graph $G$. In this paper, we also give a new algorithm for deciding, for a Bayesian net that is semi-Markovian, whether a probability $P(s|do(t))$ is identifiable, and, if it is identifiable, for expressing it without allusions to confounding variables. Our algorithm is closely based on a previous algorithm by Tian and Pearl, but seems to correct a small flaw in theirs. In this paper, we also find a {\\it necessary and sufficient graphical condition} for a probability $P(s|do(t))$ to be identifiable when $t$ is a singleton set. So far, in the prior literature, it appears that only a {\\it sufficient graphical condition} has been given for this. By \"graphical\" we mean that it is directly based on Judea Pearl's 3 rules of do-calculus."
      },
      {
        "node_idx": 37335,
        "score_0_10": 10,
        "title": "botornot a system to evaluate social bots",
        "abstract": "While most online social media accounts are controlled by humans, these platforms also host automated agents called social bots or sybil accounts. Recent literature reported on cases of social bots imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate the extent to which a Twitter account exhibits similarity to the known characteristics of social bots. Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs."
      },
      {
        "node_idx": 34471,
        "score_0_10": 9,
        "title": "satellite atm network architectural considerations and tcp ip performance",
        "abstract": "In this paper, we have provided a summary of the design options in Satellite-ATM technology. A satellite ATM network consists of a space segment of satellites connected by inter-satellite crosslinks, and a ground segment of the various ATM networks. A satellite-ATM interface module connects the satellite network to the ATM networks and performs various call and control functions. A network control center performs various network management and resource allocation functions. Several issues such as the ATM service model, media access protocols, and traffic management issues must be considered when designing a satellite ATM network to effectively transport Internet traffic. We have presented the buffer requirements for TCP/IP traffic over ATM-UBR for satellite latencies. Our results are based on TCP with selective acknowledgments and a per-VC buffer management policy at the switches. A buffer size of about 0.5 * RTT to 1 * RTT is sufficient to provide over 98% throughput to infinite TCP traffic for long latency networks and a large number of sources. This buffer requirement is independent of the number of sources. The fairness is high for a large numbers of sources because of the per-VC buffer management performed at the switches and the nature of TCP traffic."
      },
      {
        "node_idx": 129248,
        "score_0_10": 9,
        "title": "the interplay between human and machine agency",
        "abstract": "Human-machine networks affect many aspects of our lives: from sharing experiences with family and friends, knowledge creation and distance learning, and managing utility bills or providing feedback on retail items, to more specialised networks providing decision support to human operators and the delivery of health care via a network of clinicians, family, friends, and both physical and virtual social robots. Such networks rely on increasingly sophisticated machine algorithms, e.g., to recommend friends or purchases, to track our online activities in order to optimise the services available, and assessing risk to help maintain or even enhance people's health. Users are being offered ever increasing power and reach through these networks by machines which have to support and allow users to be able to achieve goals such as maintaining contact, making better decisions, and monitoring their health. As such, this comes down to a synergy between human and machine agency in which one is dependent in complex ways on the other. With that agency questions arise about trust, risk and regulation, as well as social influence and potential for computer-mediated self-efficacy. In this paper, we explore these constructs and their relationships and present a model based on review of the literature which seeks to identify the various dependencies between them."
      }
    ]
  },
  "354": {
    "explanation": "advanced blockchain consensus and cryptographic security protocols",
    "topk": [
      {
        "node_idx": 57558,
        "score_0_10": 10,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 96849,
        "score_0_10": 9,
        "title": "enhancing bitcoin security and performance with strong consistency via collective signing",
        "abstract": "While showing great promise, Bitcoin requires users to wait tens of minutes for transactions to commit, and even then, offering only probabilistic guarantees. This paper introduces ByzCoin, a novel Byzantine consensus protocol that leverages scalable collective signing to commit Bitcoin transactions irreversibly within seconds. ByzCoin achieves Byzantine consensus while preserving Bitcoin's open membership by dynamically forming hash power-proportionate consensus groups that represent recently-successful block miners. ByzCoin employs communication trees to optimize transaction commitment and verification under normal operation while guaranteeing safety and liveness under Byzantine faults, up to a near-optimal tolerance of f faulty group members among 3f + 2 total. ByzCoin mitigates double spending and selfish mining attacks by producing collectively signed transaction blocks within one minute of transaction submission. Tree-structured communication further reduces this latency to less than 30 seconds. Due to these optimizations, ByzCoin achieves a throughput higher than PayPal currently handles, with a confirmation latency of 15-20 seconds."
      },
      {
        "node_idx": 162969,
        "score_0_10": 9,
        "title": "on the cost of concurrency in transactional memory",
        "abstract": "Traditional techniques for synchronization are based on \\emph{locking} that provides threads with exclusive access to shared data. \\emph{Coarse-grained} locking typically forces threads to access large amounts of data sequentially and, thus, does not fully exploit hardware concurrency. Program-specific \\emph{fine-grained} locking or \\emph{non-blocking} (\\emph{i.e.}, not using locks) synchronization, on the other hand, is a dark art to most programmers and trusted to the wisdom of a few computing experts. Thus, it is appealing to seek a middle ground between these two extremes: a synchronization mechanism that relieves the programmer of the overhead of reasoning about data conflicts that may arise from concurrent operations without severely limiting the program's performance. The \\emph{Transactional Memory (TM)} abstraction is proposed as such a mechanism: it intends to combine an easy-to-use programming interface with an efficient utilization of the concurrent-computing abilities provided by multicore architectures. TM allows the programmer to \\emph{speculatively} execute sequences of shared-memory operations as \\emph{atomic transactions} with \\emph{all-or-nothing} semantics: the transaction can either \\emph{commit}, in which case it appears as executed sequentially, or \\emph{abort}, in which case its update operations do not take effect. Thus, the programmer can design software having only sequential semantics in mind and let TM take care, at run-time, of resolving the conflicts in concurrent executions. #R##N#Intuitively, we want TMs to allow for as much \\emph{concurrency} as possible: in the absence of severe data conflicts, transactions should be able to progress in parallel. But what are the inherent costs associated with providing high degrees of concurrency in TMs? This is the central question of the thesis."
      },
      {
        "node_idx": 102043,
        "score_0_10": 9,
        "title": "bigdatabench a big data benchmark suite from internet services",
        "abstract": "As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above."
      },
      {
        "node_idx": 46715,
        "score_0_10": 9,
        "title": "bitcoin ng a scalable blockchain protocol",
        "abstract": "Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential. #R##N#This paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem. #R##N#In addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network."
      },
      {
        "node_idx": 51514,
        "score_0_10": 9,
        "title": "consensus on transaction commit",
        "abstract": "The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. Running a Paxos consensus algorithm on the commit/abort decision of each participant yields a transaction commit protocol that uses 2F +1 coordinators and makes progress if at least F +1 of them are working. In the fault-free case, this algorithm requires one extra message delay but has the same stable-storage write delay as Two-Phase Commit. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the general Paxos Commit algorithm."
      },
      {
        "node_idx": 165403,
        "score_0_10": 8,
        "title": "scaliendb designing and implementing a distributed database using paxos",
        "abstract": "ScalienDB is a scalable, replicated database built on top of the Paxos algorithm. It was developed from 2010 to 2012, when the startup backing it failed. This paper discusses the design decisions of the distributed database, describes interesting parts of the C++ codebase and enumerates lessons learned putting ScalienDB into production at a handful of clients. The source code is available on Github under the AGPL license, but it is no longer developed or maintained."
      },
      {
        "node_idx": 84359,
        "score_0_10": 8,
        "title": "information theoretically secret key generation for fading wireless channels",
        "abstract": "The multipath-rich wireless environment associated with typical wireless usage scenarios is characterized by a fading channel response that is time-varying, location-sensitive, and uniquely shared by a given transmitter-receiver pair. The complexity associated with a richly scattering environment implies that the short-term fading process is inherently hard to predict and best modeled stochastically, with rapid decorrelation properties in space, time and frequency. In this paper, we demonstrate how the channel state between a wireless transmitter and receiver can be used as the basis for building practical secret key generation protocols between two entities. We begin by presenting a scheme based on level crossings of the fading process, which is well-suited for the Rayleigh and Rician fading models associated with a richly scattering environment. Our level crossing algorithm is simple, and incorporates a self-authenticating mechanism to prevent adversarial manipulation of message exchanges during the protocol. Since the level crossing algorithm is best suited for fading processes that exhibit symmetry in their underlying distribution, we present a second and more powerful approach that is suited for more general channel state distributions. This second approach is motivated by observations from quantizing jointly Gaussian processes, but exploits empirical measurements to set quantization boundaries and a heuristic log likelihood ratio estimate to achieve an improved secret key generation rate. We validate both proposed protocols through experimentations using a customized 802.11a platform, and show for the typical WiFi channel that reliable secret key establishment can be accomplished at rates on the order of 10 bits/second."
      },
      {
        "node_idx": 30590,
        "score_0_10": 8,
        "title": "exponential decreasing rate of leaked information in universal random privacy amplification",
        "abstract": "We derive a new upper bound for Eve's information in secret key generation from a common random number without communication. This bound improves on Bennett 's bound based on the Renyi entropy of order 2 because the bound obtained here uses the Renyi entropy of order 1+s for s \u2208 [0,1]. This bound is applied to a wire-tap channel. Then, we derive an exponential upper bound for Eve's information. Our exponent is compared with Hayashi 's exponent. For the additive case, the bound obtained here is better. The result is applied to secret key agreement by public discussion."
      },
      {
        "node_idx": 22646,
        "score_0_10": 8,
        "title": "secret key agreement with channel state information at the transmitter",
        "abstract": "We study the capacity of secret-key agreement over a wiretap channel with state parameters. The transmitter, the legitimate receiver, and the eavesdropper are connected by a discrete memoryless wiretap channel with a memoryless state sequence. The transmitter and the legitimate receiver generate a secret-key that must be concealed from the eavesdropper. We assume that the state sequence is known noncausally to the transmitter and no public discussion channel is available. We derive lower and upper bounds on the secret-key capacity. The lower bound involves a source-channel codebook for constructing a common reconstruction sequence at the legitimate terminals and then mapping this sequence to a secret-key using a secret-key codebook. For the special case of Gaussian channels with additive interference (secret-keys from dirty paper channel) our bounds differ by 0.5 bit/symbol and coincide in the high signal-to-noise-ratio and high interference-to-noise-ratio regimes. In another special case-symmetric channel state information (CSI)-when the legitimate receiver is also revealed the state sequence, we establish optimality of our lower bound. In addition, only causal side information at the transmitter and the receiver suffices to attain the secret-key capacity in the case of symmetric CSI."
      }
    ]
  },
  "356": {
    "explanation": "Network protocol detection and anomaly analysis in DNS tunneling",
    "topk": [
      {
        "node_idx": 54224,
        "score_0_10": 10,
        "title": "identifying dns tunneled traffic with predictive models",
        "abstract": "DNS is a distributed, fault tolerant system that avoids a single point of failure. As such it is an integral part of the internet as we use it today and hence deemed a safe protocol which is let through firewalls and proxies with no or little checks. This can be exploited by malicious agents. Network forensics is effective but struggles due to size of data and manual labour. This paper explores to what extent predictive models can be used to predict network traffic, what protocols are tunneled in the DNS protocol and more specifically whether the predictive performance is enhanced when analyzing DNS-queries and responses together and which feature set that can be used for DNS-tunneled network prediction. The tested protocols are SSH, SFTP and Telnet and the machine learning models used are Multi Layered Perceptron and Random Forests. To train the models we extract the IP Packet length, Name length and Name entropy of both the queries and responses in the DNS traffic. With an experimental research strategy it is empirically shown that the performance of the models increases when training the models on the query and respose pairs rather than using only queries or responses. The accuracy of the models is >83% and reduction in data size when features are extracted is roughly 95%. Our results provides evidence that machine learning is a valuable tool in detecting network protocols in a DNS tunnel and that only an small subset of network traffic is needed to detect this anomaly."
      },
      {
        "node_idx": 126092,
        "score_0_10": 10,
        "title": "entropy based prediction of network protocols in the forensic analysis of dns tunnels",
        "abstract": "DNS tunneling techniques are often used for malicious purposes but network security mechanisms have struggled to detect these. Network forensic analysis has thus been used but has proved slow and effort intensive as Network Forensics Analysis Tools struggle to deal with undocumented or new network tunneling techniques. In this paper we present a method to aid forensic analysis through automating the inference of protocols tunneled within DNS tunneling techniques. We analyze the internal packet structure of DNS tunneling techniques and characterize the information entropy of different network protocols and their DNS tunneled equivalents. From this, we present our protocol prediction method that uses entropy distribution averaging. Finally we apply our method on a dataset to measure its performance and show that it has a prediction accuracy of 75%. Our method also preserves privacy as it does not parse the actual tunneled content, rather it only calculates the information entropy."
      },
      {
        "node_idx": 47935,
        "score_0_10": 10,
        "title": "perceptual copyright protection using multiresolution wavelet based watermarking and fuzzy logic",
        "abstract": "In this paper, an efficiently DWT-based watermarking technique is proposed to embed signatures in images to attest the owner identification and discourage the unauthorized copying. This paper deals with a fuzzy inference filter to choose the larger entropy of coefficients to embed watermarks. Unlike most previous watermarking frameworks which embedded watermarks in the larger coefficients of inner coarser subbands, the proposed technique is based on utilizing a context model and fuzzy inference filter by embedding watermarks in the larger-entropy coefficients of coarser DWT subbands. The proposed approaches allow us to embed adaptive casting degree of watermarks for transparency and robustness to the general image-processing attacks such as smoothing, sharpening, and JPEG compression. The approach has no need the original host image to extract watermarks. Our schemes have been shown to provide very good results in both image transparency and robustness."
      },
      {
        "node_idx": 53918,
        "score_0_10": 10,
        "title": "judicious qos using cloud overlays",
        "abstract": "We revisit the long-standing problem of providing network QoS to applications, and propose the concept of judicious QoS -- combining the cheaper, best effort IP service with the cloud, which offers a highly reliable infrastructure and the ability to add in-network services, albeit at higher cost. Our proposed J-QoS framework offers a range of reliability services with different cost vs. delay trade-offs, including: i) a forwarding service that forwards packets over the cloud overlay, ii) a caching service, which stores packets inside the cloud and allows them to be pulled in case of packet loss or disruption on the Internet, and iii) a novel coding service that provides the least expensive packet recovery option by combining packets of multiple application streams and sending a small number of coded packets across the more expensive cloud paths. We demonstrate the feasibility of these services using measurements from RIPE Atlas and a live deployment on PlanetLab. We also consider case studies on how J-QoS works with services up and down the network stack, including Skype video conferencing, TCP-based web transfers, and cellular access networks."
      },
      {
        "node_idx": 79147,
        "score_0_10": 10,
        "title": "tromino demand and drf aware multi tenant queue manager for apache mesos cluster",
        "abstract": "Apache Mesos, a two-level resource scheduler, provides resource sharing across multiple users in a multi-tenant clustered environment. Computational resources (i.e., CPU, memory, disk, etc.) are distributed according to the Dominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive resources based on their current usage and are responsible for scheduling their tasks within the allocation. We have observed that multiple frameworks can cause fairness imbalance in a multi-user environment. For example, a greedy framework consuming more than its fair share of resources can deny resource fairness to others. The user with the least Dominant Share is considered first by the DRF module to get its resource allocation. However, the default DRF implementation, in Apache Mesos' Master allocation module, does not consider the overall resource demands of the tasks in the queue for each user/framework. This lack of awareness can lead to poor performance as users without any pending task may receive more resource offers, and users with a queue of pending tasks can starve due to their high dominant shares. In a multi-tenant environment, the characteristics of frameworks and workloads must be understood by cluster managers to be able to define fairness based on not only resource share but also resource demand and queue wait time. We have developed a policy driven queue manager, Tromino, for an Apache Mesos cluster where tasks for individual frameworks can be scheduled based on each framework's overall resource demands and current resource consumption. Dominant Share and demand awareness of Tromino and scheduling based on these attributes can reduce (1) the impact of unfairness due to a framework specific configuration, and (2) unfair waiting time due to higher resource demand in a pending task queue. In the best case, Tromino can significantly reduce the average waiting time of a framework by using the proposed Demand-DRF aware policy."
      },
      {
        "node_idx": 23931,
        "score_0_10": 10,
        "title": "negative selection approach application in network intrusion detection systems",
        "abstract": "Abstract\u2014Nature has always been an inspiration to researcherswith its diversity and robustness of its systems, and Arti\ufb01cialImmune Systems are one of them. Many algorithms were inspiredby ongoing discoveries of biological immune systems techniquesand approaches. One of the basic and most common approachis the Negative Selection Approach, which is simple and easy toimplement. It was applied in many \ufb01elds, but mostly in anomalydetection for the similarity of its basic idea. In this paper, areview is given on the application of negative selection approachin network security, speci\ufb01cally the intrusion detection system.As the work in this \ufb01eld is limited, we need to understand whatthe challenges of this approach are. Recommendations are givenby the end of the paper for future work. I. I NTRODUCTION Networks are more vulnerable by time to intrusions andattacks, from inside and outside. Cyber-attacks are makingnews headlines worldwide, as threats to networks are gettingbolder and more sophisticated. Reports of 2011 and 2012are showing an increase in network attacks, with Denial ofService (DoS) and targeted attacks having a big share in it.As reported by many web sites like [1] [2] [3], \ufb01gures 1 and 2show motivations behind attacks and targeted customer typesrespectively.Internal threats and Advanced Persistent Threats (APT)are the biggest threats to a network, as they are carefullyconstructed and dangerous, due to internal users\u2019 privilegesto access network resources. Figure 3 shows internal networksecurity concerns. With this in mind, and the increasing so-phistication of attacks, new approaches to protect the networkresources are always under investigation, and the one thatis concerned with inside and outside threats is the IntrusionDetection System.Intrusion detection systems [4] [5] [6] have been around forquite some time, as a successful security system. An IntrusionDetection System (IDS) is a system that de\ufb01nes and detectspossible threats within a computer or a network, by gatheringand analysing information from the surrounding environment."
      },
      {
        "node_idx": 101511,
        "score_0_10": 10,
        "title": "steganalysis of transcoding steganography",
        "abstract": "TranSteg (Trancoding Steganography) is a fairly new IP telephony steganographic method that functions by compressing overt (voice) data to make space for the steganogram by means of transcoding. It offers high steganographic bandwidth, retains good voice quality and is generally harder to detect than other existing VoIP steganographic methods. In TranSteg, after the steganogram reaches the receiver, the hidden information is extracted and the speech data is practically restored to what was originally sent. This is a huge advantage compared with other existing VoIP steganographic methods, where the hidden data can be extracted and removed but the original data cannot be restored because it was previously erased due to a hidden data insertion process. In this paper we address the issue of steganalysis of TranSteg. Various TranSteg scenarios and possibilities of warden(s) localization are analyzed with regards to the TranSteg detection. A steganalysis method based on MFCC (Mel-Frequency Cepstral Coefficients) parameters and GMMs (Gaussian Mixture Models) was developed and tested for various overt/covert codec pairs in a single warden scenario with double transcoding. The proposed method allowed for efficient detection of some codec pairs (e.g., G.711/G.729), whilst some others remained more resistant to detection (e.g., iLBC/AMR)."
      },
      {
        "node_idx": 33940,
        "score_0_10": 10,
        "title": "influence of speech codecs selection on transcoding steganography",
        "abstract": "The typical approach to steganography is to compress the covert data in order to limit its size, which is reasonable in the context of a limited steganographic bandwidth. TranSteg (Trancoding Steganography) is a new IP telephony steganographic method that was recently proposed that offers high steganographic bandwidth while retaining good voice quality. In TranSteg, compression of the overt data is used to make space for the steganogram. In this paper we focus on analyzing the influence of the selection of speech codecs on hidden transmission performance, that is, which codecs would be the most advantageous ones for TranSteg. Therefore, by considering the codecs which are currently most popular for IP telephony we aim to find out which codecs should be chosen for transcoding to minimize the negative influence on voice quality while maximizing the obtained steganographic bandwidth."
      },
      {
        "node_idx": 91474,
        "score_0_10": 10,
        "title": "sensing danger innate immunology for intrusion detection",
        "abstract": "The immune system provides an ideal metaphor for anomaly detection in general and computer securities in particular. Based on this idea, artificial immune systems have been used for a number of years for intrusion detection, unfortunately so far with little success. However, these previous systems were largely based on immunological theory from the 1970s and 1980s and over the last decade our understanding of immunological processes has vastly improved. In this paper we present two new immune-inspired algorithms based on the latest immunological discoveries, such as the behaviour of Dendritic Cells. The resultant algorithms are applied to real-world intrusion problems and show encouraging results. Overall, we believe that there is a bright future for these next-generation artificial immune algorithms."
      },
      {
        "node_idx": 63359,
        "score_0_10": 10,
        "title": "allocation and admission policies for service streams",
        "abstract": "A service provisioning system is examined, where a number of servers are used to offer different types of services to paying customers. A customer is charged for the execution of a stream of jobs; the number of jobs in the stream and the rate of their submission is specified. On the other hand, the provider promises a certain quality of service (QoS), measured by the average waiting time of the jobs in the stream. A penalty is paid if the agreed QoS requirement is not met. The objective is to maximize the total average revenue per unit time. Dynamic policies for making server allocation and stream admission decisions are introduced and evaluated. The results of several simulations are described."
      }
    ]
  },
  "358": {
    "explanation": "symbol grounding in hybrid symbolic-connectionist cognitive models",
    "topk": [
      {
        "node_idx": 168237,
        "score_0_10": 10,
        "title": "the symbol grounding problem",
        "abstract": "There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the \"symbol grounding problem\": How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) \"iconic representations\" , which are analogs of the proximal sensory projections of distal objects and events, and (2) \"categorical representations\" , which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) \"symbolic representations\" , grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., \"An X is a Y that is Z\"). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic \"module,\" however; the symbolic functions would emerge as an intrinsically \"dedicated\" symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded."
      },
      {
        "node_idx": 43354,
        "score_0_10": 10,
        "title": "neural turing machines",
        "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."
      },
      {
        "node_idx": 150622,
        "score_0_10": 9,
        "title": "boolean logic gates from a single memristor via low level sequential logic",
        "abstract": "By using the memristor's memory to both store a bit and perform an operation with a second input bit, simple Boolean logic gates have been built with a single memristor. The operation makes use of the interaction of current spikes (occasionally called current transients) found in both memristors and other devices. The sequential time-based logic methodology allows two logical input bits to be used on a one-port by sending the bits separated in time. The resulting logic gate is faster than one relying on memristor's state switching, low power and requires only one memristor. We experimentally demonstrate working OR and XOR gates made with a single flexible Titanium dioxide sol-gel memristor."
      },
      {
        "node_idx": 116210,
        "score_0_10": 9,
        "title": "social capital predicts corruption risk in towns",
        "abstract": "Corruption is a social plague: gains accrue to small groups, while its costs are borne by everyone. Significant variation in its level between and within countries suggests a relationship between social structure and the prevalence of corruption, yet, large scale empirical studies thereof have been missing due to lack of data. In this paper we relate the structural characteristics of social capital of towns with corruption in their local governments. Using datasets from Hungary, we quantify corruption risk by suppressed competition and lack of transparency in the town's awarded public contracts. We characterize social capital using social network data from a popular online platform. Controlling for social, economic, and political factors, we find that settlements with fragmented social networks, indicating an excess of \\textit{bonding social capital} have higher corruption risk and towns with more diverse external connectivity, suggesting a surplus of \\textit{bridging social capital} are less exposed to corruption. We interpret fragmentation as fostering in-group favoritism and conformity, which increase corruption, while diversity facilitates impartiality in public life and stifles corruption."
      },
      {
        "node_idx": 48296,
        "score_0_10": 9,
        "title": "spiking memristor logic gates are a type of time variant perceptron",
        "abstract": "Memristors are low-power memory-holding resistors thought to be useful for neuromophic computing, which can compute via spike-interactions mediated through the device's short-term memory. Using interacting spikes, it is possible to build an AND gate that computes OR at the same time, similarly a full adder can be built that computes the arithmetical sum of its inputs. Here we show how these gates can be understood by modelling the memristors as a novel type of perceptron: one which is sensitive to input order. The memristor's memory can change the input weights for later inputs, and thus the memristor gates cannot be accurately described by a single perceptron, requiring either a network of time-invarient perceptrons or a complex time-varying self-reprogrammable perceptron. This work demonstrates the high functionality of memristor logic gates, and also that the addition of theasholding could enable the creation of a standard perceptron in hardware, which may have use in building neural net chips."
      },
      {
        "node_idx": 135057,
        "score_0_10": 8,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 153214,
        "score_0_10": 8,
        "title": "belief and surprise a belief function formulation",
        "abstract": "We motivate and describe a theory of belief in this paper. This theory is developed with the following view of human belief in mind. Consider the belief that an event E will occur (or has occurred or is occurring). An agent either entertains this belief or does not entertain this belief (i.e., there is no \"grade\" in entertaining the belief). If the agent chooses to exercise \"the will to believe\" and entertain this belief, he/she/it is entitled to a degree of confidence c (1 > c > 0) in doing so. Adopting this view of human belief, we conjecture that whenever an agent entertains the belief that E will occur with c degree of confidence, the agent will be surprised (to the extent c) upon realizing that E did not occur."
      },
      {
        "node_idx": 29312,
        "score_0_10": 8,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 55316,
        "score_0_10": 8,
        "title": "overcoming catastrophic forgetting in neural networks",
        "abstract": "The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially."
      },
      {
        "node_idx": 88157,
        "score_0_10": 8,
        "title": "probabilistic belief change expansion conditioning and constraining",
        "abstract": "The AGM theory of belief revision has become an important paradigm for investigating rational belief changes. Unfortunately, researchers working in this paradigm have restricted much of their attention to rather simple representations of belief states, namely logically closed sets of propositional sentences. In our opinion, this has resulted in a too abstract categorisation of belief change operations: expansion, revision, or contraction. Occasionally, in the AGM paradigm, also probabilistic belief changes have been considered, and it is widely accepted that the probabilistic version of expansion is conditioning. However, we argue that it may be more correct to view conditioning and expansion as two essentially different kinds of belief change, and that what we call constraining is a better candidate for being considered probabilistic expansion."
      }
    ]
  },
  "360": {
    "explanation": "neural network optimization and architecture innovations",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 130623,
        "score_0_10": 8,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      }
    ]
  },
  "361": {
    "explanation": "adaptive learning rate optimization techniques",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 43376,
        "score_0_10": 9,
        "title": "skip thought vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 88762,
        "score_0_10": 8,
        "title": "computational approach to anaphora resolution in spanish dialogues",
        "abstract": "This paper presents an algorithm for identifying noun-phrase antecedents of pronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora resolution requires numerous sources of information in order to find the correct antecedent of the anaphor. These sources can be of different kinds, e.g., linguistic information, discourse/dialogue structure information, or topic information. For this reason, our algorithm uses various different kinds of information (hybrid information). The algorithm is based on linguistic constraints and preferences and uses an anaphoric accessibility space within which the algorithm finds the noun phrase. We present some experiments related to this algorithm and this space using a corpus of 204 dialogues. The algorithm is implemented in Prolog. According to this study, 95.9% of antecedents were located in the proposed space, a precision of 81.3% was obtained for pronominal anaphora resolution, and 81.5% for adjectival anaphora."
      },
      {
        "node_idx": 135057,
        "score_0_10": 8,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 34794,
        "score_0_10": 8,
        "title": "supervised learning of universal sentence representations from natural language inference data",
        "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available."
      },
      {
        "node_idx": 140427,
        "score_0_10": 8,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      }
    ]
  },
  "364": {
    "explanation": "theory and complexity of well-covered graphs and game determinacy",
    "topk": [
      {
        "node_idx": 64530,
        "score_0_10": 10,
        "title": "theory of atomata",
        "abstract": "We show that every regular language defines a unique nondeterministic finite automaton (NFA), which we call \"\\'atomaton\", whose states are the \"atoms\" of the language, that is, non-empty intersections of complemented or uncomplemented left quotients of the language. We describe methods of constructing the \\'atomaton, and prove that it is isomorphic to the reverse automaton of the minimal deterministic finite automaton (DFA) of the reverse language. We study \"atomic\" NFAs in which the right language of every state is a union of atoms. We generalize Brzozowski's double-reversal method for minimizing a deterministic finite automaton (DFA), showing that the result of applying the subset construction to an NFA is a minimal DFA if and only if the reverse of the NFA is atomic. We prove that Sengoku's claim that his method always finds a minimal NFA is false."
      },
      {
        "node_idx": 66081,
        "score_0_10": 10,
        "title": "a new approach to updating beliefs",
        "abstract": "We define a new notion of conditional belief, which plays the same role for Dempster-Shafer belief functions as conditional probability does for probability functions. Our definition is different from the standard definition given by Dempster, and avoids many of the well-known problems of that definition. Just as the conditional probability Pr (lB) is a probability function which is the result of conditioning on B being true, so too our conditional belief function Bel (lB) is a belief function which is the result of conditioning on B being true. We define the conditional belief as the lower envelope (that is, the inf) of a family of conditional probability functions, and provide a closed form expression for it. An alternate way of understanding our definition of conditional belief is provided by considering ideas from an earlier paper [Fagin and Halpern, 1989], where we connect belief functions with inner measures. In particular, we show here how to extend the definition of conditional probability to non measurable sets, in order to get notions of inner and outer conditional probabilities, which can be viewed as best approximations to the true conditional probability, given our lack of information. Our definition of conditional belief turns out to be an exact analogue of our definition of inner conditional probability."
      },
      {
        "node_idx": 58432,
        "score_0_10": 10,
        "title": "complexity results on w well covered graphs",
        "abstract": "A graph G is well-covered if all its maximal independent sets are of the same cardinality. Assume that a weight function w is defined on its vertices. Then G is w-well-covered if all maximal independent sets are of the same weight. #R##N#For every graph G, the set of weight functions w such that G is w-well-covered is a vector space, denoted WCW(G). Let B be a complete bipartite induced subgraph of G on vertex sets of bipartition B_X and B_Y. Then B is generating if there exists an independent set S such that S \\cup B_X and S \\cup B_Y are both maximal independent sets of G. A relating edge is a generating subgraph in the restricted case that B = K_{1,1}. #R##N#Deciding whether an input graph G is well-covered is co-NP-complete. Therefore finding WCW(G) is co-NP-hard. Deciding whether an edge is relating is co-NP-complete. Therefore, deciding whether a subgraph is generating is co-NP-complete as well. #R##N#In this article we discuss the connections among these problems, provide proofs for NP-completeness for several restricted cases, and present polynomial characterizations for some other cases."
      },
      {
        "node_idx": 136959,
        "score_0_10": 10,
        "title": "well indumatched trees and graphs of bounded girth",
        "abstract": "A graph G is called well-indumatched if all of its maximal induced matchings have the same size. In this paper we characterize all well-indumatched trees. We provide a linear time algorithm to decide if a tree is well-indumatched or not. Then, we characterize minimal well-indumatched graphs of girth at least 9 and show subsequently that for an odd integer g greater than or equal to 9 and different from 11, there is no well-indumatched graph of girth g. On the other hand, there are infinitely many well-indumatched unicyclic graphs of girth k, where k is in {3, 5, 7} or k is an even integer greater than 2. We also show that, although the recognition of well-indumatched graphs is known to be co-NP-complete in general, one can recognize in polynomial time well-indumatched graphs where the size of maximal induced matchings is fixed."
      },
      {
        "node_idx": 118601,
        "score_0_10": 10,
        "title": "representing knowledge about norms",
        "abstract": "Norms are essential to extend inference: inferences based on norms are far richer than those based on logical implications. In the recent decades, much effort has been devoted to reason on a domain, once its norms are represented. How to extract and express those norms has received far less attention. Extraction is difficult: as the readers are supposed to know them, the norms of a domain are seldom made explicit. For one thing, extracting norms requires a language to represent them, and this is the topic of this paper. We apply this language to represent norms in the domain of driving, and show that it is adequate to reason on the causes of accidents, as described by car-crash reports."
      },
      {
        "node_idx": 59755,
        "score_0_10": 10,
        "title": "macwilliams type identities for some new m spotty weight enumerators over finite commutative frobenius rings",
        "abstract": "Past few years have seen an extensive use of RAM chips with wide I/O data (e.g. 16, 32, 64 bits) in computer memory systems. These chips are highly vulnerable to a special type of byte error, called an $m$-spotty byte error, which can be effectively detected or corrected using byte error-control codes. The MacWilliams identity provides the relationship between the weight distribution of a code and that of its dual. This paper introduces $m$-spotty Hamming weight enumerator, joint $m$-spotty Hamming weight enumerator and split $m$-spotty Hamming weight enumerator for byte error-control codes over finite commutative Frobenius rings as well as $m$-spotty Lee weight enumerator over an infinite family of rings. In addition, MacWilliams type identities are also derived for these enumerators."
      },
      {
        "node_idx": 160303,
        "score_0_10": 10,
        "title": "well covered graphs without cycles of lengths 4 5 and 6",
        "abstract": "A graph G is well-covered if all its maximal independent sets are of the same cardinality. Assume that a weight function w is defined on its vertices. Then G is w-well-covered if all maximal independent sets are of the same weight. For every graph G, the set of weight functions w such that G is w-well-covered is a vector space. Given an input graph G without cycles of length 4, 5, and 6, we characterize polynomially the vector space of weight functions w for which G is w-well-covered. Let B be an induced complete bipartite subgraph of G on vertex sets of bipartition B_{X} and B_{Y}. Assume that there exists an independent set S such that both the union of S and B_{X} and the union of S and B_{Y} are maximal independent sets of G. Then B is a generating subgraph of G, and it produces the restriction w(B_{X})=w(B_{Y}). It is known that for every weight function w, if G is w-well-covered, then the above restriction is satisfied. In the special case, where B_{X}={x} and B_{Y}={y}, we say that xy is a relating edge. Recognizing relating edges and generating subgraphs is an NP-complete problem. However, we provide a polynomial algorithm for recognizing generating subgraphs of an input graph without cycles of length 5, 6 and 7. We also present a polynomial algorithm for recognizing relating edges in an input graph without cycles of length 5 and 6."
      },
      {
        "node_idx": 71790,
        "score_0_10": 10,
        "title": "positional determinacy of games with infinitely many priorities",
        "abstract": "We study two-player games of infinite duration that are played on finite or infinite game graphs. A winning strategy for such a game is positional if it only depends on the current position, and not on the history of the play. A game is positionally determined if, from each position, one of the two players has a positional winning strategy. The theory of such games is well studied for winning conditions that are defined in terms of a mapping that assigns to each position a priority from a finite set C. Specifically, in Muller games the winner of a play is determined by the set of those priorities that have been seen infinitely often; an important special case are parity games where the least (or greatest) priority occurring infinitely often determines the winner. It is well-known that parity games are positionally determined whereas Muller games are determined via finite-memory strategies. In this paper, we extend this theory to the case of games with infinitely many priorities. Such games arise in several application areas, for instance in pushdown games with winning conditions depending on stack contents. For parity games there are several generalisations to the case of infinitely many pri- orities. While max-parity games over ! or min-parity games over larger ordinals than ! require strategies with infinite memory, we can prove that min-parity games with priorities in ! are positionally determined. Indeed, it turns out that the min-parity condition over ! is the only infinitary Muller condition that guarantees positional determinacy on all game graphs. 1. Motivation The problem of computing winning positions and winning strategies in infinite games has numerous applications in computing, most notably for the synthesis and verification of reactive controllers and for the model-checking of the \u00b5-calculus and other logics. Of special importance are parity games, due to several reasons."
      },
      {
        "node_idx": 66464,
        "score_0_10": 10,
        "title": "lumpings of markov chains entropy rate preservation and higher order lumpability",
        "abstract": "A lumping of a Markov chain is a coordinate-wise projection of the chain. We characterise the entropy rate preservation of a lumping of an aperiodic and irreducible Markov chain on a finite state space by the random growth rate of the cardinality of the realisable preimage of a finite-length trajectory of the lumped chain and by the information needed to reconstruct original trajectories from their lumped images. Both are purely combinatorial criteria, depending only on the transition graph of the Markov chain and the lumping function. A lumping is strongly k-lumpable, iff the lumped process is a k-th order Markov chain for each starting distribution of the original Markov chain. We characterise strong k-lumpability via tightness of stationary entropic bounds. In the sparse setting, we give sufficient conditions on the lumping to both preserve the entropy rate and be strongly k-lumpable."
      },
      {
        "node_idx": 129761,
        "score_0_10": 10,
        "title": "renyi divergence and kullback leibler divergence",
        "abstract": "Renyi divergence is related to Renyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by Renyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the Renyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of Renyi divergence and Kullback-Leibler divergence, including convexity, continuity, limits of    \\(\\sigma \\)   -algebras, and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results."
      }
    ]
  },
  "365": {
    "explanation": "video action recognition and temporal modeling techniques",
    "topk": [
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 45381,
        "score_0_10": 9,
        "title": "end to end learning for self driving cars",
        "abstract": "We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. #R##N#The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. #R##N#Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. #R##N#We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS)."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 121343,
        "score_0_10": 8,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 49351,
        "score_0_10": 8,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 108093,
        "score_0_10": 8,
        "title": "temporal segment networks towards good practices for deep action recognition",
        "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices."
      }
    ]
  },
  "366": {
    "explanation": "automated multi-agent execution monitoring and alert systems",
    "topk": [
      {
        "node_idx": 74519,
        "score_0_10": 10,
        "title": "interactive execution monitoring of agent teams",
        "abstract": "There is an increasing need for automated support for humans monitoring the activity of distributed teams of cooperating agents, both human and machine. We characterize the domain-independent challenges posed by this problem, and describe how properties of domains influence the challenges and their solutions. We will concentrate on dynamic, data-rich domains where humans are ultimately responsible for team behavior. Thus, the automated aid should interactively support effective and timely decision making by the human. We present a domain-independent categorization of the types of alerts a plan-based monitoring system might issue to a user, where each type generally requires different monitoring techniques. We describe a monitoring framework for integrating many domain-specific and task-specific monitoring techniques and then using the concept of value of an alert to avoid operator overload.#R##N##R##N#We use this framework to describe an execution monitoring approach we have used to implement Execution Assistants (EAs) in two different dynamic, data-rich, real-world domains to assist a human in monitoring team behavior. One domain (Army small unit operations) has hundreds of mobile, geographically distributed agents, a combination of humans, robots, and vehicles. The other domain (teams of unmanned ground and air vehicles) has a handful of cooperating robots. Both domains involve unpredictable adversaries in the vicinity. Our approach customizes monitoring behavior for each specific task, plan, and situation, as well as for user preferences. Our EAs alert the human controller when reported events threaten plan execution or physically threaten team members. Alerts were generated in a timely manner without inundating the user with too many alerts (less than 10% of alerts are unwanted, as judged by domain experts)."
      },
      {
        "node_idx": 69444,
        "score_0_10": 10,
        "title": "state space modeling of inverter based microgrids considering distributed secondary voltage control",
        "abstract": "Appropriate control of high penetration renewable energies in power systems requires a complete modeling of the system. In this paper, a comprehensive state space modeling of voltage source inverters, networks and loads are studied. We have modeled a secondary voltage control that utilizes the nominal set points of the output voltage as the input for the controller design. A distributed control algorithm only requiring a sparse communication between neighboring distributed generators (DGs) is used. The proposed secondary voltage model is applied on a microgrid with three DGs and two loads to verify the results."
      },
      {
        "node_idx": 62420,
        "score_0_10": 9,
        "title": "goal oriented data visualization with software project control centers",
        "abstract": "Many software development organizations still lack support for obtaining intellec- tual control over their software development processes and for determining the performance of their processes and the quality of the produced products. System- atic support for detecting and reacting to critical project states in order to achieve planned goals is usually missing. One means to institutionalize measurement on the basis of explicit models is the development and establishment of a so-called Soft- ware Project Control Center (SPCC) for systematic quality assurance and man- agement support. An SPCC is comparable to a control room, which is a well known term in the mechanical production domain. Its tasks include collecting, in- terpreting, and visualizing measurement data in order to provide context-, pur- pose-, and role-oriented information for all stakeholders (e.g., project managers, quality assurance manager, developers) during the execution of a software devel-"
      },
      {
        "node_idx": 62609,
        "score_0_10": 9,
        "title": "autonomous scanning for endomicroscopic mosaicing and 3d fusion",
        "abstract": "Robot-assisted minimally invasive surgery can benefit from the automation of common, repetitive or well-defined but ergonomically difficult tasks. One such task is the scanning of a pick-up endomicroscopy probe over a complex, undulating tissue surface to enhance the effective field-of-view through video mosaicing. In this paper, the da Vinci\u00ae surgical robot, through the dVRK framework, is used for autonomous scanning and 2D mosaicing over a user-defined region of interest. To achieve the level of precision required for high quality mosaic generation, which relies on sufficient overlap between consecutive image frames, visual servoing is performed using a combination of a tracking marker attached to the probe and the endomicroscopy images themselves. The resulting sub-millimetre accuracy of the probe motion allows for the generation of large mosaics with minimal intervention from the surgeon. Images are streamed from the endomicroscope and overlaid live onto the surgeons view, while 2D mosaics are generated in real-time, and fused into a 3D stereo reconstruction of the surgical scene, thus providing intuitive visualisation and fusion of the multi-scale images. The system therefore offers significant potential to enhance surgical procedures, by providing the operator with cellular-scale information over a larger area than could typically be achieved by manual scanning."
      },
      {
        "node_idx": 66726,
        "score_0_10": 9,
        "title": "smart inverter impacts on california distribution feeders with increasing pv penetration a case study",
        "abstract": "The impacts of high PV penetration on distribution feeders have been well documented within the last decade. To mitigate these impacts, interconnection standards have been amended to allow PV inverters to regulate voltage locally. However, there is a deficiency of literature discussing how these inverters will behave on real feeders under increasing PV penetration. In this paper, we simulate several deployment scenarios of these inverters on a real California distribution feeder. We show that minimum and maximum voltage, tap operations, and voltage variability are improved due to the inverters. Line losses were shown to increase at high PV penetrations as a side effect. Furthermore, we find inverter sizing was shown to be important as PV penetration increased. Finally we show that increasing the number of inverters and removing the deadband from the Volt/VAr control curve improves the effectiveness."
      },
      {
        "node_idx": 93175,
        "score_0_10": 9,
        "title": "optimal oltc voltage control scheme to enable high solar penetrations",
        "abstract": "Abstract   High solar Photovoltaic (PV) penetration on distribution systems can cause over-voltage problems. To this end, an Optimal Tap Control (OTC) method is proposed to regulate On-Load Tap Changers (OLTCs) by minimizing the maximum deviation of the voltage profile from 1\u00a0p.u. on the entire feeder. A secondary objective is to reduce the number of tap operations (TOs), which is implemented for the optimization horizon based on voltage forecasts derived from high resolution PV generation forecasts. A linearization technique is applied to make the optimization problem convex and able to be solved at operational timescales. Simulations on a PC show the solution time for one time step is only 1.1\u00a0s for a large feeder with 4 OLTCs and 1623 buses. OTC results are compared against existing methods through simulations on two feeders in the Californian network. OTC is firstly compared against an advanced rule-based Voltage Level Control (VLC) method. OTC and VLC achieve the same reduction of voltage violations, but unlike VLC, OTC is capable of coordinating multiple OLTCs. Scalability to multiple OLTCs is therefore demonstrated against a basic conventional rule-based control method called Autonomous Tap Control (ATC). Comparing to ATC, the test feeder under control of OTC can accommodate around 67% more PV without over-voltage issues. Though a side effect of OTC is an increase in tap operations, the secondary objective functionally balances operations between all OLTCs such that impacts on their lifetime and maintenance are minimized."
      },
      {
        "node_idx": 3416,
        "score_0_10": 9,
        "title": "synergy based hand pose sensing reconstruction enhancement",
        "abstract": "Low-cost sensing gloves for reconstruction posture provide measurements which are limited under several regards. They are generated through an imperfectly known model, are subject to noise, and may be less than the number of Degrees of Freedom (DoFs) of the hand. Under these conditions, direct reconstruction of the hand posture is an ill-posed problem, and performance can be very poor. This paper examines the problem of estimating the posture of a human hand using(low-cost) sensing gloves, and how to improve their performance by exploiting the knowledge on how humans most frequently use their hands. To increase the accuracy of pose reconstruction without modifying the glove hardware - hence basically at no extra cost - we propose to collect, organize, and exploit information on the probabilistic distribution of human hand poses in common tasks. We discuss how a database of such an a priori information can be built, represented in a hierarchy of correlation patterns or postural synergies, and fused with glove data in a consistent way, so as to provide a good hand pose reconstruction in spite of insufficient and inaccurate sensing data. Simulations and experiments on a low-cost glove are reported which demonstrate the effectiveness of the proposed techniques."
      },
      {
        "node_idx": 19561,
        "score_0_10": 9,
        "title": "neuro fuzzy systems sate of the art modeling techniques",
        "abstract": "Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS) have attracted the growing interest of researchers in various scientific and engineering areas due to the growing need of adaptive intelligent systems to solve the real world problems. ANN learns from scratch by adjusting the interconnections between layers. FIS is a popular computing framework based on the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The advantages of a combination of ANN and FIS are obvious. There are several approaches to integrate ANN and FIS and very often it depends on the application. We broadly classify the integration of ANN and FIS into three categories namely concurrent model, cooperative model and fully fused model. This paper starts with a discussion of the features of each model and generalize the advantages and deficiencies of each model. We further focus the review on the different types of fused neuro-fuzzy systems and citing the advantages and disadvantages of each model."
      },
      {
        "node_idx": 78315,
        "score_0_10": 9,
        "title": "hybrid methodology for hourly global radiation forecasting in mediterranean area",
        "abstract": "The renewable energies prediction and particularly global radiation forecasting is a challenge studied by a growing number of research teams. This paper proposes an original technique to model the insolation time series based on combining Artificial Neural Network (ANN) and Auto-Regressive and Moving Average (ARMA) model. While ANN by its non-linear nature is effective to predict cloudy days, ARMA techniques are more dedicated to sunny days without cloud occurrences. Thus, three hybrids models are suggested: the first proposes simply to use ARMA for 6 months in spring and summer and to use an optimized ANN for the other part of the year; the second model is equivalent to the first but with a seasonal learning; the last model depends on the error occurred the previous hour. These models were used to forecast the hourly global radiation for five places in Mediterranean area. The forecasting performance was compared among several models: the 3 above mentioned models, the best ANN and ARMA for each location. In the best configuration, the coupling of ANN and ARMA allows an improvement of more than 1%, with a maximum in autumn (3.4%) and a minimum in winter (0.9%) where ANN alone is the best."
      },
      {
        "node_idx": 105528,
        "score_0_10": 9,
        "title": "optimal inverter var control in distribution systems with high pv penetration",
        "abstract": "The intent of the study detailed in this paper is to demonstrate the benefits of inverter var control on a fast timescale to mitigate rapid and large voltage fluctuations due to the high penetration of photovoltaic generation and the resulting reverse power flow. Our approach is to formulate the volt/var control as a radial optimal power flow (OPF) problem to minimize line losses and energy consumption, subject to constraints on voltage magnitudes. An efficient solution to the radial OPF problem is presented and used to study the structure of optimal inverter var injection and the net benefits, taking into account the additional cost of inverter losses when operating at non-unity power factor. This paper will illustrate how, depending on the circuit topology and its loading condition, the inverter's optimal reactive power injection is not necessarily monotone with respect to their real power output. The results are demonstrated on a distribution feeder on the Southern California Edison system that has a very light load and a 5 MW photovoltaic (PV) system installed away from the substation."
      }
    ]
  },
  "367": {
    "explanation": "robustness and efficiency of auction mechanisms under Knightian uncertainty",
    "topk": [
      {
        "node_idx": 67704,
        "score_0_10": 10,
        "title": "knightian robustness of the vickrey mechanism",
        "abstract": "We investigate the resilience of some classical mechanisms to alternative specifications of preferences and information structures. Specifically, we analyze the Vickrey mechanism for auctions of multiple identical goods when the only information a player $i$ has about the profile of true valuations, $\\theta^*$, consists of a set of distributions, from one of which $\\theta_i^*$ has been drawn. #R##N#In this setting, the players no longer have complete preferences, and the Vickrey mechanism is no longer dominant-strategy. However, we prove that its efficiency performance is excellent, and essentially optimal, in undominated strategies."
      },
      {
        "node_idx": 13884,
        "score_0_10": 10,
        "title": "symmetry and approximability of submodular maximization problems",
        "abstract": "A number of recent results on optimization problems involving submodular functions have made use of the multilinear relaxation of the problem. These results hold typically in the value oracle model, where the objective function is accessible via a black box returning f(S) for a given S. We present a general approach to deriving inapproximability results in the value oracle model, based on the notion of symmetry gap. Our main result is that for any fixed instance that exhibits a certain symmetry gap in its multilinear relaxation, there is a naturally related class of instances for which a better approximation factor than the symmetry gap would require exponentially many oracle queries. This unifies several known hardness results for submodular maximization, and implies several new ones. In particular, we prove that there is no constant-factor approximation for the problem of maximizing a non-negative submodular function over the bases of a matroid. We also provide a closely matching approximation algorithm for this problem."
      },
      {
        "node_idx": 46136,
        "score_0_10": 10,
        "title": "hierarchical cooperation achieves optimal capacity scaling in ad hoc networks",
        "abstract": "n source and destination pairs randomly located in an area want to communicate with each other. Signals transmitted from one user to another at distance r apart are subject to a power loss of r^{-alpha}, as well as a random phase. We identify the scaling laws of the information theoretic capacity of the network. In the case of dense networks, where the area is fixed and the density of nodes increasing, we show that the total capacity of the network scales linearly with n. This improves on the best known achievability result of n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where the density of nodes is fixed and the area increasing linearly with n, we show that this capacity scales as n^{2-alpha/2} for 2 3. The best known earlier result (Xie and Kumar 2006) identified the scaling law for alpha > 4. Thus, much better scaling than multihop can be achieved in dense networks, as well as in extended networks with low attenuation. The performance gain is achieved by intelligent node cooperation and distributed MIMO communication. The key ingredient is a hierarchical and digital architecture for nodal exchange of information for realizing the cooperation."
      },
      {
        "node_idx": 150960,
        "score_0_10": 10,
        "title": "knightian robustness from regret minimization",
        "abstract": "We consider auctions in which the players have very limited knowledge about their own valuations. Specifically, the only information that a Knightian player $i$ has about the profile of true valuations, $\\theta^*$, consists of a set of distributions, from one of which $\\theta_i^*$ has been drawn. #R##N#We analyze the social-welfare performance of the VCG mechanism, for unrestricted combinatorial auctions, when Knightian players that either (a) choose a regret-minimizing strategy, or (b) resort to regret minimization only to refine further their own sets of undominated strategies, if needed. We prove that this performance is very good."
      },
      {
        "node_idx": 163667,
        "score_0_10": 9,
        "title": "rate region of the quadratic gaussian two encoder source coding problem",
        "abstract": "We determine the rate region of the quadratic Gaussian two-encoder source-coding problem. This rate region is achieved by a simple architecture that separates the analog and digital aspects of the compression. Furthermore, this architecture requires higher rates to send a Gaussian source than it does to send any other source with the same covariance. Our techniques can also be used to determine the sum rate of some generalizations of this classical problem. Our approach involves coupling the problem to a quadratic Gaussian ``CEO problem.''"
      },
      {
        "node_idx": 140495,
        "score_0_10": 9,
        "title": "the capacity of private information retrieval",
        "abstract": "In the private information retrieval (PIR) problem a user wishes to retrieve, as efficiently as possible, one out of $K$ messages from $N$ non-communicating databases (each holds all $K$ messages) while revealing nothing about the identity of the desired message index to any individual database. The information theoretic capacity of PIR is the maximum number of bits of desired information that can be privately retrieved per bit of downloaded information. For $K$ messages and $N$ databases, we show that the PIR capacity is $(1+1/N+1/N^2+\\cdots+1/N^{K-1})^{-1}$. A remarkable feature of the capacity achieving scheme is that if we eliminate any subset of messages (by setting the message symbols to zero), the resulting scheme also achieves the PIR capacity for the remaining subset of messages."
      },
      {
        "node_idx": 68420,
        "score_0_10": 9,
        "title": "femtocaching wireless video content delivery through distributed caching helpers",
        "abstract": "Video on-demand streaming from Internet-based servers is becoming one of the most important services offered by wireless networks today. In order to improve the area spectral efficiency of video transmission in cellular systems, small cells heterogeneous architectures (e.g., femtocells, WiFi off-loading) are being proposed, such that video traffic to nomadic users can be handled by short-range links to the nearest small cell access points (referred to as \"helpers\"). As the helper deployment density increases, the backhaul capacity becomes the system bottleneck. In order to alleviate such bottleneck we propose a system where helpers with low-rate backhaul but high storage capacity cache popular video files. Files not available from helpers are transmitted by the cellular base station. We analyze the optimum way of assigning files to the helpers, in order to minimize the expected downloading time for files. We distinguish between the uncoded case (where only complete files are stored) and the coded case, where segments of Fountain-encoded versions of the video files are stored at helpers. We show that the uncoded optimum file assignment is NP-hard, and develop a greedy strategy that is provably within a factor 2 of the optimum. Further, for a special case we provide an efficient algorithm achieving a provably better approximation ratio of $1-(1-1/d)^d$, where $d$ is the maximum number of helpers a user can be connected to. We also show that the coded optimum cache assignment problem is convex that can be further reduced to a linear program. We present numerical results comparing the proposed schemes."
      },
      {
        "node_idx": 93459,
        "score_0_10": 9,
        "title": "deterministic algorithms for submodular maximization problems",
        "abstract": "Randomization is a fundamental tool used in many theoretical and practical areas of computer science. We study here the role of randomization in the area of submodular function maximization. In this area most algorithms are randomized, and in almost all cases the approximation ratios obtained by current randomized algorithms are superior to the best results obtained by known deterministic algorithms. Derandomization of algorithms for general submodular function maximization seems hard since the access to the function is done via a value oracle. This makes it hard, for example, to apply standard derandomization techniques such as conditional expectations. Therefore, an interesting fundamental problem in this area is whether randomization is inherently necessary for obtaining good approximation ratios. #R##N#In this work we give evidence that randomization is not necessary for obtaining good algorithms by presenting a new technique for derandomization of algorithms for submodular function maximization. Our high level idea is to maintain explicitly a (small) distribution over the states of the algorithm, and carefully update it using marginal values obtained from an extreme point solution of a suitable linear formulation. We demonstrate our technique on two recent algorithms for unconstrained submodular maximization and for maximizing submodular function subject to a cardinality constraint. In particular, for unconstrained submodular maximization we obtain an optimal deterministic $1/2$-approximation showing that randomization is unnecessary for obtaining optimal results for this setting."
      },
      {
        "node_idx": 94987,
        "score_0_10": 9,
        "title": "fundamental limits of caching in wireless d2d networks",
        "abstract": "We consider a wireless Device-to-Device (D2D) network where communication is restricted to be single-hop. Users make arbitrary requests from a finite library of files and have pre-cached information on their devices, subject to a per-node storage capacity constraint. A similar problem has already been considered in an ``infrastructure'' setting, where all users receive a common multicast (coded) message from a single omniscient server (e.g., a base station having all the files in the library) through a shared bottleneck link. In this work, we consider a D2D ``infrastructure-less'' version of the problem. We propose a caching strategy based on deterministic assignment of subpackets of the library files, and a coded delivery strategy where the users send linearly coded messages to each other in order to collectively satisfy their demands. We also consider a random caching strategy, which is more suitable to a fully decentralized implementation. Under certain conditions, both approaches can achieve the information theoretic outer bound within a constant multiplicative factor. In our previous work, we showed that a caching D2D wireless network with one-hop communication, random caching, and uncoded delivery, achieves the same throughput scaling law of the infrastructure-based coded multicasting scheme, in the regime of large number of users and files in the library. This shows that the spatial reuse gain of the D2D network is order-equivalent to the coded multicasting gain of single base station transmission. It is therefore natural to ask whether these two gains are cumulative, i.e.,if a D2D network with both local communication (spatial reuse) and coded multicasting can provide an improved scaling law. Somewhat counterintuitively, we show that these gains do not cumulate (in terms of throughput scaling law)."
      },
      {
        "node_idx": 85494,
        "score_0_10": 9,
        "title": "knightian analysis of the vickrey mechanism",
        "abstract": "We analyze the Vickrey mechanism for auctions of multiple identical goods when the players have both Knightian uncertainty over their own valuations and incomplete preferences. In this model, the Vickrey mechanism is no longer dominant-strategy, and we prove that all dominant-strategy mechanisms are inadequate. However, we also prove that, in undominated strategies, the social welfare produced by the Vickrey mechanism in the worst case is not only very good, but also essentially optimal."
      }
    ]
  },
  "369": {
    "explanation": "linearizability and verification of concurrent data structures",
    "topk": [
      {
        "node_idx": 76162,
        "score_0_10": 10,
        "title": "strict linearizability and abstract atomicity",
        "abstract": "Linearizability is a commonly accepted consistency condition for concurrent objects. Filipovic et al. show that linearizability is equivalent to observational refinement. However, linearizability does not permit concurrent objects to share memory spaces with their client programs. We show that linearizability (or observational refinement) can be broken even though a client program of an object accesses the shared memory spaces without interference from the methods of the object. In this paper, we present strict linearizability which lifts this limitation and can ensure client-side traces and final-states equivalence even in a relaxed program model allowing clients to directly access the states of concurrent objects. We also investigate several important properties of strict linearizability. #R##N#At a high level of abstraction, a concurrent object can be viewed as a concurrent implementation of an abstract data type (ADT). We also present a correctness criterion for relating an ADT and its concurrent implementation, which is the combination of linearizability and data abstraction and can ensure observational equivalence. We also investigate its relationship with strict linearizability."
      },
      {
        "node_idx": 160287,
        "score_0_10": 10,
        "title": "linearizability with ownership transfer",
        "abstract": "Linearizability is a commonly accepted notion of correctness for libraries of#N#concurrent algorithms. Unfortunately, it assumes a complete isolation between a#N#library and its client, with interactions limited to passing values of a given#N#data type. This is inappropriate for common programming languages, where#N#libraries and their clients can communicate via the heap, transferring the#N#ownership of data structures, and can even run in a shared address space#N#without any memory protection. In this paper, we present the first definition#N#of linearizability that lifts this limitation and establish an Abstraction#N#Theorem: while proving a property of a client of a concurrent library, we can#N#soundly replace the library by its abstract implementation related to the#N#original one by our generalisation of linearizability. This allows abstracting#N#from the details of the library implementation while reasoning about the#N#client. We also prove that linearizability with ownership transfer can be#N#derived from the classical one if the library does not access some of data#N#structures transferred to it by the client."
      },
      {
        "node_idx": 154461,
        "score_0_10": 10,
        "title": "coinductive big step operational semantics",
        "abstract": "Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results."
      },
      {
        "node_idx": 75302,
        "score_0_10": 10,
        "title": "bpa bisimilarity is exptime hard",
        "abstract": "Given a basic process algebra (BPA) and two stack symbols, the BPA bisimilarity problem asks whether the two stack symbols are bisimilar. We show that this problem is EXPTIME-hard."
      },
      {
        "node_idx": 165377,
        "score_0_10": 10,
        "title": "simple executions of snapshot implementations",
        "abstract": "The well known snapshot primitive in concurrent programming allows for n-asynchronous processes to write values to an array of single-writer registers and, for each process, to take a snapshot of these registers. In this paper we provide a formulation of the well known linearizability condition for snapshot algorithms in terms of the existence of certain mathematical functions. In addition, we identify a simplifying property of snapshot implementations we call \"schedule-based algorithms\". This property is natural to assume in the sense that as far as we know, every published snapshot algorithm is schedule-based. Based on this, we prove that when dealing with schedule-based algorithms, it suffices to consider only a small class of very simple executions to prove or disprove correctness in terms of linearizability. We believe that the ideas developed in this paper may help to design automatic verification of snapshot algorithms. Since verifying linearizability was recently proved to be EXPSPACE-complete, focusing on unique objects (snapshot in our case) can potentially lead to designing restricted, but feasible verification methods."
      },
      {
        "node_idx": 74998,
        "score_0_10": 10,
        "title": "parametric schedulability analysis of a launcher flight control system under reactivity constraints",
        "abstract": "The next generation of space systems will have to achieve more and more complex missions. In order to master the development cost and duration of such systems, an alternative to a manual design is to automatically synthesize the main parameters of the system. In this paper, we present an approach on the specific case of the scheduling of the flight control of a space launcher. The approach requires two successive steps: (1) the formalization of the problem to be solved in a parametric formal model and (2) the synthesis of the model parameters with a tool. We first describe the problematic of the scheduling of a launcher flight control, then we show how this problematic can be formalized with parametric stopwatch automata; we then present the results computed by IMITATOR. We compare the results to the ones obtained by other tools classically used in scheduling."
      },
      {
        "node_idx": 103170,
        "score_0_10": 10,
        "title": "heap abstractions for static analysis",
        "abstract": "Heap data is potentially unbounded and seemingly arbitrary. Hence, unlike stack and static data, heap data cannot be abstracted in terms of a fixed set of program variables. This makes it an interesting topic of study and there is an abundance of literature employing heap abstractions. Although most studies have addressed similar concerns, insights gained in one description of heap abstraction may not directly carry over to some other description.   In our search of a unified theme, we view heap abstraction as consisting of two steps: (a) heap modelling, which is the process of representing a heap memory (i.e., an unbounded set of concrete locations) as a heap model (i.e., an unbounded set of abstract locations), and (b) summarization, which is the process of bounding the heap model by merging multiple abstract locations into summary locations. We classify the heap models as storeless, store based, and hybrid. We describe various summarization techniques based on k-limiting, allocation sites, patterns, variables, other generic instrumentation predicates, and higher-order logics. This approach allows us to compare the insights of a large number of seemingly dissimilar heap abstractions and also paves the way for creating new abstractions by mix and match of models and summarization techniques."
      },
      {
        "node_idx": 2923,
        "score_0_10": 10,
        "title": "feedback scheduling for energy efficient real time homogeneous multiprocessor systems",
        "abstract": "Real-time scheduling algorithms proposed in the literature are often based on worst-case estimates of task parameters and the performance of an open-loop scheme can therefore be poor. To improve on such a situation, one can instead apply a closed-loop scheme, where feedback is exploited to dynamically adjust the system parameters at run-time. We propose an optimal control framework that takes advantage of feeding back information of finished tasks to solve a real-time multiprocessor scheduling problem with uncertainty in task execution times, with the objective of minimizing the total energy consumption. Specifically, we propose a linear programming-based algorithm to solve a workload partitioning problem and adopt McNaughton's wrap around algorithm to find the task execution order. Simulation results for a PowerPC 405LP and an XScale processor illustrate that our feedback scheduling algorithm can result in an energy saving of approximately 40% compared to an open-loop method."
      },
      {
        "node_idx": 46807,
        "score_0_10": 10,
        "title": "verifying linearizability a comparative survey",
        "abstract": "Linearizability is a key correctness criterion for concurrent data structures, ensuring that each history of the concurrent object under consideration is consistent with respect to a history of the corresponding abstract data structure. Linearizability allows concurrent (i.e., overlapping) operation calls take effect in any order, but requires the real-time order of non-overlapping to be preserved. The sophisticated nature of concurrent objects means that linearizability is difficult to judge, and hence, over the years, numerous techniques for verifying linearizability have been developed using a variety of formal foundations such as data refinement, shape analysis, reduction, etc. However, because the underlying framework, nomenclature and terminology for each method is different, it has become difficult for practitioners to evaluate the differences between each approach, and hence, evaluate the methodology most appropriate for verifying the data structure at hand. In this paper, we compare the major of methods for verifying linearizability, describe the main contribution of each method, and compare their advantages and limitations."
      },
      {
        "node_idx": 150966,
        "score_0_10": 10,
        "title": "correctness and completeness of logic programs",
        "abstract": "We discuss proving correctness and completeness of definite clause logic programs. We propose a method for proving completeness, while for proving correctness we employ a method that should be well known but is often neglected. Also, we show how to prove completeness and correctness in the presence of SLD-tree pruning, and point out that approximate specifications simplify specifications and proofs.   We compare the proof methods to declarative diagnosis (algorithmic debugging), showing that approximate specifications eliminate a major drawback of the latter. We argue that our proof methods reflect natural declarative thinking about programs, and that they can be used, formally or informally, in everyday programming."
      }
    ]
  },
  "370": {
    "explanation": "epipolar geometry estimation using motion barcodes and line correspondences",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 85296,
        "score_0_10": 10,
        "title": "two view constraints on the epipoles from few correspondences",
        "abstract": "In general it requires at least 7 point correspondences to compute the fundamental matrix between views. We use the cross ratio invariance between corresponding epipolar lines, stemming from epipolar line homography, to derive a simple formulation for the relationship between epipoles and corresponding points. We show how it can be used to reduce the number of required points for the epipolar geometry when some information about the epipoles is available and demonstrate this with a buddy search app."
      },
      {
        "node_idx": 84579,
        "score_0_10": 10,
        "title": "cameras viewing cameras geometry",
        "abstract": "A basic problem in computer vision is to understand the structure of a real-world scene given several images of it. Here we study several theoretical aspects of the intra multi-view geometry of calibrated cameras when all that they can reliably recognize is each other. With the proliferation of wearable cameras, autonomous vehicles and drones, the geometry of these multiple cameras is a timely and relevant problem to study."
      },
      {
        "node_idx": 49376,
        "score_0_10": 9,
        "title": "camera calibration from dynamic silhouettes using motion barcodes",
        "abstract": "Computing the epipolar geometry between cameras with very different viewpoints is often problematic as matching points are hard to find. In these cases, it has been proposed to use information from dynamic objects in the scene for suggesting point and line correspondences. #R##N#We propose a speed up of about two orders of magnitude, as well as an increase in robustness and accuracy, to methods computing epipolar geometry from dynamic silhouettes. This improvement is based on a new temporal signature: motion barcode for lines. Motion barcode is a binary temporal sequence for lines, indicating for each frame the existence of at least one foreground pixel on that line. The motion barcodes of two corresponding epipolar lines are very similar, so the search for corresponding epipolar lines can be limited only to lines having similar barcodes. The use of motion barcodes leads to increased speed, accuracy, and robustness in computing the epipolar geometry."
      },
      {
        "node_idx": 102743,
        "score_0_10": 9,
        "title": "fundamental matrices from moving objects using line motion barcodes",
        "abstract": "Computing the epipolar geometry between cameras with very different viewpoints is often very difficult. The appearance of objects can vary greatly, and it is difficult to find corresponding feature points. Prior methods searched for corresponding epipolar lines using points on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the \"Motion Barcodes\", a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction."
      },
      {
        "node_idx": 61221,
        "score_0_10": 9,
        "title": "point set registration coherent point drift",
        "abstract": "Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 67267,
        "score_0_10": 8,
        "title": "unsupervised learning of depth and ego motion from video",
        "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings."
      },
      {
        "node_idx": 9161,
        "score_0_10": 8,
        "title": "event retrieval using motion barcodes",
        "abstract": "We introduce a simple and effective method for retrieval of videos showing a specific event, even when the videos of that event were captured from significantly different viewpoints. Appearance-based methods fail in such cases, as appearances change with large changes of viewpoints. #R##N#Our method is based on a pixel-based feature, \"motion barcode\", which records the existence/non-existence of motion as a function of time. While appearance, motion magnitude, and motion direction can vary greatly between disparate viewpoints, the existence of motion is viewpoint invariant. Based on the motion barcode, a similarity measure is developed for videos of the same event taken from very different viewpoints. This measure is robust to occlusions common under different viewpoints, and can be computed efficiently. #R##N#Event retrieval is demonstrated using challenging videos from stationary and hand held cameras."
      },
      {
        "node_idx": 96902,
        "score_0_10": 8,
        "title": "epipolar geometry based on line similarity",
        "abstract": "It is known that epipolar geometry can be computed from three epipolar line correspondences but this computation is rarely used in practice since there are no simple methods to find corresponding lines. Instead, methods for finding corresponding points are widely used. This paper proposes a similarity measure between lines that indicates whether two lines are corresponding epipolar lines and enables finding epipolar line correspondences as needed for the computation of epipolar geometry. #R##N#A similarity measure between two lines, suitable for video sequences of a dynamic scene, has been previously described. This paper suggests a stereo matching similarity measure suitable for images. It is based on the quality of stereo matching between the two lines, as corresponding epipolar lines yield a good stereo correspondence. #R##N#Instead of an exhaustive search over all possible pairs of lines, the search space is substantially reduced when two corresponding point pairs are given. #R##N#We validate the proposed method using real-world images and compare it to state-of-the-art methods. We found this method to be more accurate by a factor of five compared to the standard method using seven corresponding points and comparable to the 8-points algorithm."
      }
    ]
  },
  "371": {
    "explanation": "deep convolutional neural network architectures and spatial feature transformations",
    "topk": [
      {
        "node_idx": 67928,
        "score_0_10": 10,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 53950,
        "score_0_10": 10,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 52018,
        "score_0_10": 10,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 103461,
        "score_0_10": 9,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      }
    ]
  },
  "372": {
    "explanation": "multi-scale context aggregation for semantic segmentation",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 55399,
        "score_0_10": 8,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 163164,
        "score_0_10": 8,
        "title": "rethinking atrous convolution for semantic image segmentation",
        "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
      },
      {
        "node_idx": 80414,
        "score_0_10": 8,
        "title": "3d u net learning dense volumetric segmentation from sparse annotation",
        "abstract": "This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases."
      },
      {
        "node_idx": 120825,
        "score_0_10": 8,
        "title": "encoder decoder with atrous separable convolution for semantic image segmentation",
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
      },
      {
        "node_idx": 69942,
        "score_0_10": 8,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 162377,
        "score_0_10": 7,
        "title": "r fcn object detection via region based fully convolutional networks",
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: this https URL"
      }
    ]
  },
  "374": {
    "explanation": "online algorithms and competitive ratios for clique clustering",
    "topk": [
      {
        "node_idx": 127295,
        "score_0_10": 10,
        "title": "online clique clustering",
        "abstract": "Clique clustering is the problem of partitioning the vertices of a graph into disjoint clusters, where each cluster forms a clique in the graph, while optimizing some objective function. In online clustering, the input graph is given one vertex at a time, and any vertices that have previously been clustered together are not allowed to be separated. The goal is to maintain a clustering with an objective value close to the optimal solution. For the variant where we want to maximize the number of edges in the clusters, we propose an online strategy based on the doubling technique. It has an asymptotic competitive ratio at most 15.646 and an absolute competitive ratio at most 22.641. We also show that no deterministic strategy can have an asymptotic competitive ratio better than 6. For the variant where we want to minimize the number of edges between clusters, we show that the deterministic competitive ratio of the problem is $n-\\omega(1)$, where n is the number of vertices in the graph."
      },
      {
        "node_idx": 124030,
        "score_0_10": 10,
        "title": "competitive strategies for online clique clustering",
        "abstract": "A clique clustering of a graph is a partitioning of its vertices into disjoint cliques. The quality of a clique clustering is measured by the total number of edges in its cliques. We consider the online variant of the clique clustering problem, where the vertices of the input graph arrive one at a time. At each step, the newly arrived vertex forms a singleton clique, and the algorithm can merge any existing cliques in its partitioning into larger cliques, but splitting cliques is not allowed. We give an online algorithm with competitive ratio 15.645 and we prove a lower bound of 6 on the competitive ratio, improving the previous respective bounds of 31 and 2."
      },
      {
        "node_idx": 40714,
        "score_0_10": 9,
        "title": "tweeting biomedicine an analysis of tweets and citations in the biomedical literature",
        "abstract": "Data collected by social media platforms have recently been introduced as a new source for indicators to help measure the impact of scholarly research in ways that are complementary to traditional citation-based indicators. Data generated from social media activities related to scholarly content can be used to reflect broad types of impact. This paper aims to provide systematic evidence regarding how often Twitter is used to diffuse journal articles in the biomedical and life sciences. The analysis is based on a set of 1.4 million documents covered by both PubMed and Web of Science (WoS) and published between 2010 and 2012. The number of tweets containing links to these documents was analyzed to evaluate the degree to which certain journals, disciplines, and specialties were represented on Twitter. It is shown that, with less than 10% of PubMed articles mentioned on Twitter, its uptake is low in general. The relationship between tweets and WoS citations was examined for each document at the level of journals and specialties. The results show that tweeting behavior varies between journals and specialties and correlations between tweets and citations are low, implying that impact metrics based on tweets are different from those based on citations. A framework utilizing the coverage of articles and the correlation between Twitter mentions and citations is proposed to facilitate the evaluation of novel social-media based metrics and to shed light on the question in how far the number of tweets is a valid metric to measure research impact."
      },
      {
        "node_idx": 51298,
        "score_0_10": 8,
        "title": "methods for estimating the size of google scholar",
        "abstract": "The emergence of academic search engines (mainly Google Scholar and Microsoft Academic Search) that aspire to index the entirety of current academic knowledge has revived and increased interest in the size of the academic web. The main objective of this paper is to propose various methods to estimate the current size (number of indexed documents) of Google Scholar (May 2014) and to determine its validity, precision and reliability. To do this, we present, apply and discuss three empirical methods: an external estimate based on empirical studies of Google Scholar coverage, and two internal estimate methods based on direct, empty and absurd queries, respectively. The results, despite providing disparate values, place the estimated size of Google Scholar at around 160 to 165 million documents. However, all the methods show considerable limitations and uncertainties due to inconsistencies in the Google Scholar search functionalities."
      },
      {
        "node_idx": 124619,
        "score_0_10": 8,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 15530,
        "score_0_10": 8,
        "title": "empirical comparison of algorithms for network community detection",
        "abstract": "Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that \"look like\" good communities for the application of interest. In this paper, we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify. We evaluate several common objective functions that are used to formalize the notion of a network community, and we examine several different classes of approximation algorithms that aim to optimize such objective functions. In addition, rather than simply fixing an objective and asking for an approximation to the best cluster of any size, we consider a size-resolved version of the optimization problem. Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms, since objective functions and approximation algorithms often have non-obvious size-dependent behavior."
      },
      {
        "node_idx": 168873,
        "score_0_10": 8,
        "title": "towards a book publishers citation reports first approach using the book citation index",
        "abstract": "The absence of books and book chapters in the Web of Science Citation Indexes (SCI, SSCI and A&HCI) has always been considered an important flaw but the Thomson Reuters 'Book Citation Index' database was finally available in October of 2010 indexing 29,618 books and 379,082 book chapters. The Book Citation Index opens a new window of opportunities for analyzing these fields from a bibliometric point of view. The main objective of this article is to analyze different impact indicators referred to the scientific publishers included in the Book Citation Index for the Social Sciences and Humanities fields during 2006-2011. This way we construct what we have called the 'Book Publishers Citation Reports'. For this, we present a total of 19 rankings according to the different disciplines in Humanities & Arts and Social Sciences & Law with six indicators for scientific publishers"
      },
      {
        "node_idx": 5409,
        "score_0_10": 8,
        "title": "incremental medians via online bidding",
        "abstract": "In the k-median problem we are given sets of facilities and customers, and distances between them. For a given set F of facilities, the cost of serving a customer u is the minimum distance between u and a facility in F. The goal is to find a set F of k facilities that minimizes the sum, over all customers, of their service costs.#R##N##R##N#Following the work of Mettu and Plaxton, we study the incremental medians problem, where k is not known in advance. An incremental algorithm produces a nested sequence of facility sets F1\u2286F2\u2286\u22c5\u22c5\u22c5\u2286Fn, where |Fk|=k for each\u00a0k. Such an algorithm is called c-cost-competitive if the cost of each Fk is at most c times the optimum k-median cost. We give improved incremental algorithms for the metric version of this problem: an 8-cost-competitive deterministic algorithm, a 2e\u22485.44-cost-competitive randomized algorithm, a (24+e)-cost-competitive, polynomial-time deterministic algorithm, and a 6e+e\u224816.31-cost-competitive, polynomial-time randomized algorithm.#R##N##R##N#We also consider the competitive ratio with respect to size. An algorithm is s-size-competitive if the cost of each Fk is at most the minimum cost of any set of k facilities, while the size of Fk is at most sk. We show that the optimal size-competitive ratios for this problem, in the deterministic and randomized cases, are 4 and e. For polynomial-time algorithms, we present the first polynomial-time O(log\u2009m)-size-approximation algorithm for the offline problem, as well as a polynomial-time O(log\u2009m)-size-competitive algorithm for the incremental problem.#R##N##R##N#Our upper bound proofs reduce the incremental medians problem to the following online bidding problem: faced with some unknown threshold T\u2208\u211d+, an algorithm must submit \u201cbids\u201d b\u2208\u211d+ until it submits a bid b\u2265T, paying the sum of all its bids. We present folklore algorithms for online bidding and prove that they are optimally competitive.#R##N##R##N#We extend some of the above results for incremental medians to approximately metric distance functions and to incremental fractional medians. Finally, we consider a restricted version of the incremental medians problem where k is restricted to one of two given values, for which we give a deterministic algorithm with a nearly optimal cost-competitive ratio."
      },
      {
        "node_idx": 100147,
        "score_0_10": 8,
        "title": "communication efficient learning of deep networks from decentralized data",
        "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. #R##N#We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
      },
      {
        "node_idx": 46715,
        "score_0_10": 8,
        "title": "bitcoin ng a scalable blockchain protocol",
        "abstract": "Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential. #R##N#This paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem. #R##N#In addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network."
      }
    ]
  },
  "375": {
    "explanation": "neural network architectures for named entity recognition and sequence labeling",
    "topk": [
      {
        "node_idx": 79523,
        "score_0_10": 10,
        "title": "source to source optimizing transformations of prolog programs based on abstract interpretation",
        "abstract": "Making a Prolog program more efficient by transforming its source code, without changing its operational semantics, is not an obvious task. It requires the user to have a clear understanding of how the Prolog compiler works, and in particular, of the effects of impure features like the cut. The way a Prolog code is written - e.g., the order of clauses, the order of literals in a clause, the use of cuts or negations - influences its efficiency. Furthermore, different optimization techniques may be redundant or conflicting when they are applied together, depending on the way a procedure is called - e.g., inserting cuts and enabling indexing. We present an optimiser, based on abstract interpretation, that automatically performs safe code transformations of Prolog procedures in the context of some class of input calls. The method is more effective if procedures are annotated with additional information about modes, types, sharing, number of solutions and the like. Thus the approach is similar to Mercury. It applies to any Prolog program, however."
      },
      {
        "node_idx": 46746,
        "score_0_10": 10,
        "title": "aperiodic crosscorrelation of sequences derived from characters",
        "abstract": "It is shown that pairs of maximal linear recursive sequences (m-sequences) typically have mean square aperiodic crosscorrelation on par with that of random sequences, but that if one takes a pair of m-sequences where one is the reverse of the other, and shifts them appropriately, one can get significantly lower mean square aperiodic crosscorrelation. Sequence pairs with even lower mean square aperiodic crosscorrelation are constructed by taking a Legendre sequence, cyclically shifting it, and then cutting it (approximately) in half and using the halves as the sequences of the pair. In some of these constructions, the mean square aperiodic crosscorrelation can be lowered further if one truncates or periodically extends (appends) the sequences. Exact asymptotic formulae for mean squared aperiodic crosscorrelation are proved for sequences derived from additive characters (including m-sequences and modified versions thereof) and multiplicative characters (including Legendre sequences and their relatives). Data is presented that shows that sequences of modest length have performance that closely approximates the asymptotic formulae."
      },
      {
        "node_idx": 106185,
        "score_0_10": 10,
        "title": "using english as pivot to extract persian italian parallel sentences from non parallel corpora",
        "abstract": "The effectiveness of a statistical machine translation system (SMT) is very dependent upon the amount of parallel corpus used in the training phase. For low-resource language pairs there are not enough parallel corpora to build an accurate SMT. In this paper, a novel approach is presented to extract bilingual Persian-Italian parallel sentences from a non-parallel (comparable) corpus. In this study, English is used as the pivot language to compute the matching scores between source and target sentences and candidate selection phase. Additionally, a new monolingual sentence similarity metric, Normalized Google Distance (NGD) is proposed to improve the matching process. Moreover, some extensions of the baseline system are applied to improve the quality of extracted sentences measured with BLEU. Experimental results show that using the new pivot based extraction can increase the quality of bilingual corpus significantly and consequently improves the performance of the Persian-Italian SMT system."
      },
      {
        "node_idx": 63929,
        "score_0_10": 10,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 112674,
        "score_0_10": 10,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 26180,
        "score_0_10": 10,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 48761,
        "score_0_10": 9,
        "title": "syntactic variation of support verb constructions",
        "abstract": "We report experiments about the syntactic variations of support verb constructions, a special type of multiword expressions (MWEs) containing predicative nouns. In these expressions, the noun can occur with or without the verb, with no clear-cut semantic difference. We extracted from a large French corpus a set of examples of the two situations and derived statistical results from these data. The extraction involved large-coverage language resources and finite-state techniques. The results show that, most frequently, predicative nouns occur without a support verb. This fact has consequences on methods of extracting or recognising MWEs."
      },
      {
        "node_idx": 71244,
        "score_0_10": 9,
        "title": "high throughput snp genotyping by sbe sbh",
        "abstract": "Despite much progress over the past decade, current Single Nucleotide Polymorphism (SNP) genotyping technologies still offer an insufficient degree of multiplexing when required to handle user-selected sets of SNPs. In this paper we propose a new genotyping assay architecture combining multiplexed solution-phase single-base extension (SBE) reactions with sequencing by hybridization (SBH) using universal DNA arrays such as all $k$-mer arrays. In addition to PCR amplification of genomic DNA, SNP genotyping using SBE/SBH assays involves the following steps: (1) Synthesizing primers complementing the genomic sequence immediately preceding SNPs of interest; (2) Hybridizing these primers with the genomic DNA; (3) Extending each primer by a single base using polymerase enzyme and dideoxynucleotides labeled with 4 different fluorescent dyes; and finally (4) Hybridizing extended primers to a universal DNA array and determining the identity of the bases that extend each primer by hybridization pattern analysis. Our contributions include a study of multiplexing algorithms for SBE/SBH genotyping assays and preliminary experimental results showing the achievable tradeoffs between the number of array probes and primer length on one hand and the number of SNPs that can be assayed simultaneously on the other. Simulation results on datasets both randomly generated and extracted from the NCBI dbSNP database suggest that the SBE/SBH architecture provides a flexible and cost-effective alternative to genotyping assays currently used in the industry, enabling genotyping of up to hundreds of thousands of user-specified SNPs per assay."
      },
      {
        "node_idx": 54614,
        "score_0_10": 9,
        "title": "finding approximate palindromes in strings",
        "abstract": "Abstract   We introduce a novel definition of approximate palindromes in strings, and provide an algorithm to find all maximal approximate palindromes in a string with up to  k  errors. Our definition is based on the usual edit operations of approximate pattern matching, and the algorithm we give, for a string of size  n  on a fixed alphabet, runs in  O ( k  2  n ) time. We also discuss two implementation-related improvements to the algorithm, and demonstrate their efficacy in practice by means of both experiments and an average-case analysis."
      },
      {
        "node_idx": 115239,
        "score_0_10": 9,
        "title": "integrating selectional preferences in wordnet",
        "abstract": "Selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous statistical models to class-to-class preferences, and presents a model that learns selectional preferences for classes of verbs, together with an algorithm to integrate the learned preferences in WordNet. The theoretical motivation is twofold: different senses of a verb may have different preferences, and classes of verbs may share preferences. On the practical side, class-to-class selectional preferences can be learned from untagged corpora (the same as word-to-class), they provide selectional preferences for less frequent word senses via inheritance, and more important, they allow for easy integration in WordNet. The model is trained on subject-verb and object-verb relationships extracted from a small corpus disambiguated with WordNet senses. Examples are provided illustrating that the theoretical motivations are well founded, and showing that the approach is feasible. Experimental results on a word sense disambiguation task are also provided."
      }
    ]
  },
  "376": {
    "explanation": "advanced video action recognition and efficient spatial-temporal feature extraction",
    "topk": [
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 73053,
        "score_0_10": 9,
        "title": "semantic image segmentation with deep convolutional nets and fully connected crfs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 26460,
        "score_0_10": 9,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 108093,
        "score_0_10": 8,
        "title": "temporal segment networks towards good practices for deep action recognition",
        "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices."
      },
      {
        "node_idx": 59347,
        "score_0_10": 8,
        "title": "netvlad cnn architecture for weakly supervised place recognition",
        "abstract": "We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the \"Vector of Locally Aggregated Descriptors\" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks."
      },
      {
        "node_idx": 162377,
        "score_0_10": 8,
        "title": "r fcn object detection via region based fully convolutional networks",
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: this https URL"
      }
    ]
  },
  "377": {
    "explanation": "advanced coding and information theory for communication systems",
    "topk": [
      {
        "node_idx": 163667,
        "score_0_10": 10,
        "title": "rate region of the quadratic gaussian two encoder source coding problem",
        "abstract": "We determine the rate region of the quadratic Gaussian two-encoder source-coding problem. This rate region is achieved by a simple architecture that separates the analog and digital aspects of the compression. Furthermore, this architecture requires higher rates to send a Gaussian source than it does to send any other source with the same covariance. Our techniques can also be used to determine the sum rate of some generalizations of this classical problem. Our approach involves coupling the problem to a quadratic Gaussian ``CEO problem.''"
      },
      {
        "node_idx": 145848,
        "score_0_10": 10,
        "title": "lecture notes on network information theory",
        "abstract": "These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at this http URL The previous (and obsolete) version of the lecture notes can be found at this http URL"
      },
      {
        "node_idx": 127804,
        "score_0_10": 10,
        "title": "polarization for arbitrary discrete memoryless channels",
        "abstract": "Channel polarization, originally proposed for binary-input channels, is generalized to arbitrary discrete memoryless channels. Specifically, it is shown that when the input alphabet size is a prime number, a similar construction to that for the binary case leads to polarization. This method can be extended to channels of composite input alphabet sizes by decomposing such channels into a set of channels with prime input alphabet sizes. It is also shown that all discrete memoryless channels can be polarized by randomized constructions. The introduction of randomness does not change the order of complexity of polar code construction, encoding, and decoding. A previous result on the error probability behavior of polar codes is also extended to the case of arbitrary discrete memoryless channels. The generalization of polarization to channels with arbitrary finite input alphabet sizes leads to polar-coding methods for approaching the true (as opposed to symmetric) channel capacity of arbitrary channels with discrete or continuous input alphabets."
      },
      {
        "node_idx": 100147,
        "score_0_10": 9,
        "title": "communication efficient learning of deep networks from decentralized data",
        "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. #R##N#We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
      },
      {
        "node_idx": 41252,
        "score_0_10": 9,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 100857,
        "score_0_10": 9,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 31334,
        "score_0_10": 9,
        "title": "feedback capacity of stationary gaussian channels",
        "abstract": "The feedback capacity of additive stationary Gaussian noise channels is characterized as the solution to a variational problem. Toward this end, it is proved that the optimal feedback coding scheme is stationary. When specialized to the first-order autoregressive moving average noise spectrum, this variational characterization yields a closed-form expression for the feedback capacity. In particular, this result shows that the celebrated Schalkwijk-Kailath coding scheme achieves the feedback capacity for the first-order autoregressive moving average Gaussian channel, positively answering a long-standing open problem studied by Butman, Schalkwijk-Tiernan, Wolfowitz, Ozarow, Ordentlich, Yang-Kavcic-Tatikonda, and others. More generally, it is shown that a k-dimensional generalization of the Schalkwijk-Kailath coding scheme achieves the feedback capacity for any autoregressive moving average noise spectrum of order k. Simply put, the optimal transmitter iteratively refines the receiver's knowledge of the intended message."
      },
      {
        "node_idx": 23523,
        "score_0_10": 8,
        "title": "polar codes are optimal for lossy source coding",
        "abstract": "We consider lossy source compression of a binary symmetric source using polar codes and the low-complexity successive encoding algorithm. It was recently shown by Arikan that polar codes achieve the capacity of arbitrary symmetric binary-input discrete memoryless channels under a successive decoding strategy. We show the equivalent result for lossy source compression, i.e., we show that this combination achieves the rate-distortion bound for a binary symmetric source. We further show the optimality of polar codes for various problems including the binary Wyner-Ziv and the binary Gelfand-Pinsker problem"
      },
      {
        "node_idx": 30590,
        "score_0_10": 8,
        "title": "exponential decreasing rate of leaked information in universal random privacy amplification",
        "abstract": "We derive a new upper bound for Eve's information in secret key generation from a common random number without communication. This bound improves on Bennett 's bound based on the Renyi entropy of order 2 because the bound obtained here uses the Renyi entropy of order 1+s for s \u2208 [0,1]. This bound is applied to a wire-tap channel. Then, we derive an exponential upper bound for Eve's information. Our exponent is compared with Hayashi 's exponent. For the additive case, the bound obtained here is better. The result is applied to secret key agreement by public discussion."
      },
      {
        "node_idx": 8618,
        "score_0_10": 8,
        "title": "capacity results for arbitrarily varying wiretap channels",
        "abstract": "In this work the arbitrarily varying wiretap channel AVWC is studied. We derive a lower bound on the random code secrecy capacity for the average error criterion and the strong secrecy criterion in the case of a best channel to the eavesdropper by using Ahlswede's robustification technique for ordinary AVCs. We show that in the case of a non-symmetrisable channel to the legitimate receiver the deterministic code secrecy capacity equals the random code secrecy capacity, a result similar to Ahlswede's dichotomy result for ordinary AVCs. Using this we can derive that the lower bound is also valid for the deterministic code capacity of the AVWC. The proof of the dichotomy result is based on the elimination technique introduced by Ahlswede for ordinary AVCs. We further prove upper bounds on the deterministic code secrecy capacity in the general case, which results in a multi-letter expression for the secrecy capacity in the case of a best channel to the eavesdropper. Using techniques of Ahlswede, developed to guarantee the validity of a reliability criterion, the main contribution of this work is to integrate the strong secrecy criterion into these techniques."
      }
    ]
  },
  "378": {
    "explanation": "multi-scale spatial feature aggregation and transformation for image segmentation",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 121343,
        "score_0_10": 8,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 26460,
        "score_0_10": 8,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 53950,
        "score_0_10": 8,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 54179,
        "score_0_10": 8,
        "title": "efficient inference in fully connected crfs with gaussian edge potentials",
        "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy."
      },
      {
        "node_idx": 112726,
        "score_0_10": 8,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 66578,
        "score_0_10": 8,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 69942,
        "score_0_10": 8,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 73053,
        "score_0_10": 8,
        "title": "semantic image segmentation with deep convolutional nets and fully connected crfs",
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
      }
    ]
  },
  "381": {
    "explanation": "reversibility and modeling of finite cellular automata",
    "topk": [
      {
        "node_idx": 92656,
        "score_0_10": 10,
        "title": "reversibility of d state finite cellular automata",
        "abstract": "This paper investigates reversibility properties of 1-dimensional 3-neighborhood d-state finite cellular automata (CAs) of length n under periodic boundary condition. A tool named reachability tree has been developed from de Bruijn graph which represents all possible reachable configurations of an n-cell CA. This tool has been used to test reversibility of CAs. We have identified a large set of reversible CAs using this tool by following some greedy strategies."
      },
      {
        "node_idx": 103328,
        "score_0_10": 10,
        "title": "open problems from cccg 2002",
        "abstract": "A list of the problems presented on August 12, 2002 at the open-problem session of the 14th Canadian Conference on Computational Geometry held in Lethbridge, Alberta, Canada."
      },
      {
        "node_idx": 57303,
        "score_0_10": 10,
        "title": "fundamental concepts in the cyclus nuclear fuel cycle simulation framework",
        "abstract": "Nuclear fuel cycle modeling generality and robustness are improved by a modular, agent based modeling framework.Discrete material and facility tracking rather than fleet-based modeling improve nuclear fuel cycle simulation fidelity.A free, open source paradigm encourages technical experts to contribute software to the Cyclus modeling ecosystem.The flexibility of the Cyclus tool from the simulator user perspective is demonstrated with both open and closed fuel cycle examples. As nuclear power expands, technical, economic, political, and environmental analyses of nuclear fuel cycles by simulators increase in importance. To date, however, current tools are often fleet-based rather than discrete and restrictively licensed rather than open source. Each of these choices presents a challenge to modeling fidelity, generality, efficiency, robustness, and scientific transparency. The Cyclus nuclear fuel cycle simulator framework and its modeling ecosystem incorporate modern insights from simulation science and software architecture to solve these problems so that challenges in nuclear fuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle simulator framework and its modeling ecosystem are presented. Additionally, the implementation of each is discussed in the context of motivating challenges in nuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are demonstrated for both open and closed fuel cycles."
      },
      {
        "node_idx": 54181,
        "score_0_10": 10,
        "title": "epsilon unfolding orthogonal polyhedra",
        "abstract": "An unfolding of a polyhedron is produced by cutting the surface and flattening to a single, connected, planar piece without overlap (except possibly at boundary points). It is a long unsolved problem to determine whether every polyhedron may be unfolded. Here we prove, via an algorithm, that every orthogonal polyhedron (one whose faces meet at right angles) of genus zero may be unfolded. Our cuts are not necessarily along edges of the polyhedron, but they are always parallel to polyhedron edges. For a polyhedron of n vertices, portions of the unfolding will be rectangular strips which, in the worst case, may need to be as thin as epsilon = 1/2^{Omega(n)}."
      },
      {
        "node_idx": 127797,
        "score_0_10": 10,
        "title": "equilateral l contact graphs",
        "abstract": "We consider {\\em L-graphs}, that is contact graphs of axis-aligned L-shapes in the plane, all with the same rotation. We provide several characterizations of L-graphs, drawing connections to Schnyder realizers and canonical orders of maximally planar graphs. We show that every contact system of L's can always be converted to an equivalent one with equilateral L's. This can be used to show a stronger version of a result of Thomassen, namely, that every planar graph can be represented as a contact system of square-based cuboids. #R##N#We also study a slightly more restricted version of equilateral L-contact systems and show that these are equivalent to homothetic triangle contact representations of maximally planar graphs. We believe that this new interpretation of the problem might allow for efficient algorithms to find homothetic triangle contact representations, that do not use Schramm's monster packing theorem."
      },
      {
        "node_idx": 99964,
        "score_0_10": 9,
        "title": "on finite 1 dimensional cellular automata reversibility and semi reversibility",
        "abstract": "Reversibility of a one-dimensional finite cellular automaton (CA) is dependent on lattice size. A finite CA can be reversible for a set of lattice sizes. On the other hand, reversibility of an infinite CA, which is decided by exploring the rule only, is different in its kind from that of finite CA. Can we, however, link the reversibility of finite CA to that of infinite CA? In order to address this issue, we introduce a new notion, named semi-reversibility. We classify the CAs into three types with respect to reversibility property -- reversible, semi-reversible and strictly irreversible. A tool, reachability tree, has been used to decide the reversibility class of any CA. Finally, relation among the existing cases of reversibility is established."
      },
      {
        "node_idx": 166853,
        "score_0_10": 9,
        "title": "optimized quality factor of fractional order analog filters with band pass and band stop characteristics",
        "abstract": "Fractional order (FO) filters have been investigated in this paper, with band-pass (BP) and band-stop (BS) characteristics, which can not be achieved with conventional integer order filters with orders lesser then two. The quality factors for symmetric and asymmetric magnitude response have been optimized using real coded Genetic Algorithm (GA) for a user specified center frequency. Parametric influence of the FO filters on the magnitude response is also illustrated with credible numerical simulations."
      },
      {
        "node_idx": 100673,
        "score_0_10": 9,
        "title": "geometrically nonlinear isogeometric analysis of laminated composite plates based on higher order shear deformation theory",
        "abstract": "Abstract   In this paper, we present an effectively numerical approach based on isogeometric analysis (IGA) and higher-order shear deformation theory (HSDT) for geometrically nonlinear analysis of laminated composite plates. The HSDT allows us to approximate displacement field that ensures by itself the realistic shear strain energy part without shear correction factors (SCFs). IGA utilizing basis functions namely B-splines or non-uniform rational B-splines (NURBS) enables to satisfy easily the stringent continuity requirement of the HSDT model without any additional variables. The nonlinearity of the plates is formed in the total Lagrange approach based on the small strain assumptions. Numerous numerical validations for the isotropic, orthotropic, cross-ply and angle-ply laminated plates are provided to demonstrate the effectiveness of the proposed method."
      },
      {
        "node_idx": 50166,
        "score_0_10": 9,
        "title": "image based modelling of organogenesis",
        "abstract": "One of the major challenges in biology concerns the integration of data across length and time scales into a consistent framework: how do macroscopic properties and functionalities arise from the molecular regulatory networks and how can they change as a result of mutations? Morphogenesis provides an excellent model system to study how simple molecular networks robustly control complex processes on the macroscopic scale in spite of molecular noise, and how important functional variants can emerge from small genetic changes. Recent advancements in 3D imaging technologies, computer algorithms, and computer power now allow us to develop and analyse increasingly realistic models of biological control. Here we present our pipeline for image-based modeling that includes the segmentation of images, the determination of displacement fields, and the solution of systems of partial differential equations (PDEs) on the growing, embryonic domains. The development of suitable mathematical models, the data-based inference of parameter sets, and the evaluation of competing models are still challenging, and current approaches are discussed."
      },
      {
        "node_idx": 122867,
        "score_0_10": 9,
        "title": "towards implementation of cellular automata in microbial fuel cells",
        "abstract": "The Microbial Fuel Cell (MFC) is a bio-electrochemical transducer converting waste products into electricity using microbial communities. Cellular Automaton (CA) is a uniform array of finite-state machines that update their states in discrete time depending on states of their closest neighbors by the same rule. Arrays of MFCs could, in principle, act as massive-parallel computing devices with local connectivity between elementary processors. We provide a theoretical design of such a parallel processor by implementing CA in MFCs. We have chosen Conway's Game of Life as the 'benchmark' CA because this is the most popular CA which also exhibits an enormously rich spectrum of patterns. Each cell of the Game of Life CA is realized using two MFCs. The MFCs are linked electrically and hydraulically. The model is verified via simulation of an electrical circuit demonstrating equivalent behaviors. The design is a first step towards future implementations of fully autonomous biological computing devices with massive parallelism. The energy independence of such devices counteracts their somewhat slow transitions - compared to silicon circuitry - between the different states during computation."
      }
    ]
  },
  "384": {
    "explanation": "advanced proof complexity and formal system embeddings",
    "topk": [
      {
        "node_idx": 8773,
        "score_0_10": 10,
        "title": "feasible interpolation for qbf resolution calculi",
        "abstract": "In sharp contrast to classical proof complexity we are currently short of lower bound techniques for QBF proof systems. In this paper we establish the feasible interpolation technique for all resolution-based QBF systems, whether modelling CDCL or expansion-based solving. This both provides the first general lower bound method for QBF proof systems as well as largely extends the scope of classical feasible interpolation. We apply our technique to obtain new exponential lower bounds to all resolution-based QBF systems for a new class of QBF formulas based on the clique problem. Finally, we show how feasible interpolation relates to the recently established lower bound method based on strategy extraction."
      },
      {
        "node_idx": 7934,
        "score_0_10": 10,
        "title": "models and termination of proof reduction in the \u03bb\u03c0 calculus modulo theory",
        "abstract": "We define a notion of model for the $\\lambda \\Pi$-calculus modulo#R##N#theory, a notion of super-consistent theory, and prove that#R##N#proof-reduction terminates in the $\\lambda \\Pi$-calculus modulo a#R##N#super-consistent theory. We prove this way the termination of#R##N#proof-reduction in two theories in the $\\lambda \\Pi$-calculus modulo#R##N#theory, and their consistency: an embedding of Simple type theory and#R##N#an embedding of the Calculus of constructions."
      },
      {
        "node_idx": 6703,
        "score_0_10": 9,
        "title": "index reduction of differential algebraic equations by differential algebraic elimination",
        "abstract": "High index differential algebraic equations (DAEs) are ordinary differential equations (ODEs) with constraints and arise frequently from many mathematical models of physical phenomenons and engineering fields. In this paper, we generalize the idea of differential elimination with Dixon resultant to polynomially nonlinear DAEs. We propose a new algorithm for index reduction of DAEs and establish the notion of differential algebraic elimination, which can provide the differential algebraic resultant of the enlarged system of original equations. To make use of structure of DAEs, variable pencil technique is given to determine the termination of differentiation. Moreover, we also provide a heuristics method for removing the extraneous factors from differential algebraic resultant. The experimentation shows that the proposed algorithm outperforms existing ones for many examples taken from the literature."
      },
      {
        "node_idx": 82239,
        "score_0_10": 9,
        "title": "multiplicative drift analysis",
        "abstract": "We introduce multiplicative drift analysis as a suitable way to analyze the runtime of randomized search heuristics such as evolutionary algorithms. Our multiplicative version of the classical drift theorem allows easier analyses in the often encountered situation that the optimization progress is roughly proportional to the current distance to the optimum."
      },
      {
        "node_idx": 114060,
        "score_0_10": 9,
        "title": "binomial difference ideals",
        "abstract": "In this paper, binomial difference ideals are studied. Three canonical representations for Laurent binomial difference ideals are given in terms of the reduced Groebner basis of Z[x]-lattices, regular and coherent difference ascending chains, and partial characters over Z[x]-lattices, respectively. Criteria for a Laurent binomial difference ideal to be reflexive, prime, well-mixed, and perfect are given in terms of their support lattices. The reflexive, well-mixed, and perfect closures of a Laurent binomial difference ideal are shown to be binomial. Most of the properties of Laurent binomial difference ideals are extended to the case of difference binomial ideals. Finally, algorithms are given to check whether a given Laurent binomial difference ideal I is reflexive, prime, well-mixed, or perfect, and in the negative case, to compute the reflexive, well-mixed, and perfect closures of I. An algorithm is given to decompose a finitely generated perfect binomial difference ideal as the intersection of reflexive prime binomial difference ideals."
      },
      {
        "node_idx": 84843,
        "score_0_10": 9,
        "title": "geo indistinguishability differential privacy for location based systems",
        "abstract": "The growing popularity of location-based systems, allowing unknown/untrusted servers to easily collect huge amounts of information regarding users' location, has recently started raising serious privacy concerns. In this paper we introduce geoind, a formal notion of privacy for location-based systems that protects the user's exact location, while allowing approximate information -- typically needed to obtain a certain desired service -- to be released.   This privacy definition formalizes the intuitive notion of protecting the user's location within a radius $r$ with a level of privacy that depends on r, and corresponds to a generalized version of the well-known concept of differential privacy. Furthermore, we present a mechanism for achieving geoind by adding controlled random noise to the user's location.   We describe how to use our mechanism to enhance LBS applications with geo-indistinguishability guarantees without compromising the quality of the application results. Finally, we compare state-of-the-art mechanisms from the literature with ours. It turns out that, among all mechanisms independent of the prior, our mechanism offers the best privacy guarantees."
      },
      {
        "node_idx": 3949,
        "score_0_10": 9,
        "title": "generic traces and constraints gentra4cp revisited",
        "abstract": "The generic trace format GenTra4CP has been defined in 2004 with the goal of becoming a standard trace format for the observation of constraint solvers over finite domains. It has not been used since. This paper defines the concept of generic trace formally, based on simple transformations of traces. It then analyzes, and occasionally corrects, shortcomings of the proposed initial format and shows the interest that a generic tracer may bring to develop portable applications or to standardization efforts, in particular in the field of constraints."
      },
      {
        "node_idx": 97729,
        "score_0_10": 9,
        "title": "new wing stroke and wing pitch approaches for milligram scale aerial devices",
        "abstract": "Here we report the construction of the simplest transmission mechanism ever designed capable of converting linear motions of any actuator to $\\pm$60$^\\circ$ rotary wing stroke motion. It is planar, compliant, can be fabricated in a single step and requires no assembly. Further, its design is universal in nature, that is, it can be used with any linear actuator capable of delivering sufficient power, irrespective of the magnitude of actuator displacements. We also report a novel passive wing pitch mechanism whose motion has little dependence on the aerodynamic loading on the wing. This exponentially simplifies the job of the designer by decoupling the as of yet highly coupled wing morphology, wing kinematics and flexure stiffness parameters. Like the contemporary flexure-based methods it is an add-on to a given wing stroke mechanism. Moreover, the intended wing pitch amplitude could easily be changed post-fabrication by tuning the resonance mass in the mechanism."
      },
      {
        "node_idx": 89900,
        "score_0_10": 9,
        "title": "a characterisation of system wide propagation in the malware landscape",
        "abstract": "System-wide propagation is frequently observed in malware, and there are several resources, like blog posts and similar, that detail some of the techniques used. However, there is currently no thorough study on the subject at large, and the full extent of system-wide malware propagation remains unknown. In this paper, we perform a systematic study on many real-world samples to comprehensively characterise system-wide propagation within the malware landscape and the goal is to use detailed and precise analyses to derive high-level views. We achieve this by collecting a diverse set of malware samples, analyse them in our Minerva malware analysis framework and then extract vast amounts of statistics about the results. We use these results to provide an in-depth discussion centred on four main research questions."
      },
      {
        "node_idx": 129863,
        "score_0_10": 9,
        "title": "conservativity of embeddings in the lambda pi calculus modulo rewriting long version",
        "abstract": "The lambda-Pi calculus can be extended with rewrite rules to embed any other functional pure type system. The normalization and conserva-tivity properties of the embedding is an open problem. In this paper, we show that the embedding is conservative. We define an inverse translation into a pure type system completion and show that the completion is con-servative using the reducibility method. This result further justifies the use of the lambda-Pi calculus modulo rewriting as a logical framework."
      }
    ]
  },
  "385": {
    "explanation": "Secrecy capacity and secure communication in Gaussian MIMO and fading channels",
    "topk": [
      {
        "node_idx": 32700,
        "score_0_10": 10,
        "title": "the secrecy capacity region of the gaussian mimo multi receiver wiretap channel",
        "abstract": "In this paper, we consider the Gaussian multiple-input multiple-output (MIMO) multi-receiver wiretap channel in which a transmitter wants to have confidential communication with an arbitrary number of users in the presence of an external eavesdropper. We derive the secrecy capacity region of this channel for the most general case. We first show that even for the single-input single-output (SISO) case, existing converse techniques for the Gaussian scalar broadcast channel cannot be extended to this secrecy context, to emphasize the need for a new proof technique. Our new proof technique makes use of the relationships between the minimum-mean-square-error and the mutual information, and equivalently, the relationships between the Fisher information and the differential entropy. Using the intuition gained from the converse proof of the SISO channel, we first prove the secrecy capacity region of the degraded MIMO channel, in which all receivers have the same number of antennas, and the noise covariance matrices can be arranged according to a positive semi-definite order. We then generalize this result to the aligned case, in which all receivers have the same number of antennas, however there is no order among the noise covariance matrices. We accomplish this task by using the channel enhancement technique. Finally, we find the secrecy capacity region of the general MIMO channel by using some limiting arguments on the secrecy capacity region of the aligned MIMO channel. We show that the capacity achieving coding scheme is a variant of dirty-paper coding with Gaussian signals."
      },
      {
        "node_idx": 53332,
        "score_0_10": 10,
        "title": "towards the secrecy capacity of the gaussian mimo wire tap channel the 2 2 1 channel",
        "abstract": "We find the secrecy capacity of the 2-2-1 Gaussian MIMO wiretap channel, which consists of a transmitter and a receiver with two antennas each, and an eavesdropper with a single antenna. We determine the secrecy capacity of this channel by proposing an achievable scheme and then developing a tight upper bound that meets the proposed achievable secrecy rate. We show that, for this channel, Gaussian signalling in the form of beam-forming is optimal, and no pre-processing of information is necessary."
      },
      {
        "node_idx": 85766,
        "score_0_10": 10,
        "title": "on the secrecy capacity of fading channels",
        "abstract": "We consider the secure transmission of information over an ergodic fading channel in the presence of an eavesdropper. Our eavesdropper can be viewed as the wireless counterpart of Wyner's wiretapper. The secrecy capacity of such a system is characterized under the assumption of asymptotically long coherence intervals. We first consider the full Channel State Information (CSI) case, where the transmitter has access to the channel gains of the legitimate receiver and the eavesdropper. The secrecy capacity under this full CSI assumption serves as an upper bound for the secrecy capacity when only the CSI of the legitimate receiver is known at the transmitter, which is characterized next. In each scenario, the perfect secrecy capacity is obtained along with the optimal power and rate allocation strategies. We then propose a low-complexity on/off power allocation strategy that achieves near-optimal performance with only the main channel CSI. More specifically, this scheme is shown to be asymptotically optimal as the average SNR goes to infinity, and interestingly, is shown to attain the secrecy capacity under the full CSI assumption. Remarkably, our results reveal the positive impact of fading on the secrecy capacity and establish the critical role of rate adaptation, based on the main channel CSI, in facilitating secure communications over slow fading channels."
      },
      {
        "node_idx": 11141,
        "score_0_10": 9,
        "title": "secure communication over fading channels",
        "abstract": "The fading broadcast channel with confidential messages (BCC) is investigated, where a source node has common information for two receivers (receivers 1 and 2), and has confidential information intended only for receiver 1. The confidential information needs to be kept as secret as possible from receiver 2. The broadcast channel from the source node to receivers 1 and 2 is corrupted by multiplicative fading gain coefficients in addition to additive Gaussian noise terms. The channel state information (CSI) is assumed to be known at both the transmitter and the receivers. The parallel BCC with independent subchannels is first studied, which serves as an information-theoretic model for the fading BCC. The secrecy capacity region of the parallel BCC is established. This result is then specialized to give the secrecy capacity region of the parallel BCC with degraded subchannels. The secrecy capacity region is then established for the parallel Gaussian BCC, and the optimal source power allocations that achieve the boundary of the secrecy capacity region are derived. In particular, the secrecy capacity region is established for the basic Gaussian BCC. The secrecy capacity results are then applied to study the fading BCC. Both the ergodic and outage performances are studied."
      },
      {
        "node_idx": 145848,
        "score_0_10": 9,
        "title": "lecture notes on network information theory",
        "abstract": "These lecture notes have been converted to a book titled Network Information Theory published recently by Cambridge University Press. This book provides a significantly expanded exposition of the material in the lecture notes as well as problems and bibliographic notes at the end of each chapter. The authors are currently preparing a set of slides based on the book that will be posted in the second half of 2012. More information about the book can be found at this http URL The previous (and obsolete) version of the lecture notes can be found at this http URL"
      },
      {
        "node_idx": 149847,
        "score_0_10": 9,
        "title": "de anonymizing social networks",
        "abstract": "Operators of online social networks are increasingly sharing potentially sensitive information about users and their relationships with advertisers, application developers, and data-mining researchers. Privacy is typically protected by anonymization, i.e., removing names, addresses, etc.We present a framework for analyzing privacy and anonymity in social networks and develop a new re-identification algorithm targeting anonymized social-network graphs. To demonstrate its effectiveness on real-world networks, we show that a third of the users who can be verified to have accounts on both Twitter, a popular microblogging service, and Flickr, an online photo-sharing site, can be re-identified in the anonymous Twitter graph with only a 12% error rate.Our de-anonymization algorithm is based purely on the network topology, does not require creation of a large number of dummy \"sybil\" nodes, is robust to noise and all existing defenses, and works even when the overlap between the target network and the adversary's auxiliary information is small."
      },
      {
        "node_idx": 30590,
        "score_0_10": 9,
        "title": "exponential decreasing rate of leaked information in universal random privacy amplification",
        "abstract": "We derive a new upper bound for Eve's information in secret key generation from a common random number without communication. This bound improves on Bennett 's bound based on the Renyi entropy of order 2 because the bound obtained here uses the Renyi entropy of order 1+s for s \u2208 [0,1]. This bound is applied to a wire-tap channel. Then, we derive an exponential upper bound for Eve's information. Our exponent is compared with Hayashi 's exponent. For the additive case, the bound obtained here is better. The result is applied to secret key agreement by public discussion."
      },
      {
        "node_idx": 119197,
        "score_0_10": 9,
        "title": "cooperation with an untrusted relay a secrecy perspective",
        "abstract": "We consider the communication scenario where a source-destination pair wishes to keep the information secret from a relay node despite wanting to enlist its help. For this scenario, an interesting question is whether the relay node should be deployed at all. That is, whether cooperation with an untrusted relay node can ever be beneficial. We first provide an achievable secrecy rate for the general untrusted relay channel, and proceed to investigate this question for two types of relay networks with orthogonal components. For the first model, there is an orthogonal link from the source to the relay. For the second model, there is an orthogonal link from the relay to the destination. For the first model, we find the equivocation capacity region and show that answer is negative. In contrast, for the second model, we find that the answer is positive. Specifically, we show, by means of the achievable secrecy rate based on compress-and-forward, that by asking the untrusted relay node to relay information, we can achieve a higher secrecy rate than just treating the relay as an eavesdropper. For a special class of the second model, where the relay is not interfering itself, we derive an upper bound for the secrecy rate using an argument whose net effect is to separate the eavesdropper from the relay. The merit of the new upper bound is demonstrated on two channels that belong to this special class. The Gaussian case of the second model mentioned above benefits from this approach in that the new upper bound improves the previously known bounds. For the Cover-Kim deterministic relay channel, the new upper bound finds the secrecy capacity when the source-destination link is not worse than the source-relay link, by matching with achievable rate we present."
      },
      {
        "node_idx": 139020,
        "score_0_10": 9,
        "title": "semantically secure lattice codes for the gaussian wiretap channel",
        "abstract": "We propose a new scheme of wiretap lattice coding that achieves semantic security and strong secrecy over the Gaussian wiretap channel. The key tool in our security proof is the flatness factor, which characterizes the convergence of the conditional output distributions corresponding to different messages and leads to an upper bound on the information leakage. We not only introduce the notion of secrecy-good lattices, but also propose the flatness factor as a design criterion of such lattices. Both the modulo-lattice Gaussian channel and genuine Gaussian channel are considered. In the latter case, we propose a novel secrecy coding scheme based on the discrete Gaussian distribution over a lattice, which achieves the secrecy capacity to within a half nat under mild conditions. No a priori distribution of the message is assumed, and no dither is used in our proposed schemes."
      },
      {
        "node_idx": 48784,
        "score_0_10": 9,
        "title": "secrecy in cooperative relay broadcast channels",
        "abstract": "We investigate the effects of user cooperation on the secrecy of broadcast channels by considering a cooperative relay broadcast channel. We show that user cooperation can increase the achievable secrecy region. We propose an achievable scheme that combines Marton's coding scheme for broadcast channels and Cover and El Gamal's compress-and-forward scheme for relay channels. We derive outer bounds for the rate-equivocation region using auxiliary random variables for single-letterization. Finally, we consider a Gaussian channel and show that both users can have positive secrecy rates, which is not possible for scalar Gaussian broadcast channels without cooperation."
      }
    ]
  },
  "386": {
    "explanation": "locating forced oscillation sources in power systems",
    "topk": [
      {
        "node_idx": 163176,
        "score_0_10": 10,
        "title": "using effective generator impedance for forced oscillation source location",
        "abstract": "Locating the sources of forced low-frequency oscillations in power systems is an important problem. A number of proposed methods demonstrate their practical usefulness, but many of them rely on strong modeling assumptions and provide poor performance in certain cases for reasons still not well understood. This paper proposes a systematic method for locating the source of a forced oscillation by considering a generator's response to fluctuations of its terminal voltages and currents. It is shown that a generator can be represented as an effective admittance matrix with respect to low-frequency oscillations, and an explicit form for this matrix, for various generator models, is derived. Furthermore, it is shown that a source generator, in addition to its effective admittance, is characterized by the presence of an effective current source thus giving a natural qualitative distinction between source and nonsource generators. Detailed descriptions are given of a source detection procedure based on this developed representation, and the method's effectiveness is confirmed by simulations on the recommended testbeds (eg. WECC 179-bus system). This method is free of strong modeling assumptions and is also shown to be robust in the presence of measurement noise and generator parameter uncertainty."
      },
      {
        "node_idx": 49951,
        "score_0_10": 10,
        "title": "simulation leagues analysis of competition formats",
        "abstract": "The selection of an appropriate competition format is critical for both the success and credibility of any competition, both real and simulated. In this paper, the automated parallelism offered by the RoboCupSoccer 2D simulation league is leveraged to conduct a 28,000 game round-robin between the top 8 teams from RoboCup 2012 and 2013. A proposed new competition format is found to reduce variation from the resultant statistically significant team performance rankings by 75% and 67%, when compared to the actual competition results from RoboCup 2012 and 2013 respectively. These results are statistically validated by generating 10,000 random tournaments for each of the three considered formats and comparing the respective distributions of ranking discrepancy."
      },
      {
        "node_idx": 54574,
        "score_0_10": 10,
        "title": "memetic artificial bee colony algorithm for large scale global optimization",
        "abstract": "Memetic computation (MC) has emerged recently as a new paradigm of efficient algorithms for solving the hardest optimization problems. On the other hand, artificial bees colony (ABC) algorithms demonstrate good performances when solving continuous and combinatorial optimization problems. This study tries to use these technologies under the same roof. As a result, a memetic ABC (MABC) algorithm has been developed that is hybridized with two local search heuristics: the Nelder-Mead algorithm (NMA) and the random walk with direction exploitation (RWDE). The former is attended more towards exploration, while the latter more towards exploitation of the search space. The stochastic adaptation rule was employed in order to control the balancing between exploration and exploitation. This MABC algorithm was applied to a Special suite on Large Scale Continuous Global Optimization at the 2012 IEEE Congress on Evolutionary Computation. The obtained results the MABC are comparable with the results of DECC-G, DECC-G*, and MLCC."
      },
      {
        "node_idx": 30977,
        "score_0_10": 10,
        "title": "a bayesian approach to forced oscillation source location given uncertain generator parameters",
        "abstract": "Since forced oscillations are exogenous to dynamic power system models, the models by themselves cannot predict when or where a forced oscillation will occur. Locating the sources of these oscillations, therefore, is a challenging problem which requires analytical methods capable of using real time power system data to trace an observed oscillation back to its source. The difficulty of this problem is exacerbated by the fact that the parameters associated with a given power system model can range from slightly uncertain to entirely unknown. In this paper, a Bayesian framework, via a two-stage Maximum A Posteriori optimization routine, is employed in order to locate the most probable source of a forced oscillation given an uncertain prior model. The approach leverages an equivalent circuit representation of the system in the frequency domain and employs a numerical procedure which makes the problem suitable for real time application. The derived framework lends itself to successful performance in the presence of PMU measurement noise, high generator parameter uncertainty, and multiple forced oscillations occurring simultaneously. The approach is tested on a 4-bus system with a single forced oscillation source and on the WECC 179-bus system with multiple oscillation sources."
      },
      {
        "node_idx": 12391,
        "score_0_10": 10,
        "title": "using passivity theory to interpret the dissipating energy flow method",
        "abstract": "Despite wide-scale deployment of phasor measurement unit technology, locating the sources of low frequency forced oscillations in power systems is still an open research topic. The dissipating energy flow method is one source location technique which has performed remarkably well in both simulation and real time application at ISO New England. The method has several deficiencies, though, which are still poorly understood. This paper borrows the concepts of passivity and positive realness from the controls literature in order to interpret the dissipating energy flow method, pinpoint the reasons for its deficiencies, and set up a framework for improving the method. The theorems presented in this paper are then tested via simulation on a simple infinite bus power system model."
      },
      {
        "node_idx": 99925,
        "score_0_10": 10,
        "title": "model free active input output feedback linearization of a single link flexible joint manipulator an improved adrc approach",
        "abstract": "Traditional Input-Output Feedback Linearization (IOFL) requires full knowledge of system dynamics and assumes no disturbance at the input channel and no system's uncertainties. In this paper, a model-free Active Input-Output Feedback Linearization (AIOFL) technique based on an Improved Active Disturbance Rejection Control (IADRC) paradigm is proposed to design feedback linearization control law for a generalized nonlinear system with known relative degree. The Linearization Control Law(LCL) is composed of a scaled generalized disturbance estimated by an Improved Nonlinear Extended State Observer (INLESO) with saturation-like behavior and the nominal control law produced by an Improved Nonlinear State Error Feedback (INLSEF). The proposed AIOFL cancels in real-time fashion the generalized disturbances which represent all the unwanted dynamics, exogenous disturbances, and system uncertainties and transforms the system into a chain of integrators up to the relative degree of the system, the only information required about the nonlinear system. Stability analysis has been conducted based on Lyapunov functions and revealed the convergence of the INLESO and the asymptotic stability of the closed-loop system. Verification of the outcomes has been achieved by applying the proposed AIOFL technique on the Flexible Joint Single Link Manipulator (SLFJM). The simulations results validated the effectiveness of the proposed AIOFL tool based on IADRC as compared to the conventional ADRC based AIOFL and the traditional IOFL techniques."
      },
      {
        "node_idx": 16751,
        "score_0_10": 9,
        "title": "dynamic magnetometer calibration and alignment to inertial sensors by kalman filtering",
        "abstract": "Magnetometer and inertial sensors are widely used for orientation estimation. Magnetometer usage is often troublesome, as it is prone to be interfered by onboard or ambient magnetic disturbance. The onboard soft-iron material distorts not only the magnetic field, but the magnetometer sensor frame coordinate and the cross-sensor misalignment relative to inertial sensors. It is desirable to conveniently put magnetic and inertial sensors information in a common frame. Existing methods either split the problem into successive intrinsic and cross-sensor calibrations, or rely on stationary accelerometer measurements which is infeasible in dynamic conditions. This paper formulates the magnetometer calibration and alignment to inertial sensors as a state estimation problem, and collectively solves the magnetometer intrinsic and cross-sensor calibrations, as well as the gyroscope bias estimation. Sufficient conditions are derived for the problem to be globally observable, even when no accelerometer information is used at all. An extended Kalman filter is designed to implement the state estimation and comprehensive test data results show the superior performance of the proposed approach. It is immune to acceleration disturbance and applicable potentially in any dynamic conditions."
      },
      {
        "node_idx": 78493,
        "score_0_10": 9,
        "title": "analysis and control of aircraft longitudinal dynamics subject to steady aerodynamic forces",
        "abstract": "The paper contributes towards the development of a unified control approach for longitudinal aircraft dynamics. Prior to the control design, we analyze the existence and the uniqueness of the equilibrium orientation along a desired reference velocity. We show that shape symmetries and aerodynamic stall phenomena imply the existence of the equilibrium orientation irrespective of the reference velocity. The equilibrium orientation, however, is not in general unique, and this may trigger an aircraft loss-of-control for specific reference velocities. Conditions that ensure the local and the global uniqueness of the equilibrium orientation are stated. We show that the uniqueness of the equilibrium orientation is intimately related to the possibility of applying the spherical equivalency, i.e. a thrust change of variable rendering the direction of the transformed external force independent of the vehicle's orientation, as in the case of spherical shapes. Once this transformation is applied, control laws for reference velocities can be designed. We show that these laws extend the thrust direction control paradigm developed for systems with orientation-independent external forces, e.g. spherical shapes, to orientation-dependent external forces, e.g. generic shapes."
      },
      {
        "node_idx": 82343,
        "score_0_10": 9,
        "title": "improved accent classification combining phonetic vowels with acoustic features",
        "abstract": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field."
      },
      {
        "node_idx": 146057,
        "score_0_10": 9,
        "title": "a control performance index for multicopters under off nominal conditions",
        "abstract": "In order to prevent loss of control (LOC) accidents,the real-time control performance monitoring problem is studied for multicopters. Different from the existing work, this paper does not try to monitor the performance of the controllers directly. In turn, the disturbances of multicopters under off-nominal conditions are estimated to affect a proposed index to tell the user whether the multicopter will be LOC or not. Firstly, a new degree of controllability (DoC) will be proposed for multicopters subject to control constrains and off-nominal conditions. Then a control performance index (CPI) is defined based on the new DoC to reflect the control performance for multicopters. Besides, the proposed CPI is applied to a new switching control framework to guide the control decision of multicopter under off-nominal conditions. Finally, simulation and experimental results show the effectiveness of the CPI and the proposed switching control framework."
      }
    ]
  },
  "388": {
    "explanation": "performance optimization tools for multicore and reconfigurable computing",
    "topk": [
      {
        "node_idx": 24673,
        "score_0_10": 10,
        "title": "likwid a lightweight performance oriented tool suite for x86 multicore environments",
        "abstract": "Exploiting the performance of today's processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command-line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and toggling hardware prefetchers. An API for using the performance counting features from user code is also included. We clearly state the differences to the widely used PAPI interface. To demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well-known OpenMP STREAM triad benchmark, and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips."
      },
      {
        "node_idx": 74433,
        "score_0_10": 10,
        "title": "likwid a lightweight performance oriented tool suite for x86 multicore environments",
        "abstract": "Exploiting the performance of today's processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command-line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and toggling hardware prefetchers. An API for using the performance counting features from user code is also included. We clearly state the differences to the widely used PAPI interface. To demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well-known OpenMP STREAM triad benchmark, and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips."
      },
      {
        "node_idx": 8434,
        "score_0_10": 10,
        "title": "high performance reconfigurable computing systems",
        "abstract": "The rapid progress and advancement in electronic chips technology provide a variety of new implementation options for system engineers. The choice varies between the flexible programs running on a general-purpose processor (GPP) and the fixed hardware implementation using an application specific integrated circuit (ASIC). Many other implementation options present, for instance, a system with a RISC processor and a DSP core. Other options include graphics processors and microcontrollers. Specialist processors certainly improve performance over general-purpose ones, but this comes as a quid pro quo for flexibility. Combining the flexibility of GPPs and the high performance of ASICs leads to the introduction of reconfigurable computing (RC) as a new implementation option with a balance between versatility and speed. The focus of this chapter is on introducing reconfigurable computers as modern super computing architectures. The chapter also investigates the main reasons behind the current advancement in the development of RC-systems. Furthermore, a technical survey of various RC-systems is included laying common grounds for comparisons. In addition, this chapter mainly presents case studies implemented under the MorphoSys RC-system. The selected case studies belong to different areas of application, such as, computer graphics and information coding. Parallel versions of the studied algorithms are developed to match the topologies supported by the MorphoSys. Performance evaluation and results analyses are included for implementations with different characteristics."
      },
      {
        "node_idx": 108606,
        "score_0_10": 10,
        "title": "cyclic coding algorithms under morphosys reconfigurable computing system",
        "abstract": "This paper introduces reconfigurable computing (RC) and specifically chooses one of the prototypes in this field, MorphoSys (M1) [1-5]. The paper addresses the results obtained when using RC in mapping algorithms pertaining to digital coding in relation to previous research [6-10]. The chosen algorithms relate to cyclic coding techniques, namely the CCITT CRC-16 and the CRC-16. A performance analysis study of the M1 RC system is also presented to evaluate the efficiency of the algorithm execution on the M1 system. For comparison purposes, three other systems were used to map the same algorithms showing the advantages and disadvantages of each compared with the M1 system. The algorithms were run on the 8 \u00d7 8 RC (reconfigurable) array of the M1 (MorphoSys) system; numerical examples were simulated to validate our results, using the MorphoSys mULATE program, which simulates MorphoSys operations."
      },
      {
        "node_idx": 36142,
        "score_0_10": 10,
        "title": "joint trajectory and communication design for multi uav enabled wireless networks",
        "abstract": "Unmanned aerial vehicles (UAVs) have attracted significant interest recently in assisting wireless communication due to their high maneuverability, flexible deployment, and low cost. This paper considers a multi-UAV enabled wireless communication system, where multiple UAV-mounted aerial base stations (BSs) are employed to serve a group of users on the ground. To achieve fair performance among users, we maximize the minimum throughput over all ground users in the downlink communication by optimizing the multiuser communication scheduling and association jointly with the UAVs' trajectory and power control. The formulated problem is a mixed integer non-convex optimization problem that is challenging to solve. As such, we propose an efficient iterative algorithm for solving it by applying the block coordinate descent and successive convex optimization techniques. Specifically, the user scheduling and association, UAV trajectory, and transmit power are alternately optimized in each iteration. In particular, for the non-convex UAV trajectory and transmit power optimization problems, two approximate convex optimization problems are solved, respectively. We further show that the proposed algorithm is guaranteed to converge to at least a locally optimal solution. To speed up the algorithm convergence and achieve good throughput, a low-complexity and systematic initialization scheme is also proposed for the UAV trajectory design based on the simple circular trajectory and the circle packing scheme. Extensive simulation results are provided to demonstrate the significant throughput gains of the proposed design as compared to other benchmark schemes."
      },
      {
        "node_idx": 156364,
        "score_0_10": 10,
        "title": "a comprehensive survey of recent advancements in molecular communication",
        "abstract": "With much advancement in the field of nanotechnology, bioengineering, and synthetic biology over the past decade, microscales and nanoscales devices are becoming a reality. Yet the problem of engineering a reliable communication system between tiny devices is still an open problem. At the same time, despite the prevalence of radio communication, there are still areas where traditional electromagnetic waves find it difficult or expensive to reach. Points of interest in industry, cities, and medical applications often lie in embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or they are located in such a way that directional high frequency systems are ineffective. Inspired by nature, one solution to these problems is molecular communication (MC), where chemical signals are used to transfer information. Although biologists have studied MC for decades, it has only been researched for roughly 10 year from a communication engineering lens. Significant number of papers have been published to date, but owing to the need for interdisciplinary work, much of the results are preliminary. In this survey, the recent advancements in the field of MC engineering are highlighted. First, the biological, chemical, and physical processes used by an MC system are discussed. This includes different components of the MC transmitter and receiver, as well as the propagation and transport mechanisms. Then, a comprehensive survey of some of the recent works on MC through a communication engineering lens is provided. The survey ends with a technology readiness analysis of MC and future research directions."
      },
      {
        "node_idx": 102043,
        "score_0_10": 9,
        "title": "bigdatabench a big data benchmark suite from internet services",
        "abstract": "As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above."
      },
      {
        "node_idx": 129115,
        "score_0_10": 9,
        "title": "gridsim a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing",
        "abstract": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular paradigms for next generation parallel and distributed computing. The management of resources and scheduling of applications in such large-scale distributed systems is a complex undertaking. In order to prove the effectiveness of resource brokers and associated scheduling algorithms, their performance needs to be evaluated under different scenarios such as varying number of resources and users with different requirements. In a grid environment, it is hard and even impossible to perform scheduler performance evaluation in a repeatable and controllable manner as resources and users are distributed across multiple organizations with their own policies. To overcome this limitation, we have developed a Java-based discrete-event grid simulation toolkit called GridSim. The toolkit supports modeling and simulation of heterogeneous grid resources (both time- and space-shared), users and application models. It provides primitives for creation of application tasks, mapping of tasks to resources, and their management. To demonstrate suitability of the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker and evaluated the performance of deadline and budget constrained cost- and time-minimization scheduling algorithms."
      },
      {
        "node_idx": 162347,
        "score_0_10": 9,
        "title": "implementing large scale agile frameworks challenges and recommendations",
        "abstract": "Based on 13 agile transformation cases over 15 years, this article identifies nine challenges associated with implementing SAFe, Scrum-at-Scale, Spotify, LeSS, Nexus, and other mixed or customised large-scale agile frameworks. These challenges should be considered by organizations aspiring to pursue a large-scale agile strategy. This article also provides recommendations for practitioners and agile researchers."
      },
      {
        "node_idx": 14289,
        "score_0_10": 9,
        "title": "agility is responsiveness to change an essential definition",
        "abstract": "There is some ambiguity of what agile means in both research and practice. Authors have suggested a diversity of different definitions, and many practitioners have their own idea of what it is. It is, therefore, difficult to interpret what agile really is. The concept, however, exists in its implementation through agile practices. In this paper, we draw on our experience from several agile transformations and argue that adopting an agile approach boils down to being responsive to change. That is the core purpose of any agile transformation. To support this claim, we relate agile principles, practices, and the agile manifesto to this core definition. Drawing on our practices of organizational changes in software companies, we also show that many misunderstandings about agility can be avoided through this definition. That, in turn, increases the likelihood of successful agile transformations."
      }
    ]
  },
  "390": {
    "explanation": "scheduling and structural graph algorithms in computing systems",
    "topk": [
      {
        "node_idx": 16420,
        "score_0_10": 10,
        "title": "a survey on graph drawing beyond planarity",
        "abstract": "Graph Drawing Beyond Planarity is a rapidly growing research area that classifies and studies geometric representations of non-planar graphs in terms of forbidden crossing configurations. Aim of this survey is to describe the main research directions in this area, the most prominent known results, and some of the most challenging open problems."
      },
      {
        "node_idx": 91602,
        "score_0_10": 10,
        "title": "edf vd scheduling of mixed criticality systems with degraded quality guarantees",
        "abstract": "This paper studies real-time scheduling of mixed-criticality systems where low-criticality tasks are still guaranteed some service in the high-criticality mode, with reduced execution budgets. First, we present a utilization-based schedulability test for such systems under EDF-VD scheduling. Second, we quantify the suboptimality of EDF-VD (with our test condition) in terms of speedup factors. In general, the speedup factor is a function with respect to the ratio between the amount of resource required by different types of tasks in different criticality modes, and reaches 4/3 in the worst case. Furthermore, we show that the proposed utilization-based schedulability test and speedup factor results apply to the elastic mixed-criticality model as well. Experiments show effectiveness of our proposed method and confirm the theoretical suboptimality results."
      },
      {
        "node_idx": 133635,
        "score_0_10": 9,
        "title": "combining task level and system level scheduling modes for mixed criticality systems",
        "abstract": "Different scheduling algorithms for mixed criticality systems have been recently proposed. The common denominator of these algorithms is to discard low critical tasks whenever high critical tasks are in lack of computation resources. This is achieved upon a switch of the scheduling mode from Normal to Critical. We distinguish two main categories of the algorithms: system-level mode switch and task-level mode switch. System-level mode algorithms allow low criticality (LC) tasks to execute only in normal mode. Task-level mode switch algorithms enable to switch the mode of an individual high criticality task (HC), from low (LO) to high (HI), to obtain priority over all LC tasks. This paper investigates an online scheduling algorithm for mixed-criticality systems that supports dynamic mode switches for both task level and system level. When a HC task job overruns its LC budget, then only that particular job is switched to HI mode. If the job cannot be accommodated, then the system switches to Critical mode. To accommodate for resource availability of the HC jobs, the LC tasks are degraded by stretching their periods until the Critical mode exhibiting job complete its execution. The stretching will be carried out until the resource availability is met. We have mechanized and implemented the proposed algorithm using Uppaal. To study the efficiency of our scheduling algorithm, we examine a case study and compare our results to the state of the art algorithms."
      },
      {
        "node_idx": 144816,
        "score_0_10": 9,
        "title": "work stealing with latency",
        "abstract": "We study in this paper the impact of communication latency on the classical Work Stealing load balancing algorithm. Our approach considers existing performance models and the underlying algorithms. We introduce a latency parameter in the model and study its overall impact by careful observations of simulation results. Using this method we are able to derive a new expression of the expected running time of divisible load applications. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors one should use for a given work platform combination. We also consider the impact of several algorithmic variants like simultaneous transfers of work or thresholds for avoiding useless transfers. All our results are validated through simulation on a wide range of parameters."
      },
      {
        "node_idx": 135780,
        "score_0_10": 9,
        "title": "practical graph isomorphism ii",
        "abstract": "We report the current state of the graph isomorphism problem from the practical point of view. After describing the general principles of the refinement-individualization paradigm and proving its validity, we explain how it is implemented in several of the key programs. In particular, we bring the description of the best known program nauty up to date and describe an innovative approach called Traces that outperforms the competitors for many difficult graph classes. Detailed comparisons against saucy, Bliss and conauto are presented."
      },
      {
        "node_idx": 3579,
        "score_0_10": 9,
        "title": "arc disjoint in and out branchings rooted at the same vertex in compositions of digraphs",
        "abstract": "A digraph $D=(V, A)$ has a good pair at a vertex $r$ if $D$ has a pair of arc-disjoint in- and out-branchings rooted at $r$. Let $T$ be a digraph with $t$ vertices $u_1,\\dots , u_t$ and let $H_1,\\dots H_t$ be digraphs such that $H_i$ has vertices $u_{i,j_i},\\ 1\\le j_i\\le n_i.$ Then the composition $Q=T[H_1,\\dots , H_t]$ is a digraph with vertex set $\\{u_{i,j_i}\\mid 1\\le i\\le t, 1\\le j_i\\le n_i\\}$ and arc set $$A(Q)=\\cup^t_{i=1}A(H_i)\\cup \\{u_{ij_i}u_{pq_p}\\mid u_iu_p\\in A(T), 1\\le j_i\\le n_i, 1\\le q_p\\le n_p\\}.$$ #R##N#When $T$ is arbitrary, we obtain the following result: every strong digraph composition $Q$ in which $n_i\\ge 2$ for every $1\\leq i\\leq t$, has a good pair at every vertex of $Q.$ The condition of $n_i\\ge 2$ in this result cannot be relaxed. When $T$ is semicomplete, we characterize semicomplete compositions with a good pair, which generalizes the corresponding characterization by Bang-Jensen and Huang (J. Graph Theory, 1995) for quasi-transitive digraphs. As a result, we can decide in polynomial time whether a given semicomplete composition has a good pair rooted at a given vertex."
      },
      {
        "node_idx": 140115,
        "score_0_10": 9,
        "title": "label dependent session types",
        "abstract": "Session types have emerged as a typing discipline for communication protocols. Existing calculi with session types come equipped with many different primitives that combine communication with the introduction or elimination of the transmitted value. #R##N#We present a foundational session type calculus with a lightweight operational semantics. It fully decouples communication from the introduction and elimination of data and thus features a single communication reduction, which acts as a rendezvous between senders and receivers. We achieve this decoupling by introducing label-dependent session types, a minimalist value-dependent session type system with subtyping. The system is sufficiently powerful to simulate existing functional session type systems. Compared to such systems, label-dependent session types place fewer restrictions on the code. We further introduce primitive recursion over natural numbers at the type level, thus allowing to describe protocols whose behaviour depends on numbers exchanged in messages. An algorithmic type checking system is introduced and proved equivalent to its declarative counterpart. The new calculus showcases a novel lightweight integration of dependent types and linear typing, with has uses beyond session type systems."
      },
      {
        "node_idx": 119815,
        "score_0_10": 9,
        "title": "combinatorial properties of triangle free rectangle arrangements and the squarability problem",
        "abstract": "We consider arrangements of axis-aligned rectangles in the plane. A geometric arrangement specifies the coordinates of all rectangles, while a combinatorial arrangement specifies only the respective intersection type in which each pair of rectangles intersects. First, we investigate combinatorial contact arrangements, i.e., arrangements of interior-disjoint rectangles, with a triangle-free intersection graph. We show that such rectangle arrangements are in bijection with the 4-orientations of an underlying planar multigraph and prove that there is a corresponding geometric rectangle contact arrangement. Moreover, we prove that every triangle-free planar graph is the contact graph of such an arrangement. Secondly, we introduce the question whether a given rectangle arrangement has a combinatorially equivalent square arrangement. In addition to some necessary conditions and counterexamples, we show that rectangle arrangements pierced by a horizontal line are squarable under certain sufficient conditions."
      },
      {
        "node_idx": 50806,
        "score_0_10": 9,
        "title": "squarability of rectangle arrangements",
        "abstract": "We study when an arrangement of axis-aligned rectangles can be transformed into an arrangement of axis-aligned squares in $\\mathbb{R}^2$ while preserving its structure. We found a counterexample to the conjecture of J. Klawitter, M. N\\\"{o}llenburg and T. Ueckerdt whether all arrangements without crossing and side-piercing can be squared. Our counterexample also works in a more general case when we only need to preserve the intersection graph and we forbid side-piercing between squares. We also show counterexamples for transforming box arrangements into combinatorially equivalent hypercube arrangements. Finally, we introduce a linear program deciding whether an arrangement of rectangles can be squared in a more restrictive version where the order of all sides is preserved."
      },
      {
        "node_idx": 49580,
        "score_0_10": 9,
        "title": "computing properties of stable configurations of thermodynamic binding networks",
        "abstract": "Models of molecular computing generally embed computation in kinetics--the specific time evolution of a chemical system. However, if the desired output is not thermodynamically stable, basic physical chemistry dictates that thermodynamic forces will drive the system toward error throughout the computation. The Thermodynamic Binding Network (TBN) model was introduced to formally study how the thermodynamic equilibrium can be made consistent with the desired computation, and it idealizes binding interactions subject to enthalpy and entropy tradeoffs. Here we consider the computational complexity of natural questions about TBNs, and develop a practical algorithm for verifying the correctness of constructions based on a SAT solver. The TBN model together with automated verification tools will help inform strategies for leak and error reduction in strand displacement cascades and algorithmic tile assembly."
      }
    ]
  },
  "392": {
    "explanation": "handwritten character recognition and segmentation techniques",
    "topk": [
      {
        "node_idx": 17274,
        "score_0_10": 10,
        "title": "classification of gradient change features using mlp for handwritten character recognition",
        "abstract": "A novel, generic scheme for off-line handwritten English alphabets character images is proposed. The advantage of the technique is that it can be applied in a generic manner to different applications and is expected to perform better in uncertain and noisy environments. The recognition scheme is using a multilayer perceptron(MLP) neural networks. The system was trained and tested on a database of 300 samples of handwritten characters. For improved generalization and to avoid overtraining, the whole available dataset has been divided into two subsets: training set and test set. We achieved 99.10% and 94.15% correct recognition rates on training and test sets respectively. The purposed scheme is robust with respect to various writing styles and size as well as presence of considerable noise."
      },
      {
        "node_idx": 481,
        "score_0_10": 10,
        "title": "text text extractor tool for handwritten document transcription and annotation",
        "abstract": "This paper presents a framework for semi-automatic transcription of large-scale historical handwritten documents and proposes a simple user-friendly text extractor tool, TexT for transcription. The proposed approach provides a quick and easy transcription of text using computer assisted interactive technique. The algorithm finds multiple occurrences of the marked text on-the-fly using a word spotting system. TexT is also capable of performing on-the-fly annotation of handwritten text with automatic generation of ground truth labels, and dynamic adjustment and correction of user generated bounding box annotations with the word being perfectly encapsulated. The user can view the document and the found words in the original form or with background noise removed for easier visualization of transcription results. The effectiveness of TexT is demonstrated on an archival manuscript collection from well-known publicly available dataset."
      },
      {
        "node_idx": 11752,
        "score_0_10": 10,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 128298,
        "score_0_10": 10,
        "title": "devnagari handwritten numeral recognition using geometric features and statistical combination classifier",
        "abstract": "This paper presents a Devnagari Numerical recognition method based on statistical discriminant functions. 17 geometric features based on pixel connectivity, lines, line directions, holes, image area, perimeter, eccentricity, solidity, orientation etc. are used for representing the numerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear, Diagquadratic and Mahalanobis distance are used for classification. 1500 handwritten numerals are used for training. Another 1500 handwritten numerals are used for testing. Experimental results show that Linear, Quadratic and Mahalanobis discriminant functions provide better results. Results of these three Discriminants are fed to a majority voting type Combination classifier. It is found that Combination classifier offers better results over individual classifiers."
      },
      {
        "node_idx": 115866,
        "score_0_10": 10,
        "title": "text line segmentation in compressed representation of handwritten document using tunneling algorithm",
        "abstract": "Operating directly on the compressed document images without decompression would be an additional advantage for storage and transmission. In this research work, we perform text line segmentation directly in compressed representation of an unconstraint handwritten document image using tunneling algorithm. In this relation, we make use of text line terminal point which is the current state-of-the-art that enables text line segmentation. The terminal points spotted along both margins (left and right) of a document image for every text line are considered as source and target respectively. The effort in spotting the terminal positions is performed directly in the compressed domain. The tunneling algorithm uses a single agent to identify the coordinate positions in the compressed representation to perform text-line segmentation of the document. The agent starts at a source point and progressively tunnels a path routing in between two adjacent text lines and reaches the probable target. The agent\u2019s navigation path from source to the target bypassing obstacles, if any, results in segregating the two adjacent text lines. However, the target point would be known only when the agent reaches destination; this is applicable for all source points and henceforth we could analyze the correspondence between source and target nodes. In compressed representation of a document image, the continuous pixel values in a spatial domain are available in the form of batches known as white-runs (background) and black-runs (foreground). These batches are considered as features of a document image represented in a Grid map. Performing text-line segmentation using these features makes the system inexpensive compared to spatial domain processing. Artificial Intelligence in Expert systems with dynamic programming and greedy strategies is employed for every search space for tunneling. An exhaustive experimentation is carried out on various benchmark datasets including ICDAR13 and the performances are reported."
      },
      {
        "node_idx": 166913,
        "score_0_10": 10,
        "title": "performance comparison of svm and ann for handwritten devnagari character recognition",
        "abstract": "Classification methods based on learning from examples have been widely applied to character recognition from the 1990s and have brought forth significant improvements of recognition accuracies. This class of methods includes statistical methods, artificial neural networks, support vector machines (SVM), multiple classifier combination, etc. In this paper, we discuss the characteristics of the some classification methods that have been successfully applied to handwritten Devnagari character recognition and results of SVM and ANNs classification method, applied on Handwritten Devnagari characters. After preprocessing the character image, we extracted shadow features, chain code histogram features, view based features and longest run features. These features are then fed to Neural classifier and in support vector machine for classification. In neural classifier, we explored three ways of combining decisions of four MLP\u2019s, designed for four different features."
      },
      {
        "node_idx": 27604,
        "score_0_10": 9,
        "title": "word and character segmentation directly in run length compressed handwritten document images",
        "abstract": "From the literature, it is demonstrated that performing text-line segmentation directly in the run-length compressed handwritten document images significantly reduces the computational time and memory space. In this paper, we investigate the issues of word and character segmentation directly on the run-length compressed document images. Primarily, the spreads of the characters are intelligently extracted from the foreground runs of the compressed data and subsequently connected components are established. The spacing between the connected components would be larger between the adjacent words when compared to that of intra-words. With this knowledge, a threshold is empirically chosen for inter-word separation. Every connected component within a word is further analysed for character segmentation. Here, min-cut graph concept is used for separating the touching characters. Over-segmentation and under-segmentation issues are addressed by insertion and deletion operations respectively. The approach has been developed particularly for compressed handwritten English document images. However, the model has been tested on non-English document images."
      },
      {
        "node_idx": 9894,
        "score_0_10": 9,
        "title": "fast searching in packed strings",
        "abstract": "Given strings $P$ and $Q$ the (exact) string matching problem is to find all positions of substrings in $Q$ matching $P$. The classical Knuth-Morris-Pratt algorithm [SIAM J. Comput., 1977] solves the string matching problem in linear time which is optimal if we can only read one character at the time. However, most strings are stored in a computer in a packed representation with several characters in a single word, giving us the opportunity to read multiple characters simultaneously. In this paper we study the worst-case complexity of string matching on strings given in packed representation. Let $m \\leq n$ be the lengths $P$ and $Q$, respectively, and let $\\sigma$ denote the size of the alphabet. On a standard unit-cost word-RAM with logarithmic word size we present an algorithm using time $$ O\\left(\\frac{n}{\\log_\\sigma n} + m + \\occ\\right). $$ Here $\\occ$ is the number of occurrences of $P$ in $Q$. For $m = o(n)$ this improves the $O(n)$ bound of the Knuth-Morris-Pratt algorithm. Furthermore, if $m = O(n/\\log_\\sigma n)$ our algorithm is optimal since any algorithm must spend at least $\\Omega(\\frac{(n+m)\\log #R##N#\\sigma}{\\log n} + \\occ) = \\Omega(\\frac{n}{\\log_\\sigma n} + \\occ)$ time to read the input and report all occurrences. The result is obtained by a novel automaton construction based on the Knuth-Morris-Pratt algorithm combined with a new compact representation of subautomata allowing an optimal tabulation-based simulation."
      },
      {
        "node_idx": 61506,
        "score_0_10": 9,
        "title": "automatic document image binarization using bayesian optimization",
        "abstract": "Document image binarization is often a challenging task due to various forms of degradation. Although there exist several binarization techniques in literature, the binarized image is typically sensitive to control parameter settings of the employed technique. This paper presents an automatic document image binarization algorithm to segment the text from heavily degraded document images. The proposed technique uses a two band-pass filtering approach for background noise removal, and Bayesian optimization for automatic hyperparameter selection for optimal results. The effectiveness of the proposed binarization technique is empirically demonstrated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets."
      },
      {
        "node_idx": 69815,
        "score_0_10": 9,
        "title": "fast packed string matching for short patterns",
        "abstract": "Searching for all occurrences of a pattern in a text is a fundamental problem in computer science with applications in many other fields, like natural language processing, information retrieval and computational biology. In the last two decades a general trend has appeared trying to exploit the power of the word RAM model to speed-up the performances of classical string matching algorithms. In this model an algorithm operates on words of length w, grouping blocks of characters, and arithmetic and logic operations on the words take one unit of time. In this paper we use specialized word-size packed string matching instructions, based on the Intel streaming SIMD extensions (SSE) technology, to design very fast string matching algorithms in the case of short patterns. From our experimental results it turns out that, despite their quadratic worst case time complexity, the new presented algorithms become the clear winners on the average for short patterns, when compared against the most effective algorithms known in literature."
      }
    ]
  },
  "394": {
    "explanation": "multi-armed bandit algorithms and combinatorial optimization",
    "topk": [
      {
        "node_idx": 75807,
        "score_0_10": 10,
        "title": "combinatorial bandits revisited",
        "abstract": "This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose \\textsc{CombEXP}, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems."
      },
      {
        "node_idx": 96349,
        "score_0_10": 10,
        "title": "thompson sampling for contextual bandits with linear payoffs",
        "abstract": "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of $\\tilde{O}(d^{3/2}\\sqrt{T})$ (or $\\tilde{O}(d\\sqrt{T \\log(N)})$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of $\\sqrt{d}$ (or $\\sqrt{\\log(N)}$) of the information-theoretic lower bound for this problem."
      },
      {
        "node_idx": 31779,
        "score_0_10": 10,
        "title": "random almost popular matchings",
        "abstract": "For a set $A$ of $n$ people and a set $B$ of $m$ items, with each person having a preference list that ranks all items from most wanted to least wanted, we consider the problem of matching every person with a unique item. A matching $M$ is called $\\epsilon$-popular if for any other matching $M'$, the number of people who prefer $M'$ to $M$ is at most $\\epsilon n$ plus the number of those who prefer $M$ to $M'$. In 2006, Mahdian showed that when randomly generating people's preference lists, if $m/n > 1.42$, then a 0-popular matching exists with $1-o(1)$ probability; and if $m/n   1-\\epsilon$, then an $\\epsilon$-popular matching exists with $1-o(1)$ probability (upper bound); and if $\\alpha(1-e^{-(1+e^{1/\\alpha})/\\alpha}) < 1-2\\epsilon$, then an $\\epsilon$-popular matching exists with $o(1)$ probability (lower bound)."
      },
      {
        "node_idx": 30682,
        "score_0_10": 10,
        "title": "universum learning for svm regression",
        "abstract": "This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach."
      },
      {
        "node_idx": 58620,
        "score_0_10": 9,
        "title": "capacity per unit energy of fading channels with a peak constraint",
        "abstract": "A discrete-time single-user scalar channel with temporally correlated Rayleigh fading is analyzed. There is no side information at the transmitter or the receiver. A simple expression is given for the capacity per unit energy, in the presence of a peak constraint. The simple formula of Verdu/spl acute/ for capacity per unit cost is adapted to a channel with memory, and is used in the proof. In addition to bounding the capacity of a channel with correlated fading, the result gives some insight into the relationship between the correlation in the fading process and the channel capacity. The results are extended to a channel with side information, showing that the capacity per unit energy is one nat per joule, independently of the peak power constraint. A continuous-time version of the model is also considered. The capacity per unit energy subject to a peak constraint (but no bandwidth constraint) is given by an expression similar to that for discrete time, and is evaluated for Gauss-Markov and Clarke fading channels."
      },
      {
        "node_idx": 166314,
        "score_0_10": 9,
        "title": "reducing dueling bandits to cardinal bandits",
        "abstract": "We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form \"A is preferred to B\" (as opposed to cardinal feedback like \"A has value 2.5\"), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions -- named $\\Doubler$, $\\MultiSbm$ and $\\DoubleSbm$ -- provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For $\\Doubler$ and $\\MultiSbm$ we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of $\\DoubleSbm$ which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms."
      },
      {
        "node_idx": 17426,
        "score_0_10": 9,
        "title": "noncoherent capacity of underspread fading channels",
        "abstract": "We derive bounds on the noncoherent capacity of wide-sense stationary uncorrelated scattering (WSSUS) channels that are selective both in time and frequency, and are underspread, i.e., the product of the channel's delay spread and Doppler spread is small. The underspread assumption is satisfied by virtually all wireless communication channels. For input signals that are peak constrained in time and frequency, we obtain upper and lower bounds on capacity that are explicit in the channel's scattering function, are accurate for a large range of bandwidth, and allow to coarsely identify the capacity-optimal bandwidth as a function of the peak power and the channel's scattering function. We also obtain a closed-form expression for the first-order Taylor series expansion of capacity in the infinite-bandwidth limit, and show that our bounds are tight in the wideband regime. For input signals that are peak constrained in time only (and, hence, allowed to be peaky in frequency), we provide upper and lower bounds on the infinite-bandwidth capacity. Our lower bound is closely related to a result by Viterbi (1967). We find cases where the bounds coincide and, hence, the infinite-bandwidth capacity is characterized exactly. The analysis in this paper is based on a discrete-time discrete-frequency approximation of WSSUS time- and frequency-selective channels. This discretization takes the underspread property of the channel explicitly into account."
      },
      {
        "node_idx": 78813,
        "score_0_10": 9,
        "title": "maximum width empty square and rectangular annulus",
        "abstract": "An annulus is, informally, a ring-shaped region, often described by two concentric circles. The maximum-width empty annulus problem asks to find an annulus of a certain shape with the maximum possible width that avoids a given set of $n$ points in the plane. This problem can also be interpreted as the problem of finding an optimal location of a ring-shaped obnoxious facility among the input points. In this paper, we study square and rectangular variants of the maximum-width empty anuulus problem, and present first nontrivial algorithms. Specifically, our algorithms run in $O(n^3)$ and $O(n^2 \\log n)$ time for computing a maximum-width empty axis-parallel square and rectangular annulus, respectively. Both algorithms use only $O(n)$ space."
      },
      {
        "node_idx": 17404,
        "score_0_10": 9,
        "title": "on the second order statistics of the multihop rayleigh fading channel",
        "abstract": "Second order statistics provides a dynamic representation of a fading channel and plays an important role in the evaluation and design of the wireless communication systems. In this paper, we present a novel analytical framework for the evaluation of important second order statistical parameters, as the level crossing rate (LCR) and the average fade duration (AFD) of the amplify-and-forward multihop Rayleigh fading channel. More specifically, motivated by the fact that this channel is a cascaded one and can be modeled as the product of N fading amplitudes, we derive novel analytical expressions for the average LCR and the AFD of the product of N Rayleigh fading envelopes (or of the recently so-called NRayleigh channel). Furthermore, we derive simple and efficient closed-form approximations to the aforementioned parameters, using the multivariate Laplace approximation theorem. It is shown that our general results reduce to the corresponding ones of the specific dual-hop case, previously published. Numerical and computer simulation examples verify the accuracy of the presented mathematical analysis and show the tightness of the proposed approximations."
      },
      {
        "node_idx": 90178,
        "score_0_10": 9,
        "title": "scma codebook design",
        "abstract": "Multicarrier CDMA is a multiple access scheme in which modulated QAM symbols are spread over OFDMA tones by using a generally complex spreading sequence. Effectively, a QAM symbol is repeated over multiple tones. Low density signature (LDS) is a version of CDMA with low density spreading sequences allowing us to take advantage of a near optimal message passing algorithm (MPA) receiver with practically feasible complexity. Sparse code multiple access (SCMA) is a multi-dimensional codebook-based non-orthogonal spreading technique. In SCMA, the procedure of bit to QAM symbol mapping and spreading are combined together and incoming bits are directly mapped to multi-dimensional codewords of SCMA codebook sets. Each layer has its dedicated codebook. Shaping gain of a multi-dimensional constellation is one of the main sources of the performance improvement in comparison to the simple repetition of QAM symbols in LDS. Meanwhile, like LDS, SCMA enjoys the low complexity reception techniques due to the sparsity of SCMA codewords. In this paper a systematic approach is proposed to design SCMA codebooks mainly based on the design principles of lattice constellations. Simulation results are presented to show the performance gain of SCMA compared to LDS and OFDMA."
      }
    ]
  },
  "396": {
    "explanation": "optimization and control in power and communication networks under uncertainty",
    "topk": [
      {
        "node_idx": 116998,
        "score_0_10": 10,
        "title": "distributed load side control coping with variation of renewable generations",
        "abstract": "This paper addresses the distributed load frequency control of multi-area power system, where controllable loads are utilized to recover nominal frequencies for the advantage of fast response speed. The imbalanced power causing frequency deviation is decomposed into three parts: a known constant part, an unknown low-frequency variation part and a high-frequency residual. The known steady part is usually the prediction of power imbalance. The variation may result from the fluctuation of renewable resources, electric vehicle charging, etc., which is usually unknown to operators. The high-frequency residual is usually unknown and treated as an external disturbance. Correspondingly, in this paper, we resolve the following three problems in different timescales: 1) allocate the steady part of power imbalance economically; 2) mitigate the effect of unknown low-frequency power variation locally; 3) attenuate unknown high-frequency disturbances. To this end, a distributed controller combining consensus method with adaptive internal model control is proposed. We first prove that the closed-loop system is asymptotically stable and converges to the optimal solution of an optimization counterpart problem if the external disturbance is not included. We then prove that the power variation can be mitigated accurately. Furthermore, we show the closed-loop system is robust against both parameter uncertainty and external disturbances. The New England system is used to verifiy the efficacy of our design."
      },
      {
        "node_idx": 20355,
        "score_0_10": 10,
        "title": "coordinated ac dc microgrid optimal scheduling",
        "abstract": "This paper proposes a coordinated optimal scheduling model for hybrid AC/DC microgrids. The objective of the proposed model is to minimize the total microgrid operation cost when considering interactions between AC and DC sub-systems of the microgrid network. Nonlinear power flow equations for AC and DC networks have been linearized through a proposed model to enable formulating the problem by mixed integer linear programming (MILP) which expedites the solution process and ensures better solutions in terms of optimality. The proposed model is tested on the modified IEEE 33-bus test system. Numerical simulations exhibit the merits of the proposed coordinated AC/DC optimal scheduling model and further analyze its sensitivity to various decisive operational parameters."
      },
      {
        "node_idx": 139141,
        "score_0_10": 10,
        "title": "model predictive control of autonomous mobility on demand systems",
        "abstract": "In this paper we present a model predictive control (MPC) approach to optimize vehicle scheduling and routing in an autonomous mobility-on-demand (AMoD) system. In AMoD systems, robotic, self-driving vehicles transport customers within an urban environment and are coordinated to optimize service throughout the entire network. Specifically, we first propose a novel discrete-time model of an AMoD system and we show that this formulation allows the easy integration of a number of real-world constraints, e.g., electric vehicle charging constraints. Second, leveraging our model, we design a model predictive control algorithm for the optimal coordination of an AMoD system and prove its stability in the sense of Lyapunov. At each optimization step, the vehicle scheduling and routing problem is solved as a mixed integer linear program (MILP) where the decision variables are binary variables representing whether a vehicle will 1) wait at a station, 2) service a customer, or 3) rebalance to another station. Finally, by using real-world data, we show that the MPC algorithm can be run in real-time for moderately-sized systems and outperforms previous control strategies for AMoD systems."
      },
      {
        "node_idx": 157174,
        "score_0_10": 9,
        "title": "stackelberg network pricing games",
        "abstract": "We study a multi-player one-round game termed Stackelberg Network Pricing Game, in which a leader can set prices for a subset of $m$ priceable edges in a graph. The other edges have a fixed cost. Based on the leader's decision one or more followers optimize a polynomial-time solvable combinatorial minimization problem and choose a minimum cost solution satisfying their requirements based on the fixed costs and the leader's prices. The leader receives as revenue the total amount of prices paid by the followers for priceable edges in their solutions, and the problem is to find revenue maximizing prices. Our model extends several known pricing problems, including single-minded and unit-demand pricing, as well as Stackelberg pricing for certain follower problems like shortest path or minimum spanning tree. Our first main result is a tight analysis of a single-price algorithm for the single follower game, which provides a $(1+\\epsilon) \\log m$-approximation for any $\\epsilon >0$. This can be extended to provide a $(1+\\epsilon)(\\log k + \\log m)$-approximation for the general problem and $k$ followers. The latter result is essentially best possible, as the problem is shown to be hard to approximate within $\\mathcal{O(\\log^\\epsilon k + \\log^\\epsilon m)$. If followers have demands, the single-price algorithm provides a $(1+\\epsilon)m^2$-approximation, and the problem is hard to approximate within $\\mathcal{O(m^\\epsilon)$ for some $\\epsilon >0$. Our second main result is a polynomial time algorithm for revenue maximization in the special case of Stackelberg bipartite vertex cover, which is based on non-trivial max-flow and LP-duality techniques. Our results can be extended to provide constant-factor approximations for any constant number of followers."
      },
      {
        "node_idx": 58620,
        "score_0_10": 9,
        "title": "capacity per unit energy of fading channels with a peak constraint",
        "abstract": "A discrete-time single-user scalar channel with temporally correlated Rayleigh fading is analyzed. There is no side information at the transmitter or the receiver. A simple expression is given for the capacity per unit energy, in the presence of a peak constraint. The simple formula of Verdu/spl acute/ for capacity per unit cost is adapted to a channel with memory, and is used in the proof. In addition to bounding the capacity of a channel with correlated fading, the result gives some insight into the relationship between the correlation in the fading process and the channel capacity. The results are extended to a channel with side information, showing that the capacity per unit energy is one nat per joule, independently of the peak power constraint. A continuous-time version of the model is also considered. The capacity per unit energy subject to a peak constraint (but no bandwidth constraint) is given by an expression similar to that for discrete time, and is evaluated for Gauss-Markov and Clarke fading channels."
      },
      {
        "node_idx": 28821,
        "score_0_10": 9,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 16157,
        "score_0_10": 9,
        "title": "interdependent scheduling games",
        "abstract": "We propose a model of interdependent scheduling games in which each player controls a set of services that they schedule independently. A player is free to schedule his own services at any time; however, each of these services only begins to accrue reward for the player when all predecessor services, which may or may not be controlled by the same player, have been activated. This model, where players have interdependent services, is motivated by the problems faced in planning and coordinating large-scale infrastructures, e.g., restoring electricity and gas to residents after a natural disaster or providing medical care in a crisis when different agencies are responsible for the delivery of staff, equipment, and medicine. We undertake a game-theoretic analysis of this setting and in particular consider the issues of welfare maximization, computing best responses, Nash dynamics, and existence and computation of Nash equilibria."
      },
      {
        "node_idx": 17404,
        "score_0_10": 9,
        "title": "on the second order statistics of the multihop rayleigh fading channel",
        "abstract": "Second order statistics provides a dynamic representation of a fading channel and plays an important role in the evaluation and design of the wireless communication systems. In this paper, we present a novel analytical framework for the evaluation of important second order statistical parameters, as the level crossing rate (LCR) and the average fade duration (AFD) of the amplify-and-forward multihop Rayleigh fading channel. More specifically, motivated by the fact that this channel is a cascaded one and can be modeled as the product of N fading amplitudes, we derive novel analytical expressions for the average LCR and the AFD of the product of N Rayleigh fading envelopes (or of the recently so-called NRayleigh channel). Furthermore, we derive simple and efficient closed-form approximations to the aforementioned parameters, using the multivariate Laplace approximation theorem. It is shown that our general results reduce to the corresponding ones of the specific dual-hop case, previously published. Numerical and computer simulation examples verify the accuracy of the presented mathematical analysis and show the tightness of the proposed approximations."
      },
      {
        "node_idx": 17426,
        "score_0_10": 9,
        "title": "noncoherent capacity of underspread fading channels",
        "abstract": "We derive bounds on the noncoherent capacity of wide-sense stationary uncorrelated scattering (WSSUS) channels that are selective both in time and frequency, and are underspread, i.e., the product of the channel's delay spread and Doppler spread is small. The underspread assumption is satisfied by virtually all wireless communication channels. For input signals that are peak constrained in time and frequency, we obtain upper and lower bounds on capacity that are explicit in the channel's scattering function, are accurate for a large range of bandwidth, and allow to coarsely identify the capacity-optimal bandwidth as a function of the peak power and the channel's scattering function. We also obtain a closed-form expression for the first-order Taylor series expansion of capacity in the infinite-bandwidth limit, and show that our bounds are tight in the wideband regime. For input signals that are peak constrained in time only (and, hence, allowed to be peaky in frequency), we provide upper and lower bounds on the infinite-bandwidth capacity. Our lower bound is closely related to a result by Viterbi (1967). We find cases where the bounds coincide and, hence, the infinite-bandwidth capacity is characterized exactly. The analysis in this paper is based on a discrete-time discrete-frequency approximation of WSSUS time- and frequency-selective channels. This discretization takes the underspread property of the channel explicitly into account."
      },
      {
        "node_idx": 27779,
        "score_0_10": 9,
        "title": "on outage behavior of wideband slow fading channels",
        "abstract": "This paper investigates point-to-point information transmission over a wideband slow-fading channel, modeled as an (asymptotically) large number of independent identically distributed parallel channels, with the random channel fading realizations remaining constant over the entire coding block. On the one hand, in the wideband limit the minimum achievable energy per nat required for reliable transmission, as a random variable, converges in probability to certain deterministic quantity. On the other hand, the exponential decay rate of the outage probability, termed as the wideband outage exponent, characterizes how the number of parallel channels, {\\it i.e.}, the ``bandwidth'', should asymptotically scale in order to achieve a targeted outage probability at a targeted energy per nat. We examine two scenarios: when the transmitter has no channel state information and adopts uniform transmit power allocation among parallel channels; and when the transmitter is endowed with an one-bit channel state feedback for each parallel channel and accordingly allocates its transmit power. For both scenarios, we evaluate the wideband minimum energy per nat and the wideband outage exponent, and discuss their implication for system performance."
      }
    ]
  },
  "399": {
    "explanation": "efficient low-precision and sparsity-aware neural network training",
    "topk": [
      {
        "node_idx": 131036,
        "score_0_10": 10,
        "title": "deep learning with limited numerical precision",
        "abstract": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding."
      },
      {
        "node_idx": 153811,
        "score_0_10": 10,
        "title": "pruning filters for efficient convnets",
        "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
      },
      {
        "node_idx": 94990,
        "score_0_10": 10,
        "title": "the sl synchronous language revisited",
        "abstract": "We revisit the SL synchronous programming model introduced by Boussinot and De Simone (IEEE, Trans. on Soft. Eng., 1996). We discuss an alternative design of the model including thread spawning and recursive definitions and we explore some basic properties of the revised model: determinism, reactivity, CPS translation to a tail recursive form, computational expressivity, and a compositional notion of program equivalence."
      },
      {
        "node_idx": 138513,
        "score_0_10": 9,
        "title": "quantized neural networks training neural networks with low precision weights and activations",
        "abstract": "We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online."
      },
      {
        "node_idx": 165790,
        "score_0_10": 9,
        "title": "channel pruning for accelerating very deep neural networks",
        "abstract": "In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant. Code has been made publicly available."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 141138,
        "score_0_10": 9,
        "title": "training deep spiking neural networks using backpropagation",
        "abstract": "Deep spiking neural networks (SNNs) hold great potential for improving the latency and energy efficiency of deep neural networks through event-based computation. However, training such networks is difficult due to the non-differentiable nature of asynchronous spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are only considered as noise. This enables an error backpropagation mechanism for deep SNNs, which works directly on spike signals and membrane potentials. Thus, compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statics of spikes more precisely. Our novel framework outperforms all previously reported results for SNNs on the permutation invariant MNIST benchmark, as well as the N-MNIST benchmark recorded with event-based vision sensors."
      },
      {
        "node_idx": 70552,
        "score_0_10": 9,
        "title": "estimating or propagating gradients through stochastic neurons for conditional computation",
        "abstract": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful."
      },
      {
        "node_idx": 17564,
        "score_0_10": 8,
        "title": "dorefa net training low bitwidth convolutional neural networks with low bitwidth gradients",
        "abstract": "We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly."
      },
      {
        "node_idx": 111390,
        "score_0_10": 8,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      }
    ]
  },
  "401": {
    "explanation": "modeling and prediction of gene regulatory networks using neural and evolutionary methods",
    "topk": [
      {
        "node_idx": 96674,
        "score_0_10": 10,
        "title": "recurrent neural network based hybrid model for reconstructing gene regulatory network",
        "abstract": "Display Omitted We proposed a recurrent neural network (RNN) based hybrid model of gene regulatory network (GRN). Due to noise in microarray data, extended Kalman filer has been introduced in the weight update equation of backpropagation through time (BPTT) training algorithm.We tested the proposed model on four different benchmark datasets - two real networks and two simulated network. On all four datasets, the proposed model outperforms the other similar kind of techniques.Also, 5% Gaussian noise has been injected into the dataset to test the performance of the propose model. The result proves that noises in the data have negligible effect on the performance of the model. This model can be much useful for disease response prediction, novel drug development and treatments. One of the exciting problems in systems biology research is to decipher how genome controls the development of complex biological system. The gene regulatory networks (GRNs) help in the identification of regulatory interactions between genes and offer fruitful information related to functional role of individual gene in a cellular system. Discovering GRNs lead to a wide range of applications, including identification of disease related pathways providing novel tentative drug targets, helps to predict disease response, and also assists in diagnosing various diseases including cancer. Reconstruction of GRNs from available biological data is still an open problem. This paper proposes a recurrent neural network (RNN) based model of GRN, hybridized with generalized extended Kalman filter for weight update in backpropagation through time training algorithm. The RNN is a complex neural network that gives a better settlement between biological closeness and mathematical flexibility to model GRN; and is also able to capture complex, non-linear and dynamic relationships among variables. Gene expression data are inherently noisy and Kalman filter performs well for estimation problem even in noisy data. Hence, we applied non-linear version of Kalman filter, known as generalized extended Kalman filter, for weight update during RNN training. The developed model has been tested on four benchmark networks such as DNA SOS repair network, IRMA network, and two synthetic networks from DREAM Challenge. We performed a comparison of our results with other state-of-the-art techniques which shows superiority of our proposed model. Further, 5% Gaussian noise has been induced in the dataset and result of the proposed model shows negligible effect of noise on results, demonstrating the noise tolerance capability of the model."
      },
      {
        "node_idx": 46589,
        "score_0_10": 10,
        "title": "evolutionary algorithms in genetic regulatory networks model",
        "abstract": "Genetic Regulatory Networks (GRNs) plays a vital role in the understanding of complex biological processes. Modeling GRNs is significantly important in order to reveal fundamental cellular processes, examine gene functions and understanding their complex relationships. Understanding the interactions between genes gives rise to develop better method for drug discovery and diagnosis of the disease since many diseases are characterized by abnormal behaviour of the genes. In this paper we have reviewed various evolutionary algorithms-based approach for modeling GRNs and discussed various opportunities and challenges."
      },
      {
        "node_idx": 96615,
        "score_0_10": 10,
        "title": "clustering based predictive process monitoring",
        "abstract": "Business process enactment is generally supported by information systems that record data about process executions, which can be extracted as event logs. Predictive process monitoring is concerned with exploiting such event logs to predict how running (uncompleted) cases will unfold up to their completion. In this paper, we propose a predictive process monitoring framework for estimating the probability that a given predicate will be fulfilled upon completion of a running case. The predicate can be, for example, a temporal logic constraint or a time constraint, or any predicate that can be evaluated over a completed trace. The framework takes into account both the sequence of events observed in the current trace, as well as data attributes associated to these events. The prediction problem is approached in two phases. First, prefixes of previous traces are clustered according to control flow information. Secondly, a classifier is built for each cluster using event data to discriminate between fulfillments and violations. At runtime, a prediction is made on a running case by mapping it to a cluster and applying the corresponding classifier. The framework has been implemented in the ProM toolset and validated on a log pertaining to the treatment of cancer patients in a large hospital."
      },
      {
        "node_idx": 84341,
        "score_0_10": 9,
        "title": "neural message passing for quantum chemistry",
        "abstract": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels."
      },
      {
        "node_idx": 45291,
        "score_0_10": 9,
        "title": "kf metamodel formalization",
        "abstract": "The KF metamodel is a comprehensive unifying metamodel covering the static structural entities and constraints of UML Class Diagrams (v2.4.1), ER, EER, ORM, and ORM2, and intended to boost interoperability of common conceptual data modelling languages. It was originally designed in UML with textual constraints, and in this report we present its formalisations in FOL and OWL, which accompanies the paper that describes, discusses, and analyses the KF metamodel in detail. These new formalizations contribute to give a precise meaning to the metamodel, to understand its complexity properties and to provide a basis for future implementations."
      },
      {
        "node_idx": 136279,
        "score_0_10": 9,
        "title": "analysis of microarray data using artificial intelligence based techniques",
        "abstract": "Microarray is one of the essential technologies used by the biologist to measure genome-wide expression levels of genes in a particular organism under some particular conditions or stimuli. As microarrays technologies have become more prevalent, the challenges of analyzing these data for getting better insight about biological processes have essentially increased. Due to availability of artificial intelligence based sophisticated computational techniques, such as artificial neural networks, fuzzy logic, genetic algorithms, and many other nature-inspired algorithms, it is possible to analyse microarray gene expression data in more better way. Here, we reviewed artificial intelligence based techniques for the analysis of microarray gene expression data. Further, challenges in the field and future work direction have also been suggested."
      },
      {
        "node_idx": 115597,
        "score_0_10": 9,
        "title": "outcome oriented predictive process monitoring review and benchmark",
        "abstract": "Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible outcomes - e.g. Will the customer complain or not? Will an order be delivered, cancelled or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures and baselines to assess their proposals, resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods. To address this gap, this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and a comparative experimental evaluation of eleven representative methods using a benchmark covering twelve predictive process monitoring tasks based on four real-life event logs."
      },
      {
        "node_idx": 91727,
        "score_0_10": 9,
        "title": "preference reasoning in matching procedures application to the admission post baccalaureat platform",
        "abstract": "Because preferences naturally arise and play an important role in many real-life decisions, they are at the backbone of various fields. In particular preferences are increasingly used in almost all matching procedures-based applications. In this work we highlight the benefit of using AI insights on preferences in a large scale application, namely the French Admission Post-Baccalaureat Platform (APB). Each year APB allocates hundreds of thousands first year applicants to universities. This is done automatically by matching applicants preferences to university seats. In practice, APB can be unable to distinguish between applicants which leads to the introduction of random selection. This has created frustration in the French public since randomness, even used as a last mean does not fare well with the republican egalitarian principle. In this work, we provide a solution to this problem. We take advantage of recent AI Preferences Theory results to show how to enhance APB in order to improve expressiveness of applicants preferences and reduce their exposure to random decisions."
      },
      {
        "node_idx": 25310,
        "score_0_10": 9,
        "title": "ant colony optimization for inferring key gene interactions",
        "abstract": "Inferring gene interaction network from gene expression data is an important task in systems biology research. The gene interaction network, especially key interactions, plays an important role in identifying biomarkers for disease that further helps in drug design. Ant colony optimization is an optimization algorithm based on natural evolution and has been used in many optimization problems. In this paper, we applied ant colony optimization algorithm for inferring the key gene interactions from gene expression data. The algorithm has been tested on two different kinds of benchmark datasets and observed that it successfully identify some key gene interactions."
      },
      {
        "node_idx": 103410,
        "score_0_10": 9,
        "title": "column oriented datalog materialization for large knowledge graphs extended technical report",
        "abstract": "The evaluation of Datalog rules over large Knowledge Graphs (KGs) is essential for many applications. In this paper, we present a new method of materializing Datalog inferences, which combines a column-based memory layout with novel optimization methods that avoid redundant inferences at runtime. The pro-active caching of certain subqueries further increases efficiency. Our empirical evaluation shows that this approach can often match or even surpass the performance of state-of-the-art systems, especially under restricted resources."
      }
    ]
  },
  "402": {
    "explanation": "polynomial depth and complexity theory concepts",
    "topk": [
      {
        "node_idx": 139838,
        "score_0_10": 10,
        "title": "on the polynomial depth of various sets of random strings",
        "abstract": "This paper proposes new notions of polynomial depth (called monotone poly depth), based on a polynomial version of monotone Kolmogorov complexity. We show that monotone poly depth satisfies all desirable properties of depth notions i.e., both trivial and random sequences are not monotone poly deep, monotone poly depth satisfies the slow growth law i.e., no simple process can transform a non deep sequence into a deep one, and monotone poly deep sequences exist (unconditionally). We give two natural examples of deep sets, by showing that both the set of Levin-random strings and the set of Kolmogorov random strings are monotone poly deep."
      },
      {
        "node_idx": 87579,
        "score_0_10": 10,
        "title": "polynomial depth highness and lowness for e",
        "abstract": "We study the relations between the notions of highness, lowness and logical depth in the setting of complexity theory. We introduce a new notion of polynomial depth based on time bounded Kolmogorov complexity. We show our polynomial depth notion satisfies all basic logical depth properties, namely neither sets in P nor sets random for EXP are polynomial deep, and only polynomial deep sets can polynomially Turing compute a polynomial deep set. We prove all EXP- complete sets are poly-deep, and under the assumption that NP does not have p-measure zero, then NP contains a polynomial deep set. We show that every high set for E contains a polynomial deep set in its polynomial Turing degree, and that there exists low for E polynomial deep sets."
      },
      {
        "node_idx": 120115,
        "score_0_10": 10,
        "title": "witness delaunay graphs",
        "abstract": "Proximity graphs are used in several areas in which a neighborliness relationship for input data sets is a useful tool in their analysis, and have also received substantial attention from the graph drawing community, as they are a natural way of implicitly representing graphs. However, as a tool for graph representation, proximity graphs have some limitations that may be overcome with suitable generalizations. We introduce a generalization, witness graphs, that encompasses both the goal of more power and flexibility for graph drawing issues and a wider spectrum for neighborhood analysis. We study in detail two concrete examples, both related to Delaunay graphs, and consider as well some problems on stabbing geometric objects and point set discrimination, that can be naturally described in terms of witness graphs."
      },
      {
        "node_idx": 44824,
        "score_0_10": 10,
        "title": "equational axiomatization of algebras with structure",
        "abstract": "This paper proposes a new category theoretic account of equationally axiomatizable classes of algebras. Our approach is well-suited for the treatment of algebras equipped with additional computationally relevant structure, such as ordered algebras, continuous algebras, quantitative algebras, nominal algebras, or profinite algebras. Our main contributions are a generic HSP theorem and a sound and complete equational logic, which are shown to encompass numerous flavors of equational axiomizations studied in the literature."
      },
      {
        "node_idx": 124031,
        "score_0_10": 10,
        "title": "borel hierarchy and omega context free languages",
        "abstract": "We give in this paper additional answers to questions of Lescow and Thomas [Logical Specifications of Infinite Computations, In:\"A Decade of Concurrency\", Springer LNCS 803 (1994), 583-621], proving new topological properties of omega context free languages : there exist some omega-CFL which are non Borel sets. And one cannot decide whether an omega-CFL is a Borel set. We give also an answer to questions of Niwinski and Simonnet about omega powers of finitary languages, giving an example of a finitary context free language L such that L^omega is not a Borel set. Then we prove some recursive analogues to preceding properties: in particular one cannot decide whether an omega-CFL is an arithmetical set."
      },
      {
        "node_idx": 7286,
        "score_0_10": 10,
        "title": "2d lyndon words and applications",
        "abstract": "A Lyndon word is a primitive string which is lexicographically smallest among cyclic permutations of its characters. Lyndon words are used for constructing bases in free Lie algebras, constructing de Bruijn sequences, finding the lexicographically smallest or largest substring in a string, and succinct suffix-prefix matching of highly periodic strings. In this paper, we extend the concept of the Lyndon word to two dimensions. We introduce the 2D Lyndon word and use it to capture 2D horizontal periodicity of a matrix in which each row is highly periodic, and to efficiently solve 2D horizontal suffix-prefix matching among a set of patterns. This yields a succinct and efficient algorithm for 2D dictionary matching. #R##N#We present several algorithms that compute the 2D Lyndon word that represents a matrix. The final algorithm achieves linear time complexity even when the least common multiple of the periods of the rows is exponential in the matrix width."
      },
      {
        "node_idx": 69221,
        "score_0_10": 10,
        "title": "constacyclic codes over finite principal ideal rings",
        "abstract": "In this paper, we give an important isomorphism between contacyclic codes and cyclic codes over finite principal ideal rings. Necessary and sufficient conditions for the existence of non-trivial cyclic self-dual codes over finite principal ideal rings are given."
      },
      {
        "node_idx": 25393,
        "score_0_10": 10,
        "title": "one eilenberg theorem to rule them all",
        "abstract": "Eilenberg-type correspondences, relating varieties of languages (e.g. of finite words, infinite words, or trees) to pseudovarieties of finite algebras, form the backbone of algebraic language theory. Numerous such correspondences are known in the literature. We demonstrate that they all arise from the same recipe: one models languages and the algebras recognizing them by monads on an algebraic category, and applies a Stone-type duality. Our main contribution is a generic variety theorem that covers e.g. Wilke's and Pin's work on $\\infty$-languages, the variety theorem for cost functions of Daviaud, Kuperberg, and Pin, and unifies the two previous categorical approaches of Boja\\'nczyk and of Ad\\'amek et al. In addition it gives a number of new results, such as an extension of the local variety theorem of Gehrke, Grigorieff, and Pin from finite to infinite words."
      },
      {
        "node_idx": 110075,
        "score_0_10": 9,
        "title": "bracketing numbers of convex functions on polytopes",
        "abstract": "We study bracketing numbers for spaces of bounded convex functions in the $L_p$ norms. We impose no Lipschitz constraint. Previous results gave bounds when the domain of the functions is a hyperrectangle. We extend these results to the case wherein the domain is a polytope. Bracketing numbers are crucial quantities for understanding asymptotic behavior for many statistical nonparametric estimators. Our results are of interest in particular in many multidimensional estimation problems based on convexity shape constraints."
      },
      {
        "node_idx": 36758,
        "score_0_10": 9,
        "title": "algebraic recognizability of languages",
        "abstract": "Recognizable languages of finite words are part of every computer science cursus, and they are routinely described as a cornerstone for applications and for theory. We would like to briefly explore why that is, and how this word-related notion extends to more complex models, such as those developed for modeling distributed or timed behaviors."
      }
    ]
  },
  "403": {
    "explanation": "mathematical and computational foundations of advanced type and logic theories",
    "topk": [
      {
        "node_idx": 28289,
        "score_0_10": 10,
        "title": "cellular cohomology in homotopy type theory",
        "abstract": "We present a development of cellular cohomology in homotopy type theory. Cohomology associates to each space a sequence of abelian groups capturing part of its structure, and has the advantage over homotopy groups in that these abelian groups of many common spaces are easier to compute in many cases. Cellular cohomology is a special kind of cohomology designed for cell complexes: these are built in stages by attaching spheres of progressively higher dimension, and cellular cohomology defines the groups out of the combinatorial description of how spheres are attached. Our main result is that for finite cell complexes, a wide class of cohomology theories (including the ones defined through Eilenberg-MacLane spaces) can be calculated via cellular cohomology. This result was formalized in the Agda proof assistant."
      },
      {
        "node_idx": 44110,
        "score_0_10": 10,
        "title": "thermodynamic graph rewriting",
        "abstract": "We develop a new thermodynamic approach to stochastic graph-rewriting. The ingredients are a finite set of reversible graph-rewriting rules called generating rules, a finite set of connected graphs P called energy patterns and an energy cost function. The idea is that the generators define the qualitative dynamics, by showing which transformations are possible, while the energy patterns and cost function specify the long-term probability $\\pi$ of any reachable graph. Given the generators and energy patterns, we construct a finite set of rules which (i) has the same qualitative transition system as the generators; and (ii) when equipped with suitable rates, defines a continuous-time Markov chain of which $\\pi$ is the unique fixed point. The construction relies on the use of site graphs and a technique of `growth policy' for quantitative rule refinement which is of independent interest. This division of labour between the qualitative and long-term quantitative aspects of the dynamics leads to intuitive and concise descriptions for realistic models (see the examples in S4 and S5). It also guarantees thermodynamical consistency (AKA detailed balance), otherwise known to be undecidable, which is important for some applications. Finally, it leads to parsimonious parameterizations of models, again an important point in some applications."
      },
      {
        "node_idx": 112891,
        "score_0_10": 10,
        "title": "mathematical foundations of matrix syntax",
        "abstract": "Matrix syntax is a formal model of syntactic relations in language. The purpose of this paper is to explain its mathematical foundations, for an audience with some formal background. We make an axiomatic presentation, motivating each axiom on linguistic and practical grounds. The resulting mathematical structure resembles some aspects of quantum mechanics. Matrix syntax allows us to describe a number of language phenomena that are otherwise very difficult to explain, such as linguistic chains, and is arguably a more economical theory of language than most of the theories proposed in the context of the minimalist program in linguistics. In particular, sentences are naturally modelled as vectors in a Hilbert space with a tensor product structure, built from 2x2 matrices belonging to some specific group."
      },
      {
        "node_idx": 156965,
        "score_0_10": 10,
        "title": "axioms for modelling cubical type theory in a topos",
        "abstract": "The homotopical approach to intensional type theory views proofs of equality as paths. We explore what is required of an object $I$ in a topos to give such a path-based model of type theory in which paths are just functions with domain $I$. Cohen, Coquand, Huber and M\\\"ortberg give such a model using a particular category of presheaves. We investigate the extent to which their model construction can be expressed in the internal type theory of any topos and identify a collection of quite weak axioms for this purpose. This clarifies the definition and properties of the notion of uniform Kan filling that lies at the heart of their constructive interpretation of Voevodsky's univalence axiom. (This paper is a revised and expanded version of a paper of the same name that appeared in the proceedings of the 25th EACSL Annual Conference on Computer Science Logic, CSL 2016.)"
      },
      {
        "node_idx": 160960,
        "score_0_10": 10,
        "title": "on the complexity of elementary modal logics",
        "abstract": "Modal logics are widely used in computer science. The complexity of modal satisfiability problems has been investigated since the 1970s, usually proving results on a case-by-case basis. We prove a very general classification for a wide class of relevant logics: Many important subclasses of modal logics can be obtained by restricting the allowed models with first-order Horn formulas. We show that the satisfiability problem for each of these logics is either NP-complete or PSPACE-hard, and exhibit a simple classification criterion. Further, we prove matching PSPACE upper bounds for many of the PSPACE-hard logics."
      },
      {
        "node_idx": 163099,
        "score_0_10": 9,
        "title": "game semantics for martin lof type theory",
        "abstract": "We present a new game semantics for Martin-L\\\"of type theory (MLTT), our aim is to give a mathematical and intensional explanation of MLTT. Specifically, we propose a category with families of a novel variant of games, which induces a surjective and injective (when Id-types are excluded) interpretation of the intensional variant of MLTT equipped with unit-, empty-, N-, dependent product, dependent sum and Id-types as well as the cumulative hierarchy of universes for the first time in the literature (as far as we are aware), though the surjectivity is accomplished merely by an inductive definition of a certain class of games and strategies. Our games generalize the existing notion of games, and achieve an interpretation of dependent types and the hierarchy of universes in an intuitive yet mathematically precise manner, our strategies can be seen as algorithms underlying programs (or proofs) in MLTT. A more fine-grained interpretation of Id-types is left as future work."
      },
      {
        "node_idx": 117903,
        "score_0_10": 9,
        "title": "on irrelevance and algorithmic equality in predicative type theory",
        "abstract": "Dependently typed programs contain an excessive amount of static terms which#N#are necessary to please the type checker but irrelevant for computation. To#N#separate static and dynamic code, several static analyses and type systems have#N#been put forward. We consider Pfenning's type theory with irrelevant#N#quantification which is compatible with a type-based notion of equality that#N#respects eta-laws. We extend Pfenning's theory to universes and large#N#eliminations and develop its meta-theory. Subject reduction, normalization and#N#consistency are obtained by a Kripke model over the typed equality judgement.#N#Finally, a type-directed equality algorithm is described whose completeness is#N#proven by a second Kripke model."
      },
      {
        "node_idx": 162915,
        "score_0_10": 9,
        "title": "equilibrium and termination",
        "abstract": "We present a reduction of the termination problem for a Turing machine (in the simplified form of the Post correspondence problem) to the problem of determining whether a continuous-time Markov chain presented as a set of Kappa graph-rewriting rules has an equilibrium. It follows that the problem of whether a computable CTMC is dissipative (ie does not have an equilibrium) is undecidable."
      },
      {
        "node_idx": 40574,
        "score_0_10": 9,
        "title": "every computably enumerable random real is provably computably enumerable random",
        "abstract": "We prove that every computably enumerable (c.e.) random real is provable in Peano Arithmetic (PA) to be c.e. random. A major step in the proof is to show that the theorem stating that \"a real is c.e. and random iff it is the halting probability of a universal prefix-free Turing machine\" can be proven in PA. Our proof, which is simpler than the standard one, can also be used for the original theorem. #R##N#Our positive result can be contrasted with the case of computable functions, where not every computable function is provably computable in PA, or even more interestingly, with the fact that almost all random finite strings are not provably random in PA. #R##N#We also prove two negative results: a) there exists a universal machine whose universality cannot be proved in PA, b) there exists a universal machine $U$ such that, based on $U$, PA cannot prove the randomness of its halting probability. #R##N#The paper also includes a sharper form of the Kraft-Chaitin Theorem, as well as a formal proof of this theorem written with the proof assistant Isabelle."
      },
      {
        "node_idx": 106892,
        "score_0_10": 9,
        "title": "the transferable belief model and other interpretations of dempster shafer s model",
        "abstract": "Dempster-Shafer's model aims at quantifying degrees of belief But there are so many interpretations of Dempster-Shafer's theory in the literature that it seems useful to present the various contenders in order to clarify their respective positions. We shall successively consider the classical probability model, the upper and lower probabilities model, Dempster's model, the transferable belief model, the evidentiary value model, the provability or necessity model. None of these models has received the qualification of Dempster-Shafer. In fact the transferable belief model is our interpretation not of Dempster's work but of Shafer's work as presented in his book (Shafer 1976, Smets 1988). It is a ?purified' form of Dempster-Shafer's model in which any connection with probability concept has been deleted. Any model for belief has at least two components: one static that describes our state of belief, the other dynamic that explains how to update our belief given new pieces of information. We insist on the fact that both components must be considered in order to study these models. Too many authors restrict themselves to the static component and conclude that Dempster-Shafer theory is the same as some other theory. But once the dynamic component is considered, these conclusions break down. Any comparison based only on the static component is too restricted. The dynamic component must also be considered as the originality of the models based on belief functions lies in its dynamic component."
      }
    ]
  },
  "404": {
    "explanation": "optimization algorithms and meta-learning for efficient model training",
    "topk": [
      {
        "node_idx": 165679,
        "score_0_10": 10,
        "title": "an overview of gradient descent optimization algorithms",
        "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent."
      },
      {
        "node_idx": 48820,
        "score_0_10": 10,
        "title": "julia a fresh approach to numerical computing",
        "abstract": "Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast. Julia questions notions generally held as \"laws of nature\" by practitioners of numerical computing: #R##N#1. High-level dynamic programs have to be slow. #R##N#2. One must prototype in one language and then rewrite in another language for speed or deployment, and #R##N#3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. #R##N#We introduce the Julia programming language and its design --- a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. #R##N#Julia shows that one can have machine performance without sacrificing human convenience."
      },
      {
        "node_idx": 115288,
        "score_0_10": 10,
        "title": "benchmarking data warehouses",
        "abstract": "Database benchmarks can either help users in comparing the performances of different systems, or help engineers in testing the effect of various design choices. In the field of data warehouses, the Transaction Processing Performance Council's standard benchmarks address the first point, but they are not tunable enough to address the second one. We present in this paper the Data Warehouse Engineering Benchmark (DWEB), which allows generating various ad hoc synthetic data warehouses and workloads. We detail DWEB's full specifications, as well as the experiments we performed to illustrate how it may be used. DWEB is a Java free software."
      },
      {
        "node_idx": 27774,
        "score_0_10": 10,
        "title": "high dimensional continuous control using generalized advantage estimation",
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. #R##N#Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
      },
      {
        "node_idx": 28271,
        "score_0_10": 10,
        "title": "resolving zero divisors using hensel lifting",
        "abstract": "Algorithms which compute modulo triangular sets must respect the presence of zero-divisors. We present Hensel lifting as a tool for dealing with them. We give an application: a modular algorithm for computing GCDs of univariate polynomials with coefficients modulo a radical triangular set over the rationals. Our modular algorithm naturally generalizes previous work from algebraic number theory. We have implemented our algorithm using Maple's RECDEN package. We compare our implementation with the procedure RegularGcd in the RegularChains package."
      },
      {
        "node_idx": 155778,
        "score_0_10": 10,
        "title": "model agnostic meta learning for fast adaptation of deep networks",
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
      },
      {
        "node_idx": 54025,
        "score_0_10": 9,
        "title": "algorithms for visualizing phylogenetic networks",
        "abstract": "We study the problem of visualizing phylogenetic networks, which are extensions of the Tree of Life in biology. We use a space filling visualization method, called DAGmaps, in order to obtain clear visualizations using limited space. In this paper, we restrict our attention to galled trees and galled networks and present linear time algorithms for visualizing them as DAGmaps."
      },
      {
        "node_idx": 52179,
        "score_0_10": 9,
        "title": "pricing social goods",
        "abstract": "Social goods are goods that grant value not only to their owners but also to the owners' surroundings, be it their families, friends or office mates. The benefit a non-owner derives from the good is affected by many factors, including the type of the good, its availability, and the social status of the non-owner. Depending on the magnitude of the benefit and on the price of the good, a potential buyer might stay away from purchasing the good, hoping to free ride on others' purchases. A revenue-maximizing seller who sells social goods must take these considerations into account when setting prices for the good. The literature on optimal pricing has advanced considerably over the last decade, but little is known about optimal pricing schemes for selling social goods. In this paper, we conduct a systematic study of revenue-maximizing pricing schemes for social goods: we introduce a Bayesian model for this scenario, and devise nearly-optimal pricing schemes for various types of externalities, both for simultaneous sales and for sequential sales."
      },
      {
        "node_idx": 106315,
        "score_0_10": 9,
        "title": "soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor",
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
      },
      {
        "node_idx": 107304,
        "score_0_10": 9,
        "title": "d4m 2 0 schema a general purpose high performance schema for the accumulo database",
        "abstract": "Non-traditional, relaxed consistency, triple store databases are the backbone of many web companies (e.g., Google Big Table, Amazon Dynamo, and Facebook Cassandra). The Apache Accumulo database is a high performance open source relaxed consistency database that is widely used for government applications. Obtaining the full benefits of Accumulo requires using novel schemas. The Dynamic Distributed Dimensional Data Model (D4M)[http://www.mit.edu/~kepner/D4M] provides a uniform mathematical framework based on associative arrays that encompasses both traditional (i.e., SQL) and non-traditional databases. For non-traditional databases D4M naturally leads to a general purpose schema that can be used to fully index and rapidly query every unique string in a dataset. The D4M 2.0 Schema has been applied with little or no customization to cyber, bioinformatics, scientific citation, free text, and social media data. The D4M 2.0 Schema is simple, requires minimal parsing, and achieves the highest published Accumulo ingest rates. The benefits of the D4M 2.0 Schema are independent of the D4M interface. Any interface to Accumulo can achieve these benefits by using the D4M 2.0 Schema."
      }
    ]
  },
  "407": {
    "explanation": "neural network architectures and methods for text and image processing tasks",
    "topk": [
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 11752,
        "score_0_10": 9,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 63929,
        "score_0_10": 9,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 112674,
        "score_0_10": 9,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 109276,
        "score_0_10": 9,
        "title": "the author topic model for authors and documents",
        "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output."
      },
      {
        "node_idx": 26180,
        "score_0_10": 9,
        "title": "introduction to the conll 2003 shared task language independent named entity recognition",
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 74534,
        "score_0_10": 9,
        "title": "unsupervised deep embedding for clustering analysis",
        "abstract": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods."
      }
    ]
  },
  "408": {
    "explanation": "image super-resolution with deep convolutional networks and efficient upsampling",
    "topk": [
      {
        "node_idx": 117249,
        "score_0_10": 10,
        "title": "deeply recursive convolutional network for image super resolution",
        "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin."
      },
      {
        "node_idx": 55272,
        "score_0_10": 10,
        "title": "image super resolution using deep convolutional networks",
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."
      },
      {
        "node_idx": 77683,
        "score_0_10": 9,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      },
      {
        "node_idx": 105042,
        "score_0_10": 9,
        "title": "convexity shape prior for level set based image segmentation method",
        "abstract": "We propose a geometric convexity shape prior preservation method for variational level set based image segmentation methods. Our method is built upon the fact that the level set of a convex signed distanced function must be convex. This property enables us to transfer a complicated geometrical convexity prior into a simple inequality constraint on the function. An active set based Gauss-Seidel iteration is used to handle this constrained minimization problem to get an efficient algorithm. We apply our method to region and edge based level set segmentation models including Chan-Vese (CV) model with guarantee that the segmented region will be convex. Experimental results show the effectiveness and quality of the proposed model and algorithm."
      },
      {
        "node_idx": 32692,
        "score_0_10": 9,
        "title": "deep laplacian pyramid networks for fast and accurate super resolution",
        "abstract": "Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy."
      },
      {
        "node_idx": 146640,
        "score_0_10": 9,
        "title": "the core of games on ordered structures and graphs",
        "abstract": "In cooperative games, the core is the most popular solution concept, and its properties are well known. In the classical setting of cooperative games, it is generally assumed that all coalitions can form, i.e., they are all feasible. In many situations, this assumption is too strong and one has to deal with some unfeasible coalitions. Defining a game on a subcollection of the power set of the set of players has many implications on the mathematical structure of the core, depending on the precise structure of the subcollection of feasible coalitions. Many authors have contributed to this topic, and we give a unified view of these different results."
      },
      {
        "node_idx": 131915,
        "score_0_10": 9,
        "title": "golden angle modulation",
        "abstract": "Quadrature amplitude modulation (QAM) exhibits a shaping-loss of $\\pi \\mathrm{e}/6$, ($\\approx1.53$ dB) compared to the AWGN Shannon capacity. With inspiration gained from special (leaf, flower petal, and seed) packing arrangements (spiral phyllotaxis) found among plants, a novel, shape-versatile, circular symmetric, modulation scheme, the Golden Angle Modulation (GAM) is introduced. Disc-shaped, and complex Gaussian approximating bell-shaped, GAM-signal constellations are considered. For bell-GAM, a high-rate approximation, and a mutual information optimization formulation, are developed. Bell-GAM overcomes the asymptotic shaping-loss seen in QAM, and offers Shannon capacity approaching performance. Transmitter resource limited links, such as space probe-to-earth, and mobile-to-basestation, are cases where GAM could be particularly valuable."
      },
      {
        "node_idx": 102778,
        "score_0_10": 9,
        "title": "learning deep cnn denoiser prior for image restoration",
        "abstract": "Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications."
      },
      {
        "node_idx": 143105,
        "score_0_10": 9,
        "title": "image deblurring and super resolution by adaptive sparse domain selection and adaptive regularization",
        "abstract": "As a powerful statistical image modeling technique, sparse representation has been successfully used in various image restoration applications. The success of sparse representation owes to the development of the l1-norm optimization techniques and the fact that natural images are intrinsically sparse in some domains. The image restoration quality largely depends on whether the employed sparse domain can represent well the underlying image. Considering that the contents can vary significantly across different images or different patches in a single image, we propose to learn various sets of bases from a precollected dataset of example image patches, and then, for a given patch to be processed, one set of bases are adaptively selected to characterize the local sparse domain. We further introduce two adaptive regularization terms into the sparse representation framework. First, a set of autoregressive (AR) models are learned from the dataset of example image patches. The best fitted AR models to a given patch are adaptively selected to regularize the image local structures. Second, the image nonlocal self-similarity is introduced as another regularization term. In addition, the sparsity regularization parameter is adaptively estimated for better image restoration performance. Extensive experiments on image deblurring and super-resolution validate that by using adaptive sparse domain selection and adaptive regularization, the proposed method achieves much better results than many state-of-the-art algorithms in terms of both PSNR and visual perception."
      },
      {
        "node_idx": 21251,
        "score_0_10": 9,
        "title": "relaxed disk packing",
        "abstract": "Motivated by biological questions, we study configurations of equal-sized disks in the Euclidean plane that neither pack nor cover. Measuring the quality by the probability that a random point lies in exactly one disk, we show that the regular hexagonal grid gives the maximum among lattice configurations."
      }
    ]
  },
  "409": {
    "explanation": "image super-resolution with efficient convolutional neural networks",
    "topk": [
      {
        "node_idx": 55272,
        "score_0_10": 10,
        "title": "image super resolution using deep convolutional networks",
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."
      },
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 30682,
        "score_0_10": 9,
        "title": "universum learning for svm regression",
        "abstract": "This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach."
      },
      {
        "node_idx": 104202,
        "score_0_10": 9,
        "title": "lenses and learners",
        "abstract": "Lenses are a well-established structure for modelling bidirectional transformations, such as the interactions between a database and a view of it. Lenses may be symmetric or asymmetric, and may be composed, forming the morphisms of a monoidal category. More recently, the notion of a learner has been proposed: these provide a compositional way of modelling supervised learning algorithms, and again form the morphisms of a monoidal category. In this paper, we show that the two concepts are tightly linked. We show both that there is a faithful, identity-on-objects symmetric monoidal functor embedding a category of asymmetric lenses into the category of learners, and furthermore there is such a functor embedding the category of learners into a category of symmetric lenses."
      },
      {
        "node_idx": 84091,
        "score_0_10": 9,
        "title": "a noise aware enhancement method for underexposed images",
        "abstract": "A novel method of contrast enhancement is proposed for underexposed images, in which heavy noise is hidden. Under low light conditions, images taken by digital cameras have low contrast in dark or bright regions. This is due to a limited dynamic range that imaging sensors have. For these reasons, various contrast enhancement methods have been proposed so far. These methods, however, have two problems: (1) The loss of details in bright regions due to over-enhancement of contrast. (2) The noise is amplified in dark regions because conventional enhancement methods do not consider noise included in images. The proposed method aims to overcome these problems. In the proposed method, a shadow-up function is applied to adaptive gamma correction with weighting distribution, and a denoising filter is also used to avoid noise being amplified in dark regions. As a result, the proposed method allows us not only to enhance contrast of dark regions, but also to avoid amplifying noise, even under strong noise environments."
      },
      {
        "node_idx": 77683,
        "score_0_10": 8,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      },
      {
        "node_idx": 17274,
        "score_0_10": 8,
        "title": "classification of gradient change features using mlp for handwritten character recognition",
        "abstract": "A novel, generic scheme for off-line handwritten English alphabets character images is proposed. The advantage of the technique is that it can be applied in a generic manner to different applications and is expected to perform better in uncertain and noisy environments. The recognition scheme is using a multilayer perceptron(MLP) neural networks. The system was trained and tested on a database of 300 samples of handwritten characters. For improved generalization and to avoid overtraining, the whole available dataset has been divided into two subsets: training set and test set. We achieved 99.10% and 94.15% correct recognition rates on training and test sets respectively. The purposed scheme is robust with respect to various writing styles and size as well as presence of considerable noise."
      },
      {
        "node_idx": 112726,
        "score_0_10": 8,
        "title": "context encoders feature learning by inpainting",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
      },
      {
        "node_idx": 97937,
        "score_0_10": 8,
        "title": "non binary local gradient contours for face recognition",
        "abstract": "As the features from the traditional Local Binary Patterns (LBP) and Local Directional Patterns (LDP) are found to be ineffective for face recognition, we have proposed a new approach derived on the basis of Information sets whereby the loss of information that occurs during the binarization is eliminated. The information sets expand the scope of fuzzy sets by connecting the attribute and the corresponding membership function value as a product. Since face is having smooth texture in a limited area, the extracted features must be highly discernible. To limit the number of features, we consider only the non overlapping windows. By the application of the information set theory we can reduce the number of feature of an image. The derived features are shown to work fairly well over eigenface, fisherface and LBP methods."
      },
      {
        "node_idx": 32771,
        "score_0_10": 8,
        "title": "keep it smpl automatic estimation of 3d human pose and shape from a single image",
        "abstract": "We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art."
      }
    ]
  },
  "410": {
    "explanation": "integrated convolutional network object recognition and localization",
    "topk": [
      {
        "node_idx": 141525,
        "score_0_10": 10,
        "title": "selecting and designing grippers for an assembly task in a structured approach",
        "abstract": "In this paper, we present a structured approach of selecting and designing a set of grippers for an assembly task. Compared to current experience-based gripper design method, our approach accelerates the design process by automatically generating a set of initial design options on gripper type and parameters according to the CAD models of assembly components. We use mesh segmentation techniques to segment the assembly components and fit the segmented parts with shape primitives, according to the predefined correspondence between primitive shape and gripper type, suitable gripper types and parameters can be selected and extracted from the fitted shape primitives. Then considering the assembly constraints, applicable gripper types and parameters can be filtered from the initial options. Among the applicable gripper configurations, we further minimize the required number of grippers for performing the assembly task, by exploring the gripper that is able to handle multiple assembly components during the assembly. Finally, the feasibility of the designed grippers are experimentally verified by assembling a part of an industrial product."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 162377,
        "score_0_10": 9,
        "title": "r fcn object detection via region based fully convolutional networks",
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: this https URL"
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 122957,
        "score_0_10": 9,
        "title": "open tee an open virtual trusted execution environment",
        "abstract": "Hardware-based Trusted Execution Environments (TEEs) are widely deployed in mobile devices. Yet their use has been limited primarily to applications developed by the device vendors. Recent standardization of TEE interfaces by GlobalPlatform (GP) promises to partially address this problem by enabling GP-compliant trusted applications to run on TEEs from different vendors. Nevertheless ordinary developers wishing to develop trusted applications face significant challenges. Access to hardware TEE interfaces are difficult to obtain without support from vendors. Tools and software needed to develop and debug trusted applications may be expensive or non-existent. In this paper, we describe Open-TEE, a virtual, hardware-independent TEE implemented in software. Open-TEE conforms to GP specifications. It allows developers to develop and debug trusted applications with the same tools they use for developing software in general. Once a trusted application is fully debugged, it can be compiled for any actual hardware TEE. Through performance measurements and a user study we demonstrate that Open-TEE is efficient and easy to use. We have made Open-TEE freely available as open source."
      },
      {
        "node_idx": 105089,
        "score_0_10": 8,
        "title": "making bubbling practical",
        "abstract": "Bubbling is a run-time graph transformation studied for the execution of non-deterministic steps in functional logic computations. This transformation has been proven correct, but as currently formulated it requires information about the entire context of a step, even when the step affects only a handful of nodes. Therefore, despite some advantages, it does not appear to be competitive with approaches that require only localized information, such as backtracking and pull-tabbing. We propose a novel algorithm that executes bubbling steps accessing only local information. To this aim, we define graphs that have an additional attribute, a dominator of each node, and we maintain this attribute when a rewrite and/or bubbling step is executed. When a bubbling step is executed, the dominator is available at no cost, and only local information is accessed. Our work makes bubbling practical, and theoretically competitive, for implementing non-determinism in functional logic computations."
      },
      {
        "node_idx": 123722,
        "score_0_10": 8,
        "title": "highway to hal open sourcing the first extendable gate level netlist reverse engineering framework",
        "abstract": "Since hardware oftentimes serves as the root of trust in our modern interconnected world, malicious hardware manipulations constitute a ubiquitous threat in the context of the Internet of Things (IoT). Hardware reverse engineering is a prevalent technique to detect such manipulations.   Over the last years, an active research community has significantly advanced the field of hardware reverse engineering. Notably, many open research questions regarding the extraction of functionally correct netlists from Field Programmable Gate Arrays (FPGAs) or Application Specific Integrated Circuits (ASICs) have been tackled. In order to facilitate further analysis of recovered netlists, a software framework is required, serving as the foundation for specialized algorithms. Currently, no such framework is publicly available.   Therefore, we provide the first open-source gate-library agnostic framework for gate-level netlist analysis. In this positional paper, we demonstrate the workflow of our modular framework HAL on the basis of two case studies and provide profound insights on its technical foundations."
      },
      {
        "node_idx": 161808,
        "score_0_10": 8,
        "title": "a new functional logic compiler for curry sprite",
        "abstract": "We introduce a new native code compiler for Curry codenamed Sprite. Sprite is based on the Fair Scheme, a compilation strategy that provides instructions for transforming declarative, non-deterministic programs of a certain class into imperative, deterministic code. We outline salient features of Sprite, discuss its implementation of Curry programs, and present benchmarking results. Sprite is the first-to-date operationally complete implementation of Curry. Preliminary results show that ensuring this property does not incur a significant penalty."
      },
      {
        "node_idx": 22502,
        "score_0_10": 8,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 95713,
        "score_0_10": 8,
        "title": "a look at the dark side of hardware reverse engineering a case study",
        "abstract": "A massive threat to the modern and complex IC production chain is the use of untrusted off-shore foundries which are able to infringe valuable hardware design IP or to inject hardware Trojans causing severe loss of safety and security. Similarly, market dominating SRAM-based FPGAs are vulnerable to both attacks since the crucial gate-level netlist can be retrieved even in field for the majority of deployed device series. In order to perform IP infringement or Trojan injection, reverse engineering (parts of) the hardware design is necessary to understand its internal workings. Even though IP protection and obfuscation techniques exist to hinder both attacks, the security of most techniques is doubtful since realistic capabilities of reverse engineering are often neglected. The contribution of our work is twofold: first, we carefully review an IP watermarking scheme tailored to FPGAs and improve its security by using opaque predicates. In addition, we show novel reverse engineering strategies on proposed opaque predicate implementations that again enables to automatically detect and alter watermarks. Second, we demonstrate automatic injection of hardware Trojans specifically tailored for third-party cryptographic IP gate-level netlists. More precisely, we extend our understanding of adversary's capabilities by presenting how block and stream cipher implementations can be surreptitiously weakened."
      }
    ]
  },
  "412": {
    "explanation": "combining word- and character-level features for sequence labeling tasks",
    "topk": [
      {
        "node_idx": 63929,
        "score_0_10": 10,
        "title": "end to end sequence labeling via bi directional lstm cnns crf",
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
      },
      {
        "node_idx": 112674,
        "score_0_10": 10,
        "title": "named entity recognition with bidirectional lstm cnns",
        "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
      },
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 35496,
        "score_0_10": 9,
        "title": "aggregated residual transformations for deep neural networks",
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      }
    ]
  },
  "414": {
    "explanation": "deep learning architectures and training optimizations",
    "topk": [
      {
        "node_idx": 143330,
        "score_0_10": 10,
        "title": "deepest neural networks",
        "abstract": "This paper shows that a long chain of perceptrons (that is, a multilayer perceptron, or MLP, with many hidden layers of width one) can be a universal classifier. The classification procedure is not necessarily computationally efficient, but the technique throws some light on the kind of computations possible with narrow and deep MLPs."
      },
      {
        "node_idx": 111390,
        "score_0_10": 10,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 68710,
        "score_0_10": 8,
        "title": "the largest cognitive systems will be optoelectronic",
        "abstract": "Electrons and photons offer complementary strengths for information processing. Photons are excellent for communication, while electrons are superior for computation and memory. Cognition requires distributed computation to be communicated across the system for information integration. We present reasoning from neuroscience, network theory, and device physics supporting the conjecture that large-scale cognitive systems will benefit from electronic devices performing synaptic, dendritic, and neuronal information processing operating in conjunction with photonic communication. On the chip scale, integrated dielectric waveguides enable fan-out to thousands of connections. On the system scale, fiber and free-space optics can be employed. The largest cognitive systems will be limited by the distance light can travel during the period of a network oscillation. We calculate that optoelectronic networks the area of a large data center ($10^5$\\,m$^2$) will be capable of system-wide information integration at $1$\\,MHz. At frequencies of cortex-wide integration in the human brain ($4$\\,Hz, theta band), optoelectronic systems could integrate information across the surface of the earth."
      },
      {
        "node_idx": 45355,
        "score_0_10": 8,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 73035,
        "score_0_10": 8,
        "title": "network in network",
        "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
      },
      {
        "node_idx": 37625,
        "score_0_10": 8,
        "title": "persistent memory transactions",
        "abstract": "This paper presents a comprehensive analysis of performance trade offs between implementation choices for transaction runtime systems on persistent memory. We compare three implementations of transaction runtimes: undo logging, redo logging, and copy-on-write. We also present a memory allocator that plugs into these runtimes. Our microbenchmark based evaluation focuses on understanding the interplay between various factors that contribute to performance differences between the three runtimes -- read/write access patterns of workloads, size of the persistence domain (portion of the memory hierarchy where the data is effectively persistent), cache locality, and transaction runtime bookkeeping overheads. No single runtime emerges as a clear winner. We confirm our analysis in more realistic settings of three \"real world\" applications we developed with our transactional API: (i) a key-value store we implemented from scratch, (ii) a SQLite port, and (iii) a persistified version of memcached, a popular key-value store. These findings are not only consistent with our microbenchmark analysis, but also provide additional interesting insights into other factors (e.g. effects of multithreading and synchronization) that affect application performance."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 119322,
        "score_0_10": 8,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      }
    ]
  },
  "415": {
    "explanation": "event attendance optimization under NP-hard constraints",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 42046,
        "score_0_10": 10,
        "title": "advanced automata minimization",
        "abstract": "We present an efficient algorithm to reduce the size of nondeterministic Buchi word automata, while retaining their language. Additionally, we describe methods to solve PSPACE-complete automata problems like universality, equivalence and inclusion for much larger instances (1-3 orders of magnitude) than before. This can be used to scale up applications of automata in formal verification tools and decision procedures for logical theories. The algorithm is based on new transition pruning techniques. These use criteria based on combinations of backward and forward trace inclusions. Since these relations are themselves PSPACE-complete, we describe methods to compute good approximations of them in polynomial time. Extensive experiments show that the average-case complexity of our algorithm scales quadratically. The size reduction of the automata depends very much on the class of instances, but our algorithm consistently outperforms all previous techniques by a wide margin. We tested our algorithm on Buchi automata derived from LTL-formulae, many classes of random automata and automata derived from mutual exclusion protocols, and compared its performance to the well-known automata tool GOAL."
      },
      {
        "node_idx": 124823,
        "score_0_10": 9,
        "title": "social event scheduling",
        "abstract": "A major challenge for social event organizers (e.g., event planning companies, venues) is attracting the maximum number of participants, since it has great impact on the success of the event, and, consequently, the expected gains (e.g., revenue, artist/brand publicity). In this paper, we introduce the Social Event Scheduling (SES) problem, which schedules a set of social events considering user preferences and behavior, events' spatiotemporal conflicts, and competing events, in order to maximize the overall number of participants. We show that SES is strongly NP-hard, even in highly restricted instances. To cope with the hardness of the SES problem we design a greedy approximation algorithm. Finally, we evaluate our method experimentally using a real dataset."
      },
      {
        "node_idx": 92159,
        "score_0_10": 9,
        "title": "quasirandom rumor spreading",
        "abstract": "We propose and analyze a quasirandom analogue of the classical push model for disseminating information in networks (\"randomized rumor spreading\"). In the classical model, in each round each informed vertex chooses a neighbor at random and informs it, if it was not informed before. It is known that this simple protocol succeeds in spreading a rumor from one vertex to all others within O(log n) rounds on complete graphs, hypercubes, random regular graphs, Erdos-Renyi random graph and Ramanujan graphs with probability 1-o(1). In the quasirandom model, we assume that each vertex has a (cyclic) list of its neighbors. Once informed, it starts at a random position on the list, but from then on informs its neighbors in the order of the list. Surprisingly, irrespective of the orders of the lists, the above-mentioned bounds still hold. In some cases, even better bounds than for the classical model can be shown."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 7401,
        "score_0_10": 9,
        "title": "timed pushdown automata revisited",
        "abstract": "This paper contains two results on timed extensions of pushdown automata (PDA). As our first result we prove that the model of dense-timed PDA of Abdulla et al. collapses: it is expressively equivalent to dense-timed PDA with timeless stack. Motivated by this result, we advocate the framework of first-order definable PDA, a specialization of PDA in sets with atoms, as the right setting to define and investigate timed extensions of PDA. The general model obtained in this way is Turing complete. As our second result we prove NEXPTIME upper complexity bound for the non-emptiness problem for an expressive subclass. As a byproduct, we obtain a tight EXPTIME complexity bound for a more restrictive subclass of PDA with timeless stack, thus subsuming the complexity bound known for dense-timed PDA."
      },
      {
        "node_idx": 47889,
        "score_0_10": 9,
        "title": "second order belief hidden markov models",
        "abstract": "Hidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. Previous works in belief HMMs have been focused on the first-order HMMs. We extend them to the second-order model."
      },
      {
        "node_idx": 42226,
        "score_0_10": 9,
        "title": "minimization of visibly pushdown automata is np complete",
        "abstract": "We show that the minimization of visibly pushdown automata is NP-complete. This result is obtained by introducing immersions, that recognize multiple languages (over a usual, non-visible alphabet) using a common deterministic transition graph, such that each language is associated with an initial state and a set of final states. We show that minimizing immersions is NP-complete, and reduce this problem to the minimization of visibly pushdown automata."
      },
      {
        "node_idx": 8358,
        "score_0_10": 8,
        "title": "attendance maximization for successful social event planning",
        "abstract": "Social event planning has received a great deal of attention in recent years where various entities, such as event planners and marketing companies, organizations, venues, or users in Event-based Social Networks, organize numerous social events (e.g., festivals, conferences, promotion parties). Recent studies show that \"attendance\" is the most common metric used to capture the success of social events, since the number of attendees has great impact on the event's expected gains (e.g., revenue, artist/brand publicity). In this work, we study the Social Event Scheduling (SES) problem which aims at identifying and assigning social events to appropriate time slots, so that the number of events attendees is maximized. We show that, even in highly restricted instances, the SES problem is NP-hard to be approximated over a factor. To solve the SES problem, we design three efficient and scalable algorithms. These algorithms exploit several novel schemes that we design. We conduct extensive experiments using several real and synthetic datasets, and demonstrate that the proposed algorithms perform on average half the computations compared to the existing solution and, in several cases, are 3-5 times faster."
      }
    ]
  },
  "416": {
    "explanation": "distributed consensus and estimation under noisy feedback and communication constraints",
    "topk": [
      {
        "node_idx": 117327,
        "score_0_10": 10,
        "title": "multi agent consensus with relative state dependent measurement noises",
        "abstract": "In this note, the distributed consensus corrupted by relative-state-dependent measurement noises is considered. Each agent can measure or receive its neighbors' state information with random noises, whose intensity is a vector function of agents' relative states. By investigating the structure of this interaction and the tools of stochastic differential equations, we develop several small consensus gain theorems to give sufficient conditions in terms of the control gain, the number of agents and the noise intensity function to ensure mean square (m. s.) and almost sure (a. s.) consensus and quantify the convergence rate and the steady-state error. Especially, for the case with homogeneous communication and control channels, a necessary and sufficient condition to ensure m. s. consensus on the control gain is given and it is shown that the control gain is independent of the specific network topology, but only depends on the number of nodes and the noise coefficient constant. For symmetric measurement models, the almost sure convergence rate is estimated by the Iterated Logarithm Law of Brownian motions."
      },
      {
        "node_idx": 100650,
        "score_0_10": 10,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 55398,
        "score_0_10": 10,
        "title": "the capacity of channels with feedback",
        "abstract": "We introduce a general framework for treating channels with memory and feedback. First, we generalize Massey's concept of directed information and use it to characterize the feedback capacity of general channels. Second, we present coding results for Markov channels. This requires determining appropriate sufficient statistics at the encoder and decoder. Third, a dynamic programming framework for computing the capacity of Markov channels is presented. Fourth, it is shown that the average cost optimality equation (ACOE) can be viewed as an implicit single-letter characterization of the capacity. Fifth, scenarios with simple sufficient statistics are described."
      },
      {
        "node_idx": 25990,
        "score_0_10": 10,
        "title": "distributed parameter estimation in sensor networks nonlinear observation models and imperfect communication",
        "abstract": "The paper studies distributed static parameter (vector) estimation in sensor networks with nonlinear observation models and noisy inter-sensor communication. It introduces \\emph{separably estimable} observation models that generalize the observability condition in linear centralized estimation to nonlinear distributed estimation. It studies two distributed estimation algorithms in separably estimable models, the $\\mathcal{NU}$ (with its linear counterpart $\\mathcal{LU}$) and the $\\mathcal{NLU}$. Their update rule combines a \\emph{consensus} step (where each sensor updates the state by weight averaging it with its neighbors' states) and an \\emph{innovation} step (where each sensor processes its local current observation.) This makes the three algorithms of the \\textit{consensus + innovations} type, very different from traditional consensus. The paper proves consistency (all sensors reach consensus almost surely and converge to the true parameter value,) efficiency, and asymptotic unbiasedness. For $\\mathcal{LU}$ and $\\mathcal{NU}$, it proves asymptotic normality and provides convergence rate guarantees. The three algorithms are characterized by appropriately chosen decaying weight sequences. Algorithms $\\mathcal{LU}$ and $\\mathcal{NU}$ are analyzed in the framework of stochastic approximation theory; algorithm $\\mathcal{NLU}$ exhibits mixed time-scale behavior and biased perturbations, and its analysis requires a different approach that is developed in the paper."
      },
      {
        "node_idx": 45949,
        "score_0_10": 10,
        "title": "influence maximization near optimal time complexity meets practical efficiency",
        "abstract": "Given a social network G and a constant k, the influence maximization problem asks for k nodes in G that (directly and indirectly) influence the largest number of nodes under a pre-defined diffusion model. This problem finds important applications in viral marketing, and has been extensively studied in the literature. Existing algorithms for influence maximization, however, either trade approximation guarantees for practical efficiency, or vice versa. In particular, among the algorithms that achieve constant factor approximations under the prominent independent cascade (IC) model or linear threshold (LT) model, none can handle a million-node graph without incurring prohibitive overheads. #R##N#This paper presents TIM, an algorithm that aims to bridge the theory and practice in influence maximization. On the theory side, we show that TIM runs in O((k+\\ell) (n+m) \\log n / \\epsilon^2) expected time and returns a (1-1/e-\\epsilon)-approximate solution with at least 1 - n^{-\\ell} probability. The time complexity of TIM is near-optimal under the IC model, as it is only a \\log n factor larger than the \\Omega(m + n) lower-bound established in previous work (for fixed k, \\ell, and \\epsilon). Moreover, TIM supports the triggering model, which is a general diffusion model that includes both IC and LT as special cases. On the practice side, TIM incorporates novel heuristics that significantly improve its empirical efficiency without compromising its asymptotic performance. We experimentally evaluate TIM with the largest datasets ever tested in the literature, and show that it outperforms the state-of-the-art solutions (with approximation guarantees) by up to four orders of magnitude in terms of running time. In particular, when k = 50, \\epsilon = 0.2, and \\ell = 1, TIM requires less than one hour on a commodity machine to process a network with 41.6 million nodes and 1.4 billion edges."
      },
      {
        "node_idx": 161760,
        "score_0_10": 10,
        "title": "collaborative distributed hypothesis testing",
        "abstract": "A collaborative distributed binary decision problem is considered. Two statisticians are required to declare the correct probability measure of two jointly distributed memoryless process, denoted by Xn=(X1,\u2026,Xn) and Yn=(Y1,\u2026,Yn), out of two possible probability measures on finite alphabets, namely PXY and PX\u00afY\u00af. The marginal samples given by Xn and Yn are assumed to be available at different locations. The statisticians are allowed to exchange limited amount of data over multiple rounds of interactions, which differs from previous work that deals mainly with unidirectional communication. A single round of interaction is considered before the result is generalized to any finite number of communication rounds. A feasibility result is shown, guaranteeing the feasibility of an error exponent for general hypotheses, through information-theoretic methods. The special case of testing against independence is revisited as being an instance of this result for which also an unfeasibility result is proven. A second special case is studied where zero-rate communication is imposed (data exchanges grow sub-exponentially with n) for which it is shown that interaction does not improve asymptotic performance."
      },
      {
        "node_idx": 31334,
        "score_0_10": 10,
        "title": "feedback capacity of stationary gaussian channels",
        "abstract": "The feedback capacity of additive stationary Gaussian noise channels is characterized as the solution to a variational problem. Toward this end, it is proved that the optimal feedback coding scheme is stationary. When specialized to the first-order autoregressive moving average noise spectrum, this variational characterization yields a closed-form expression for the feedback capacity. In particular, this result shows that the celebrated Schalkwijk-Kailath coding scheme achieves the feedback capacity for the first-order autoregressive moving average Gaussian channel, positively answering a long-standing open problem studied by Butman, Schalkwijk-Tiernan, Wolfowitz, Ozarow, Ordentlich, Yang-Kavcic-Tatikonda, and others. More generally, it is shown that a k-dimensional generalization of the Schalkwijk-Kailath coding scheme achieves the feedback capacity for any autoregressive moving average noise spectrum of order k. Simply put, the optimal transmitter iteratively refines the receiver's knowledge of the intended message."
      },
      {
        "node_idx": 19427,
        "score_0_10": 9,
        "title": "finite state channels with time invariant deterministic feedback",
        "abstract": "We consider capacity of discrete-time channels with feedback for the general case where the feedback is a time-invariant deterministic function of the output samples. Under the assumption that the channel states take values in a finite alphabet, we find an achievable rate and an upper bound on the capacity. We further show that when the channel is indecomposable, and has no intersymbol interference (ISI), its capacity is given by the limit of the maximum of the (normalized) directed information between the input $X^N$ and the output $Y^N$, i.e. $C = \\lim_{N \\to \\infty} \\frac{1}{N} \\max I(X^N \\to Y^N)$, where the maximization is taken over the causal conditioning probability $Q(x^N||z^{N-1})$ defined in this paper. The capacity result is used to show that the source-channel separation theorem holds for time-invariant determinist feedback. We also show that if the state of the channel is known both at the encoder and the decoder then feedback does not increase capacity."
      },
      {
        "node_idx": 49292,
        "score_0_10": 9,
        "title": "distributed storage allocations",
        "abstract": "We examine the problem of allocating a given total storage budget in a distributed storage system for maximum reliability. A source has a single data object that is to be coded and stored over a set of storage nodes; it is allowed to store any amount of coded data in each node, as long as the total amount of storage used does not exceed the given budget. A data collector subsequently attempts to recover the original data object by accessing only the data stored in a random subset of the nodes. By using an appropriate code, successful recovery can be achieved whenever the total amount of data accessed is at least the size of the original data object. The goal is to find an optimal storage allocation that maximizes the probability of successful recovery. This optimization problem is challenging in general because of its combinatorial nature, despite its simple formulation. We study several variations of the problem, assuming different allocation models and access models. The optimal allocation and the optimal symmetric allocation (in which all nonempty nodes store the same amount of data) are determined for a variety of cases. Our results indicate that the optimal allocations often have nonintuitive structure and are difficult to specify. We also show that depending on the circumstances, coding may or may not be beneficial for reliable storage."
      },
      {
        "node_idx": 47973,
        "score_0_10": 9,
        "title": "solving the apparent diversity accuracy dilemma of recommender systems",
        "abstract": "Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations."
      }
    ]
  },
  "417": {
    "explanation": "handwritten character recognition using neural networks and pattern analysis",
    "topk": [
      {
        "node_idx": 116566,
        "score_0_10": 10,
        "title": "time efficient approach to offline hand written character recognition using associative memory net",
        "abstract": "In this paper, an efficient Offline Hand Written Character Recognition algorithm is proposed based on Associative Memory Net (AMN). The AMN used in this work is basically auto associative. The implementation is carried out completely in 'C' language. To make the system perform to its best with minimal computation time, a Parallel algorithm is also developed using an API package OpenMP. Characters are mainly English alphabets (Small (26), Capital (26)) collected from system (52) and from different persons (52). The characters collected from system are used to train the AMN and characters collected from different persons are used for testing the recognition ability of the net. The detailed analysis showed that the network recognizes the hand written characters with recognition rate of 72.20% in average case. However, in best case, it recognizes the collected hand written characters with 88.5%. The developed network consumes 3.57 sec (average) in Serial implementation and 1.16 sec (average) in Parallel implementation using OpenMP."
      },
      {
        "node_idx": 4891,
        "score_0_10": 10,
        "title": "growth rate of binary words avoiding xxx r",
        "abstract": "Consider the set of those binary words with no non-empty factors of the form $xxx^R$. Du, Mousavi, Schaeffer, and Shallit asked whether this set of words grows polynomially or exponentially with length. In this paper, we demonstrate the existence of upper and lower bounds on the number of such words of length $n$, where each of these bounds is asymptotically equivalent to a (different) function of the form $Cn^{\\lg n+c}$, where $C$, $c$ are constants."
      },
      {
        "node_idx": 100669,
        "score_0_10": 10,
        "title": "pattern avoidability with involution",
        "abstract": "An infinte word w avoids a pattern p with the involution f if there is no substitution for the variables in p and no involution f such that the resulting word is a factor of w. We investigate the avoidance of patterns with respect to the size of the alphabet. For example, it is shown that the pattern x f(x) x can be avoided over three letters but not two letters, whereas it is well known that x x x is avoidable over two letters."
      },
      {
        "node_idx": 15936,
        "score_0_10": 10,
        "title": "english character recognition using artificial neural network",
        "abstract": "This work focuses on development of a Offline Hand Written English Character Recognition algorithm based on Artificial Neural Network (ANN). The ANN implemented in this work has single output neuron which shows whether the tested character belongs to a particular cluster or not. The implementation is carried out completely in 'C' language. Ten sets of English alphabets (small-26, capital-26) were used to train the ANN and 5 sets of English alphabets were used to test the network. The characters were collected from different persons over duration of about 25 days. The algorithm was tested with 5 capital letters and 5 small letter sets. However, the result showed that the algorithm recognized English alphabet patterns with maximum accuracy of 92.59% and False Rejection Rate (FRR) of 0%."
      },
      {
        "node_idx": 128298,
        "score_0_10": 10,
        "title": "devnagari handwritten numeral recognition using geometric features and statistical combination classifier",
        "abstract": "This paper presents a Devnagari Numerical recognition method based on statistical discriminant functions. 17 geometric features based on pixel connectivity, lines, line directions, holes, image area, perimeter, eccentricity, solidity, orientation etc. are used for representing the numerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear, Diagquadratic and Mahalanobis distance are used for classification. 1500 handwritten numerals are used for training. Another 1500 handwritten numerals are used for testing. Experimental results show that Linear, Quadratic and Mahalanobis discriminant functions provide better results. Results of these three Discriminants are fed to a majority voting type Combination classifier. It is found that Combination classifier offers better results over individual classifiers."
      },
      {
        "node_idx": 91394,
        "score_0_10": 9,
        "title": "a review on handwritten character and numeral recognition for roman arabic chinese and indian scripts",
        "abstract": "There are a lot of intensive researches on handwritten character recognition (HCR) for almost past four decades. The research has been done on some of popular scripts such as Roman, Arabic, Chinese and Indian. In this paper we present a review on HCR work on the four popular scripts. We have summarized most of the published paper from 2005 to recent and also analyzed the various methods in creating a robust HCR system. We also added some future direction of research on HCR."
      },
      {
        "node_idx": 90119,
        "score_0_10": 9,
        "title": "a software for aging faces applied to ancient marble busts",
        "abstract": "The study and development of software able to show the effect of aging of faces is one of the tasks of face recognition technologies. Some software solutions are used for investigations, some others to show the effects of drugs on healthy appearance, however some other applications can be proposed for the analysis of visual arts. Here we use a freely available software, which is providing interesting results, for the comparison of ancient marble busts. An analysis of Augustus busts is proposed."
      },
      {
        "node_idx": 121657,
        "score_0_10": 9,
        "title": "facial transformations of ancient portraits the face of caesar",
        "abstract": "Some software solutions used to obtain the facial transformations can help investigating the artistic metamorphosis of the ancient portraits of the same person. An analysis with a freely available software of portraitures of Julius Caesar is proposed, showing his several \"morphs\". The software helps enhancing the mood the artist added to a portrait."
      },
      {
        "node_idx": 128383,
        "score_0_10": 9,
        "title": "recognition of non compound handwritten devnagari characters using a combination of mlp and minimum edit distance",
        "abstract": "This paper deals with a new method for recognition of offline Handwritten non-compound Devnagari Characters in two stages. It uses two well known and established pattern recognition techniques: one using neural networks and the other one using minimum edit distance. Each of these techniques is applied on different sets of characters for recognition. In the first stage, two sets of features are computed and two classifiers are applied to get higher recognition accuracy. Two MLP's are used separately to recognize the characters. For one of the MLP's the characters are represented with their shadow features and for the other chain code histogram feature is used. The decision of both MLP's is combined using weighted majority scheme. Top three results produced by combined MLP's in the first stage are used to calculate the relative difference values. In the second stage, based on these relative differences character set is divided into two. First set consists of the characters with distinct shapes and second set consists of confused characters, which appear very similar in shapes. Characters of distinct shapes of first set are classified using MLP. Confused characters in second set are classified using minimum edit distance method. Method of minimum edit distance makes use of corner detected in a character image using modified Harris corner detection technique. Experiment on this method is carried out on a database of 7154 samples. The overall recognition is found to be 90.74%."
      },
      {
        "node_idx": 6477,
        "score_0_10": 8,
        "title": "a self portrait of young leonardo",
        "abstract": "One of the most famous drawings by Leonardo da Vinci is a self-portrait in red chalk, where he looks quite old. In fact, there is a sketch in one of his notebooks, partially covered by written notes, that can be a self-portrait of the artist when he was young. The use of image processing, to remove the handwritten text and improve the image, allows a comparison of the two portraits"
      }
    ]
  },
  "418": {
    "explanation": "multi-scale context aggregation for scene parsing",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 131345,
        "score_0_10": 9,
        "title": "provenance threat modeling",
        "abstract": "Provenance systems are used to capture history metadata, applications include ownership attribution and determining the quality of a particular data set. Provenance systems are also used for debugging, process improvement, understanding data proof of ownership, certification of validity, etc. The provenance of data includes information about the processes and source data that leads to the current representation. In this paper we study the security risks provenance systems might be exposed to and recommend security solutions to better protect the provenance information."
      },
      {
        "node_idx": 163252,
        "score_0_10": 9,
        "title": "a first look at quic in the wild",
        "abstract": "For the first time since the establishment of TCP and UDP, the Internet transport layer is subject to a major change by the introduction of QUIC. Initiated by Google in 2012, QUIC provides a reliable, connection-oriented low-latency and fully encrypted transport. In this paper, we provide the first broad assessment of QUIC usage in the wild. We monitor the entire IPv4 address space since August 2016 and about 46% of the DNS namespace to detected QUIC-capable infrastructures. Our scans show that the number of QUIC-capable IPs has more than tripled since then to over 617.59 K. We find around 161 K domains hosted on QUIC-enabled infrastructure, but only 15 K of them present valid certificates over QUIC. Second, we analyze one year of traffic traces provided by MAWI, one day of a major European tier-1 ISP and from a large IXP to understand the dominance of QUIC in the Internet traffic mix. We find QUIC to account for 2.6% to 9.1% of the current Internet traffic, depending on the vantage point. This share is dominated by Google pushing up to 42.1% of its traffic via QUIC."
      },
      {
        "node_idx": 54478,
        "score_0_10": 9,
        "title": "publicly verifiable secret sharing using non abelian groups",
        "abstract": "In his paper (9), Stadler develops techniques for improving the security of existing secret sharing protocols by allowing to check whether the secret shares given out by the dealer are valid. In particular, the secret sharing is executed over abelian groups. In this paper we develop similar methods over non-abelian groups."
      },
      {
        "node_idx": 150975,
        "score_0_10": 9,
        "title": "a secret sharing scheme using groups",
        "abstract": "In this paper a secret sharing scheme based on the word problem in groups is introduced. The security of the scheme and possible variations are discussed in section 2. The article concludes with the suggestion of two categories of platform groups for the implementation of the scheme."
      },
      {
        "node_idx": 51514,
        "score_0_10": 9,
        "title": "consensus on transaction commit",
        "abstract": "The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. Running a Paxos consensus algorithm on the commit/abort decision of each participant yields a transaction commit protocol that uses 2F +1 coordinators and makes progress if at least F +1 of them are working. In the fault-free case, this algorithm requires one extra message delay but has the same stable-storage write delay as Two-Phase Commit. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the general Paxos Commit algorithm."
      },
      {
        "node_idx": 58412,
        "score_0_10": 9,
        "title": "semantic security and indistinguishability in the quantum world",
        "abstract": "At CRYPTO 2013, Boneh and Zhandry initiated the study of quantum-secure encryption. They proposed first indistinguishability definitions for the quantum world where the actual indistinguishability only holds for classical messages, and they provide arguments why it might be hard to achieve a stronger notion. In this work, we show that stronger notions are achievable, where the indistinguishability holds for quantum superpositions of messages. We investigate exhaustively the possibilities and subtle differences in defining such a quantum indistinguishability notion for symmetric-key encryption schemes. We justify our stronger definition by showing its equivalence to novel quantum semantic-security notions that we introduce. Furthermore, we show that our new security definitions cannot be achieved by a large class of ciphers --- those which are quasi-preserving the message length. On the other hand, we provide a secure construction based on quantum-resistant pseudorandom permutations; this construction can be used as a generic transformation for turning a large class of encryption schemes into quantum indistinguishable and hence quantum semantically secure ones. Moreover, our construction is the first completely classical encryption scheme shown to be secure against an even stronger notion of indistinguishability, which was previously known to be achievable only by using quantum messages and arbitrary quantum encryption circuits."
      },
      {
        "node_idx": 42653,
        "score_0_10": 8,
        "title": "on the security defects of an image encryption scheme",
        "abstract": "This paper studies the security of a recently-proposed chaos-based image encryption scheme and points out the following problems: (1) there exist a number of invalid keys and weak keys, and some keys are partially equivalent for encryption/decryption; (2) given one chosen plain-image, a subkey K\"1\"0 can be guessed with a smaller computational complexity than that of the simple brute-force attack; (3) given at most 128 chosen plain-images, a chosen-plaintext attack can possibly break the following part of the secret key: K\"imod128\"i\"=\"4^1^0, which works very well when K\"1\"0 is not too large; (4) when K\"1\"0 is relatively small, a known-plaintext attack can be carried out with only one known plain-image to recover some visual information of any other plain-images encrypted by the same key."
      },
      {
        "node_idx": 165527,
        "score_0_10": 8,
        "title": "cryptanalysis of an image encryption scheme based on a compound chaotic sequence",
        "abstract": "Recently, an image encryption scheme based on a compound chaotic sequence was proposed. In this paper, the security of the scheme is studied and the following problems are found: (1) a differential chosen-plaintext attack can break the scheme with only three chosen plain-images; (2) there is a number of weak keys and some equivalent keys for encryption; (3) the scheme is not sensitive to the changes of plain-images; and (4) the compound chaotic sequence does not work as a good random number source."
      },
      {
        "node_idx": 28023,
        "score_0_10": 8,
        "title": "joint defogging and demosaicking",
        "abstract": "Image defogging is a technique used extensively for enhancing visual quality of images in bad weather conditions. Even though defogging algorithms have been well studied, defogging performance is degraded by demosaicking artifacts and sensor noise amplification in distant scenes. In order to improve the visual quality of restored images, we propose a novel approach to perform defogging and demosaicking simultaneously. We conclude that better defogging performance with fewer artifacts can be achieved when a defogging algorithm is combined with a demosaicking algorithm simultaneously. We also demonstrate that the proposed joint algorithm has the benefit of suppressing noise amplification in distant scenes. In addition, we validate our theoretical analysis and observations for both synthesized data sets with ground truth fog-free images and natural scene data sets captured in a raw format."
      }
    ]
  },
  "421": {
    "explanation": "high-resolution conditional image synthesis using generative adversarial networks",
    "topk": [
      {
        "node_idx": 167239,
        "score_0_10": 10,
        "title": "high resolution image synthesis and semantic manipulation with conditional gans",
        "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing."
      },
      {
        "node_idx": 76583,
        "score_0_10": 10,
        "title": "learning to discover cross domain relations with generative adversarial networks",
        "abstract": "While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available this https URL"
      },
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 123935,
        "score_0_10": 10,
        "title": "bottom up and top down attention for image captioning and visual question answering",
        "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge."
      },
      {
        "node_idx": 140101,
        "score_0_10": 10,
        "title": "papg personalized anti phishing guard",
        "abstract": "Security and privacy have been considered a corner stone in all electronic transactions nowadays. People are becoming very cautious when conducting electronic transactions over internet. One of the major issues that frightens them is identity theft. Identity theft might be conducted using phishing techniques that aims to trick the user to provide his credentials in a well-organized tactic. Efforts have been done towards fighting against phishing attacks and hence identify theft. However, most of these efforts are either computationally exhaustive to the electronic device or depend on a third party to perform the task. In this paper, we propose a plugin called Personalized Anti-Phishing Guard - PAPG that is managed personally on the device and is used to guard the user against phishing attacks. The plugin maintains data locally and may not need to synchronize with a third party. Besides, PAPG depends on the user's feedback to build the local knowledge base that is used to support the decision. The user might also store his profile and reuse it with other devices and from different locations without having to configure it again"
      },
      {
        "node_idx": 43464,
        "score_0_10": 10,
        "title": "bidirectional attention flow for machine comprehension",
        "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."
      },
      {
        "node_idx": 108255,
        "score_0_10": 10,
        "title": "current induced dynamics of multiple skyrmions with domain wall pair and skyrmion based majority gate design",
        "abstract": "As an intriguing ultra-small particle-like magnetic texture, skyrmion has attracted lots of research interests in next-generation ultra-dense and low power magnetic memory/logic designs. Previous studies have demonstrated a single skyrmion-domain wall pair collision in a specially designed magnetic racetrack junction. In this work, we investigate the dynamics of multiple skyrmions with domain wall pair in a magnetic racetrack. The numerical micromagnetic simulation results indicate that the domain wall pair could be pinned or depinned by the rectangular notch pinning site depending on both the number of skyrmions in the racetrack and the magnitude of driving current density. Such emergent dynamical property could be used to implement a threshold-tunable step function, in which the inputs are skyrmions and threshold could be tuned by the driving current density. The threshold-tunable step function is widely used in logic and neural network applications. We also present a three-input skyrmion-based majority logic gate design to demonstrate the potential application of such dynamic interaction of multiple skyrmions and domain wall pair."
      },
      {
        "node_idx": 62634,
        "score_0_10": 10,
        "title": "my camera can see through fences a deep learning approach for image de fencing",
        "abstract": "In recent times, the availability of inexpensive image capturing devices such as smartphones/tablets has led to an exponential increase in the number of images/videos captured. However, sometimes the amateur photographer is hindered by fences in the scene which have to be removed after the image has been captured. Conventional approaches to image de-fencing suffer from inaccurate and non-robust fence detection apart from being limited to processing images of only static occluded scenes. In this paper, we propose a semi-automated de-fencing algorithm using a video of the dynamic scene. We use convolutional neural networks for detecting fence pixels. We provide qualitative as well as quantitative comparison results with existing lattice detection algorithms on the existing PSU NRT data set and a proposed challenging fenced image dataset. The inverse problem of fence removal is solved using split Bregman technique assuming total variation of the de-fenced image as the regularization constraint."
      },
      {
        "node_idx": 154569,
        "score_0_10": 9,
        "title": "it is undecidable if two regular tree languages can be separated by a deterministic tree walking automaton",
        "abstract": "The following problem is shown undecidable: given regular languages L,K of finite trees, decide if there exists a deterministic tree-walking automaton which accepts all trees in L and rejects all trees in K. The proof uses a technique of Kopczy\\'nski from LICS 2016."
      },
      {
        "node_idx": 165763,
        "score_0_10": 9,
        "title": "developing all skyrmion spiking neural network",
        "abstract": "In this work, we have proposed a revolutionary neuromorphic computing methodology to implement All-Skyrmion Spiking Neural Network (AS-SNN). Such proposed methodology is based on our finding that skyrmion is a topological stable spin texture and its spatiotemporal motion along the magnetic nano-track intuitively interprets the pulse signal transmission between two interconnected neurons. In such design, spike train in SNN could be encoded as particle-like skyrmion train and further processed by the proposed skyrmion-synapse and skyrmion-neuron within the same magnetic nano-track to generate output skyrmion as post-spike. Then, both pre-neuron spikes and post-neuron spikes are encoded as particle-like skyrmions without conversion between charge and spin signals, which fundamentally differentiates our proposed design from other hybrid Spin-CMOS designs. The system level simulation shows 87.1% inference accuracy for handwritten digit recognition task, while the energy dissipation is ~1 fJ/per spike which is 3 orders smaller in comparison with CMOS based IBM TrueNorth system."
      }
    ]
  },
  "423": {
    "explanation": "interactive and semi-automatic volumetric medical image segmentation techniques",
    "topk": [
      {
        "node_idx": 80414,
        "score_0_10": 10,
        "title": "3d u net learning dense volumetric segmentation from sparse annotation",
        "abstract": "This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases."
      },
      {
        "node_idx": 138038,
        "score_0_10": 10,
        "title": "in depth assessment of an interactive graph based approach for the segmentation for pancreatic metastasis in ultrasound acquisitions of the liver with two specialists in internal medicine",
        "abstract": "The manual outlining of hepatic metastasis in (US) ultrasound acquisitions from patients suffering from pancreatic cancer is common practice. However, such pure manual measurements are often very time consuming, and the results repeatedly differ between the raters. In this contribution, we study the in-depth assessment of an interactive graph-based approach for the segmentation for pancreatic metastasis in US images of the liver with two specialists in Internal Medicine. Thereby, evaluating the approach with over one hundred different acquisitions of metastases. The two physicians or the algorithm had never assessed the acquisitions before the evaluation. In summary, the physicians first performed a pure manual outlining followed by an algorithmic segmentation over one month later. As a result, the experts satisfied in up to ninety percent of algorithmic segmentation results. Furthermore, the algorithmic segmentation was much faster than manual outlining and achieved a median Dice Similarity Coefficient (DSC) of over eighty percent. Ultimately, the algorithm enables a fast and accurate segmentation of liver metastasis in clinical US images, which can support the manual outlining in daily practice."
      },
      {
        "node_idx": 103461,
        "score_0_10": 10,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      },
      {
        "node_idx": 128509,
        "score_0_10": 10,
        "title": "completely reducible sets",
        "abstract": "We study the family of rational sets of words, called completely reducible and which are such that the syntactic representation of their characteristic series is completely reducible. This family contains, by a result of Reutenauer, the submonoids generated by bifix codes and, by a result of Berstel and Reutenauer, the cyclic sets. We study the closure properties of this family. We prove a result on linear representations of monoids which gives a generalization of the result concerning the complete reducibility of the submonoid generated by a bifix code to sets called birecurrent. We also give a new proof of the result concerning cyclic sets."
      },
      {
        "node_idx": 66067,
        "score_0_10": 10,
        "title": "refinement cut user guided segmentation algorithm for translational science",
        "abstract": "In this contribution, a semi-automatic segmentation algorithm for (medical) image analysis is presented. More precise, the approach belongs to the category of interactive contouring algorithms, which provide real-time feedback of the segmentation result. However, even with interactive real-time contouring approaches there are always cases where the user cannot find a satisfying segmentation, e.g. due to homogeneous appearances between the object and the background, or noise inside the object. For these difficult cases the algorithm still needs additional user support. However, this additional user support should be intuitive and rapid integrated into the segmentation process, without breaking the interactive real-time segmentation feedback. I propose a solution where the user can support the algorithm by an easy and fast placement of one or more seed points to guide the algorithm to a satisfying segmentation result also in difficult cases. These additional seed(s) restrict(s) the calculation of the segmentation for the algorithm, but at the same time, still enable to continue with the interactive real-time feedback segmentation. For a practical and genuine application in translational science, the approach has been tested on medical data from the clinical routine in 2D and 3D."
      },
      {
        "node_idx": 141809,
        "score_0_10": 10,
        "title": "us cut interactive algorithm for rapid detection and segmentation of liver tumors in ultrasound acquisitions",
        "abstract": "Ultrasound (US) is the most commonly used liver imaging modality worldwide. It plays an important role in follow-up of cancer patients with liver metastases. We present an interactive segmentation approach for liver tumors in US acquisitions. Due to the low image quality and the low contrast between the tumors and the surrounding tissue in US images, the segmentation is very challenging. Thus, the clinical practice still relies on manual measurement and outlining of the tumors in the US images. We target this problem by applying an interactive segmentation algorithm to the US data, allowing the user to get real-time feedback of the segmentation results. The algorithm has been developed and tested hand-in-hand by physicians and computer scientists to make sure a future practical usage in a clinical setting is feasible. To cover typical acquisitions from the clinical routine, the approach has been evaluated with dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic (darker) or isoechoic (similar) in comparison to the surrounding liver tissue. Due to the interactive real-time behavior of the approach, it was possible even in difficult cases to find satisfying segmentations of the tumors within seconds and without parameter settings, and the average tumor deviation was only 1.4mm compared with manual measurements. However, the long term goal is to ease the volumetric acquisition of liver tumors in order to evaluate for treatment response. Additional aim is the registration of intraoperative US images via the interactive segmentations to the patient's pre-interventional CT acquisitions."
      },
      {
        "node_idx": 86125,
        "score_0_10": 10,
        "title": "square cut a segmentation algorithm on the basis of a rectangle shape",
        "abstract": "We present a rectangle-based segmentation algorithm that sets up a graph and performs a graph cut to separate an object from the background. However, graph-based algorithms distribute the graph's nodes uniformly and equidistantly on the image. Then, a smoothness term is added to force the cut to prefer a particular shape. This strategy does not allow the cut to prefer a certain structure, especially when areas of the object are indistinguishable from the background. We solve this problem by referring to a rectangle shape of the object when sampling the graph nodes, i.e., the nodes are distributed non-uniformly and non-equidistantly on the image. This strategy can be useful, when areas of the object are indistinguishable from the background. For evaluation, we focus on vertebrae images from Magnetic Resonance Imaging (MRI) datasets to support the time consuming manual slice-by-slice segmentation performed by physicians. The ground truth of the vertebrae boundaries were manually extracted by two clinical experts (neurological surgeons) with several years of experience in spine surgery and afterwards compared with the automatic segmentation results of the proposed scheme yielding an average Dice Similarity Coefficient (DSC) of 90.97\u00b12.2%."
      },
      {
        "node_idx": 140315,
        "score_0_10": 9,
        "title": "a flexible semi automatic approach for glioblastoma multiforme segmentation",
        "abstract": "Gliomas are the most common primary brain tumors, evolving from the cerebral supportive cells. For clinical follow-up, the evaluation of the preoperative tumor volume is essential. Volumetric assessment of tumor volume with manual segmentation of its outlines is a time-consuming process that can be overcome with the help of segmentation methods. In this paper, a flexible semi-automatic approach for grade IV glioma segmentation is presented. The approach uses a novel segmentation scheme for spherical objects that creates a directed 3D graph. Thereafter, the minimal cost closed set on the graph is computed via a polynomial time s-t cut, creating an optimal segmentation of the tumor. The user can improve the results by specifying an arbitrary number of additional seed points to support the algorithm with grey value information and geometrical constraints. The presented method is tested on 12 magnetic resonance imaging datasets. The ground truth of the tumor boundaries are manually extracted by neurosurgeons. The segmented gliomas are compared with a one click method, and the semi-automatic approach yields an average Dice Similarity Coefficient (DSC) of 77.72% and 83.91%, respectively."
      },
      {
        "node_idx": 100896,
        "score_0_10": 9,
        "title": "interactive outlining of pancreatic cancer liver metastases in ultrasound images",
        "abstract": "Ultrasound (US) is the most commonly used liver imaging modality worldwide. Due to its low cost, it is increasingly used in the follow-up of cancer patients with metastases localized in the liver. In this contribution, we present the results of an interactive segmentation approach for liver metastases in US acquisitions. A (semi-) automatic segmentation is still very challenging because of the low image quality and the low contrast between the metastasis and the surrounding liver tissue. Thus, the state of the art in clinical practice is still manual measurement and outlining of the metastases in the US images. We tackle the problem by providing an interactive segmentation approach providing real-time feedback of the segmentation results. The approach has been evaluated with typical US acquisitions from the clinical routine, and the datasets consisted of pancreatic cancer metastases. Even for difficult cases, satisfying segmentations results could be achieved because of the interactive real-time behavior of the approach. In total, 40 clinical images have been evaluated with our method by comparing the results against manual ground truth segmentations. This evaluation yielded to an average Dice Score of 85% and an average Hausdorff Distance of 13 pixels."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      }
    ]
  },
  "425": {
    "explanation": "Memetic and hybrid metaheuristic optimization techniques",
    "topk": [
      {
        "node_idx": 147143,
        "score_0_10": 10,
        "title": "memetic search in differential evolution algorithm",
        "abstract": "Differential Evolution (DE) is a renowned optimization stratagem that can easily solve nonlinear and comprehensive problems. DE is a well known and uncomplicated population based probabilistic approach for comprehensive optimization. It has apparently outperformed a number of Evolutionary Algorithms and further search heuristics in the vein of Particle Swarm Optimization at what time of testing over both yardstick and actual world problems. Nevertheless, DE, like other probabilistic optimization algorithms, from time to time exhibits precipitate convergence and stagnates at suboptimal position. In order to stay away from stagnation behavior while maintaining an excellent convergence speed, an innovative search strategy is introduced, named memetic search in DE. In the planned strategy, positions update equation customized as per a memetic search stratagem. In this strategy a better solution participates more times in the position modernize procedure. The position update equation is inspired from the memetic search in artificial bee colony algorithm. The proposed strategy is named as Memetic Search in Differential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it is tested over 8 benchmark optimization problems and three real world optimization problems. A comparative analysis has also been carried out among proposed MSDE and original DE. Results show that the anticipated algorithm go one better than the basic DE and its recent deviations in a good number of the experiments."
      },
      {
        "node_idx": 54574,
        "score_0_10": 9,
        "title": "memetic artificial bee colony algorithm for large scale global optimization",
        "abstract": "Memetic computation (MC) has emerged recently as a new paradigm of efficient algorithms for solving the hardest optimization problems. On the other hand, artificial bees colony (ABC) algorithms demonstrate good performances when solving continuous and combinatorial optimization problems. This study tries to use these technologies under the same roof. As a result, a memetic ABC (MABC) algorithm has been developed that is hybridized with two local search heuristics: the Nelder-Mead algorithm (NMA) and the random walk with direction exploitation (RWDE). The former is attended more towards exploration, while the latter more towards exploitation of the search space. The stochastic adaptation rule was employed in order to control the balancing between exploration and exploitation. This MABC algorithm was applied to a Special suite on Large Scale Continuous Global Optimization at the 2012 IEEE Congress on Evolutionary Computation. The obtained results the MABC are comparable with the results of DECC-G, DECC-G*, and MLCC."
      },
      {
        "node_idx": 53273,
        "score_0_10": 9,
        "title": "accelerating the ant colony optimization by smart ants using genetic operator",
        "abstract": "This paper research review Ant colony optimization (ACO) and Genetic Algorithm (GA), both are two powerful meta-heuristics. This paper explains some major defects of these two algorithm at first then proposes a new model for ACO in which, artificial ants use a quick genetic operator and accelerate their actions in selecting next state. Experimental results show that proposed hybrid algorithm is effective and its performance including speed and accuracy beats other version."
      },
      {
        "node_idx": 106114,
        "score_0_10": 9,
        "title": "matching networks for one shot learning",
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
      },
      {
        "node_idx": 134689,
        "score_0_10": 9,
        "title": "solving single digit sudoku subproblems",
        "abstract": "We show that single-digit \"Nishio\" subproblems in nxn Sudoku puzzles may be solved in time o(2^n), faster than previous solutions such as the pattern overlay method. We also show that single-digit deduction in Sudoku is NP-hard."
      },
      {
        "node_idx": 147021,
        "score_0_10": 9,
        "title": "tensor manipulation in gpl maxima",
        "abstract": "GPL Maxima is an open-source computer algebra system based on DOE-MACSYMA. GPL Maxima included two tensor manipulation packages from DOE-MACSYMA, but these were in various states of disrepair. One of the two packages, CTENSOR, implemented component-based tensor manipulation; the other, ITENSOR, treated tensor symbols as opaque, manipulating them based on their index properties. The present paper describes the state in which these packages were found, the steps that were needed to make the packages fully functional again, and the new functionality that was implemented to make them more versatile. A third package, ATENSOR, was also implemented; fully compatible with the identically named package in the commercial version of MACSYMA, ATENSOR implements abstract tensor algebras."
      },
      {
        "node_idx": 161764,
        "score_0_10": 9,
        "title": "sparse representations of clifford and tensor algebras in maxima",
        "abstract": "Clifford algebras have broad applications in science and engineering. The use of Clifford algebras can be further promoted in these fields by availability of computational tools that automate tedious routine calculations. We offer an extensive demonstration of the applications of Clifford algebras in electromagnetism using the geometric algebra \\({\\mathbb{G}^3 \\equiv C\\ell_{3,0}}\\) as a computational model in the Maxima computer algebra system. We compare the geometric algebra-based approach with conventional symbolic tensor calculations supported by Maxima, based on the itensor package. The Clifford algebra functionality of Maxima is distributed as two new packages called clifford\u2014for basic simplification of Clifford products, outer products, scalar products and inverses; and cliffordan\u2014for applications of geometric calculus."
      },
      {
        "node_idx": 117388,
        "score_0_10": 9,
        "title": "solving connectivity problems parameterized by treewidth in single exponential time",
        "abstract": "For the vast majority of local graph problems standard dynamic programming techniques give c^tw V^O(1) algorithms, where tw is the treewidth of the input graph. On the other hand, for problems with a global requirement (usually connectivity) the best-known algorithms were naive dynamic programming schemes running in tw^O(tw) V^O(1) time. #R##N#We breach this gap by introducing a technique we dubbed Cut&Count that allows to produce c^tw V^O(1) Monte Carlo algorithms for most connectivity-type problems, including Hamiltonian Path, Feedback Vertex Set and Connected Dominating Set, consequently answering the question raised by Lokshtanov, Marx and Saurabh [SODA'11] in a surprising way. We also show that (under reasonable complexity assumptions) the gap cannot be breached for some problems for which Cut&Count does not work, like CYCLE PACKING. #R##N#The constant c we obtain is in all cases small (at most 4 for undirected problems and at most 6 for directed ones), and in several cases we are able to show that improving those constants would cause the Strong Exponential Time Hypothesis to fail. #R##N#Our results have numerous consequences in various fields, like FPT algorithms, exact and approximate algorithms on planar and H-minor-free graphs and algorithms on graphs of bounded degree. In all these fields we are able to improve the best-known results for some problems."
      },
      {
        "node_idx": 22691,
        "score_0_10": 9,
        "title": "artificial ant species on solving optimization problems",
        "abstract": "During the last years several ant-based techniques were involved to solve hard and complex optimization problems. The current paper is a short study about the influence of artificial ant species in solving optimization problems. There are studied the artificial Pharaoh Ants, Lasius Niger and also artificial ants with no special specificity used commonly in Ant Colony Optimization."
      },
      {
        "node_idx": 58417,
        "score_0_10": 9,
        "title": "nurse rostering with genetic algorithms",
        "abstract": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming. This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital.#R##N##R##N# #R##N##R##N#The hospital is made up of wards of up to 30 nurses. Each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis. In addition to fulfilling the minimum demand for staff over three daily shifts, nurses\u2019 wishes and qualifications have to be taken into account. The schedules must also be seen to be fair, in that unpopular shifts have to be spread evenly amongst all nurses, and other restrictions, such as team nursing and special conditions for senior staff, have to be satisfied.#R##N##R##N# #R##N##R##N#The basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover, single-bit mutation and a rank-based selection. The solution space consists of all schedules in which each nurse works the required number of shifts, but the remaining constraints, both hard and soft, are relaxed and penalised in the fitness function.#R##N##R##N# #R##N##R##N#The talk will start with a detailed description of the problem and the initial implementation and will go on to highlight the shortcomings of such an approach, in terms of the key element of balancing feasibility, i.e. covering the demand and work regulations, and quality, as measured by the nurses\u2019 preferences. A series of experiments involving parameter adaptation, niching, intelligent weights, delta coding, local hill climbing, migration and special selection rules will then be outlined and it will be shown how a series of these enhancements were able to eradicate these difficulties.#R##N##R##N# #R##N##R##N#Results based on several months\u2019 real data will be used to measure the impact of each modification, and to show that the final algorithm is able to compete with a tabu search approach currently employed at the hospital. The talk will conclude with some observations as to the overall quality of this approach to this and similar problems."
      }
    ]
  },
  "428": {
    "explanation": "Digital forensic investigation and analysis of cloud storage services",
    "topk": [
      {
        "node_idx": 28810,
        "score_0_10": 10,
        "title": "digital forensic investigation of cloud storage services",
        "abstract": "The demand for cloud computing is increasing because of the popularity of digital devices and the wide use of the Internet. Among cloud computing services, most consumers use cloud storage services that provide mass storage. This is because these services give them various additional functions as well as storage. It is easy to access cloud storage services using smartphones. With increasing utilization, it is possible for malicious users to abuse cloud storage services. Therefore, a study on digital forensic investigation of cloud storage services is necessary. This paper proposes new procedure for investigating and analyzing the artifacts of all accessible devices, such as Windows, Mac, iPhone, and Android smartphone."
      },
      {
        "node_idx": 35005,
        "score_0_10": 10,
        "title": "twitter sentiment analysis system",
        "abstract": "Social media is increasingly used by humans to express their feelings and opinions in the form of short text messages. Detecting sentiments in the text has a wide range of applications including identifying anxiety or depression of individuals and measuring well-being or mood of a community. Sentiments can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Sentiment Analysis in text documents is essentially a content-based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning. In this paper, sentiment recognition based on textual data and the techniques used in sentiment analysis are discussed."
      },
      {
        "node_idx": 129115,
        "score_0_10": 9,
        "title": "gridsim a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing",
        "abstract": "Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular paradigms for next generation parallel and distributed computing. The management of resources and scheduling of applications in such large-scale distributed systems is a complex undertaking. In order to prove the effectiveness of resource brokers and associated scheduling algorithms, their performance needs to be evaluated under different scenarios such as varying number of resources and users with different requirements. In a grid environment, it is hard and even impossible to perform scheduler performance evaluation in a repeatable and controllable manner as resources and users are distributed across multiple organizations with their own policies. To overcome this limitation, we have developed a Java-based discrete-event grid simulation toolkit called GridSim. The toolkit supports modeling and simulation of heterogeneous grid resources (both time- and space-shared), users and application models. It provides primitives for creation of application tasks, mapping of tasks to resources, and their management. To demonstrate suitability of the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker and evaluated the performance of deadline and budget constrained cost- and time-minimization scheduling algorithms."
      },
      {
        "node_idx": 37335,
        "score_0_10": 9,
        "title": "botornot a system to evaluate social bots",
        "abstract": "While most online social media accounts are controlled by humans, these platforms also host automated agents called social bots or sybil accounts. Recent literature reported on cases of social bots imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate the extent to which a Twitter account exhibits similarity to the known characteristics of social bots. Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs."
      },
      {
        "node_idx": 46715,
        "score_0_10": 9,
        "title": "bitcoin ng a scalable blockchain protocol",
        "abstract": "Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential. #R##N#This paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem. #R##N#In addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network."
      },
      {
        "node_idx": 137724,
        "score_0_10": 9,
        "title": "market oriented cloud computing vision hype and reality for delivering it services as computing utilities",
        "abstract": "This keynote paper: presents a 21st century vision of computing; identifies various computing paradigms promising to deliver the vision of computing utilities; defines Cloud computing and provides the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; presents some representative Cloud platforms especially those developed in industries along with our current work towards realising market-oriented resource allocation of Clouds by leveraging the 3rd generation Aneka enterprise Grid technology; reveals our early thoughts on interconnecting Clouds for dynamically creating an atmospheric computing environment along with pointers to future community research; and concludes with the need for convergence of competing IT paradigms for delivering our 21st century vision."
      },
      {
        "node_idx": 88816,
        "score_0_10": 9,
        "title": "calm before the storm the challenges of cloud computing in digital forensics",
        "abstract": "Cloud computing is a rapidly evolving information technology (IT) phenomenon. Rather than procure, deploy and manage a physical IT infrastructure to host their software applications, organizations are increasingly deploying their infrastructure into remote, virtualized environments, often hosted and managed by third parties. This development has significant implications for digital forensic investigators, equipment vendors, law enforcement, as well as corporate compliance and audit departments (among others). Much of digital forensic practice assumes careful control and management of IT assets (particularly data storage) during the conduct of an investigation. This paper summarises the key aspects of cloud computing and analyses how established digital forensic procedures will be invalidated in this new environment. Several new research challenges addressing this changing context are also identified and discussed."
      },
      {
        "node_idx": 59988,
        "score_0_10": 9,
        "title": "cloud storage forensic hubic as a case study",
        "abstract": "In today's society where we live in a world of constant connectivity, many people are now looking to cloud services in order to store their files so they can have access to them wherever they are. By using cloud services, users can access files anywhere with an internet connection. However, while cloud storage is convenient, it also presents security risks. From a forensics perspective, the increasing popularity of cloud storage platforms, makes investigation into such exploits much more difficult, especially since many platforms such as mobile devices as well as computers are able to use these services. This paper presents investigation of hubiC as one of popular cloud platforms running on Microsoft Windows 8.1. Remaining artefacts pertaining different usage of hubiC namely upload, download, installation and uninstallation on Microsoft Windows 8.1 are presented."
      },
      {
        "node_idx": 167109,
        "score_0_10": 9,
        "title": "online human bot interactions detection estimation and characterization",
        "abstract": "Increasing evidence suggests that a growing amount of social media content is generated by autonomous entities known as social bots. In this work we present a framework to detect such entities on Twitter. We leverage more than a thousand features extracted from public data and meta-data about users: friends, tweet content and sentiment, network patterns, and activity time series. We benchmark the classification framework by using a publicly available dataset of Twitter bots. This training data is enriched by a manually annotated collection of active Twitter users that include both humans and bots of varying sophistication. Our models yield high accuracy and agreement with each other and can detect bots of different nature. Our estimates suggest that between 9% and 15% of active Twitter accounts are bots. Characterizing ties among accounts, we observe that simple bots tend to interact with bots that exhibit more human-like behaviors. Analysis of content flows reveals retweet and mention strategies adopted by bots to interact with different target groups. Using clustering analysis, we characterize several subclasses of accounts, including spammers, self promoters, and accounts that post content from connected applications."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      }
    ]
  },
  "429": {
    "explanation": "multi-scale context aggregation for semantic segmentation",
    "topk": [
      {
        "node_idx": 121343,
        "score_0_10": 10,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 67928,
        "score_0_10": 8,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 163164,
        "score_0_10": 8,
        "title": "rethinking atrous convolution for semantic image segmentation",
        "abstract": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
      },
      {
        "node_idx": 132436,
        "score_0_10": 8,
        "title": "why johnny still still can t encrypt evaluating the usability of a modern pgp client",
        "abstract": "This paper presents the results of a laboratory study involving Mailvelope, a modern PGP client that integrates tightly with existing webmail providers. In our study, we brought in pairs of participants and had them attempt to use Mailvelope to communicate with each other. Our results shown that more than a decade and a half after \\textit{Why Johnny Can't Encrypt}, modern PGP tools are still unusable for the masses. We finish with a discussion of pain points encountered using Mailvelope, and discuss what might be done to address them in future PGP systems."
      },
      {
        "node_idx": 120825,
        "score_0_10": 8,
        "title": "encoder decoder with atrous separable convolution for semantic image segmentation",
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
      },
      {
        "node_idx": 116677,
        "score_0_10": 8,
        "title": "model checking contractual protocols",
        "abstract": "This paper discusses how model checking, a technique used for the verification of behavioural requirements of dynamic systems, can be usefully deployed for the verification of contracts. A process view of agreements between parties is taken, whereby a contract is modelled as it evolves over time in terms of actions or more generally events that effect changes in its state. Modelling is done with Petri Nets in the spirit of other research work on the representation of trade procedures. The paper illustrates all the phases of the verification technique through an example and argues that the approach is useful particularly in the context of pre-contractual negotiation and contract drafting. The work reported here is part of a broader project on the development of logic-based tools for the analysis and representation of legal contracts."
      },
      {
        "node_idx": 21714,
        "score_0_10": 8,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 52018,
        "score_0_10": 7,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 119987,
        "score_0_10": 7,
        "title": "xception deep learning with depthwise separable convolutions",
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
      },
      {
        "node_idx": 31226,
        "score_0_10": 7,
        "title": "effectiveness of crypto transcoding for h 264 avc and hevc video bit streams",
        "abstract": "To avoid delays arising from a need to decrypt a video prior to transcoding and then re-encrypt it afterwards, this paper assesses a selective encryption (SE) content protection scheme. The scheme is suited to both recent standardized codecs, namely H.264/Advanced Video Coding (AVC) and High Efficiency Video Coding (HEVC). Specifically, the paper outlines a joint crypto-transcoding scheme for secure transrating of a video bitstream. That is to say it generates new video bitrates, possibly as part of an HTTP Adaptive Streaming (HAS) content delivery network. The scheme will reduce the bitrate to one or more lower desired bit-rate without consuming time in the encryption/decryption process, which would be the case when full encryption is used. In addition, the decryption key no longer needs to be exposed at intermediate middleboxes, including when transrating is performed in a cloud datacenter. The effectiveness of the scheme is variously evaluated: by examination of the SE generated visual distortion; by the extent of computational and bitrate overheads; and by choice of cipher when encrypting the selected elements within the bitstream. Results indicate that there remains: a content; quantization level (after transrating of an encrypted video); and codec-type dependency to any distortion introduced. A further recommendation is that the Advanced Encryption Standard (AES) is preferred for SE to lightweight XOR encryption, despite it being taken up elsewhere as a real-time encryption method."
      }
    ]
  },
  "430": {
    "explanation": "advanced error correction and decoding in network coding",
    "topk": [
      {
        "node_idx": 41252,
        "score_0_10": 10,
        "title": "a rank metric approach to error control in random network coding",
        "abstract": "The problem of error control in random linear network coding is addressed from a matrix perspective that is closely related to the subspace perspective of Rotter and Kschischang. A large class of constant-dimension subspace codes is investigated. It is shown that codes in this class can be easily constructed from rank-metric codes, while preserving their distance properties. Moreover, it is shown that minimum distance decoding of such subspace codes can be reformulated as a generalized decoding problem for rank-metric codes where partial information about the error is available. This partial information may be in the form of erasures (knowledge of an error location but not its value) and deviations (knowledge of an error value but not its location). Taking erasures and deviations into account (when they occur) strictly increases the error correction capability of a code: if mu erasures and delta deviations occur, then errors of rank t can always be corrected provided that 2t les d - 1 + mu + delta, where d is the minimum rank distance of the code. For Gabidulin codes, an important family of maximum rank distance codes, an efficient decoding algorithm is proposed that can properly exploit erasures and deviations. In a network coding application, where n packets of length M over F(q) are transmitted, the complexity of the decoding algorithm is given by O(dM) operations in an extension field F(qn)."
      },
      {
        "node_idx": 163667,
        "score_0_10": 9,
        "title": "rate region of the quadratic gaussian two encoder source coding problem",
        "abstract": "We determine the rate region of the quadratic Gaussian two-encoder source-coding problem. This rate region is achieved by a simple architecture that separates the analog and digital aspects of the compression. Furthermore, this architecture requires higher rates to send a Gaussian source than it does to send any other source with the same covariance. Our techniques can also be used to determine the sum rate of some generalizations of this classical problem. Our approach involves coupling the problem to a quadratic Gaussian ``CEO problem.''"
      },
      {
        "node_idx": 73918,
        "score_0_10": 9,
        "title": "generalized approximate message passing for estimation with random linear mixing",
        "abstract": "We consider the estimation of an i.i.d.\\ random vector observed through a linear transform followed by a componentwise, probabilistic (possibly nonlinear) measurement channel. A novel algorithm, called generalized approximate message passing (GAMP), is presented that provides computationally efficient approximate implementations of max-sum and sum-problem loopy belief propagation for such problems. The algorithm extends earlier approximate message passing methods to incorporate arbitrary distributions on both the input and output of the transform and can be applied to a wide range of problems in nonlinear compressed sensing and learning. #R##N#Extending an analysis by Bayati and Montanari, we argue that the asymptotic componentwise behavior of the GAMP method under large, i.i.d. Gaussian transforms is described by a simple set of state evolution (SE) equations. From the SE equations, one can \\emph{exactly} predict the asymptotic value of virtually any componentwise performance metric including mean-squared error or detection accuracy. Moreover, the analysis is valid for arbitrary input and output distributions, even when the corresponding optimization problems are non-convex. The results match predictions by Guo and Wang for relaxed belief propagation on large sparse matrices and, in certain instances, also agree with the optimal performance predicted by the replica method. The GAMP methodology thus provides a computationally efficient methodology, applicable to a large class of non-Gaussian estimation problems with precise asymptotic performance guarantees."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 123813,
        "score_0_10": 9,
        "title": "robust 1 bit compressive sensing via binary stable embeddings of sparse vectors",
        "abstract": "The Compressive Sensing (CS) framework aims to ease the burden on analog-to-digital converters (ADCs) by reducing the sampling rate required to acquire and stably recover sparse signals. Practical ADCs not only sample but also quantize each measurement to a finite number of bits; moreover, there is an inverse relationship between the achievable sampling rate and the bit depth. In this paper, we investigate an alternative CS approach that shifts the emphasis from the sampling rate to the number of bits per measurement. In particular, we explore the extreme case of 1-bit CS measurements, which capture just their sign. Our results come in two flavors. First, we consider ideal reconstruction from noiseless 1-bit measurements and provide a lower bound on the best achievable reconstruction error. We also demonstrate that i.i.d. random Gaussian matrices describe measurement mappings achieving, with overwhelming probability, nearly optimal error decay. Next, we consider reconstruction robustness to measurement errors and noise and introduce the Binary $\\epsilon$-Stable Embedding (B$\\epsilon$SE) property, which characterizes the robustness measurement process to sign changes. We show the same class of matrices that provide almost optimal noiseless performance also enable such a robust mapping. On the practical side, we introduce the Binary Iterative Hard Thresholding (BIHT) algorithm for signal reconstruction from 1-bit measurements that offers state-of-the-art performance."
      },
      {
        "node_idx": 3489,
        "score_0_10": 9,
        "title": "weight distributions of cyclic codes with respect to pairwise coprime order elements",
        "abstract": "Let $\\Bbb F_r$ be an extension of a finite field $\\Bbb F_q$ with $r=q^m$. Let each $g_i$ be of order $n_i$ in $\\Bbb F_r^*$ and $\\gcd(n_i, n_j)=1$ for $1\\leq i \\neq j \\leq u$. #R##N#We define a cyclic code over $\\Bbb F_q$ by #R##N#$$\\mathcal C_{(q, m, n_1,n_2, ..., n_u)}=\\{c(a_1, a_2, ..., a_u) : a_1, a_2, ..., a_u \\in \\Bbb F_r\\},$$ where #R##N#$$c(a_1, a_2, ..., a_u)=({Tr}_{r/q}(\\sum_{i=1}^ua_ig_i^0), ..., {Tr}_{r/q}(\\sum_{i=1}^ua_ig_i^{n-1}))$$ and $n=n_1n_2... n_u$. In this paper, we present a method to compute the weights of $\\mathcal C_{(q, m, n_1,n_2, ..., n_u)}$. Further, we determine the weight distributions of the cyclic codes $\\mathcal C_{(q, m, n_1,n_2)}$ and $\\mathcal C_{(q, m, n_1,n_2,1)}$."
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 136642,
        "score_0_10": 9,
        "title": "the dynamics of message passing on dense graphs with applications to compressed sensing",
        "abstract": "\u201cApproximate message passing\u201d (AMP) algorithms have proved to be effective in reconstructing sparse signals from a small number of incoherent linear measurements. Extensive numerical experiments further showed that their dynamics is accurately tracked by a simple one-dimensional iteration termed state evolution. In this paper, we provide rigorous foundation to state evolution. We prove that indeed it holds asymptotically in the large system limit for sensing matrices with independent and identically distributed Gaussian entries. While our focus is on message passing algorithms for compressed sensing, the analysis extends beyond this setting, to a general class of algorithms on dense graphs. In this context, state evolution plays the role that density evolution has for sparse graphs. The proof technique is fundamentally different from the standard approach to density evolution, in that it copes with a large number of short cycles in the underlying factor graph. It relies instead on a conditioning technique recently developed by Erwin Bolthausen in the context of spin glass theory."
      },
      {
        "node_idx": 77683,
        "score_0_10": 9,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      }
    ]
  },
  "431": {
    "explanation": "structural tractability of constraint satisfaction problems",
    "topk": [
      {
        "node_idx": 48886,
        "score_0_10": 10,
        "title": "constraint solving via fractional edge covers",
        "abstract": "Many important combinatorial problems can be modeled as constraint satisfaction problems. Hence identifying polynomial-time solvable classes of constraint satisfaction problems has received a lot of attention. In this paper, we are interested in structural properties that can make the problem tractable. So far, the largest structural class that is known to be polynomial-time solvable is the class of bounded hypertree width instances introduced by Gottlob et al. Here we identify a new class of polynomial-time solvable instances: those having bounded fractional edge cover number. #R##N#Combining hypertree width and fractional edge cover number, we then introduce the notion of fractional hypertree width. We prove that constraint satisfaction problems with bounded fractional hypertree width can be solved in polynomial time (provided that a the tree decomposition is given in the input). Together with a recent approximation algorithm for finding such decompositions by Marx, it follows that bounded fractional hypertree width is now the most general known structural property that guarantees polynomial-time solvability."
      },
      {
        "node_idx": 152319,
        "score_0_10": 10,
        "title": "the classification of complementary information set codes of lengths 14 and 16",
        "abstract": "In the paper \"A new class of codes for Boolean masking of cryptographic computations,\" Carlet, Gaborit, Kim, and Sol\\'{e} defined a new class of rate one-half binary codes called \\emph{complementary information set} (or CIS) codes. The authors then classified all CIS codes of length less than or equal to 12. CIS codes have relations to classical Coding Theory as they are a generalization of self-dual codes. As stated in the paper, CIS codes also have important practical applications as they may improve the cost of masking cryptographic algorithms against side channel attacks. In this paper, we give a complete classification result for length 14 CIS codes using an equivalence relation on $GL(n,\\FF_2)$. We also give a new classification for all binary $[16,8,3]$ and $[16,8,4]$ codes. We then complete the classification for length 16 CIS codes and give additional classifications for optimal CIS codes of lengths 20 and 26."
      },
      {
        "node_idx": 19582,
        "score_0_10": 10,
        "title": "higher order cis codes",
        "abstract": "We introduce {\\bf complementary information set codes} of higher-order. A binary linear code of length $tk$ and dimension $k$ is called a complementary information set code of order $t$ ($t$-CIS code for short) if it has $t$ pairwise disjoint information sets. The duals of such codes permit to reduce the cost of masking cryptographic algorithms against side-channel attacks. As in the case of codes for error correction, given the length and the dimension of a $t$-CIS code, we look for the highest possible minimum distance. In this paper, this new class of codes is investigated. The existence of good long CIS codes of order $3$ is derived by a counting argument. General constructions based on cyclic and quasi-cyclic codes and on the building up construction are given. A formula similar to a mass formula is given. A classification of 3-CIS codes of length $\\le 12$ is given. Nonlinear codes better than linear codes are derived by taking binary images of $\\Z_4$-codes. A general algorithm based on Edmonds' basis packing algorithm from matroid theory is developed with the following property: given a binary linear code of rate $1/t$ it either provides $t$ disjoint information sets or proves that the code is not $t$-CIS. Using this algorithm, all optimal or best known $[tk, k]$ codes where $t=3, 4, \\dots, 256$ and $1 \\le k \\le \\lfloor 256/t \\rfloor$ are shown to be $t$-CIS for all such $k$ and $t$, except for $t=3$ with $k=44$ and $t=4$ with $k=37$."
      },
      {
        "node_idx": 126617,
        "score_0_10": 10,
        "title": "order preserving matching",
        "abstract": "We introduce a new string matching problem called order-preserving matching on numeric strings where a pattern matches a text if the text contains a substring whose relative orders coincide with those of the pattern. Order-preserving matching is applicable to many scenarios such as stock price analysis and musical melody matching in which the order relations should be matched instead of the strings themselves. Solving order-preserving matching has to do with representations of order relations of a numeric string. We define prefix representation and nearest neighbor representation, which lead to efficient algorithms for order-preserving matching. We present efficient algorithms for single and multiple pattern cases. For the single pattern case, we give an O(n log m) time algorithm and optimize it further to obtain O(n + m log m) time. For the multiple pattern case, we give an O(n log m) time algorithm."
      },
      {
        "node_idx": 81243,
        "score_0_10": 9,
        "title": "relational characterisations of paths",
        "abstract": "Binary relations are one of the standard ways to encode, characterise and reason about graphs. Relation algebras provide equational axioms for a large fragment of the calculus of binary relations. Although relations are standard tools in many areas of mathematics and computing, researchers usually fall back to point-wise reasoning when it comes to arguments about paths in a graph. We present a purely algebraic way to specify different kinds of paths in relation algebras. We study the relationship between paths with a designated root vertex and paths without such a vertex. Since we stay in first-order logic this development helps with mechanising proofs. To demonstrate the applicability of the algebraic framework we verify the correctness of three basic graph algorithms. All results of this paper are formally verified using Isabelle/HOL."
      },
      {
        "node_idx": 5619,
        "score_0_10": 9,
        "title": "average stack cost of buechi pushdown automata",
        "abstract": "We study the average stack cost of Buechi pushdown automata (Buechi PDA). We associate a non-negative price with each stack symbol and define the cost of a stack as the sum of costs of all its elements. We introduce and study the average stack cost problem (ASC), which asks whether there exists an accepting run of a given Buechi PDA such that the long-run average of stack costs is below some given threshold. The ASC problem generalizes mean-payoff objective and can be used to express quantitative properties of pushdown systems. In particular, we can compute the average response time using the ASC problem. We show that the ASC problem can be solved in polynomial time."
      },
      {
        "node_idx": 80423,
        "score_0_10": 9,
        "title": "cfa2 a context free approach to control flow analysis",
        "abstract": "In a functional language, the dominant control-flow mechanism is function call and return. Most higher-order flow analyses, including k-CFA, do not handle call and return well: they remember only a bounded number of pending calls because they approximate programs with control-flow graphs. Call/return mismatch introduces precision-degrading spurious control-flow paths and increases the analysis time. We describe CFA2, the first flow analysis with precise call/return matching in the presence of higher-order functions and tail calls. We formulate CFA2 as an abstract interpretation of programs in continuation-passing style and describe a sound and complete summarization algorithm for our abstract semantics. A preliminary evaluation shows that CFA2 gives more accurate data-flow information than 0CFA and 1CFA."
      },
      {
        "node_idx": 121879,
        "score_0_10": 9,
        "title": "definability equals recognizability for graphs of bounded treewidth",
        "abstract": "We prove a conjecture of Courcelle, which states that a graph property is definable in MSO with modular counting predicates on graphs of constant treewidth if, and only if it is recognizable in the following sense: constant-width tree decompositions of graphs satisfying the property can be recognized by tree automata. While the forward implication is a classic fact known as Courcelle's theorem, the converse direction remained open"
      },
      {
        "node_idx": 7286,
        "score_0_10": 9,
        "title": "2d lyndon words and applications",
        "abstract": "A Lyndon word is a primitive string which is lexicographically smallest among cyclic permutations of its characters. Lyndon words are used for constructing bases in free Lie algebras, constructing de Bruijn sequences, finding the lexicographically smallest or largest substring in a string, and succinct suffix-prefix matching of highly periodic strings. In this paper, we extend the concept of the Lyndon word to two dimensions. We introduce the 2D Lyndon word and use it to capture 2D horizontal periodicity of a matrix in which each row is highly periodic, and to efficiently solve 2D horizontal suffix-prefix matching among a set of patterns. This yields a succinct and efficient algorithm for 2D dictionary matching. #R##N#We present several algorithms that compute the 2D Lyndon word that represents a matrix. The final algorithm achieves linear time complexity even when the least common multiple of the periods of the rows is exponential in the matrix width."
      },
      {
        "node_idx": 31738,
        "score_0_10": 9,
        "title": "the piggy bank cryptographic trope",
        "abstract": "This paper presents applications of the trope of the locked and sealed piggy-bank into which the secret can be easily inserted but from which it cannot be withdrawn without opening the box. We present a basic two-pass cryptographic scheme that can serve as template for a variety of implementations. Together with the sealed piggy-bank is sent a coded letter that lists and certifies the contents of the box. We show how this idea can help increase the security of cryptographic protocols for classical systems as well as those based on \"single-state\" systems. More specifically, we propose the use of a hashing digest (instead of the coded letter) to detect loss of key bits to the eavesdropper and use in communication systems where error correction is an important issue."
      }
    ]
  },
  "432": {
    "explanation": "3D volumetric medical image segmentation with sparse annotations",
    "topk": [
      {
        "node_idx": 71153,
        "score_0_10": 10,
        "title": "quadrilateral meshing by circle packing",
        "abstract": "We use circle-packing methods to generate quadrilateral meshes for polygonal domains, with guaranteed bounds both on the quality and the number of elements. We show that these methods can generate meshes of several types: (1) the elements form the cells of a Voronoi diagram, (2) all elements have two opposite right angles, (3) all elements are kites, or (4) all angles are at most 120 degrees. In each case the total number of elements is O(n), where n is the number of input vertices."
      },
      {
        "node_idx": 11237,
        "score_0_10": 10,
        "title": "statistical efficiency of curve fitting algorithms",
        "abstract": "We study the problem of fitting parametrized curves to noisy data. Under certain assumptions (known as Cartesian and radial functional models), we derive asymptotic expressions for the bias and the covariance matrix of the parameter estimates. We also extend Kanatani's version of the Cramer-Rao lower bound, which he proved for unbiased estimates only, to more general estimates that include many popular algorithms (most notably, the orthogonal least squares and algebraic fits). We then show that the gradient-weighted algebraic fit is statistically efficient and describe all other statistically efficient algebraic fits."
      },
      {
        "node_idx": 143592,
        "score_0_10": 10,
        "title": "molecular computers",
        "abstract": "We propose the chemlambda artificial chemistry, whose behavior strongly suggests that real molecules which embed Interaction Nets patterns and real chemical reactions which resemble Interaction Nets graph rewrites could be a realistic path towards molecular computers, in the sense explained in the article."
      },
      {
        "node_idx": 148800,
        "score_0_10": 9,
        "title": "faster range minimum queries",
        "abstract": "Range Minimum Query (RMQ) is an important building brick of many compressed data structures and string matching algorithms. Although this problem is essentially solved in theory, with sophisticated data structures allowing for constant time queries, practical performance and construction time also matter. Additionally, there are offline scenarios in which the number of queries, $q$, is rather small and given beforehand, which encourages to use a simpler approach. In this work, we present a simple data structure, with very fast construction, which allows to handle queries in constant time on average. This algorithm, however, requires access to the input data during queries (which is not the case of sophisticated RMQ solutions). We subsequently refine our technique, combining it with one of the existing succinct solutions with $O(1)$ worst-case time queries and no access to the input array. The resulting hybrid is still a memory frugal data structure, spending usually up to about $3n$ bits, and providing competitive query times, especially for wide ranges. We also show how to make our baseline data structure more compact. Experimental results demonstrate that the proposed BbST (Block-based Sparse Table) variants are competitive to existing solutions, also in the offline scenario."
      },
      {
        "node_idx": 19526,
        "score_0_10": 9,
        "title": "stabbing segments with rectilinear objects",
        "abstract": "Given a set $S$ of $n$ line segments in the plane, we say that a region $\\mathcal{R}\\subseteq \\mathbb{R}^2$ is a {\\em stabber} for $S$ if $\\mathcal{R}$ contains exactly one endpoint of each segment of $S$. In this paper we provide optimal or near-optimal algorithms for reporting all combinatorially different stabbers for several shapes of stabbers. Specifically, we consider the case in which the stabber can be described as the intersection of axis-parallel halfplanes (thus the stabbers are halfplanes, strips, quadrants, $3$-sided rectangles, or rectangles). The running times are $O(n)$ (for the halfplane case), $O(n\\log n)$ (for strips, quadrants, and 3-sided rectangles), and $O(n^2 \\log n)$ (for rectangles)."
      },
      {
        "node_idx": 819,
        "score_0_10": 9,
        "title": "lombardi drawings of graphs",
        "abstract": "We introduce the notion of Lombardi graph drawings, named after the American abstract artist Mark Lombardi. In these drawings, edges are represented as circular arcs rather than as line segments or polylines, and the vertices have perfect angular resolution: the edges are equally spaced around each vertex. We describe algorithms for finding Lombardi drawings of regular graphs, graphs of bounded degeneracy, and certain families of planar graphs."
      },
      {
        "node_idx": 156653,
        "score_0_10": 9,
        "title": "stabbing pairwise intersecting disks by five points",
        "abstract": "We present an $O(n)$ expected time algorithm and an $O(n \\log n)$ deterministic time algorithm to find a set of five points that stab a set of $n$ pairwise intersecting disks in the plane. We also give a simple construction with 13 pairwise intersecting disks that cannot be stabbed by three points."
      },
      {
        "node_idx": 27453,
        "score_0_10": 9,
        "title": "new results on stabbing segments with a polygon",
        "abstract": "We consider a natural variation of the concept of stabbing a segment by a simple polygon: a segment is stabbed by a simple polygon $\\mathcal{P}$ if at least one of its two endpoints is contained in $\\mathcal{P}$. A segment set $S$ is stabbed by $\\mathcal{P}$ if every segment of $S$ is stabbed by $\\mathcal{P}$. We show that if $S$ is a set of pairwise disjoint segments, the problem of computing the minimum perimeter polygon stabbing $S$ can be solved in polynomial time. We also prove that for general segments the problem is NP-hard. Further, an adaptation of our polynomial-time algorithm solves an open problem posed by L\\\"offler and van Kreveld [Algorithmica 56(2), 236--269 (2010)] about finding a maximum perimeter convex hull for a set of imprecise points modeled as line segments."
      },
      {
        "node_idx": 103461,
        "score_0_10": 9,
        "title": "v net fully convolutional neural networks for volumetric medical image segmentation",
        "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods."
      },
      {
        "node_idx": 80414,
        "score_0_10": 8,
        "title": "3d u net learning dense volumetric segmentation from sparse annotation",
        "abstract": "This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases."
      }
    ]
  },
  "437": {
    "explanation": "automata theory and cellular automata reversibility analysis",
    "topk": [
      {
        "node_idx": 99279,
        "score_0_10": 10,
        "title": "mechanical reasoning on infinite extensive games",
        "abstract": "In order to better understand reasoning involved in analyzing infinite games in extensive form, we performed the experiments in proof assistant Coq that are reported here."
      },
      {
        "node_idx": 14164,
        "score_0_10": 10,
        "title": "from nondeterministic b uchi and streett automata to deterministic parity automata",
        "abstract": "In this paper we revisit Safra's determinization constructions for automata on infinite words. We show how to construct deterministic automata with fewer states and, most importantly, parity acceptance conditions. Determinization is used in numerous applications, such as reasoning about tree automata, satisfiability of CTL*, and realizability and synthesis of logical specifications. The upper bounds for all these applications are reduced by using the smaller deterministic automata produced by our construction. In addition, the parity acceptance conditions allows to use more efficient algorithms (when compared to handling Rabin or Streett acceptance conditions)."
      },
      {
        "node_idx": 92656,
        "score_0_10": 10,
        "title": "reversibility of d state finite cellular automata",
        "abstract": "This paper investigates reversibility properties of 1-dimensional 3-neighborhood d-state finite cellular automata (CAs) of length n under periodic boundary condition. A tool named reachability tree has been developed from de Bruijn graph which represents all possible reachable configurations of an n-cell CA. This tool has been used to test reversibility of CAs. We have identified a large set of reversible CAs using this tool by following some greedy strategies."
      },
      {
        "node_idx": 42046,
        "score_0_10": 10,
        "title": "advanced automata minimization",
        "abstract": "We present an efficient algorithm to reduce the size of nondeterministic Buchi word automata, while retaining their language. Additionally, we describe methods to solve PSPACE-complete automata problems like universality, equivalence and inclusion for much larger instances (1-3 orders of magnitude) than before. This can be used to scale up applications of automata in formal verification tools and decision procedures for logical theories. The algorithm is based on new transition pruning techniques. These use criteria based on combinations of backward and forward trace inclusions. Since these relations are themselves PSPACE-complete, we describe methods to compute good approximations of them in polynomial time. Extensive experiments show that the average-case complexity of our algorithm scales quadratically. The size reduction of the automata depends very much on the class of instances, but our algorithm consistently outperforms all previous techniques by a wide margin. We tested our algorithm on Buchi automata derived from LTL-formulae, many classes of random automata and automata derived from mutual exclusion protocols, and compared its performance to the well-known automata tool GOAL."
      },
      {
        "node_idx": 99964,
        "score_0_10": 10,
        "title": "on finite 1 dimensional cellular automata reversibility and semi reversibility",
        "abstract": "Reversibility of a one-dimensional finite cellular automaton (CA) is dependent on lattice size. A finite CA can be reversible for a set of lattice sizes. On the other hand, reversibility of an infinite CA, which is decided by exploring the rule only, is different in its kind from that of finite CA. Can we, however, link the reversibility of finite CA to that of infinite CA? In order to address this issue, we introduce a new notion, named semi-reversibility. We classify the CAs into three types with respect to reversibility property -- reversible, semi-reversible and strictly irreversible. A tool, reachability tree, has been used to decide the reversibility class of any CA. Finally, relation among the existing cases of reversibility is established."
      },
      {
        "node_idx": 13090,
        "score_0_10": 9,
        "title": "deludedly agreeing to agree",
        "abstract": "We study conditions relating to the impossibility of agreeing to disagree in models of interactive KD45 belief (in contrast to models of S5 knowledge, which are used in nearly all the agreements literature). We show that even when the truth axiom is not assumed it turns out that players will find it impossible to agree to disagree under fairly broad conditions."
      },
      {
        "node_idx": 30423,
        "score_0_10": 9,
        "title": "consensus game acceptors",
        "abstract": "We study a game for recognising formal languages, in which two players with imperfect information need to coordinate on a common decision, given private input strings correlated by a finite graph. The players have a joint objective to avoid an inadmissible decision, in spite of the uncertainty induced by the input."
      },
      {
        "node_idx": 114105,
        "score_0_10": 9,
        "title": "parameterized complexity results for a model of theory of mind based on dynamic epistemic logic",
        "abstract": "In this paper we introduce a computational-level model of theory of mind (ToM) based on dynamic epistemic logic (DEL), and we analyze its computational complexity. The model is a special case of DEL model checking. We provide a parameterized complexity analysis, considering several aspects of DEL (e.g., number of agents, size of preconditions, etc.) as parameters. We show that model checking for DEL is PSPACE-hard, also when restricted to single-pointed models and S5 relations, thereby solving an open problem in the literature. Our approach is aimed at formalizing current intractability claims in the cognitive science literature regarding computational models of ToM."
      },
      {
        "node_idx": 15976,
        "score_0_10": 9,
        "title": "remembering chandra kintala",
        "abstract": "With this contribution we would like to remember Chandra M. R. Kintala who passed away in November 2009. We will give short overviews of his CV and his contributions to the field of theoretical and applied computer science and, given the opportunity, will attempt to present the current state of limited nondeterminism and limited resources for machines. Finally, we will briefly touch on some research topics which hopefully will be addressed in the not so distant future."
      },
      {
        "node_idx": 149204,
        "score_0_10": 9,
        "title": "boosting moving object indexing through velocity partitioning",
        "abstract": "There have been intense research interests in moving object indexing in the past decade. However, existing work did not exploit the important property of skewed velocity distributions. In many real world scenarios, objects travel predominantly along only a few directions. Examples include vehicles on road networks, flights, people walking on the streets, etc. The search space for a query is heavily dependent on the velocity distribution of the objects grouped in the nodes of an index tree. Motivated by this observation, we propose the velocity partitioning (VP) technique, which exploits the skew in velocity distribution to speed up query processing using moving object indexes. The VP technique first identifies the \"dominant velocity axes (DVAs)\" using a combination of principal components analysis (PCA) and k-means clustering. Then, a moving object index (e.g., a TPR-tree) is created based on each DVA, using the DVA as an axis of the underlying coordinate system. An object is maintained in the index whose DVA is closest to the object's current moving direction. Thus, all the objects in an index are moving in a near 1-dimensional space instead of a 2-dimensional space. As a result, the expansion of the search space with time is greatly reduced, from a quadratic function of the maximum speed (of the objects in the search range) to a near linear function of the maximum speed. The VP technique can be applied to a wide range of moving object index structures. We have implemented the VP technique on two representative ones, the TPR*-tree and the Bx-tree. Extensive experiments validate that the VP technique consistently improves the performance of those index structures."
      }
    ]
  },
  "438": {
    "explanation": "advanced unstructured spacetime mesh generation for hyperbolic PDEs",
    "topk": [
      {
        "node_idx": 29131,
        "score_0_10": 10,
        "title": "spacetime meshing for discontinuous galerkin methods",
        "abstract": "Important applications in science and engineering, such as modeling traffic flow, seismic waves, electromagnetics, and the simulation of mechanical stresses in materials, require the high-fidelity numerical solution of hyperbolic partial differential equations (PDEs) in space and time variables. Spacetime discontinuous Galerkin (SDG) finite element methods are used to solve such PDEs arising from wave propagation phenomena. #R##N#To support an accurate and efficient solution procedure using SDG methods and to exploit the flexibility of these methods, we give a meshing algorithm to construct an unstructured simplicial spacetime mesh over an arbitrary simplicial space domain. Our algorithm is the first adaptive spacetime meshing algorithm suitable for efficient solution of nonlinear phenomena using spacetime discontinuous Galerkin finite element methods. Given a triangulated d-dimensional Euclidean space domain M (a simplicial complex) corresponding to time t = 0 and initial conditions of the underlying hyperbolic spacetime PDE, we construct an unstructured simplicial mesh of the ( d + 1)-dimensional spacetime domain \u03a9. Our algorithm uses a near-optimal number of spacetime elements, each with bounded temporal aspect ratio for any finite prefix of \u03a9. When d \u2264 2, our algorithm varies the size of spacetime elements to an a posteriori numerical estimate. Certain facets of our mesh satisfy gradient constraints that allow interleaving mesh generation with the SDG salver. Our meshing algorithm thus supports an efficient parallelizable solution strategy by SDG methods."
      },
      {
        "node_idx": 131059,
        "score_0_10": 10,
        "title": "discontinuous galerkin methods on graphics processing units for nonlinear hyperbolic conservation laws",
        "abstract": "We present a novel implementation of the modal discontinuous Galerkin (DG) method for hyperbolic conservation laws in two dimensions on graphics processing units (GPUs) using NVIDIA's Compute Unified Device Architecture (CUDA). Both flexible and highly accurate, DG methods accommodate parallel architectures well as their discontinuous nature produces element-local approximations. High performance scientific computing suits GPUs well, as these powerful, massively parallel, cost-effective devices have recently included support for double-precision floating point numbers. Computed examples for Euler equations over unstructured triangle meshes demonstrate the effectiveness of our implementation on an NVIDIA GTX 580 device. Profiling of our method reveals performance comparable to an existing nodal DG-GPU implementation for linear problems."
      },
      {
        "node_idx": 76322,
        "score_0_10": 10,
        "title": "parallelizable global conformal parameterization of simply connected surfaces via partial welding",
        "abstract": "Conformal surface parameterization is useful in graphics, imaging and visualization, with applications to texture mapping, atlas construction, registration, remeshing and so on. With the increasing capability in scanning and storing data, dense 3D surface meshes are common nowadays. While meshes with higher resolution better resemble smooth surfaces, they pose computational difficulties for the existing parameterization algorithms. In this work, we propose a novel parallelizable algorithm for computing the global conformal parameterization of simply-connected surfaces via partial welding maps. A given simply-connected surface is first partitioned into smaller subdomains. The local conformal parameterizations of all subdomains are then computed in parallel. The boundaries of the parameterized subdomains are subsequently integrated consistently using a novel technique called partial welding, which is developed based on conformal welding theory. Finally, by solving the Laplace equation for each subdomain using the updated boundary conditions, we obtain a global conformal parameterization of the given surface, with bijectivity guaranteed by quasi-conformal theory. By including additional shape constraints, our method can be easily extended to achieve disk conformal parameterization for simply-connected open surfaces and spherical conformal parameterization for genus-0 closed surfaces. Experimental results are presented to demonstrate the effectiveness of our proposed algorithm. When compared to the state-of-the-art conformal parameterization methods, our method achieves a significant improvement in both computational time and accuracy."
      },
      {
        "node_idx": 125737,
        "score_0_10": 10,
        "title": "single image dehazing through improved atmospheric light estimation",
        "abstract": "Image contrast enhancement for outdoor vision is important for smart car auxiliary transport systems. The video frames captured in poor weather conditions are often characterized by poor visibility. Most image dehazing algorithms consider to use a hard threshold assumptions or user input to estimate atmospheric light. However, the brightest pixels sometimes are objects such as car lights or streetlights, especially for smart car auxiliary transport systems. Simply using a hard threshold may cause a wrong estimation. In this paper, we propose a single optimized image dehazing method that estimates atmospheric light efficiently and removes haze through the estimation of a semi-globally adaptive filter. The enhanced images are characterized with little noise and good exposure in dark regions. The textures and edges of the processed images are also enhanced significantly."
      },
      {
        "node_idx": 43073,
        "score_0_10": 10,
        "title": "optimizing b spline surfaces for developability and paneling architectural freeform surfaces",
        "abstract": "Abstract   Motivated by applications in architecture and design, we present a novel method for increasing the developability of a B-spline surface. We use the property that the Gauss image of a developable surface is 1-dimensional and can be locally well approximated by circles. This is cast into an algorithm for thinning the Gauss image by increasing the planarity of the Gauss images of appropriate neighborhoods. A variation of the main method allows us to tackle the problem of paneling a freeform architectural surface with developable panels, in particular enforcing rotational cylindrical, rotational conical and planar panels, which are the main preferred types of developable panels in architecture due to the reduced cost of manufacturing."
      },
      {
        "node_idx": 92778,
        "score_0_10": 10,
        "title": "parallel multiphysics simulations of charged particles in microfluidic flows",
        "abstract": "The article describes parallel multiphysics simulations of charged particles in microfluidic flows with the waLBerla framework. To this end, three physical effects are coupled: rigid body dynamics, fluid flow modelled by a lattice Boltzmann algorithm, and electric potentials represented by a finite volume discretisation. For solving the finite volume discretisation for the electrostatic forces, a cell-centered multigrid algorithm is developed that conforms to the lattice Boltzmann meshes and the parallel communication structure of waLBerla. The new functionality is validated with suitable benchmark scenarios. Additionally, the parallel scaling and the numerical efficiency of the algorithms are analysed on an advanced supercomputer."
      },
      {
        "node_idx": 13468,
        "score_0_10": 10,
        "title": "boundary first flattening",
        "abstract": "A conformal flattening maps a curved surface to the plane without distorting angles\u2014such maps have become a fundamental building block for problems in geometry processing, numerical simulation, and computational design. Yet existing methods provide little direct control over the shape of the flattened domain, or else demand expensive nonlinear optimization. Boundary first flattening (BFF) is a linear method for conformal parameterization that is faster than traditional linear methods, yet provides control and quality comparable to sophisticated nonlinear schemes. The key insight is that the boundary data for many conformal mapping problems can be efficiently constructed via the Cherrier formula together with a pair of Poincare-Steklov operators; once the boundary is known, the map can be easily extended over the rest of the domain. Since computation demands only a single factorization of the real Laplace matrix, the amortized cost is about 50\u00d7 less than any previously published technique for boundary-controlled conformal flattening. As a result, BFF opens the door to real-time editing or fast optimization of high-resolution maps, with direct control over boundary length or angle. We show how this method can be used to construct maps with sharp corners, cone singularities, minimal area distortion, and uniformization over the unit disk; we also demonstrate for the first time how a surface can be conformally flattened directly onto any given target shape."
      },
      {
        "node_idx": 47473,
        "score_0_10": 9,
        "title": "a continuum theory for unstructured mesh generation in two dimensions",
        "abstract": "A continuum description of unstructured meshes in two dimensions, both for planar and curved surface domains, is proposed. The meshes described are those which, in the limit of an increasingly finer mesh (smaller cells), and away from irregular vertices, have ideally-shaped cells (squares or equilateral triangles), and can therefore be completely described by two local properties: local cell size and local edge directions. The connection between the two properties is derived by defining a Riemannian manifold whose geodesics trace the edges of the mesh. A function $\\phi$, proportional to the logarithm of the cell size, is shown to obey the Poisson equation, with localized charges corresponding to irregular vertices. The problem of finding a suitable manifold for a given domain is thus shown to exactly reduce to an Inverse Poisson problem on $\\phi$, of finding a distribution of localized charges adhering to the conditions derived for boundary alignment. Possible applications to mesh generation are discussed."
      },
      {
        "node_idx": 135624,
        "score_0_10": 9,
        "title": "duneuro a software toolbox for forward modeling in neuroscience",
        "abstract": "This paper describes duneuro, a software toolbox for forward modeling in neuroscience. Its purpose is to provide extendible and easy-to-use interfaces and enable a closer integration into existing analysis pipelines. It provides implementations of fitted and unfitted finite element methods and makes use of the Dune framework. The forward problems consist of the electroencephalography (EEG) and magnetoencephalography (MEG) forward problems. For the incorporation into existing analysis pipelines, Python and Matlab interfaces are provided. The practical use is demonstrated on a source analysis example of evoked potentials."
      },
      {
        "node_idx": 146558,
        "score_0_10": 9,
        "title": "fast disk conformal parameterization of simply connected open surfaces",
        "abstract": "Surface parameterizations have been widely used in computer graphics and geometry processing. In particular, as simply-connected open surfaces are conformally equivalent to the unit disk, it is desirable to compute the disk conformal parameterizations of the surfaces. In this paper, we propose a novel algorithm for the conformal parameterization of a simply-connected open surface onto the unit disk, which significantly speeds up the computation, enhances the conformality and stability, and guarantees the bijectivity. The conformality distortions at the inner region and on the boundary are corrected by two steps, with the aid of an iterative scheme using quasi-conformal theories. Experimental results demonstrate the effectiveness of our proposed method."
      }
    ]
  },
  "439": {
    "explanation": "deep recurrent and convolutional neural network architectures and optimization techniques",
    "topk": [
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 53950,
        "score_0_10": 9,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 67928,
        "score_0_10": 9,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 73035,
        "score_0_10": 9,
        "title": "network in network",
        "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      }
    ]
  },
  "441": {
    "explanation": "detecting fake news and misinformation on social media platforms",
    "topk": [
      {
        "node_idx": 113298,
        "score_0_10": 10,
        "title": "fake news detection on social media a data mining perspective",
        "abstract": "Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of \"fake news\", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ineffective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media."
      },
      {
        "node_idx": 109276,
        "score_0_10": 10,
        "title": "the author topic model for authors and documents",
        "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 29514,
        "score_0_10": 8,
        "title": "mtil17 english to indian langauge statistical machine translation",
        "abstract": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation."
      },
      {
        "node_idx": 92275,
        "score_0_10": 8,
        "title": "predicting the popularity of online content",
        "abstract": "We present a method for accurately predicting the long time popularity of online content from early measurements of user access. Using two content sharing portals, Youtube and Digg, we show that by modeling the accrual of views and votes on content offered by these services we can predict the long-term dynamics of individual submissions from initial data. In the case of Digg, measuring access to given stories during the first two hours allows us to forecast their popularity 30 days ahead with remarkable accuracy, while downloads of Youtube videos need to be followed for 10 days to attain the same performance. The differing time scales of the predictions are shown to be due to differences in how content is consumed on the two portals: Digg stories quickly become outdated, while Youtube videos are still found long after they are initially submitted to the portal. We show that predictions are more accurate for submissions for which attention decays quickly, whereas predictions for evergreen content will be prone to larger errors."
      },
      {
        "node_idx": 167570,
        "score_0_10": 7,
        "title": "man is to computer programmer as woman is to homemaker debiasing word embeddings",
        "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to \"debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
      },
      {
        "node_idx": 165750,
        "score_0_10": 7,
        "title": "some like it hoax automated fake news detection in social networks",
        "abstract": "In recent years, the reliability of information on the Internet has emerged as a crucial issue of modern society. Social network sites (SNSs) have revolutionized the way in which information is spread by allowing users to freely share content. As a consequence, SNSs are also increasingly used as vectors for the diffusion of misinformation and hoaxes. The amount of disseminated information and the rapidity of its diffusion make it practically impossible to assess reliability in a timely manner, highlighting the need for automatic hoax detection systems. #R##N#As a contribution towards this objective, we show that Facebook posts can be classified with high accuracy as hoaxes or non-hoaxes on the basis of the users who \"liked\" them. We present two classification techniques, one based on logistic regression, the other on a novel adaptation of boolean crowdsourcing algorithms. On a dataset consisting of 15,500 Facebook posts and 909,236 users, we obtain classification accuracies exceeding 99% even when the training set contains less than 1% of the posts. We further show that our techniques are robust: they work even when we restrict our attention to the users who like both hoax and non-hoax posts. These results suggest that mapping the diffusion pattern of information can be a useful component of automatic hoax detection systems."
      },
      {
        "node_idx": 37335,
        "score_0_10": 7,
        "title": "botornot a system to evaluate social bots",
        "abstract": "While most online social media accounts are controlled by humans, these platforms also host automated agents called social bots or sybil accounts. Recent literature reported on cases of social bots imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate the extent to which a Twitter account exhibits similarity to the known characteristics of social bots. Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs."
      },
      {
        "node_idx": 140427,
        "score_0_10": 7,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 11752,
        "score_0_10": 7,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      }
    ]
  },
  "444": {
    "explanation": "advanced signal processing and machine learning for sensing and recognition",
    "topk": [
      {
        "node_idx": 55984,
        "score_0_10": 10,
        "title": "transmit energy focusing for doa estimation in mimo radar with colocated antennas",
        "abstract": "In this paper, we propose a transmit beamspace energy focusing technique for multiple-input multiple-output (MIMO) radar with application to direction finding for multiple targets. The general angular directions of the targets are assumed to be located within a certain spatial sector. We focus the energy of multiple (two or more) transmitted orthogonal waveforms within that spatial sector using transmit beamformers which are designed to improve the signal-to-noise ratio (SNR) gain at each receive antenna. The subspace decomposition-based techniques such as MUSIC can then be used for direction finding for multiple targets. Moreover, the transmit beamformers can be designed so that matched-filtering the received data to the waveforms yields multiple (two or more) data sets with rotational invariance property that allows applying search-free direction finding techniques such as ESPRIT or parallel factor analysis (PARAFAC). Unlike previously reported MIMO radar ESPRIT/PARAFAC-based direction finding techniques, our method achieves the rotational invariance property in a different manner combined also with the transmit energy focusing. As a result, it achieves better estimation performance at lower computational cost. The corresponding Cramer-Rao bound is derived and its dependence on the number of waveforms used is discussed. Simulation results also show the superiority of the proposed technique over the existing techniques."
      },
      {
        "node_idx": 55960,
        "score_0_10": 10,
        "title": "the smartphone brain scanner a portable real time neuroimaging system",
        "abstract": "Combining low-cost wireless EEG sensors with smartphones offers novel opportunities for mobile brain imaging in an everyday context. Here we present the technical details and validation of a framework for building multi-platform, portable EEG applications with real-time 3D source reconstruction. The system \u2013 Smartphone Brain Scanner \u2013 combines an off-the-shelf neuroheadset or EEG cap with a smartphone or tablet, and as such represents the first fully portable system for real-time 3D EEG imaging. We discuss the benefits and challenges, including technical limitations as well as details of real-time reconstruction of 3D images of brain activity. We present examples of brain activity captured in a simple experiment involving imagined finger tapping, which shows that the acquired signal in a relevant brain region is similar to that obtained with standard EEG lab equipment. Although the quality of the signal in a mobile solution using an off-the-shelf consumer neuroheadset is lower than the signal obtained using high-density standard EEG equipment, we propose mobile application development may offset the disadvantages and provide completely new opportunities for neuroimaging in natural settings."
      },
      {
        "node_idx": 153677,
        "score_0_10": 10,
        "title": "from theory to practice sub nyquist sampling of sparse wideband analog signals",
        "abstract": "Conventional sub-Nyquist sampling methods for analog signals exploit prior information about the spectral support. In this paper, we consider the challenging problem of blind sub-Nyquist sampling of multiband signals, whose unknown frequency support occupies only a small portion of a wide spectrum. Our primary design goals are efficient hardware implementation and low computational load on the supporting digital processing. We propose a system, named the modulated wideband converter, which first multiplies the analog signal by a bank of periodic waveforms. The product is then low-pass filtered and sampled uniformly at a low rate, which is orders of magnitude smaller than Nyquist. Perfect recovery from the proposed samples is achieved under certain necessary and sufficient conditions. We also develop a digital architecture, which allows either reconstruction of the analog input, or processing of any band of interest at a low rate, that is, without interpolating to the high Nyquist rate. Numerical simulations demonstrate many engineering aspects: robustness to noise and mismodeling, potential hardware simplifications, real-time performance for signals with time-varying support and stability to quantization effects. We compare our system with two previous approaches: periodic nonuniform sampling, which is bandwidth limited by existing hardware devices, and the random demodulator, which is restricted to discrete multitone signals and has a high computational load. In the broader context of Nyquist sampling, our scheme has the potential to break through the bandwidth barrier of state-of-the-art analog conversion technologies such as interleaved converters."
      },
      {
        "node_idx": 152497,
        "score_0_10": 10,
        "title": "utilizing bluetooth low energy to recognize proximity touch and humans",
        "abstract": "Interacting with humans is one of the main challenges for mobile robots in a human inhabited environment. To enable adaptive behavior, a robot needs to recognize touch gestures and/or the proximity to interacting individuals. Moreover, a robot interacting with two or more humans usually needs to distinguish between them. However, this remains both a configuration and cost intensive task. In this paper we utilize inexpensive Bluetooth Low Energy (BLE) devices and propose an easy and configurable technique to enhance the robot's capabilities to interact with surrounding people. In a noisy laboratory setting, a mobile spherical robot is utilized in three proof-of-concept experiments of the proposed system architecture. Firstly, we enhance the robot with proximity information about the individuals in the surrounding environment. Secondly, we exploit BLE to utilize it as a touch sensor. And lastly, we use BLE to distinguish between interacting individuals. Results show that observing the raw received signal strength (RSS) between BLE devices already enhances the robot's interaction capabilities and that the provided infrastructure can be facilitated to enable adaptive behavior in the future. We show one and the same sensor system can be used to detect different types of information relevant in human-robot interaction (HRI) experiments."
      },
      {
        "node_idx": 133863,
        "score_0_10": 10,
        "title": "a characterization of entropy in terms of information loss",
        "abstract": "There are numerous characterizations of Shannon entropy and Tsallis entropy as measures of information obeying certain properties. Using work by Faddeev and Furuichi, we derive a very simple characterization. Instead of focusing on the entropy of a probability measure on a finite set, this characterization focuses on the \"information loss\", or change in entropy, associated with a measure-preserving function. Information loss is a special case of conditional entropy: namely, it is the entropy of a random variable conditioned on some function of that variable. We show that Shannon entropy gives the only concept of information loss that is functorial, convex-linear and continuous. This characterization naturally generalizes to Tsallis entropy as well."
      },
      {
        "node_idx": 49479,
        "score_0_10": 10,
        "title": "driver drowsiness estimation from eeg signals using online weighted adaptation regularization for regression owarr",
        "abstract": "One big challenge that hinders the transition of brain\u2013computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation from EEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS."
      },
      {
        "node_idx": 24682,
        "score_0_10": 10,
        "title": "multiple human tracking in rgb d data a survey",
        "abstract": "Multiple human tracking (MHT) is a fundamental task in many computer vision applications. Appearance-based approaches, primarily formulated on RGB data, are constrained and affected by problems arising from occlusions and/or illumination variations. In recent years, the arrival of cheap RGB-Depth (RGB-D) devices has {led} to many new approaches to MHT, and many of these integrate color and depth cues to improve each and every stage of the process. In this survey, we present the common processing pipeline of these methods and review their methodology based (a) on how they implement this pipeline and (b) on what role depth plays within each stage of it. We identify and introduce existing, publicly available, benchmark datasets and software resources that fuse color and depth data for MHT. Finally, we present a brief comparative evaluation of the performance of those works that have applied their methods to these datasets."
      },
      {
        "node_idx": 77155,
        "score_0_10": 10,
        "title": "filter banks on discrete abelian groups",
        "abstract": "In this work we provide polyphase, modulation, and frame theoretical analyses of a filter bank on a discrete abelian group. Thus, multidimensional or cyclic filter banks as well as filter banks for signals in $\\ell^2(\\mathbb{Z}^d\\times \\mathbb{Z}_s)$ or $\\ell^2(\\mathbb{Z}_r \\times \\mathbb{Z}_s)$ spaces are studied in a unified way. We obtain perfect reconstruction conditions and the corresponding frame bounds."
      },
      {
        "node_idx": 3061,
        "score_0_10": 10,
        "title": "markov brains a technical introduction",
        "abstract": "Markov Brains are a class of evolvable artificial neural networks (ANN). They differ from conventional ANNs in many aspects, but the key difference is that instead of a layered architecture, with each node performing the same function, Markov Brains are networks built from individual computational components. These computational components interact with each other, receive inputs from sensors, and control motor outputs. The function of the computational components, their connections to each other, as well as connections to sensors and motors are all subject to evolutionary optimization. Here we describe in detail how a Markov Brain works, what techniques can be used to study them, and how they can be evolved."
      },
      {
        "node_idx": 129667,
        "score_0_10": 10,
        "title": "vision based human gender recognition a survey",
        "abstract": "Gender is an important demographic attribute of people. This paper provides a survey of human gender recognition in computer vision. A review of approaches exploiting information from face and whole body (either from a still image or gait sequence) is presented. We highlight the challenges faced and survey the representative methods of these approaches. Based on the results, good performance have been achieved for datasets captured under controlled environments, but there is still much work that can be done to improve the robustness of gender recognition under real-life environments."
      }
    ]
  },
  "445": {
    "explanation": "estimation bounds and unbiasedness in constrained parameter estimation",
    "topk": [
      {
        "node_idx": 151707,
        "score_0_10": 10,
        "title": "cram acute text e r rao bound for constrained parameter estimation using lehmann unbiasedness",
        "abstract": "The constrained Cram  $\\acute{\\text{e}}$  r\u2013Rao bound (CCRB) is a lower bound on the mean-squared-error (MSE) of estimators that satisfy some unbiasedness conditions. Although the CCRB unbiasedness conditions are satisfied asymptotically by the constrained maximum likelihood (CML) estimator, in the non-asymptotic region these conditions are usually too strict and the commonly used estimators, such as the CML estimator, do not satisfy them. Therefore, the CCRB may not be a lower bound on the MSE matrix of such estimators. In this paper, we propose a new definition for unbiasedness under constraints, denoted by C-unbiasedness, which is based on using Lehmann-unbiasedness with a weighted MSE (WMSE) risk and taking into account the parametric constraints. In addition to C-unbiasedness, a Cram  $\\acute{\\text{e}}$  r\u2013Rao-type bound on the WMSE of C-unbiased estimators, denoted as Lehmann-unbiased CCRB (LU-CCRB), is derived. This bound is a scalar bound that depends on the chosen weighted combination of estimation errors. It is shown that C-unbiasedness is less restrictive than the CCRB unbiasedness conditions. Thus, the set of estimators that satisfy the CCRB unbiasedness conditions is a subset of the set of C-unbiased estimators and the proposed LU-CCRB may be an informative lower bound in cases where the corresponding CCRB is not. In the simulations, we examine linear and nonlinear estimation problems under nonlinear constraints in which the CML estimator is shown to be C-unbiased and the LU-CCRB is an informative lower bound on the WMSE, while the corresponding CCRB on the WMSE is not a lower bound and is not informative in the non-asymptotic region."
      },
      {
        "node_idx": 164990,
        "score_0_10": 10,
        "title": "multi label prediction via compressed sensing",
        "abstract": "We consider multi-label prediction problems with large output spaces under the assumption of output sparsity -- that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting."
      },
      {
        "node_idx": 124412,
        "score_0_10": 10,
        "title": "an empirical evaluation of four algorithms for multi class classification mart abc mart robust logitboost and abc logitboost",
        "abstract": "This empirical study is mainly devoted to comparing four tree-based boosting algorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for multi-class classification on a variety of publicly available datasets. Some of those datasets have been thoroughly tested in prior studies using a broad range of classification algorithms including SVM, neural nets, and deep learning. #R##N#In terms of the empirical classification errors, our experiment results demonstrate: #R##N#1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably improves (robust) logitboost. 3. Robust) logitboost} considerably improves mart on most datasets. 4. Abc-logitboost considerably improves abc-mart on most datasets. 5. These four boosting algorithms (especially abc-logitboost) outperform SVM on many datasets. 6. Compared to the best deep learning methods, these four boosting algorithms (especially abc-logitboost) are competitive."
      },
      {
        "node_idx": 69525,
        "score_0_10": 10,
        "title": "pril perceptron ranking using interval labeled data",
        "abstract": "In this paper, we propose an online learning algorithm PRIL for learning ranking classifiers using interval labeled data and show its correctness. We show its convergence in finite number of steps if there exists an ideal classifier such that the rank given by it for an example always lies in its label interval. We then generalize this mistake bound result for the general case. We also provide regret bound for the proposed algorithm. We propose a multiplicative update algorithm for PRIL called M-PRIL. We provide its correctness and convergence results. We show the effectiveness of PRIL by showing its performance on various datasets."
      },
      {
        "node_idx": 79738,
        "score_0_10": 10,
        "title": "llr based successive cancellation list decoding of polar codes",
        "abstract": "We show that successive cancellation list decoding can be formulated exclusively using log-likelihood ratios. In addition to numerical stability, the log-likelihood ratio based formulation has useful properties that simplify the sorting step involved in successive cancellation list decoding. We propose a hardware architecture of the successive cancellation list decoder in the log-likelihood ratio domain which, compared with a log-likelihood domain implementation, requires less irregular and smaller memories. This simplification, together with the gains in the metric sorter, lead to    $ 56\\%$   to   $137\\%$   higher throughput per unit area than other recently proposed architectures. We then evaluate the empirical performance of the CRC-aided successive cancellation list decoder at different list sizes using different CRCs and conclude that it is important to adapt the CRC length to the list size in order to achieve the best error-rate performance of concatenated polar codes. Finally, we synthesize conventional successive cancellation decoders at large block-lengths with the same block-error probability as our proposed CRC-aided successive cancellation list decoders to demonstrate that, while our decoders have slightly lower throughput and larger area, they have a significantly smaller decoding latency."
      },
      {
        "node_idx": 147629,
        "score_0_10": 10,
        "title": "assessing the performance of leja and clenshaw curtis collocation for computational electromagnetics with random input data",
        "abstract": "We consider the problem of quantifying uncertainty regarding the output of an electromagnetic field problem in the presence of a large number of uncertain input parameters. In order to reduce the growth in complexity with the number of dimensions, we employ a dimension-adaptive stochastic collocation method based on nested univariate nodes. We examine the accuracy and performance of collocation schemes based on Clenshaw-Curtis and Leja rules, for the cases of uniform and bounded, non-uniform random inputs, respectively. Based on numerical experiments with an academic electromagnetic field model, we compare the two rules in both the univariate and multivariate case and for both quadrature and interpolation purposes. Results for a real-world electromagnetic field application featuring high-dimensional input uncertainty are also presented."
      },
      {
        "node_idx": 15986,
        "score_0_10": 10,
        "title": "fast polar decoders algorithm and implementation",
        "abstract": "Polar codes provably achieve the symmetric capacity of a memoryless channel while having an explicit construction. The adoption of polar codes however, has been hampered by the low throughput of their decoding algorithm. This work aims to increase the throughput of polar decoding hardware by an order of magnitude relative to successive-cancellation decoders and is more than 8 times faster than the current fastest polar decoder. We present an algorithm, architecture, and FPGA implementation of a flexible, gigabit-per-second polar decoder."
      },
      {
        "node_idx": 50467,
        "score_0_10": 10,
        "title": "a framework for automated pde constrained optimisation",
        "abstract": "A generic framework for the solution of PDE-constrained optimisation problems based on the FEniCS system is presented. Its main features are an intuitive mathematical interface, a high degree of automation, and an efficient implementation of the generated adjoint model. The framework is based upon the extension of a domain-specific language for variational problems to cleanly express complex optimisation problems in a compact, high-level syntax. For example, optimisation problems constrained by the time-dependent Navier-Stokes equations can be written in tens of lines of code. Based on this high-level representation, the framework derives the associated adjoint equations in the same domain-specific language, and uses the FEniCS code generation technology to emit parallel optimised low-level C++ code for the solution of the forward and adjoint systems. The functional and gradient information so computed is then passed to the optimisation algorithm to update the parameter values. This approach works both for steady-state as well as transient, and for linear as well as nonlinear governing PDEs and a wide range of functionals and control parameters. We demonstrate the applicability and efficiency of this approach on classical textbook optimisation problems and advanced examples."
      },
      {
        "node_idx": 69868,
        "score_0_10": 10,
        "title": "an adaptive successive cancellation list decoder for polar codes with cyclic redundancy check",
        "abstract": "In this letter, we propose an adaptive SC (Successive Cancellation)-List decoder for polar codes with CRC. This adaptive SC-List decoder iteratively increases the list size until the decoder outputs contain at least one survival path which can pass CRC. Simulation shows that the adaptive SC-List decoder provides significant complexity reduction. We also demonstrate that polar code (2048, 1024) with 24-bit CRC decoded by our proposed adaptive SC-List decoder with very large list size can achieve a frame error rate FER=0.001 at Eb/No=1.1dB, which is about 0.2dB from the information theoretic limit at this block length."
      },
      {
        "node_idx": 90344,
        "score_0_10": 9,
        "title": "continuous time quantum consensus quantum synchronisation",
        "abstract": "Distributed consensus algorithm over networks of quantum systems has been the focus of recent studies in the context of quantum computing and distributed control. Most of the progress in this category have been on the convergence conditions and optimizing the convergence rate of the algorithm, for quantum networks with undirected underlying topology. This paper aims to address the extension of this problem over quantum networks with directed underlying graphs. In doing so, the convergence to two different stable states namely, consensus and synchronous states have been studied. Based on the intertwining relation between the eigenvalues, it is shown that for determining the convergence rate to the consensus state, all induced graphs should be considered while for the synchronous state only the underlying graph suffices. Furthermore, it is illustrated that for the range of weights that the Aldous' conjecture holds true, the convergence rate to both states are equal. Using the Pareto region for convergence rates of the algorithm, the global and Pareto optimal points for several topologies have been provided."
      }
    ]
  },
  "448": {
    "explanation": "spatio-temporal video action recognition and adaptive learning methods",
    "topk": [
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 55399,
        "score_0_10": 9,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 21564,
        "score_0_10": 9,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 106114,
        "score_0_10": 9,
        "title": "matching networks for one shot learning",
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
      },
      {
        "node_idx": 54016,
        "score_0_10": 9,
        "title": "prototypical networks for few shot learning",
        "abstract": "We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      }
    ]
  },
  "449": {
    "explanation": "multimodal visual-text alignment and localization for image understanding",
    "topk": [
      {
        "node_idx": 130623,
        "score_0_10": 10,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 69942,
        "score_0_10": 10,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 77301,
        "score_0_10": 9,
        "title": "grad cam visual explanations from deep networks via gradient based localization",
        "abstract": "We propose a technique for producing \u2018visual explanations\u2019 for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach\u2014Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say \u2018dog\u2019 in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265\u2013290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E."
      },
      {
        "node_idx": 21714,
        "score_0_10": 9,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 123935,
        "score_0_10": 9,
        "title": "bottom up and top down attention for image captioning and visual question answering",
        "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      }
    ]
  },
  "451": {
    "explanation": "probabilistic inference and signal processing methods",
    "topk": [
      {
        "node_idx": 45941,
        "score_0_10": 10,
        "title": "possibilistic conditioning and propagation",
        "abstract": "We give an axiomatization of confidence transfer - a known conditioning scheme - from the perspective of expectation-based inference in the sense of Gardenfors and Makinson. Then, we use the notion of belief independence to \"filter out\" different proposal s of possibilistic conditioning rules, all are variations of confidence transfer. Among the three rules that we consider, only Dempster's rule of conditioning passes the test of supporting the notion of belief independence. With the use of this conditioning rule, we then show that we can use local computation for computing desired conditional marginal possibilities of the joint possibility satisfying the given constraints. It turns out that our local computation scheme is already proposed by Shenoy. However, our intuitions are completely different from that of Shenoy. While Shenoy just defines a local computation scheme that fits his framework of valuation-based systems, we derive that local computation scheme from II(,8) = tI(,8 I a) * II(a) and appropriate independence assumptions, just like how the Bayesians derive their local computation scheme."
      },
      {
        "node_idx": 165239,
        "score_0_10": 10,
        "title": "scalable and reliable multi dimensional aggregation of sensor data streams",
        "abstract": "Ever-increasing amounts of data and requirements to process them in real time lead to more and more analytics platforms and software systems being designed according to the concept of stream processing. A common area of application is the processing of continuous data streams from sensors, for example, IoT devices or performance monitoring tools. In addition to analyzing pure sensor data, analyses of data for groups of sensors often need to be performed as well. Therefore, data streams of the individual sensors have to be continuously aggregated to a data stream for a group. Motivated by a real-world application scenario, we propose that such a stream aggregation approach has to allow for aggregating sensors in hierarchical groups, support multiple such hierarchies in parallel, provide reconfiguration at runtime, and preserve the scalability and reliability qualities induced by applying stream processing techniques. We propose a stream processing architecture fulfilling these requirements, which can be integrated into existing big data architectures. We present a pilot implementation of such an extended architecture and show how it is used in industry. Furthermore, in experimental evaluations we show that our solution scales linearly with the amount of sensors and provides adequate reliability in the case of faults."
      },
      {
        "node_idx": 141503,
        "score_0_10": 10,
        "title": "holographic image sensing",
        "abstract": "Holographic representations of data enable distributed storage with progressive refinement when the stored packets of data are made available in any arbitrary order. In this paper, we propose and test patch-based transform coding holographic sensing of image data. Our proposal is optimized for progressive recovery under random order of retrieval of the stored data. The coding of the image patches relies on the design of distributed projections ensuring best image recovery, in terms of the $\\ell_2$ norm, at each retrieval stage. The performance depends only on the number of data packets that has been retrieved thus far. #R##N#Several possible options to enhance the quality of the recovery while changing the size and number of data packets are discussed and tested. This leads us to examine several interesting bit-allocation and rate-distortion trade offs, highlighted for a set of natural images with ensemble estimated statistical properties."
      },
      {
        "node_idx": 146266,
        "score_0_10": 10,
        "title": "holographic sensing",
        "abstract": "Holographic representations of data encode information in packets of equal importance that enable progressive recovery. The quality of recovered data improves as more and more packets become available. This progressive recovery of the information is independent of the order in which packets become available. Such representations are ideally suited for distributed storage and for the transmission of data packets over networks with unpredictable delays and or erasures.  Several methods for holographic representations of signals and images have been proposed over the years and multiple description information theory also deals with such representations. Surprisingly, however, these methods had not been considered in the classical framework of optimal least-squares estimation theory, until very recently. We develop a least-squares approach to the design of holographic representation for stochastic data vectors, relying on the framework widely used in modeling signals and images."
      },
      {
        "node_idx": 77683,
        "score_0_10": 10,
        "title": "real time single image and video super resolution using an efficient sub pixel convolutional neural network",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
      },
      {
        "node_idx": 90952,
        "score_0_10": 9,
        "title": "5g ultra dense cellular networks",
        "abstract": "Traditional ultra-dense wireless networks are recommended as a complement for cellular networks and are deployed in partial areas, such as hotspot and indoor scenarios. Based on the massive multiple-input multi-output (MIMO) antennas and the millimeter wavecommunication technologies, the 5G ultra-dense cellular network is proposed to deploy in overall cellular scenarios. Moreover, a distribution network architecture is presented for 5G ultra-dense cellular networks. Furthermore, the backhaul network capacity and the backhaul energy efficiency of ultra-dense cellular networks are investigated to answer an important question, i.e., how much densification can be deployed for 5G ultra-dense cellular networks. Simulation results reveal that there exist densification limits for 5G ultra-dense cellualr networks with backhaul network capacity and backhaul energy efficiency constraints."
      },
      {
        "node_idx": 61221,
        "score_0_10": 9,
        "title": "point set registration coherent point drift",
        "abstract": "Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods."
      },
      {
        "node_idx": 153214,
        "score_0_10": 9,
        "title": "belief and surprise a belief function formulation",
        "abstract": "We motivate and describe a theory of belief in this paper. This theory is developed with the following view of human belief in mind. Consider the belief that an event E will occur (or has occurred or is occurring). An agent either entertains this belief or does not entertain this belief (i.e., there is no \"grade\" in entertaining the belief). If the agent chooses to exercise \"the will to believe\" and entertain this belief, he/she/it is entitled to a degree of confidence c (1 > c > 0) in doing so. Adopting this view of human belief, we conjecture that whenever an agent entertains the belief that E will occur with c degree of confidence, the agent will be surprised (to the extent c) upon realizing that E did not occur."
      },
      {
        "node_idx": 55272,
        "score_0_10": 9,
        "title": "image super resolution using deep convolutional networks",
        "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."
      },
      {
        "node_idx": 43948,
        "score_0_10": 9,
        "title": "multi gateway cooperation in multibeam satellite systems",
        "abstract": "Multibeam systems with hundreds of beams have been recently deployed in order to provide higher capacities by employing fractional frequency reuse. Furthermore, employing full frequency reuse and precoding over multiple beams has shown great throughput potential in literature. However, feeding all this data from a single gateway is not feasible based on the current frequency allocations. In this context, we investigate a range of scenarios involving beam clusters where each cluster is managed by a single gateway. More specifically, the following cases are considered for handling intercluster interference: a) conventional frequency colouring, b) joint processing within cluster, c) partial CSI sharing among clusters, d) partial CSI and data sharing among clusters. CSI sharing does not provide considerable performance gains with respect to b) but combined with data sharing offers roughly a 40% improvement over a) and a 15% over b)."
      }
    ]
  },
  "452": {
    "explanation": "techniques for robust image filtering and object tracking in varied environments",
    "topk": [
      {
        "node_idx": 114165,
        "score_0_10": 10,
        "title": "robust guided image filtering",
        "abstract": "The process of using one image to guide the filtering process of another one is called Guided Image Filtering (GIF). The main challenge of GIF is the structure inconsistency between the guidance image and the target image. Besides, noise in the target image is also a challenging issue especially when it is heavy. In this paper, we propose a general framework for Robust Guided Image Filtering (RGIF), which contains a data term and a smoothness term, to solve the two issues mentioned above. The data term makes our model simultaneously denoise the target image and perform GIF which is robust against the heavy noise. The smoothness term is able to make use of the property of both the guidance image and the target image which is robust against the structure inconsistency. While the resulting model is highly non-convex, it can be solved through the proposed Iteratively Re-weighted Least Squares (IRLS) in an efficient manner. For challenging applications such as guided depth map upsampling, we further develop a data-driven parameter optimization scheme to properly determine the parameter in our model. This optimization scheme can help to preserve small structures and sharp depth edges even for a large upsampling factor (8x for example). Moreover, the specially designed structure of the data term and the smoothness term makes our model perform well in edge-preserving smoothing for single-image tasks (i.e., the guidance image is the target image itself). This paper is an extension of our previous work [1], [2]."
      },
      {
        "node_idx": 121750,
        "score_0_10": 10,
        "title": "meta des",
        "abstract": "Dynamic ensemble selection systems work by estimating the level of competence of each classifier from a pool of classifiers. Only the most competent ones are selected to classify a given test sample. This is achieved by defining a criterion to measure the level of competence of a base classifier, such as, its accuracy in local regions of the feature space around the query instance. However, using only one criterion about the behavior of a base classifier is not sufficient to accurately estimate its level of competence. In this paper, we present a novel dynamic ensemble selection framework using meta-learning. We propose five distinct sets of meta-features, each one corresponding to a different criterion to measure the level of competence of a classifier for the classification of input samples. The meta-features are extracted from the training data and used to train a meta-classifier to predict whether or not a base classifier is competent enough to classify an input instance. During the generalization phase, the meta-features are extracted from the query instance and passed down as input to the meta-classifier. The meta-classifier estimates, whether a base classifier is competent enough to be added to the ensemble. Experiments are conducted over several small sample size classification problems, i.e., problems with a high degree of uncertainty due to the lack of training data. Experimental results show that the proposed meta-learning framework greatly improves classification accuracy when compared against current state-of-the-art dynamic ensemble selection techniques. HighlightsWe propose a novel dynamic ensemble selection framework using meta-learning.We present five sets of meta-features to measure the competence of a classifier.Results demonstrate the proposed framework outperforms current techniques."
      },
      {
        "node_idx": 153811,
        "score_0_10": 10,
        "title": "pruning filters for efficient convnets",
        "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
      },
      {
        "node_idx": 126739,
        "score_0_10": 10,
        "title": "gaussian processes for analyzing positioned trajectories in sports",
        "abstract": "Kernel-based machine learning approaches are gaining increasing interest for exploring and modeling large dataset in recent years. Gaussian process (GP) is one example of such kernel-based approaches, which can provide very good performance for nonlinear modeling problems. In this work, we first propose a grey-box modeling approach to analyze the forces in cross country skiing races. To be more precise, a disciplined set of kinetic motion model formulae is combined with data-driven Gaussian process regression model, which accounts for everything unknown in the system. Then, a modeling approach is proposed to analyze the kinetic flow of both individual and clusters of skiers. The proposed approaches can be generally applied to use cases where positioned trajectories and kinetic measurements are available. The proposed approaches are evaluated using data collected from the Falun Nordic World Ski Championships 2015, in particular the Men's cross country $4\\times10$ km relay. Forces during the cross country skiing races are analyzed and compared. Velocity models for skiers at different competition stages are also evaluated. Finally, the comparisons between the grey-box and black-box approach are carried out, where the grey-box approach can reduce the predictive uncertainty by $30\\%$ to $40\\%$."
      },
      {
        "node_idx": 63359,
        "score_0_10": 10,
        "title": "allocation and admission policies for service streams",
        "abstract": "A service provisioning system is examined, where a number of servers are used to offer different types of services to paying customers. A customer is charged for the execution of a stream of jobs; the number of jobs in the stream and the rate of their submission is specified. On the other hand, the provider promises a certain quality of service (QoS), measured by the average waiting time of the jobs in the stream. A penalty is paid if the agreed QoS requirement is not met. The objective is to maximize the total average revenue per unit time. Dynamic policies for making server allocation and stream admission decisions are introduced and evaluated. The results of several simulations are described."
      },
      {
        "node_idx": 126533,
        "score_0_10": 10,
        "title": "repairing people trajectories based on point clustering",
        "abstract": "This paper presents a method for improving any object tracking algorithm based on machine learning. During the training phase, important trajectory features are extracted which are then used to calculate a confidence value of trajectory. The positions at which objects are usually lost and found are clustered in order to construct the set of 'lost zones' and 'found zones' in the scene. Using these zones, we construct a triplet set of zones i.e. three zones: In/Out zone (zone where an object can enter or exit the scene), 'lost zone' and 'found zone'. Thanks to these triplets, during the testing phase, we can repair the erroneous trajectories according to which triplet they are most likely to belong to. The advantage of our approach over the existing state of the art approaches is that (i) this method does not depend on a predefined contextual scene, (ii) we exploit the semantic of the scene and (iii) we have proposed a method to filter out noisy trajectories based on their confidence value."
      },
      {
        "node_idx": 82939,
        "score_0_10": 10,
        "title": "robust mobile object tracking based on multiple feature similarity and trajectory filtering",
        "abstract": "This paper presents a new algorithm to track mobile objects in different scene conditions. The main idea of the proposed tracker includes estimation, multi-features similarity measures and trajectory filtering. A feature set (distance, area, shape ratio, color histogram) is defined for each tracked object to search for the best matching object. Its best matching object and its state estimated by the Kalman filter are combined to update position and size of the tracked object. However, the mobile object trajectories are usually fragmented because of occlusions and misdetections. Therefore, we also propose a trajectory filtering, named global tracker, aims at removing the noisy trajectories and fusing the fragmented trajectories belonging to a same mobile object. The method has been tested with five videos of different scene conditions. Three of them are provided by the ETISEO benchmarking project (this http URL) in which the proposed tracker performance has been compared with other seven tracking algorithms. The advantages of our approach over the existing state of the art ones are: (i) no prior knowledge information is required (e.g. no calibration and no contextual models are needed), (ii) the tracker is more reliable by combining multiple feature similarities, (iii) the tracker can perform in different scene conditions: single/several mobile objects, weak/strong illumination, indoor/outdoor scenes, (iv) a trajectory filtering is defined and applied to improve the tracker performance, (v) the tracker performance outperforms many algorithms of the state of the art."
      },
      {
        "node_idx": 63267,
        "score_0_10": 10,
        "title": "prostate biopsy assistance system with gland deformation estimation for enhanced precision",
        "abstract": "Computer-assisted prostate biopsies became a very active research area during the last years. Prostate tracking makes it possi- ble to overcome several drawbacks of the current standard transrectal ultrasound (TRUS) biopsy procedure, namely the insufficient targeting accuracy which may lead to a biopsy distribution of poor quality, the very approximate knowledge about the actual location of the sampled tissues which makes it difficult to implement focal therapy strategies based on biopsy results, and finally the difficulty to precisely reach non-ultrasound (US) targets stemming from different modalities, statistical atlases or previous biopsy series. The prostate tracking systems presented so far are limited to rigid transformation tracking. However, the gland can get considerably deformed during the intervention because of US probe pres- sure and patient movements. We propose to use 3D US combined with image-based elastic registration to estimate these deformations. A fast elastic registration algorithm that copes with the frequently occurring US shadows is presented. A patient cohort study was performed, which yielded a statistically significant in-vivo accuracy of 0.83+-0.54mm."
      },
      {
        "node_idx": 132295,
        "score_0_10": 9,
        "title": "suod toward scalable unsupervised outlier detection",
        "abstract": "Outlier detection is a key field of machine learning for identifying abnormal data objects. Due to the high expense of acquiring ground truth, unsupervised models are often chosen in practice. To compensate for the unstable nature of unsupervised algorithms, practitioners from high-stakes fields like finance, health, and security, prefer to build a large number of models for further combination and analysis. However, this poses scalability challenges in high-dimensional large datasets. In this study, we propose a three-module acceleration framework called SUOD to expedite the training and prediction with a large number of unsupervised detection models. SUOD's Random Projection module can generate lower subspaces for high-dimensional datasets while reserving their distance relationship. Balanced Parallel Scheduling module can forecast the training and prediction cost of models with high confidence---so the task scheduler could assign nearly equal amount of taskload among workers for efficient parallelization. SUOD also comes with a Pseudo-supervised Approximation module, which can approximate fitted unsupervised models by lower time complexity supervised regressors for fast prediction on unseen data. It may be considered as an unsupervised model knowledge distillation process. Notably, all three modules are independent with great flexibility to \"mix and match\"; a combination of modules can be chosen based on use cases. Extensive experiments on more than 30 benchmark datasets have shown the efficacy of SUOD, and a comprehensive future development plan is also presented."
      },
      {
        "node_idx": 48358,
        "score_0_10": 9,
        "title": "aiwc opencl based architecture independent workload characterisation",
        "abstract": "Measuring performance-critical characteristics of application workloads is important both for developers, who must understand and optimize the performance of codes, as well as designers and integrators of HPC systems, who must ensure that compute architectures are suitable for the intended workloads. However, if these workload characteristics are tied to architectural features that are specific to a particular system, they may not generalize well to alternative or future systems. An architecture-independent method ensures an accurate characterization of inherent program behaviour, without bias due to architecture-dependent features that vary widely between different types of accelerators. This work presents the first architecture- independent workload characterization framework for heterogeneous compute platforms, proposing a set of metrics determining the suitability and performance of an application on any parallel HPC architecture. The tool, AIWC, is a plugin for the open-source Oclgrind simulator. It supports parallel workloads and is capable of characterizing OpenCL codes currently in use in the supercomputing setting. AIWC simulates an OpenCL device by directly interpreting LLVM instructions, and the resulting metrics may be used for performance prediction and developer feedback to guide device-specific optimizations. An evaluation of the metrics collected over a subset of the Extended OpenDwarfs Benchmark Suite is also presented."
      }
    ]
  },
  "454": {
    "explanation": "cloud service selection automation and decision support techniques",
    "topk": [
      {
        "node_idx": 137663,
        "score_0_10": 10,
        "title": "investigating decision support techniques for automating cloud service selection",
        "abstract": "The compass of Cloud infrastructure services advances steadily leaving users in the agony of choice. To be able to select the best mix of service offering from an abundance of possibilities, users must consider complex dependencies and heterogeneous sets of criteria. Therefore, we present a PhD thesis proposal on investigating an intelligent decision support system for selecting Cloud-based infrastructure services (e.g. storage, network, CPU). The outcomes of this will be decision support tools and techniques, which will automate and map users' specified application requirements to Cloud service configurations."
      },
      {
        "node_idx": 20337,
        "score_0_10": 10,
        "title": "detection of malicious and low throughput data exfiltration over the dns protocol",
        "abstract": "In the presence of security countermeasures, a malware designed for data exfiltration must do so using a covert channel to achieve its goal. Among existing covert channels stands the domain name system (DNS) protocol. Although the detection of covert channels over the DNS has been thoroughly studied in the last decade, previous research dealt with a specific subclass of covert channels, namely DNS tunneling. While the importance of tunneling detection is not undermined, an entire class of low throughput DNS exfiltration malware remained overlooked. The goal of this study is to propose a method for detecting both tunneling and low-throughput data exfiltration over the DNS. Towards this end, we propose a solution composed of a supervised feature selection method, and an interchangeable, and adjustable anomaly detection model trained on legitimate traffic. In the first step, a one-class classifier is applied for detecting domain-specific traffic that does not conform with the normal behavior. Then, in the second step, in order to reduce the false positive rate resulting from the attempt to detect the low-throughput data exfiltration we apply a rule-based filter that filters data exchange over DNS used by legitimate services. Our solution was evaluated on a medium-scale recursive DNS server logs, and involved more than 75,000 legitimate uses and almost 2,000 attacks. Evaluation results shows that while DNS tunneling is covered with at least 99% recall rate and less than 0.01% false positive rate, the detection of low throughput exfiltration is more difficult. While not preventing it completely, our solution limits a malware attempting to avoid detection with at most a 1kb/h of payload under the limitations of the DNS syntax (equivalent to five credit cards details, or ten user credentials per hour) which reduces the effectiveness of the attack."
      },
      {
        "node_idx": 57558,
        "score_0_10": 10,
        "title": "spectre attacks exploiting speculative execution",
        "abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. #R##N#Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. #R##N#While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak."
      },
      {
        "node_idx": 46585,
        "score_0_10": 10,
        "title": "the anatomy of the facebook social graph",
        "abstract": "We study the structure of the social graph of active Facebook users, the largest social network ever analyzed. We compute numerous features of the graph including the number of users and friendships, the degree distribution, path lengths, clustering, and mixing patterns. Our results center around three main observations. First, we characterize the global structure of the graph, determining that the social network is nearly fully connected, with 99.91% of individuals belonging to a single large connected component, and we confirm the \"six degrees of separation\" phenomenon on a global scale. Second, by studying the average local clustering coefficient and degeneracy of graph neighborhoods, we show that while the Facebook graph as a whole is clearly sparse, the graph neighborhoods of users contain surprisingly dense structure. Third, we characterize the assortativity patterns present in the graph by studying the basic demographic and network properties of users. We observe clear degree assortativity and characterize the extent to which \"your friends have more friends than you\". Furthermore, we observe a strong effect of age on friendship preferences as well as a globally modular community structure driven by nationality, but we do not find any strong gender homophily. We compare our results with those from smaller social networks and find mostly, but not entirely, agreement on common structural network characteristics."
      },
      {
        "node_idx": 20703,
        "score_0_10": 10,
        "title": "modeling bitcoin contracts by timed automata",
        "abstract": "Bitcoin is a peer-to-peer cryptographic currency system. Since its introduction in 2008, Bitcoin has gained noticeable popularity, mostly due to its following properties: (1) the transaction fees are very low, and (2) it is not controlled by any central authority, which in particular means that nobody can \"print\" the money to generate inflation. Moreover, the transaction syntax allows to create the so-called contracts, where a number of mutually-distrusting parties engage in a protocol to jointly perform some financial task, and the fairness of this process is guaranteed by the properties of Bitcoin. Although the Bitcoin contracts have several potential applications in the digital economy, so far they have not been widely used in real life. This is partly due to the fact that they are cumbersome to create and analyze, and hence risky to use. #R##N#In this paper we propose to remedy this problem by using the methods originally developed for the computer-aided analysis for hardware and software systems, in particular those based on the timed automata. More concretely, we propose a framework for modeling the Bitcoin contracts using the timed automata in the UPPAAL model checker. Our method is general and can be used to model several contracts. As a proof-of-concept we use this framework to model some of the Bitcoin contracts from our recent previous work. We then automatically verify their security in UPPAAL, finding (and correcting) some subtle errors that were difficult to spot by the manual analysis. We hope that our work can draw the attention of the researchers working on formal modeling to the problem of the Bitcoin contract verification, and spark off more research on this topic."
      },
      {
        "node_idx": 6965,
        "score_0_10": 10,
        "title": "modelling bitcoin in agda",
        "abstract": "We present two models of the block chain of Bitcoin in the interactive theorem prover Agda. The first one is based on a simple model of bank accounts, while having transactions with multiple inputs and outputs. The second model models transactions, which refer directly to unspent transaction outputs, rather than user accounts. The resulting blockchain gives rise to a transaction tree. That model is formalised using an extended form of induction-recursion, one of the unique features of Agda. The set of transaction trees and transactions is defined inductively, while simultaneously recursively defining the list of unspent transaction outputs. Both structures model standard transactions, coinbase transactions, transaction fees, the exact message to be signed by those spending money in a transaction, block rewards, blocks, and the blockchain, and the second structure models as well maturation time for coinbase transactions and Merkle trees. Hashing and cryptographic operations and their correctness are dealt with abstractly by postulating corresponding operations. An indication is given how the correctness of this model could be specified and proven in Agda."
      },
      {
        "node_idx": 81798,
        "score_0_10": 10,
        "title": "a countrywide traffic accident dataset",
        "abstract": "Reducing traffic accidents is an important public safety challenge. However, the majority of studies on traffic accident analysis and prediction have used small-scale datasets with limited coverage, which limits their impact and applicability; and existing large-scale datasets are either private, old, or do not include important contextual information such as environmental stimuli (weather, points-of-interest, etc.). In order to help the research community address these shortcomings we have - through a comprehensive process of data collection, integration, and augmentation - created a large-scale publicly available database of accident information named US-Accidents. US-Accidents currently contains data about $2.25$ million instances of traffic accidents that took place within the contiguous United States, and over the last three years. Each accident record consists of a variety of intrinsic and contextual attributes such as location, time, natural language description, weather, period-of-day, and points-of-interest. We present this dataset in this paper, along with a wide range of insights gleaned from this dataset with respect to the spatiotemporal characteristics of accidents. The dataset is publicly available at this https URL."
      },
      {
        "node_idx": 35005,
        "score_0_10": 10,
        "title": "twitter sentiment analysis system",
        "abstract": "Social media is increasingly used by humans to express their feelings and opinions in the form of short text messages. Detecting sentiments in the text has a wide range of applications including identifying anxiety or depression of individuals and measuring well-being or mood of a community. Sentiments can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Sentiment Analysis in text documents is essentially a content-based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning. In this paper, sentiment recognition based on textual data and the techniques used in sentiment analysis are discussed."
      },
      {
        "node_idx": 87088,
        "score_0_10": 10,
        "title": "a declarative recommender system for cloud infrastructure services selection",
        "abstract": "The cloud infrastructure services landscape advances steadily leaving users in the agony of choice..."
      },
      {
        "node_idx": 36666,
        "score_0_10": 10,
        "title": "the role of social networks in information diffusion",
        "abstract": "Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these technologies on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed."
      }
    ]
  },
  "455": {
    "explanation": "game-theoretic equilibrium analysis in economic and network models",
    "topk": [
      {
        "node_idx": 93242,
        "score_0_10": 10,
        "title": "nonsmooth aggregative games with coupling constraints and infinitely many classes of players",
        "abstract": "After defining a pure-action profile in a nonatomic aggregative game, where players have specific compact convex pure-action sets and nonsmooth convex cost functions, as a square-integrable function, we characterize a Wardrop equilibrium as a solution to an infinite-dimensional generalized variational inequality. We show the existence of Wardrop equilibrium and variational Wardrop equilibrium, a concept of equilibrium adapted to the presence of coupling constraints, in monotone nonatomic aggregative games. The uniqueness of (variational) Wardrop equilibrium is proved for strictly or aggregatively strictly monotone nonatomic aggregative games. We then show that, for a sequence of finite-player aggregative games with aggregative constraints, if the players' pure-action sets converge to those of a strongly (resp. aggregatively strongly) monotone nonatomic aggregative game, and the aggregative constraints in the finite-player games converge to the aggregative constraint of the nonatomic game, then a sequence of so-called variational Nash equilibria in these finite-player games converge to the variational Wardrop equilibrium in pure-action profile (resp. aggregate-action profile). In particular, it allows the construction of an auxiliary sequence of games with finite-dimensional equilibria to approximate the infinite-dimensional equilibrium in such a nonatomic game. Finally, we show how to construct auxiliary finite-player games for two general classes of nonatomic games."
      },
      {
        "node_idx": 31377,
        "score_0_10": 10,
        "title": "competitive contagion in networks",
        "abstract": "We develop a game-theoretic framework for the study of competition between firms who have budgets to \"seed\" the initial adoption of their products by consumers located in a social network. The payoffs to the firms are the eventual number of adoptions of their product through a competitive stochastic diffusion process in the network. This framework yields a rich class of competitive strategies, which depend in subtle ways on the stochastic dynamics of adoption, the relative budgets of the players, and the underlying structure of the social network. #R##N#We identify a general property of the adoption dynamics --- namely, decreasing returns to local adoption --- for which the inefficiency of resource use at equilibrium (the Price of Anarchy) is uniformly bounded above, across all networks. We also show that if this property is violated the Price of Anarchy can be unbounded, thus yielding sharp threshold behavior for a broad class of dynamics. #R##N#We also introduce a new notion, the Budget Multiplier, that measures the extent that imbalances in player budgets can be amplified at equilibrium. We again identify a general property of the adoption dynamics --- namely, proportional local adoption between competitors --- for which the (pure strategy) Budget Multiplier is uniformly bounded above, across all networks. We show that a violation of this property can lead to unbounded Budget Multiplier, again yielding sharp threshold behavior for a broad class of dynamics."
      },
      {
        "node_idx": 5574,
        "score_0_10": 10,
        "title": "nash equilibria for quadratic voting",
        "abstract": "Voters making a binary decision purchase votes from a centralized clearing house, paying the square of the number of votes purchased. The net payoff to an agent with utility $u$ who purchases $v$ votes is $\\Psi (S_{n+1})u-v^{2}$, where $\\Psi$ is a monotone function taking values between -1 and +1 and $S_{n+1}$ is the sum of all votes purchased by the $n+1$ voters participating in the election. The utilities of the voters are assumed to arise by random sampling from a probability distribution $F_{U}$ with compact support; each voter knows her own utility, but not those of the other voters, although she does know the sampling distribution $F_{U}$. Nash equilibria for this game are described. These results imply that the expected inefficiency of any Nash equilibrium decays like $1/n$."
      },
      {
        "node_idx": 149351,
        "score_0_10": 10,
        "title": "the incentive ratio in exchange economies",
        "abstract": "The incentive ratio measures the utility gains from strategic behaviour. Without any restrictions on the setup, ratios for linear, Leontief and Cobb-Douglas exchange markets are unbounded, showing that manipulating the equilibrium is a worthwhile endeavour, even if it is computationally challenging. Such unbounded improvements can be achieved even if agents only misreport their utility functions. This provides a sharp contrast with previous results from Fisher markets. When the Cobb-Douglas setup is more restrictive, the maximum utility gain is bounded by the number of commodities. By means of an example, we show that it is possible to exceed a known upper bound for Fisher markets in exchange economies."
      },
      {
        "node_idx": 146524,
        "score_0_10": 10,
        "title": "on the performance of approximate equilibria in congestion games",
        "abstract": "We study the performance of approximate Nash equilibria for linear congestion games. We consider how much the price of anarchy worsens and how much the price of stability improves as a function of the approximation factor $\\epsilon$. We give (almost) tight upper and lower bounds for both the price of anarchy and the price of stability for atomic and non-atomic congestion games. Our results not only encompass and generalize the existing results of exact equilibria to $\\epsilon$-Nash equilibria, but they also provide a unified approach which reveals the common threads of the atomic and non-atomic price of anarchy results. By expanding the spectrum, we also cast the existing results in a new light. For example, the Pigou network, which gives tight results for exact Nash equilibria of selfish routing, remains tight for the price of stability of $\\epsilon$-Nash equilibria but not for the price of anarchy."
      },
      {
        "node_idx": 157174,
        "score_0_10": 10,
        "title": "stackelberg network pricing games",
        "abstract": "We study a multi-player one-round game termed Stackelberg Network Pricing Game, in which a leader can set prices for a subset of $m$ priceable edges in a graph. The other edges have a fixed cost. Based on the leader's decision one or more followers optimize a polynomial-time solvable combinatorial minimization problem and choose a minimum cost solution satisfying their requirements based on the fixed costs and the leader's prices. The leader receives as revenue the total amount of prices paid by the followers for priceable edges in their solutions, and the problem is to find revenue maximizing prices. Our model extends several known pricing problems, including single-minded and unit-demand pricing, as well as Stackelberg pricing for certain follower problems like shortest path or minimum spanning tree. Our first main result is a tight analysis of a single-price algorithm for the single follower game, which provides a $(1+\\epsilon) \\log m$-approximation for any $\\epsilon >0$. This can be extended to provide a $(1+\\epsilon)(\\log k + \\log m)$-approximation for the general problem and $k$ followers. The latter result is essentially best possible, as the problem is shown to be hard to approximate within $\\mathcal{O(\\log^\\epsilon k + \\log^\\epsilon m)$. If followers have demands, the single-price algorithm provides a $(1+\\epsilon)m^2$-approximation, and the problem is hard to approximate within $\\mathcal{O(m^\\epsilon)$ for some $\\epsilon >0$. Our second main result is a polynomial time algorithm for revenue maximization in the special case of Stackelberg bipartite vertex cover, which is based on non-trivial max-flow and LP-duality techniques. Our results can be extended to provide constant-factor approximations for any constant number of followers."
      },
      {
        "node_idx": 56366,
        "score_0_10": 10,
        "title": "markets for public decision making",
        "abstract": "A public decision-making problem consists of a set of issues, each with multiple possible alternatives, and a set of competing agents, each with a preferred alternative for each issue. We study adaptations of market economies to this setting, focusing on binary issues. Issues have prices, and each agent is endowed with artificial currency that she can use to purchase probability for her preferred alternatives (we allow randomized outcomes). We first show that when each issue has a single price that is common to all agents, market equilibria can be arbitrarily bad. This negative result motivates a different approach. We present a novel technique called \"pairwise issue expansion\", which transforms any public decision-making instance into an equivalent Fisher market, the simplest type of private goods market. This is done by expanding each issue into many goods: one for each pair of agents who disagree on that issue. We show that the equilibrium prices in the constructed Fisher market yield a \"pairwise pricing equilibrium\" in the original public decision-making problem which maximizes Nash welfare. More broadly, pairwise issue expansion uncovers a powerful connection between the public decision-making and private goods settings; this immediately yields several interesting results about public decisions markets, and furthers the hope that we will be able to find a simple iterative voting protocol that leads to near-optimum decisions."
      },
      {
        "node_idx": 159639,
        "score_0_10": 9,
        "title": "the cooperative game theory foundations of network bargaining games",
        "abstract": "We study bargaining games between suppliers and manufacturers in a network context. Agents wish to enter into contracts in order to generate surplus which then must be divided among the participants. Potential contracts and their surplus are represented by weighted edges in our bipartite network. Each agent in the market is additionally limited by a capacity representing the number of contracts which he or she may undertake. When all agents are limited to just one contract each, prior research applied natural generalizations of the Nash bargaining solution to the networked setting, defined the new solution concepts of stable and balanced, and characterized the resulting bargaining outcomes. We simplify and generalize these results to a setting in which participants in only one side of the market are limited to one contract each. The heart of our results uses a linear-programming formulation to establish a novel connection between well-studied cooperative game theory concepts (such as core and prekernel) and the solution concepts of stable and balanced defined for the bargaining games. This immediately implies one can take advantage of the results and algorithms in cooperative game theory to reproduce results such as those of Azar et al. [1] and Kleinberg and Tardos [29] and also generalize them to our setting. The cooperative-game-theoretic connection also inspires us to refine our solution space using standard solution concepts from that literature such as nucleolus and lexicographic kernel. The nucleolus is particularly attractive as it is unique, always exists, and is supported by experimental data in the network bargaining literature. Guided by algorithms from cooperative game theory, we show how to compute the nucleolus by pruning and iteratively solving a natural linear-programming formulation."
      },
      {
        "node_idx": 126122,
        "score_0_10": 9,
        "title": "budget allocation in binary opinion dynamics",
        "abstract": "In this article we study the allocation of a budget to promote an opinion in a group of agents. We assume that their opinion dynamics are based on the well-known voter model. We are interested in finding the most efficient use of a budget over time in order to manipulate a social network. We address the problem using the theory of discounted Markov decision processes. Our contributions can be summarized as follows: (i) we introduce the discounted Markov decision process in our cases, (ii) we present the corresponding Bellman equations, and, (iii) we solve the Bellman equations via backward programming. This work is a step towards providing a solid formulation of the budget allocation in social networks."
      },
      {
        "node_idx": 157066,
        "score_0_10": 9,
        "title": "computing nash equilibria approximation and smoothed complexity",
        "abstract": "We show that the BIMATRIX game does not have a fully polynomial-time approximation scheme, unless PPAD is in P. In other words, no algorithm with time polynomial in n and 1/\\epsilon can compute an \\epsilon-approximate Nash equilibrium of an n by nbimatrix game, unless PPAD is in P. Instrumental to our proof, we introduce a new discrete fixed-point problem on a high-dimensional cube with a constant side-length, such as on an n-dimensional cube with side-length 7, and show that they are PPAD-complete. Furthermore, we prove, unless PPAD is in RP, that the smoothed complexity of the Lemke-Howson algorithm or any algorithm for computing a Nash equilibrium of a bimatrix game is polynomial in n and 1/\\sigma under perturbations with magnitude \\sigma. Our result answers a major open question in the smoothed analysis of algorithms and the approximation of Nash equilibria."
      }
    ]
  },
  "457": {
    "explanation": "advanced belief updating in uncertainty modeling",
    "topk": [
      {
        "node_idx": 66081,
        "score_0_10": 10,
        "title": "a new approach to updating beliefs",
        "abstract": "We define a new notion of conditional belief, which plays the same role for Dempster-Shafer belief functions as conditional probability does for probability functions. Our definition is different from the standard definition given by Dempster, and avoids many of the well-known problems of that definition. Just as the conditional probability Pr (lB) is a probability function which is the result of conditioning on B being true, so too our conditional belief function Bel (lB) is a belief function which is the result of conditioning on B being true. We define the conditional belief as the lower envelope (that is, the inf) of a family of conditional probability functions, and provide a closed form expression for it. An alternate way of understanding our definition of conditional belief is provided by considering ideas from an earlier paper [Fagin and Halpern, 1989], where we connect belief functions with inner measures. In particular, we show here how to extend the definition of conditional probability to non measurable sets, in order to get notions of inner and outer conditional probabilities, which can be viewed as best approximations to the true conditional probability, given our lack of information. Our definition of conditional belief turns out to be an exact analogue of our definition of inner conditional probability."
      },
      {
        "node_idx": 122540,
        "score_0_10": 10,
        "title": "bibliometric enhanced information retrieval",
        "abstract": "Bibliometric techniques are not yet widely used to enhance retrieval processes in digital libraries, although they offer value-added effects for users. In this workshop we will explore how statistical modelling of scholarship, such as Bradfordizing or network analysis of coauthorship network, can improve retrieval services for specific communities, as well as for large, cross-domain collections. This workshop aims to raise awareness of the missing link between information retrieval (IR) and bibliometrics/scientometrics and to create a common ground for the incorporation of bibliometric-enhanced services into retrieval at the digital library interface."
      },
      {
        "node_idx": 11752,
        "score_0_10": 9,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 137083,
        "score_0_10": 9,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 112891,
        "score_0_10": 9,
        "title": "mathematical foundations of matrix syntax",
        "abstract": "Matrix syntax is a formal model of syntactic relations in language. The purpose of this paper is to explain its mathematical foundations, for an audience with some formal background. We make an axiomatic presentation, motivating each axiom on linguistic and practical grounds. The resulting mathematical structure resembles some aspects of quantum mechanics. Matrix syntax allows us to describe a number of language phenomena that are otherwise very difficult to explain, such as linguistic chains, and is arguably a more economical theory of language than most of the theories proposed in the context of the minimalist program in linguistics. In particular, sentences are naturally modelled as vectors in a Hilbert space with a tensor product structure, built from 2x2 matrices belonging to some specific group."
      },
      {
        "node_idx": 58471,
        "score_0_10": 9,
        "title": "on axiomatization of inconsistency indicators for pairwise comparisons",
        "abstract": "This study examines the notion of inconsistency in pairwise comparisons for providing an axiomatization for it. It also proposes two inconsistency indicators for pairwise comparisons. The primary motivation for the inconsistency reduction is expressed by a computer industry concept \u201cgarbage in, garbage out\u201d. The quality of the output depends on the quality of the input."
      },
      {
        "node_idx": 144256,
        "score_0_10": 9,
        "title": "linguistics and some aspects of its underlying dynamics",
        "abstract": "In recent years, central components of a new approach to linguistics, the Minimalist Program (MP) have come closer to physics. Features of the Minimalist Program, such as the unconstrained nature of recursive Merge, the operation of the Labeling Algorithm that only operates at the interface of Narrow Syntax with the Conceptual-Intentional and the Sensory-Motor interfaces, the difference between pronounced and un-pronounced copies of elements in a sentence and the build-up of the Fibonacci sequence in the syntactic derivation of sentence structures, are directly accessible to representation in terms of algebraic formalism. Although in our scheme linguistic structures are classical ones, we find that an interesting and productive isomorphism can be established between the MP structure, algebraic structures and many-body field theory opening new avenues of inquiry on the dynamics underlying some central aspects of linguistics."
      },
      {
        "node_idx": 130570,
        "score_0_10": 9,
        "title": "reformulating global grammar constraints",
        "abstract": "An attractive mechanism to specify global constraints in rostering and other domains is via formal languages. For instance, the Regular and Grammar constraints specify constraints in terms of the languages accepted by an automaton and a context-free grammar respectively. Taking advantage of the fixed length of the constraint, we give an algorithm to transform a context-free grammar into an automaton. We then study the use of minimization techniques to reduce the size of such automata and speed up propagation. We show that minimizing such automata after they have been unfolded and domains initially reduced can give automata that are more compact than minimizing before unfolding and reducing. Experimental results show that such transformations can improve the size of rostering problems that we can 'model and run'."
      },
      {
        "node_idx": 78341,
        "score_0_10": 9,
        "title": "improved semantic representations from tree structured long short term memory networks",
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
      },
      {
        "node_idx": 18770,
        "score_0_10": 9,
        "title": "towards a modular recommender system for research papers written in albanian",
        "abstract": "In the recent years there has been an increase in scientific papers publications in Albania and its neighboring countries that have large communities of Albanian speaking researchers. Many of these papers are written in Albanian. It is a very time consuming task to find papers related to the researchers' work, because there is no concrete system that facilitates this process. In this paper we present the design of a modular intelligent search system for articles written in Albanian. The main part of it is the recommender module that facilitates searching by providing relevant articles to the users (in comparison with a given one). We used a cosine similarity based heuristics that differentiates the importance of term frequencies based on their location in the article. We did not notice big differences on the recommendation results when using different combinations of the importance factors of the keywords, title, abstract and body. We got similar results when using only the title and abstract in comparison with the other combinations. Because we got fairly good results in this initial approach, we believe that similar recommender systems for documents written in Albanian can be built also in contexts not related to scientific publishing."
      }
    ]
  },
  "459": {
    "explanation": "planar morphing algorithms for tree and graph drawings",
    "topk": [
      {
        "node_idx": 36570,
        "score_0_10": 10,
        "title": "largest inscribed rectangles in geometric convex sets",
        "abstract": "We consider the problem of finding inscribed boxes and axis-aligned inscribed boxes of maximum volume, inside a compact and solid convex set. Our algorithms are capable of solving these two problems in any such set that can be represented with finite number of convex inequalities. For the axis-aligned case, we formulate the problem for higher dimensions and present an exact optimization algorithm which solves the problem in $\\mathcal{O}(d^3+d^2n)$ time, where $d$ is the dimension and $n$ is the number of inequalities defining the convex set. For the general case, after formulating the problem for higher dimensions we investigate the traditional 2-dimensional problem, which is in the literature merely considered for convex polygons, for a broad range of convex sets. We first present a new exact algorithm that finds the largest inscribed axis-aligned rectangle in such convex sets for any given direction of axes in $\\mathcal{O}(n)$ time. Using this exact algorithm as a subroutine, we present an $\\epsilon$-approximation algorithm that computes $(1-\\epsilon)$-approximation to the largest inscribed rectangle with computational complexity of $\\mathcal{O}(\\epsilon^{-1}n)$. Finally, we show that how this running time can be improved to $\\mathcal{O}(\\epsilon^{-1}\\log n)$ with a $\\mathcal{O}(\\epsilon^{-1}n)$ pre-processing time when the convex set is a polygon."
      },
      {
        "node_idx": 1157,
        "score_0_10": 10,
        "title": "how to morph a tree on a small grid",
        "abstract": "In this paper we study planar morphs between straight-line planar grid drawings of trees. A morph consists of a sequence of morphing steps, where in a morphing step vertices move along straight-line trajectories at constant speed. We show how to construct planar morphs that simultaneously achieve a reduced number of morphing steps and a polynomially-bounded resolution. We assume that both the initial and final drawings lie on the grid and we ensure that each morphing step produces a grid drawing; further, we consider both upward drawings of rooted trees and drawings of arbitrary trees."
      },
      {
        "node_idx": 16420,
        "score_0_10": 9,
        "title": "a survey on graph drawing beyond planarity",
        "abstract": "Graph Drawing Beyond Planarity is a rapidly growing research area that classifies and studies geometric representations of non-planar graphs in terms of forbidden crossing configurations. Aim of this survey is to describe the main research directions in this area, the most prominent known results, and some of the most challenging open problems."
      },
      {
        "node_idx": 167769,
        "score_0_10": 9,
        "title": "morphing planar graph drawings optimally",
        "abstract": "We provide an algorithm for computing a planar morph between any two planar straight-line drawings of any $n$-vertex plane graph in $O(n)$ morphing steps, thus improving upon the previously best known $O(n^2)$ upper bound. Further, we prove that our algorithm is optimal, that is, we show that there exist two planar straight-line drawings $\\Gamma_s$ and $\\Gamma_t$ of an $n$-vertex plane graph $G$ such that any planar morph between $\\Gamma_s$ and $\\Gamma_t$ requires $\\Omega(n)$ morphing steps."
      },
      {
        "node_idx": 22502,
        "score_0_10": 9,
        "title": "overfeat integrated recognition localization and detection using convolutional networks",
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
      },
      {
        "node_idx": 3136,
        "score_0_10": 9,
        "title": "tree drawings revisited",
        "abstract": "We make progress on a number of open problems concerning the area requirement for drawing trees on a grid. We prove that #R##N#1. every tree of size $n$ (with arbitrarily large degree) has a straight-line drawing with area $n2^{O(\\sqrt{\\log\\log n\\log\\log\\log n})}$, improving the longstanding $O(n\\log n)$ bound; #R##N#2. every tree of size $n$ (with arbitrarily large degree) has a straight-line upward drawing with area $n\\sqrt{\\log n}(\\log\\log n)^{O(1)}$, improving the longstanding $O(n\\log n)$ bound; #R##N#3. every binary tree of size $n$ has a straight-line orthogonal drawing with area $n2^{O(\\log^*n)}$, improving the previous $O(n\\log\\log n)$ bound by Shin, Kim, and Chwa (1996) and Chan, Goodrich, Kosaraju, and Tamassia (1996); #R##N#4. every binary tree of size $n$ has a straight-line order-preserving drawing with area $n2^{O(\\log^*n)}$, improving the previous $O(n\\log\\log n)$ bound by Garg and Rusu (2003); #R##N#5. every binary tree of size $n$ has a straight-line orthogonal order-preserving drawing with area $n2^{O(\\sqrt{\\log n})}$, improving the $O(n^{3/2})$ previous bound by Frati (2007)."
      },
      {
        "node_idx": 42123,
        "score_0_10": 9,
        "title": "upward planar morphs",
        "abstract": "We prove that, given two topologically-equivalent upward planar straight-line drawings of an $n$-vertex directed graph $G$, there always exists a morph between them such that all the intermediate drawings of the morph are upward planar and straight-line. Such a morph consists of $O(1)$ morphing steps if $G$ is a reduced planar $st$-graph, $O(n)$ morphing steps if $G$ is a planar $st$-graph, $O(n)$ morphing steps if $G$ is a reduced upward planar graph, and $O(n^2)$ morphing steps if $G$ is a general upward planar graph. Further, we show that $\\Omega(n)$ morphing steps might be necessary for an upward planar morph between two topologically-equivalent upward planar straight-line drawings of an $n$-vertex path."
      },
      {
        "node_idx": 66578,
        "score_0_10": 9,
        "title": "spatial pyramid pooling in deep convolutional networks for visual recognition",
        "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\u00d7224) input image. This requirement is \u201cartificial\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."
      },
      {
        "node_idx": 162866,
        "score_0_10": 9,
        "title": "optimal morphs of convex drawings",
        "abstract": "We give an algorithm to compute a morph between any two convex drawings of the same plane graph. The morph preserves the convexity of the drawing at any time instant and moves each vertex along a piecewise linear curve with linear complexity. The linear bound is asymptotically optimal in the worst case."
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      }
    ]
  },
  "460": {
    "explanation": "palindromic structures and geometric data structures optimization",
    "topk": [
      {
        "node_idx": 9810,
        "score_0_10": 10,
        "title": "range closest pair search in higher dimensions",
        "abstract": "Range closest-pair (RCP) search is a range-search variant of the classical closest-pair problem, which aims to store a given set $S$ of points into some space-efficient data structure such that when a query range $Q$ is specified, the closest pair in $S \\cap Q$ can be reported quickly. RCP search has received attention over years, but the primary focus was only on $\\mathbb{R}^2$. In this paper, we study RCP search in higher dimensions. We give the first nontrivial RCP data structures for orthogonal, simplex, halfspace, and ball queries in $\\mathbb{R}^d$ for any constant $d$. Furthermore, we prove a conditional lower bound for orthogonal RCP search for $d \\geq 3$."
      },
      {
        "node_idx": 26613,
        "score_0_10": 10,
        "title": "on the least number of palindromes contained in an infinite word",
        "abstract": "We investigate the least number of palindromic factors in an infinite word. We first consider general alphabets and give answers to this problem for periodic and non-periodic words, closed or not under reversal of factors. We then investigate the same problem when the alphabet has size two."
      },
      {
        "node_idx": 3136,
        "score_0_10": 10,
        "title": "tree drawings revisited",
        "abstract": "We make progress on a number of open problems concerning the area requirement for drawing trees on a grid. We prove that #R##N#1. every tree of size $n$ (with arbitrarily large degree) has a straight-line drawing with area $n2^{O(\\sqrt{\\log\\log n\\log\\log\\log n})}$, improving the longstanding $O(n\\log n)$ bound; #R##N#2. every tree of size $n$ (with arbitrarily large degree) has a straight-line upward drawing with area $n\\sqrt{\\log n}(\\log\\log n)^{O(1)}$, improving the longstanding $O(n\\log n)$ bound; #R##N#3. every binary tree of size $n$ has a straight-line orthogonal drawing with area $n2^{O(\\log^*n)}$, improving the previous $O(n\\log\\log n)$ bound by Shin, Kim, and Chwa (1996) and Chan, Goodrich, Kosaraju, and Tamassia (1996); #R##N#4. every binary tree of size $n$ has a straight-line order-preserving drawing with area $n2^{O(\\log^*n)}$, improving the previous $O(n\\log\\log n)$ bound by Garg and Rusu (2003); #R##N#5. every binary tree of size $n$ has a straight-line orthogonal order-preserving drawing with area $n2^{O(\\sqrt{\\log n})}$, improving the $O(n^{3/2})$ previous bound by Frati (2007)."
      },
      {
        "node_idx": 63698,
        "score_0_10": 10,
        "title": "finding approximate palindromes in strings quickly and simply",
        "abstract": "Described are two algorithms to find long approximate palindromes in a string, for example a DNA sequence. A simple algorithm requires O(n)- space and almost always runs in O(k.n)-time where n is the length of the string and k is the number of \"errors\" allowed in the palindrome. Its worst- case time-complexity is O(n 2 ) but this does not oc- cur with real biological sequences. A more complex algorithm guarantees O(k.n) worst-case time com- plexity."
      },
      {
        "node_idx": 18618,
        "score_0_10": 10,
        "title": "infinite words without palindrome",
        "abstract": "We show that there exists an uniformly recurrent infinite word whose set of factors is closed under reversal and which has only finitely many palindromic factors."
      },
      {
        "node_idx": 37709,
        "score_0_10": 9,
        "title": "statistical piano reduction controlling performance difficulty",
        "abstract": "We present a statistical-modelling method for piano reduction, i.e. converting an ensemble score into piano scores, that can control performance difficulty. While previous studies have focused on describing the condition for playable piano scores, it depends on player's skill and can change continuously with the tempo. We thus computationally quantify performance difficulty as well as musical fidelity to the original score, and formulate the problem as optimization of musical fidelity under constraints on difficulty values. First, performance difficulty measures are developed by means of probabilistic generative models for piano scores and the relation to the rate of performance errors is studied. Second, to describe musical fidelity, we construct a probabilistic model integrating a prior piano-score model and a model representing how ensemble scores are likely to be edited. An iterative optimization algorithm for piano reduction is developed based on statistical inference of the model. We confirm the effect of the iterative procedure; we find that subjective difficulty and musical fidelity monotonically increase with controlled difficulty values; and we show that incorporating sequential dependence of pitches and fingering motion in the piano-score model improves the quality of reduction scores in high-difficulty cases."
      },
      {
        "node_idx": 33809,
        "score_0_10": 9,
        "title": "statistical learning and estimation of piano fingering",
        "abstract": "Automatic estimation of piano fingering is important for computationally understanding the process of music performance and applicable to performance assistance and education systems. While a natural way to formulate the quality of fingerings is to construct models of the constraints/costs of performance, it is generally difficult to find appropriate parameter values for these models. Here we study an alternative data-driven approach based on statistical modeling in which the naturalness of a given fingering is described by probabilities. Specifically, we construct two types of hidden Markov models (HMMs) and their higher-order extensions. We also study deep neural network (DNN)-based methods for comparison. Using a newly released dataset of fingering annotations, we conduct systematic evaluations of these models as well as a representative constraint-based method and find that the methods based on high-order HMMs outperform others in terms of estimation accuracies. We also quantitatively study individual difference of fingering and propose evaluation measures that can be used with multiple ground truth data. We conclude that the HMM-based methods are currently state of the art and generate acceptable fingerings in most parts and that they have certain limitations such as ignorance of phrase boundaries and interdependence of the two hands."
      },
      {
        "node_idx": 100251,
        "score_0_10": 9,
        "title": "short plane supports for spatial hypergraphs",
        "abstract": "A graph $G=(V,E)$ is a support of a hypergraph $H=(V,S)$ if every hyperedge induces a connected subgraph in $G$. Supports are used for certain types of hypergraph visualizations. In this paper we consider visualizing spatial hypergraphs, where each vertex has a fixed location in the plane. This is the case, e.g., when modeling set systems of geospatial locations as hypergraphs. By applying established aesthetic quality criteria we are interested in finding supports that yield plane straight-line drawings with minimum total edge length on the input point set $V$. We first show, from a theoretical point of view, that the problem is NP-hard already under rather mild conditions as well as a negative approximability results. Therefore, the main focus of the paper lies on practical heuristic algorithms as well as an exact, ILP-based approach for computing short plane supports. We report results from computational experiments that investigate the effect of requiring planarity and acyclicity on the resulting support length. Further, we evaluate the performance and trade-offs between solution quality and speed of several heuristics relative to each other and compared to optimal solutions."
      },
      {
        "node_idx": 73577,
        "score_0_10": 9,
        "title": "exploratory analysis of a large flamenco corpus using an ensemble of convolutional neural networks as a structural annotation backend",
        "abstract": "We present computational tools that we developed for the analysis of a large corpus of flamenco music recordings, along with the related exploratory findings. The proposed computational backend is based on a set of Convolutional Neural Networks that provide the structural annotation of each music recording with respect to the presence of vocals, guitar and hand-clapping (\"palmas\"). The resulting, automatically extracted annotations, allowed for the visualization of music recordings in structurally meaningful ways, the extraction of global statistics related to the instrumentation of flamenco music, the detection of a cappella and instrumental recordings for which no such information existed, the investigation of differences in structure and instrumentation across styles and the study of tonality across instrumentation and styles. The reported findings show that it is feasible to perform a large scale analysis of flamenco music with state-of-the-art classification technology and produce automatically extracted descriptors that are both musicologically valid and useful, in the sense that they can enhance conventional metadata schemes and assist bridging the semantic gap between audio recordings and high-level musicological concepts."
      },
      {
        "node_idx": 78813,
        "score_0_10": 9,
        "title": "maximum width empty square and rectangular annulus",
        "abstract": "An annulus is, informally, a ring-shaped region, often described by two concentric circles. The maximum-width empty annulus problem asks to find an annulus of a certain shape with the maximum possible width that avoids a given set of $n$ points in the plane. This problem can also be interpreted as the problem of finding an optimal location of a ring-shaped obnoxious facility among the input points. In this paper, we study square and rectangular variants of the maximum-width empty anuulus problem, and present first nontrivial algorithms. Specifically, our algorithms run in $O(n^3)$ and $O(n^2 \\log n)$ time for computing a maximum-width empty axis-parallel square and rectangular annulus, respectively. Both algorithms use only $O(n)$ space."
      }
    ]
  },
  "461": {
    "explanation": "graph edge coloring and interval colorings in cubic graphs",
    "topk": [
      {
        "node_idx": 79698,
        "score_0_10": 10,
        "title": "the runs theorem",
        "abstract": "We give a new characterization of maximal repetitions (or runs) in strings based on Lyndon words. The characterization leads to a proof of what was known as the \u201cruns\u201d conjecture [R. M. Kolpakov an..."
      },
      {
        "node_idx": 137633,
        "score_0_10": 10,
        "title": "cubic graphs with large circumference deficit",
        "abstract": "The circumference $c(G)$ of a graph $G$ is the length of a longest cycle. By exploiting our recent results on resistance of snarks, we construct infinite classes of cyclically $4$-, $5$- and $6$-edge-connected cubic graphs with circumference ratio $c(G)/|V(G)|$ bounded from above by $0.876$, $0.960$ and $0.990$, respectively. In contrast, the dominating cycle conjecture implies that the circumference ratio of a cyclically $4$-edge-connected cubic graph is at least $0.75$. #R##N#In addition, we construct snarks with large girth and large circumference deficit, solving Problem 1 proposed in [J. H\\\"agglund and K. Markstr\\\"om, On stable cycles and cycle double covers of graphs with large circumference, Disc. Math. 312 (2012), 2540--2544]."
      },
      {
        "node_idx": 148800,
        "score_0_10": 10,
        "title": "faster range minimum queries",
        "abstract": "Range Minimum Query (RMQ) is an important building brick of many compressed data structures and string matching algorithms. Although this problem is essentially solved in theory, with sophisticated data structures allowing for constant time queries, practical performance and construction time also matter. Additionally, there are offline scenarios in which the number of queries, $q$, is rather small and given beforehand, which encourages to use a simpler approach. In this work, we present a simple data structure, with very fast construction, which allows to handle queries in constant time on average. This algorithm, however, requires access to the input data during queries (which is not the case of sophisticated RMQ solutions). We subsequently refine our technique, combining it with one of the existing succinct solutions with $O(1)$ worst-case time queries and no access to the input array. The resulting hybrid is still a memory frugal data structure, spending usually up to about $3n$ bits, and providing competitive query times, especially for wide ranges. We also show how to make our baseline data structure more compact. Experimental results demonstrate that the proposed BbST (Block-based Sparse Table) variants are competitive to existing solutions, also in the offline scenario."
      },
      {
        "node_idx": 45166,
        "score_0_10": 10,
        "title": "interval colorings of complete bipartite graphs and trees",
        "abstract": "A translation from Russian of the work of R.R. Kamalian \"Interval colorings of complete bipartite graphs and trees\", Preprint of the Computing Centre of the Academy of Sciences of Armenia, Yerevan, 1989. (Was published by the decision of the Academic Council of the Computing Centre of the Academy of Sciences of Armenian SSR and Yerevan State University from 7.09.1989)."
      },
      {
        "node_idx": 164842,
        "score_0_10": 10,
        "title": "counting subwords and regular languages",
        "abstract": "Let $x$ and $y$ be words. We consider the languages whose words $z$ are those for which the numbers of occurrences of $x$ and $y$, as subwords of $z$, are the same (resp., the number of $x$'s is less than the number of $y$'s, resp., is less than or equal). We give a necessary and sufficient condition on $x$ and $y$ for these languages to be regular, and we show how to check this condition efficiently."
      },
      {
        "node_idx": 49880,
        "score_0_10": 10,
        "title": "palindromes in two dimensional words",
        "abstract": "A two-dimensional ($2$D) word is a $2$D palindrome if it is equal to its reverse and it is an HV-palindrome if all its columns and rows are $1$D palindromes. We study some combinatorial and structural properties of HV-palindromes and its comparison with $2$D palindromes. We investigate the maximum number number of distinct non-empty HV-palindromic sub-arrays in any finite $2$D word, thus, proving the conjecture given by Anisiua et al. We also find the least number of HV-palindromes in an infinite $2$D word over a finite alphabet size $q$."
      },
      {
        "node_idx": 24333,
        "score_0_10": 10,
        "title": "on s packing edge colorings of cubic graphs",
        "abstract": "Given a non-decreasing sequence S = (s 1,s 2,. .. ,s k) of positive integers, an S-packing edge-coloring of a graph G is a partition of the edge set of G into k subsets {X 1 ,X 2,. .. ,X k } such that for each 1 $\\le$ i $\\le$ k, the distance between two distinct edges e, e ' $\\in$ X i is at least s i + 1. This paper studies S-packing edge-colorings of cubic graphs. Among other results, we prove that cubic graphs having a 2-factor are (1,1,1,3,3)-packing edge-colorable, (1,1,1,4,4,4,4,4)-packing edge-colorable and (1,1,2,2,2,2,2)-packing edge-colorable. We determine sharper results for cubic graphs of bounded oddness and 3-edge-colorable cubic graphs and we propose many open problems."
      },
      {
        "node_idx": 20053,
        "score_0_10": 10,
        "title": "clustered planarity testing revisited",
        "abstract": "The Hanani--Tutte theorem is a classical result proved for the first time in the 1930s that characterizes planar graphs as graphs that admit a drawing in the plane in which every pair of edges not sharing a vertex cross an even number of times. We generalize this result to clustered graphs with two disjoint clusters, and show that a straightforward extension to flat clustered graphs with three or more disjoint clusters is not possible. For general clustered graphs we show a variant of the Hanani--Tutte theorem in the case when each cluster induces a connected subgraph. Di Battista and Frati proved that clustered planarity of embedded clustered graphs whose every face is incident with at most five vertices can be tested in polynomial time. We give a new and short proof of this result, using the matroid intersection algorithm."
      },
      {
        "node_idx": 85723,
        "score_0_10": 10,
        "title": "periodicity in rectangular arrays",
        "abstract": "We discuss several two-dimensional generalizations of the familiar Lyndon-Schutzenberger periodicity theorem for words. We consider the notion of primitive array (as one that cannot be expressed as the repetition of smaller arrays). We count the number of m x n arrays that are primitive. Finally, we show that one can test primitivity and compute the primitive root of an array in linear time."
      },
      {
        "node_idx": 35828,
        "score_0_10": 10,
        "title": "interval edge colorings of cubic graphs",
        "abstract": "An edge-coloring of a multigraph G with colors 1,2,...,t is called an interval t-coloring if all colors are used, and the colors of edges incident to any vertex of G are distinct and form an interval of integers. In this paper we prove that if G is a connected cubic multigraph (a connected cubic graph) that admits an interval t-coloring, then t\\leq |V(G)| +1 (t\\leq |V(G)|), where V(G) is the set of vertices of G. Moreover, if G is a connected cubic graph, G\\neq K_{4}, and G has an interval t-coloring, then t\\leq |V(G)| -1. We also show that these upper bounds are sharp. Finally, we prove that if G is a bipartite subcubic multigraph, then G has an interval edge-coloring with no more than four colors."
      }
    ]
  },
  "463": {
    "explanation": "applications of environmental sensing and graph theory in computational problems",
    "topk": [
      {
        "node_idx": 66150,
        "score_0_10": 10,
        "title": "disks in curves of bounded convex curvature",
        "abstract": "We say that a simple, closed curve $\\gamma$ in the plane has bounded convex curvature if for every point $x$ on $\\gamma$, there is an open unit disk $U_x$ and $\\varepsilon_x>0$ such that $x\\in\\partial U_x$ and $B_{\\varepsilon_x}(x)\\cap U_x\\subset\\text{Int}\\;\\gamma$. We prove that the interior of every curve of bounded convex curvature contains an open unit disk."
      },
      {
        "node_idx": 25172,
        "score_0_10": 10,
        "title": "environmental sensing by wearable device for indoor activity and location estimation",
        "abstract": "We present results from a set of experiments in this pilot study to investigate the causal influence of user activity on various environmental parameters monitored by occupant-carried multi-purpose sensors. Hypotheses with respect to each type of measurements are verified, including temperature, humidity, and light level collected during eight typical activities: sitting in lab / cubicle, indoor walking / running, resting after physical activity, climbing stairs, taking elevators, and outdoor walking. Our main contribution is the development of features for activity and location recognition based on environmental measurements, which exploit location- and activity-specific characteristics and capture the trends resulted from the underlying physiological process. The features are statistically shown to have good separability and are also information-rich. Fusing environmental sensing together with acceleration is shown to achieve classification accuracy as high as 99.13%. For building applications, this study motivates a sensor fusion paradigm for learning individualized activity, location, and environmental preferences for energy management and user comfort."
      },
      {
        "node_idx": 11647,
        "score_0_10": 10,
        "title": "building in briefcase bib",
        "abstract": "A building's environment has profound influence on occupant comfort and health. Continuous monitoring of building occupancy and environment is essential to fault detection, intelligent control, and building commissioning. Though many solutions for environmental measuring based on wireless sensor networks exist, they are not easily accessible to households and building owners who may lack time or technical expertise needed to set up a system and get quick and detailed overview of environmental conditions. #R##N#Building-in-Briefcase (BiB) is a portable sensor network platform that is trivially easy to deploy in any building environment. Once the sensors are distributed, the environmental data is collected and communicated to the BiB router via TCP/IP protocol and WiFi technology which then forwards the data to the central database securely over the internet through a 3G radio. The user, with minimal effort, can access the aggregated data and visualize the trends in real time on the BiB web portal. Paramount to the adoption and continued operation of an indoor sensing platform is battery lifetime. This design has achieved a multi-year lifespan by careful selection of components, an efficient binary communications protocol and data compression. Our BiB sensor is capable of collecting a rich set of environmental parameters, and is expandable to measure others, such as CO2. This paper describes the power characteristics of BiB sensors and their occupancy estimation and activity recognition functionality. Our vision is large-scale deployment of BiB in thousands of buildings, which would provide ample research opportunities and opportunities to identify ways to improve the building environment and energy efficiency."
      },
      {
        "node_idx": 86541,
        "score_0_10": 10,
        "title": "characterizing star pcgs",
        "abstract": "A graph $G$ is called a pairwise compatibility graph (PCG, for short) if it admits a tuple $(T,w, d_{\\min},d_{\\max})$ of a tree $T$ whose leaf set is equal to the vertex set of $G$, a non-negative edge weight $w$, and two non-negative reals $d_{\\min}\\leq d_{\\max}$ such that $G$ has an edge between two vertices $u,v\\in V$ if and only if the distance between the two leaves $u$ and $v$ in the weighted tree $(T,w)$ is in the interval $[d_{\\min}, d_{\\max}]$. The tree $T$ is also called a witness tree of the PCG $G$. The problem of testing if a given graph is a PCG is not known to be NP-hard yet. To obtain a complete characterization of PCGs is a wide open problem in computational biology and graph theory. In literature, most witness trees admitted by known PCGs are stars and caterpillars. In this paper, we give a complete characterization for a graph to be a star-PCG (a PCG that admits a star as its witness tree), which provides us the first polynomial-time algorithm for recognizing star-PCGs."
      },
      {
        "node_idx": 35803,
        "score_0_10": 10,
        "title": "closure under minors of undirected entanglement",
        "abstract": "Entanglement is a digraph complexity measure that origins in fixed-point theory. Its purpose is to count the nested depth of cycles in digraphs. #R##N#In this paper we prove that the class of undirected graphs of entanglement at most $k$, for arbitrary fixed $k \\in \\mathbb{N}$, is closed under taking minors. Our proof relies on the game theoretic characterization of entanglement in terms of Robber and Cops games."
      },
      {
        "node_idx": 64701,
        "score_0_10": 10,
        "title": "affine sessions",
        "abstract": "Session types describe the structure of communications implemented by channels. In particular, they prescribe the sequence of communications, whether they are input or output actions, and the type of value exchanged. Crucial to any language with session types is the notion of linearity, which is essential to ensure that channels exhibit the behaviour prescribed by their type without interference in the presence of concurrency. In this work we relax the condition of linearity to that of affinity, by which channels exhibit at most the behaviour prescribed by their types. This more liberal setting allows us to incorporate an elegant error handling mechanism which simplifies and improves related works on exceptions. Moreover, our treatment does not affect the progress properties of the language: sessions never get stuck."
      },
      {
        "node_idx": 42857,
        "score_0_10": 10,
        "title": "universally consistent predictive distributions",
        "abstract": "This paper describes simple universally consistent procedures of probability forecasting that satisfy a natural property of small-sample validity, under the assumption that the observations are produced independently in the IID fashion."
      },
      {
        "node_idx": 1779,
        "score_0_10": 10,
        "title": "conformal predictive distributions with kernels",
        "abstract": "This paper reviews the checkered history of predictive distributions in statistics and discusses two developments, one from recent literature and the other new. The first development is bringing predictive distributions into machine learning, whose early development was so deeply influenced by two remarkable groups at the Institute of Automation and Remote Control. The second development is combining predictive distributions with kernel methods, which were originated by one of those groups, including Emmanuel Braverman."
      },
      {
        "node_idx": 101517,
        "score_0_10": 9,
        "title": "optimal algorithms for testing closeness of discrete distributions",
        "abstract": "We study the question of closeness testing for two discrete distributions. More precisely, given samples from two distributions $p$ and $q$ over an $n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least $\\eps$-far from $q$, in either $\\ell_1$ or $\\ell_2$ distance. Batu et al. gave the first sub-linear time algorithms for these problems, which matched the lower bounds of Valiant up to a logarithmic factor in $n$, and a polynomial factor of $\\eps.$ #R##N#In this work, we present simple (and new) testers for both the $\\ell_1$ and $\\ell_2$ settings, with sample complexity that is information-theoretically optimal, to constant factors, both in the dependence on $n$, and the dependence on $\\eps$; for the $\\ell_1$ testing problem we establish that the sample complexity is $\\Theta(\\max\\{n^{2/3}/\\eps^{4/3}, n^{1/2}/\\eps^2 \\}).$"
      },
      {
        "node_idx": 44702,
        "score_0_10": 9,
        "title": "combinatorial metrics macwilliams type identities isometries and extension property",
        "abstract": "In this work we characterize the combinatorial metrics admitting a MacWilliams-type identity and describe the group of linear isometries of such metrics. Considering coverings that are not connected, we classify the metrics satisfying the MacWilliams extension property."
      }
    ]
  },
  "466": {
    "explanation": "localization and visualization of discriminative image regions in CNNs",
    "topk": [
      {
        "node_idx": 151734,
        "score_0_10": 10,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 127295,
        "score_0_10": 10,
        "title": "online clique clustering",
        "abstract": "Clique clustering is the problem of partitioning the vertices of a graph into disjoint clusters, where each cluster forms a clique in the graph, while optimizing some objective function. In online clustering, the input graph is given one vertex at a time, and any vertices that have previously been clustered together are not allowed to be separated. The goal is to maintain a clustering with an objective value close to the optimal solution. For the variant where we want to maximize the number of edges in the clusters, we propose an online strategy based on the doubling technique. It has an asymptotic competitive ratio at most 15.646 and an absolute competitive ratio at most 22.641. We also show that no deterministic strategy can have an asymptotic competitive ratio better than 6. For the variant where we want to minimize the number of edges between clusters, we show that the deterministic competitive ratio of the problem is $n-\\omega(1)$, where n is the number of vertices in the graph."
      },
      {
        "node_idx": 124030,
        "score_0_10": 10,
        "title": "competitive strategies for online clique clustering",
        "abstract": "A clique clustering of a graph is a partitioning of its vertices into disjoint cliques. The quality of a clique clustering is measured by the total number of edges in its cliques. We consider the online variant of the clique clustering problem, where the vertices of the input graph arrive one at a time. At each step, the newly arrived vertex forms a singleton clique, and the algorithm can merge any existing cliques in its partitioning into larger cliques, but splitting cliques is not allowed. We give an online algorithm with competitive ratio 15.645 and we prove a lower bound of 6 on the competitive ratio, improving the previous respective bounds of 31 and 2."
      },
      {
        "node_idx": 45949,
        "score_0_10": 9,
        "title": "influence maximization near optimal time complexity meets practical efficiency",
        "abstract": "Given a social network G and a constant k, the influence maximization problem asks for k nodes in G that (directly and indirectly) influence the largest number of nodes under a pre-defined diffusion model. This problem finds important applications in viral marketing, and has been extensively studied in the literature. Existing algorithms for influence maximization, however, either trade approximation guarantees for practical efficiency, or vice versa. In particular, among the algorithms that achieve constant factor approximations under the prominent independent cascade (IC) model or linear threshold (LT) model, none can handle a million-node graph without incurring prohibitive overheads. #R##N#This paper presents TIM, an algorithm that aims to bridge the theory and practice in influence maximization. On the theory side, we show that TIM runs in O((k+\\ell) (n+m) \\log n / \\epsilon^2) expected time and returns a (1-1/e-\\epsilon)-approximate solution with at least 1 - n^{-\\ell} probability. The time complexity of TIM is near-optimal under the IC model, as it is only a \\log n factor larger than the \\Omega(m + n) lower-bound established in previous work (for fixed k, \\ell, and \\epsilon). Moreover, TIM supports the triggering model, which is a general diffusion model that includes both IC and LT as special cases. On the practice side, TIM incorporates novel heuristics that significantly improve its empirical efficiency without compromising its asymptotic performance. We experimentally evaluate TIM with the largest datasets ever tested in the literature, and show that it outperforms the state-of-the-art solutions (with approximation guarantees) by up to four orders of magnitude in terms of running time. In particular, when k = 50, \\epsilon = 0.2, and \\ell = 1, TIM requires less than one hour on a commodity machine to process a network with 41.6 million nodes and 1.4 billion edges."
      },
      {
        "node_idx": 51298,
        "score_0_10": 9,
        "title": "methods for estimating the size of google scholar",
        "abstract": "The emergence of academic search engines (mainly Google Scholar and Microsoft Academic Search) that aspire to index the entirety of current academic knowledge has revived and increased interest in the size of the academic web. The main objective of this paper is to propose various methods to estimate the current size (number of indexed documents) of Google Scholar (May 2014) and to determine its validity, precision and reliability. To do this, we present, apply and discuss three empirical methods: an external estimate based on empirical studies of Google Scholar coverage, and two internal estimate methods based on direct, empty and absurd queries, respectively. The results, despite providing disparate values, place the estimated size of Google Scholar at around 160 to 165 million documents. However, all the methods show considerable limitations and uncertainties due to inconsistencies in the Google Scholar search functionalities."
      },
      {
        "node_idx": 77301,
        "score_0_10": 9,
        "title": "grad cam visual explanations from deep networks via gradient based localization",
        "abstract": "We propose a technique for producing \u2018visual explanations\u2019 for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach\u2014Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say \u2018dog\u2019 in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265\u2013290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E."
      },
      {
        "node_idx": 14657,
        "score_0_10": 9,
        "title": "lesion border detection in dermoscopy images",
        "abstract": "Abstract   Background  Dermoscopy is one of the major imaging modalities used in the diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty and subjectivity of human interpretation, computerized analysis of dermoscopy images has become an important research area. One of the most important steps in dermoscopy image analysis is the automated detection of lesion borders.    Methods  In this article, we present a systematic overview of the recent border detection methods in the literature paying particular attention to computational issues and evaluation aspects.    Conclusion  Common problems with the existing approaches include the acquisition, size, and diagnostic distribution of the test image set, the evaluation of the results, and the inadequate description of the employed methods. Border determination by dermatologists appears to depend upon higher-level knowledge, therefore it is likely that the incorporation of domain knowledge in automated methods will enable them to perform better, especially in sets of images with a variety of diagnoses."
      },
      {
        "node_idx": 65091,
        "score_0_10": 9,
        "title": "does google scholar contain all highly cited documents 1950 2013",
        "abstract": "The study of highly cited documents on Google Scholar (GS) has never been addressed to date in a comprehensive manner. The objective of this work is to identify the set of highly cited documents in Google Scholar and define their core characteristics: their languages, their file format, or how many of them can be accessed free of charge. We will also try to answer some additional questions that hopefully shed some light about the use of GS as a tool for assessing scientific impact through citations. The decalogue of research questions is shown below: #R##N#1. Which are the most cited documents in GS? #R##N#2. Which are the most cited document types in GS? #R##N#3. What languages are the most cited documents written in GS? #R##N#4. How many highly cited documents are freely accessible? #R##N#4.1 What file types are the most commonly used to store these highly cited documents? #R##N#4.2 Which are the main providers of these documents? #R##N#5. How many of the highly cited documents indexed by GS are also indexed by WoS? #R##N#6. Is there a correlation between the number of citations that these highly cited documents have received in GS and the number of citations they have received in WoS? #R##N#7. How many versions of these highly cited documents has GS detected? #R##N#8. Is there a correlation between the number of versions GS has detected for these documents, and the number citations they have received? #R##N#9. Is there a correlation between the number of versions GS has detected for these documents, and their position in the search engine result pages? #R##N#10. Is there some relation between the positions these documents occupy in the search engine result pages, and the number of citations they have received?"
      },
      {
        "node_idx": 39369,
        "score_0_10": 9,
        "title": "person re identification by local maximal occurrence representation and metric learning",
        "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      }
    ]
  },
  "468": {
    "explanation": "deep recurrent neural network sequence modeling and transferability",
    "topk": [
      {
        "node_idx": 51364,
        "score_0_10": 10,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 45355,
        "score_0_10": 8,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 29312,
        "score_0_10": 8,
        "title": "on the properties of neural machine translation encoder decoder approaches",
        "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 49351,
        "score_0_10": 8,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 135057,
        "score_0_10": 8,
        "title": "generating sequences with recurrent neural networks",
        "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
      },
      {
        "node_idx": 14201,
        "score_0_10": 8,
        "title": "decaf a deep convolutional activation feature for generic visual recognition",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
      },
      {
        "node_idx": 160949,
        "score_0_10": 7,
        "title": "a convolutional neural network for modelling sentences",
        "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."
      }
    ]
  },
  "469": {
    "explanation": "spatial manipulation and multi-scale context aggregation in convolutional networks",
    "topk": [
      {
        "node_idx": 55399,
        "score_0_10": 10,
        "title": "spatial transformer networks",
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
      },
      {
        "node_idx": 67928,
        "score_0_10": 10,
        "title": "multi scale context aggregation by dilated convolutions",
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
      },
      {
        "node_idx": 71636,
        "score_0_10": 10,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 139975,
        "score_0_10": 9,
        "title": "cnn features off the shelf an astounding baseline for recognition",
        "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks."
      },
      {
        "node_idx": 69942,
        "score_0_10": 9,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 45355,
        "score_0_10": 9,
        "title": "tensorflow large scale machine learning on heterogeneous distributed systems",
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 51364,
        "score_0_10": 9,
        "title": "speech recognition with deep recurrent neural networks",
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
      }
    ]
  },
  "471": {
    "explanation": "logic proof assistants and formal reasoning tools",
    "topk": [
      {
        "node_idx": 159367,
        "score_0_10": 10,
        "title": "natural deduction and the isabelle proof assistant",
        "abstract": "We describe our Natural Deduction Assistant (NaDeA) and the interfaces between the Isabelle proof assistant and NaDeA. In particular, we explain how NaDeA, using a generated prover that has been verified in Isabelle, provides feedback to the student, and also how NaDeA, for each formula proved by the student, provides a generated theorem that can be verified in Isabelle."
      },
      {
        "node_idx": 63264,
        "score_0_10": 9,
        "title": "compositional semantics for the procedural interpretation of logic",
        "abstract": "Semantics of logic programs has been given by proof theory, model theory and by fixpoint of the immediate-consequence operator. If clausal logic is a programming language, then it should also have a compositional semantics. Compositional semantics for programming languages follows the abstract syntax of programs, composing the meaning of a unit by a mathematical operation on the meanings of its constituent units. The procedural interpretation of logic has only yielded an incomplete abstract syntax for logic programs. We complete it and use the result as basis of a compositional semantics. We present for comparison Tarski's algebraization of first-order predicate logic, which is in substance the compositional semantics for his choice of syntax. We characterize our semantics by equivalence with the immediate-consequence operator."
      },
      {
        "node_idx": 76938,
        "score_0_10": 9,
        "title": "towards applicative relational programming",
        "abstract": "Functional programming comes in two flavours: one where ``functions are first-class citizens'' (we call this applicative) and one which is based on equations (we call this declarative). In relational programming clauses play the role of equations. Hence Prolog is declarative. The purpose of this paper is to provide in relational programming a mathematical basis for the relational analog of applicative functional programming. We use the cylindric semantics of first-order logic due to Tarski and provide a new notation for the required cylinders that we call tables. We define the Table/Relation Algebra with operators sufficient to translate Horn clauses into algebraic form. We establish basic mathematical properties of these operators. We show how relations can be first-class citizens, and devise mechanisms for modularity, for local scoping of predicates, and for exporting/importing relations between programs."
      },
      {
        "node_idx": 160393,
        "score_0_10": 9,
        "title": "nadea a natural deduction assistant with a formalization in isabelle",
        "abstract": "We present a new software tool for teaching logic based on natural deduction. Its proof system is formalized in the proof assistant Isabelle such that its definition is very precise. Soundness of the formalization has been proved in Isabelle. The tool is open source software developed in TypeScript / JavaScript and can thus be used directly in a browser without any further installation. Although developed for undergraduate computer science students who are used to study and program concrete computer code in a programming language we consider the approach relevant for a broader audience and for other proof systems as well."
      },
      {
        "node_idx": 112853,
        "score_0_10": 9,
        "title": "instruction set architectures for quantum processing units",
        "abstract": "Progress in quantum computing hardware raises questions about how these devices can be controlled, programmed, and integrated with existing computational workflows. We briefly describe several prominent quantum computational models, their associated quantum processing units (QPUs), and the adoption of these devices as accelerators within high-performance computing systems. Emphasizing the interface to the QPU, we analyze instruction set architectures based on reduced and complex instruction sets, i.e., RISC and CISC architectures. We clarify the role of conventional constraints on memory addressing and instruction widths within the quantum computing context. Finally, we examine existing quantum computing platforms, including the D-Wave 2000Q and IBM Quantum Experience, within the context of future ISA development and HPC needs."
      },
      {
        "node_idx": 135351,
        "score_0_10": 9,
        "title": "six challenges for neural machine translation",
        "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation."
      },
      {
        "node_idx": 28770,
        "score_0_10": 9,
        "title": "church a language for generative models",
        "abstract": "We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques."
      },
      {
        "node_idx": 71636,
        "score_0_10": 9,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 130925,
        "score_0_10": 8,
        "title": "a mobile application for self guided study of formal reasoning",
        "abstract": "In this work, we introduce AXolotl, a self-study aid designed to guide students through the basics of formal reasoning and term manipulation. Unlike most of the existing study aids for formal reasoning, AXolotl is an Android-based application with a simple touch-based interface. Part of the design goal was to minimize the possibility of user errors which distract from the learning process. Such as typos or inconsistent application of the provided rules. The system includes a zoomable proof viewer which displays the progress made so far and allows for storage of the completed proofs as a JPEG or LaTeX file. The software is available on the google play store and comes with a small library of problems. Additional problems may be opened in AXolotl using a simple input language. Currently, AXolotl supports problems that can be solved using rules which transform a single expression into a set of expressions. This covers educational scenarios found in our first-semester introduction to logic course and helps bridge the gap between propositional and first-order reasoning. Future developments will include rewrite rules which take a set of expressions and return a set of expressions, as well as a quantified first-order extension."
      },
      {
        "node_idx": 11402,
        "score_0_10": 8,
        "title": "in datacenter performance analysis of a tensor processing unit",
        "abstract": "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."
      }
    ]
  },
  "472": {
    "explanation": "interpretable CNN-based visual explanation and localization techniques",
    "topk": [
      {
        "node_idx": 124619,
        "score_0_10": 10,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 77301,
        "score_0_10": 9,
        "title": "grad cam visual explanations from deep networks via gradient based localization",
        "abstract": "We propose a technique for producing \u2018visual explanations\u2019 for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach\u2014Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say \u2018dog\u2019 in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265\u2013290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E."
      },
      {
        "node_idx": 49351,
        "score_0_10": 9,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      },
      {
        "node_idx": 151734,
        "score_0_10": 9,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 98234,
        "score_0_10": 8,
        "title": "the mythos of model interpretability",
        "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not."
      },
      {
        "node_idx": 71636,
        "score_0_10": 8,
        "title": "adadelta an adaptive learning rate method",
        "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
      },
      {
        "node_idx": 130623,
        "score_0_10": 8,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 15365,
        "score_0_10": 8,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      },
      {
        "node_idx": 43376,
        "score_0_10": 8,
        "title": "skip thought vectors",
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available."
      },
      {
        "node_idx": 11752,
        "score_0_10": 8,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      }
    ]
  },
  "474": {
    "explanation": "residual connections and architecture improvements in deep neural networks",
    "topk": [
      {
        "node_idx": 53950,
        "score_0_10": 10,
        "title": "identity mappings in deep residual networks",
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: this https URL"
      },
      {
        "node_idx": 52018,
        "score_0_10": 9,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 130623,
        "score_0_10": 9,
        "title": "deep visual semantic alignments for generating image descriptions",
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
      },
      {
        "node_idx": 106114,
        "score_0_10": 9,
        "title": "matching networks for one shot learning",
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
      },
      {
        "node_idx": 137083,
        "score_0_10": 9,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 119322,
        "score_0_10": 9,
        "title": "wide residual networks",
        "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
      },
      {
        "node_idx": 88803,
        "score_0_10": 9,
        "title": "improving neural networks by preventing co adaptation of feature detectors",
        "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
      },
      {
        "node_idx": 73035,
        "score_0_10": 9,
        "title": "network in network",
        "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
      },
      {
        "node_idx": 111390,
        "score_0_10": 9,
        "title": "squeezenet alexnet level accuracy with 50x fewer parameters and 0 5mb model size",
        "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet)."
      },
      {
        "node_idx": 15365,
        "score_0_10": 9,
        "title": "how transferable are features in deep neural networks",
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
      }
    ]
  },
  "476": {
    "explanation": "machine translation challenges and lexical resource augmentation",
    "topk": [
      {
        "node_idx": 135351,
        "score_0_10": 10,
        "title": "six challenges for neural machine translation",
        "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation."
      },
      {
        "node_idx": 88762,
        "score_0_10": 10,
        "title": "computational approach to anaphora resolution in spanish dialogues",
        "abstract": "This paper presents an algorithm for identifying noun-phrase antecedents of pronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora resolution requires numerous sources of information in order to find the correct antecedent of the anaphor. These sources can be of different kinds, e.g., linguistic information, discourse/dialogue structure information, or topic information. For this reason, our algorithm uses various different kinds of information (hybrid information). The algorithm is based on linguistic constraints and preferences and uses an anaphoric accessibility space within which the algorithm finds the noun phrase. We present some experiments related to this algorithm and this space using a corpus of 204 dialogues. The algorithm is implemented in Prolog. According to this study, 95.9% of antecedents were located in the proposed space, a precision of 81.3% was obtained for pronominal anaphora resolution, and 81.5% for adjectival anaphora."
      },
      {
        "node_idx": 79086,
        "score_0_10": 10,
        "title": "reordering rules for english hindi smt",
        "abstract": "Reordering is a preprocessing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. We are proposing a rich set of rules for better reordering. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase-based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality. We have observed significant improvements in the translation quality by using our approach over the baseline SMT. We have used BLEU, NIST, multi-reference word error rate, multi-reference position independent error rate for judging the improvements. We have exploited open source SMT toolkit MOSES to develop the system."
      },
      {
        "node_idx": 116147,
        "score_0_10": 10,
        "title": "the bisimulation problem for equational graphs of finite out degree",
        "abstract": "The \"bisimulation problem\" for equational graphs of finite out-degree is shown to be decidable. We reduce this problem to the bisimulation problem for deterministic rational (vectors of) boolean series on the alphabet of a dpda M. We then exhibit a complete formal system for deducing equivalent pairs of such vectors."
      },
      {
        "node_idx": 77766,
        "score_0_10": 10,
        "title": "lexicographic choice functions",
        "abstract": "We investigate a generalisation of the coherent choice functions considered by Seidenfeld et al. (2010), by sticking to the convexity axiom but imposing no Archimedeanity condition. We define our choice functions on vector spaces of options, which allows us to incorporate as special cases both Seidenfeld et al.'s (2010) choice functions on horse lotteries and sets of desirable gambles (Quaeghebeur, 2014), and to investigate their connections. We show that choice functions based on sets of desirable options (gambles) satisfy Seidenfeld's convexity axiom only for very particular types of sets of desirable options, which are in a one-to-one relationship with the lexicographic probabilities. We call them lexicographic choice functions. Finally, we prove that these choice functions can be used to determine the most conservative convex choice function associated with a given binary relation."
      },
      {
        "node_idx": 116697,
        "score_0_10": 10,
        "title": "extended unary coding",
        "abstract": "Extended variants of the recently introduced spread unary coding are described. These schemes, in which the length of the code word is fixed, allow representation of approximately n^2 numbers for n bits, rather than the n numbers of the standard unary coding. In the first of two proposed schemes the spread increases, whereas in the second scheme the spread remains constant."
      },
      {
        "node_idx": 29514,
        "score_0_10": 10,
        "title": "mtil17 english to indian langauge statistical machine translation",
        "abstract": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation."
      },
      {
        "node_idx": 22972,
        "score_0_10": 10,
        "title": "phrase pair mappings for hindi english statistical machine translation",
        "abstract": "In this paper, we present our work on the creation of lexical resources for the Machine Translation between English and Hindi. We describes the development of phrase pair mappings for our experiments and the comparative performance evaluation between different trained models on top of the baseline Statistical Machine Translation system. We focused on augmenting the parallel corpus with more vocabulary as well as with various inflected forms by exploring different ways. We have augmented the training corpus with various lexical resources such as lexical words, synset words, function words and verb phrases. We have described the case studies, automatic and subjective evaluations, detailed error analysis for both the English to Hindi and Hindi to English machine translation systems. We further analyzed that, there is an incremental growth in the quality of machine translation with the usage of various lexical resources. Thus lexical resources do help uplift the translation quality of resource poor langugaes."
      },
      {
        "node_idx": 151298,
        "score_0_10": 10,
        "title": "bilingual words and phrase mappings for marathi and hindi smt",
        "abstract": "Lack of proper linguistic resources is the major challenges faced by the Machine Translation system developments when dealing with the resource poor languages. In this paper, we describe effective ways to utilize the lexical resources to improve the quality of statistical machine translation. Our research on the usage of lexical resources mainly focused on two ways, such as; augmenting the parallel corpus with more vocabulary and to provide various word forms. We have augmented the training corpus with various lexical resources such as lexical words, function words, kridanta pairs and verb phrases. We have described the case studies, evaluations and detailed error analysis for both Marathi to Hindi and Hindi to Marathi machine translation systems. From the evaluations we observed that, there is an incremental growth in the quality of machine translation as the usage of various lexical resources increases. Moreover, usage of various lexical resources helps to improve the coverage and quality of machine translation where limited parallel corpus is available."
      },
      {
        "node_idx": 164989,
        "score_0_10": 9,
        "title": "google s multilingual neural machine translation system enabling zero shot translation",
        "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages."
      }
    ]
  },
  "480": {
    "explanation": "neural networks for natural language understanding and secure data storage",
    "topk": [
      {
        "node_idx": 137083,
        "score_0_10": 10,
        "title": "teaching machines to read and comprehend",
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
      },
      {
        "node_idx": 11752,
        "score_0_10": 10,
        "title": "a neural attention model for abstractive sentence summarization",
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
      },
      {
        "node_idx": 140427,
        "score_0_10": 10,
        "title": "exploiting similarities among languages for machine translation",
        "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
      },
      {
        "node_idx": 76222,
        "score_0_10": 10,
        "title": "elsa efficient long term secure storage of large datasets",
        "abstract": "An increasing amount of information today is generated, exchanged, and stored digitally. This also includes long-lived and highly sensitive information (e.g., electronic health records, governmental documents) whose integrity and confidentiality must be protected over decades or even centuries. While there is a vast amount of cryptography-based data protection schemes, only few are designed for long-term protection. Recently, Braun et al. (AsiaCCS'17) proposed the first long-term protection scheme that provides renewable integrity protection and information-theoretic confidentiality protection. However, computation and storage costs of their scheme increase significantly with the number of stored data items. As a result, their scheme appears suitable only for protecting databases with a small number of relatively large data items, but unsuitable for databases that hold a large number of relatively small data items (e.g., medical record databases). In this work, we present a solution for efficient long-term integrity and confidentiality protection of large datasets consisting of relatively small data items. First, we construct a renewable vector commitment scheme that is information-theoretically hiding under selective decommitment. We then combine this scheme with renewable timestamps and information-theoretically secure secret sharing. The resulting solution requires only a single timestamp for protecting a dataset while the state of the art requires a number of timestamps linear in the number of data items. We implemented our solution and measured its performance in a scenario where 12 000 data items are aggregated, stored, protected, and verified over a time span of 100 years. Our measurements show that our new solution completes this evaluation scenario an order of magnitude faster than the state of the art."
      },
      {
        "node_idx": 53478,
        "score_0_10": 9,
        "title": "opennmt open source toolkit for neural machine translation",
        "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques."
      },
      {
        "node_idx": 157548,
        "score_0_10": 9,
        "title": "a neural conversational model",
        "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 42662,
        "score_0_10": 9,
        "title": "the fast downward planning system",
        "abstract": "Fast Downward is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. However, unlike other PDDL planning systems, Fast Downward does not use the propositional PDDL representation of a planning task directly. Instead, the input is first translated into an alternative representation called multivalued planning tasks, which makes many of the implicit constraints of a propositional planning task explicit. Exploiting this alternative representation, Fast Downward uses hierarchical decompositions of planning tasks for computing its heuristic function, called the causal graph heuristic, which is very different from traditional HSP-like heuristics based on ignoring negative interactions of operators.#R##N##R##N#In this article, we give a full account of Fast Downward's approach to solving multivalued planning tasks. We extend our earlier discussion of the causal graph heuristic to tasks involving axioms and conditional effects and present some novel techniques for search control that are used within Fast Downward's best-first search algorithm: preferred operators transfer the idea of helpful actions from local search to global best-first search, deferred evaluation of heuristic functions mitigates the negative effect of large branching factors on search performance, and multiheuristic best-first search combines several heuristic evaluation functions within a single search algorithm in an orthogonal way. We also describe efficient data structures for fast state expansion (successor generators and axiom evaluators) and present a new non-heuristic search algorithm called focused iterative-broadening search, which utilizes the information encoded in causal graphs in a novel way.#R##N##R##N#Fast Downward has proven remarkably successful: It won the \"classical\" (i. e., propositional, non-optimising) track of the 4th International Planning Competition at ICAPS 2004, following in the footsteps of planners such as FF and LPG. Our experiments show that it also performs very well on the benchmarks of the earlier planning competitions and provide some insights about the usefulness of the new search enhancements."
      },
      {
        "node_idx": 26928,
        "score_0_10": 9,
        "title": "propyla privacy preserving long term secure storage",
        "abstract": "The amount of electronically stored information increases rapidly. Sensitive information requires integrity and confidentiality protection, sometimes for decades or even centuries (e.g., health records or governmental documents). Commonly used cryptographic schemes, however, are not designed to provide protection over such long time periods. Their security usually relies on the hardness of a specific computational problem and security cannot be maintained against unforeseeable developments in computational technology (e.g., quantum computers breaking RSA-based systems). Recently, Braun et al.\\ (\\mboxAsiaCCS'17 ) proposed the first storage architecture that supports integrity protection renewal while guaranteeing information theoretic confidentiality. However, their solution only considers the storage of unstructured data and does not allow for reading or writing subparts of the data.    Our contribution is the first long-term secure storage architecture that supports storage of structured databases and provides long-term integrity, confidentiality, and access pattern hiding security. To achieve this, we combine several cryptographic components (i.e., secret sharing, renewable timestamps, and renewable commitments) with an information-theoretically secure \\mboxORAM such that the described security properties are achieved. We also prove our construction secure and show that it only introduces a small overhead compared to standard secret sharing and ORAM based storage solutions."
      },
      {
        "node_idx": 43464,
        "score_0_10": 9,
        "title": "bidirectional attention flow for machine comprehension",
        "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."
      }
    ]
  },
  "481": {
    "explanation": "Robust sound source localization for mobile robots using microphone arrays",
    "topk": [
      {
        "node_idx": 96771,
        "score_0_10": 10,
        "title": "robust sound source localization using a microphone array on a mobile robot",
        "abstract": "The hearing sense on a mobile robot is important because it is omnidirectional and it does not require direct line-of-sight with the sound source. Such capabilities can nicely complement vision to help localize a person or an interesting event in the environment. To do so the robot auditory system must be able to work in noisy, unknown and diverse environmental conditions. In this paper, we present a robust sound source localization method in three-dimensional space using an array of 8 microphones. The method is based on time delay of arrival estimation. Results show that a mobile robot can localize in real time different types of sound sources over a range of 3 meters and with a precision of 3/spl deg/."
      },
      {
        "node_idx": 156364,
        "score_0_10": 10,
        "title": "a comprehensive survey of recent advancements in molecular communication",
        "abstract": "With much advancement in the field of nanotechnology, bioengineering, and synthetic biology over the past decade, microscales and nanoscales devices are becoming a reality. Yet the problem of engineering a reliable communication system between tiny devices is still an open problem. At the same time, despite the prevalence of radio communication, there are still areas where traditional electromagnetic waves find it difficult or expensive to reach. Points of interest in industry, cities, and medical applications often lie in embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or they are located in such a way that directional high frequency systems are ineffective. Inspired by nature, one solution to these problems is molecular communication (MC), where chemical signals are used to transfer information. Although biologists have studied MC for decades, it has only been researched for roughly 10 year from a communication engineering lens. Significant number of papers have been published to date, but owing to the need for interdisciplinary work, much of the results are preliminary. In this survey, the recent advancements in the field of MC engineering are highlighted. First, the biological, chemical, and physical processes used by an MC system are discussed. This includes different components of the MC transmitter and receiver, as well as the propagation and transport mechanisms. Then, a comprehensive survey of some of the recent works on MC through a communication engineering lens is provided. The survey ends with a technology readiness analysis of MC and future research directions."
      },
      {
        "node_idx": 95479,
        "score_0_10": 10,
        "title": "a simple semantics for haskell overloading",
        "abstract": "As originally proposed, type classes provide overloading and ad-hoc definition, but can still be understood (and implemented) in terms of strictly parametric calculi. This is not true of subsequent extensions of type classes. Functional dependencies and equality constraints allow the satisfiability of predicates to refine typing; this means that the interpretations of equivalent qualified types may not be interconvertible. Overlapping instances and instance chains allow predicates to be satisfied without determining the implementations of their associated class methods, introducing truly non-parametric behavior. We propose a new approach to the semantics of type classes, interpreting polymorphic expressions by the behavior of each of their ground instances, but without requiring that those behaviors be parametrically determined. We argue that this approach both matches the intuitive meanings of qualified types and accurately models the behavior of programs."
      },
      {
        "node_idx": 84147,
        "score_0_10": 10,
        "title": "online networks and subjective well being",
        "abstract": "We argue that the use of online networks may threaten subjective well-being in several ways, due to the inherent attributes of Internet-mediated interaction and through its effects on social trust and sociability. We test our hypotheses on a representative sample of the Italian population. We find a significantly negative correlation between online networking and well-being. This result is partially confirmed after accounting for endogeneity. We explore the direct and indirect effects of the use of social networking sites (SNS) on well-being in a SEM analysis. We find that online networking plays a positive role in subjective well-being through its impact on physical interactions, whereas SNS use is associated with lower social trust. The overall effect of networking on individual welfare is significantly negative."
      },
      {
        "node_idx": 40125,
        "score_0_10": 10,
        "title": "passive communication with ambient light",
        "abstract": "In this work, we propose a new communication system for illuminated areas, indoors and outdoors. Light sources in our environments --such as light bulbs or even the sun-- are our signal emitters, but we do not modulate data at the light source. We instead propose that the environment itself modulates the {ambient} light signals: if mobile elements `wear' patterns consisting of distinctive reflecting surfaces, single photodiode could decode the disturbed light signals to read passive information. Achieving this vision requires a deep understanding of a new type of communication channel. Many parameters can affect the performance of passive communication based on visible light: the size of reflective surfaces, the surrounding light intensity, the speed of mobile objects, the field-of-view of the receiver, to name a few. In this paper, we present our vision for a passive communication channel with visible light, the design challenges and the evaluation of an outdoor application where our receiver decodes information from a car moving at 18 km/h."
      },
      {
        "node_idx": 149351,
        "score_0_10": 10,
        "title": "the incentive ratio in exchange economies",
        "abstract": "The incentive ratio measures the utility gains from strategic behaviour. Without any restrictions on the setup, ratios for linear, Leontief and Cobb-Douglas exchange markets are unbounded, showing that manipulating the equilibrium is a worthwhile endeavour, even if it is computationally challenging. Such unbounded improvements can be achieved even if agents only misreport their utility functions. This provides a sharp contrast with previous results from Fisher markets. When the Cobb-Douglas setup is more restrictive, the maximum utility gain is bounded by the number of commodities. By means of an example, we show that it is possible to exceed a known upper bound for Fisher markets in exchange economies."
      },
      {
        "node_idx": 157671,
        "score_0_10": 9,
        "title": "telerobotic pointing gestures shape human spatial cognition",
        "abstract": "This paper aimed to explore whether human beings can understand gestures produced by telepresence robots. If it were the case, they can derive meaning conveyed in telerobotic gestures when processing spatial information. We conducted two experiments over Skype in the present study. Participants were presented with a robotic interface that had arms, which were teleoperated by an experimenter. The robot can point to virtual locations that represented certain entities. In Experiment 1, the experimenter described spatial locations of fictitious objects sequentially in two conditions: speech only condition (SO, verbal descriptions clearly indicated the spatial layout) and speech and robotic gesture condition (SR, verbal descriptions were ambiguous but accompanied by robotic pointing gestures). Participants were then asked to recall the objects\u2019 spatial locations. We found that the number of spatial locations recalled in the SR condition was on par with that in the SO condition, suggesting that telerobotic pointing gestures compensated ambiguous speech during the process of spatial information. In Experiment 2, the experimenter described spatial locations non-sequentially in the SR and SO conditions. Surprisingly, the number of spatial locations recalled in the SR condition was even higher than that in the SO condition, suggesting that telerobotic pointing gestures were more powerful than speech in conveying spatial information when information was presented in an unpredictable order. The findings provide evidence that human beings are able to comprehend telerobotic gestures, and importantly, integrate these gestures with co-occurring speech. This work promotes engaging remote collaboration among humans through a robot intermediary."
      },
      {
        "node_idx": 102163,
        "score_0_10": 9,
        "title": "emotional storytelling using virtual and robotic agents",
        "abstract": "In order to create effective storytelling agents three fundamental questions must be answered: first, is a physically embodied agent preferable to a virtual agent or a voice-only narration? Second, does a human voice have an advantage over a synthesised voice? Third, how should the emotional trajectory of the different characters in a story be related to a storyteller's facial expressions during storytelling time, and how does this correlate with the apparent emotions on the faces of the listeners? The results of two specially designed studies indicate that the physically embodied robot produces more attention to the listener as compared to a virtual embodiment, that a human voice is preferable over the current state of the art of text-to-speech, and that there is a complex yet interesting relation between the emotion lines of the story, the facial expressions of the narrating agent, and the emotions of the listener, and that the empathising of the listener is evident through its facial expressions. This work constitutes an important step towards emotional storytelling robots that can observe their listeners and adapt their style in order to maximise their effectiveness."
      },
      {
        "node_idx": 158981,
        "score_0_10": 9,
        "title": "probe and adapt rate adaptation for http video streaming at scale",
        "abstract": "Today, the technology for video streaming over the Internet is converging towards a paradigm named HTTP-based adaptive streaming (HAS), which brings two new features. First, by using HTTP/TCP, it leverages network-friendly TCP to achieve both firewall/NAT traversal and bandwidth sharing. Second, by pre-encoding and storing the video in a number of discrete rate levels, it introduces video bitrate adaptivity in a scalable way so that the video encoding is excluded from the closed-loop adaptation. A conventional wisdom in HAS design is that since the TCP throughput observed by a client would indicate the available network bandwidth, it could be used as a reliable reference for video bitrate selection. We argue that this is no longer true when HAS becomes a substantial fraction of the total network traffic. We show that when multiple HAS clients compete at a network bottleneck, the discrete nature of the video bitrates results in difficulty for a client to correctly perceive its fair-share bandwidth. Through analysis and test bed experiments, we demonstrate that this fundamental limitation leads to video bitrate oscillation and other undesirable behaviors that negatively impact the video viewing experience. We therefore argue that it is necessary to design at the application layer using a \"probe and adapt\" principle for video bitrate adaptation (where \"probe\" refers to trial increment of the data rate, instead of sending auxiliary piggybacking traffic), which is akin, but also orthogonal to the transport-layer TCP congestion control. We present PANDA - a client-side rate adaptation algorithm for HAS - as a practical embodiment of this principle. Our test bed results show that compared to conventional algorithms, PANDA is able to reduce the instability of video bitrate selection by over 75% without increasing the risk of buffer underrun."
      },
      {
        "node_idx": 39435,
        "score_0_10": 9,
        "title": "network cournot competition",
        "abstract": "Cournot competition is a fundamental economic model that represents firms competing in a single market of a homogeneous good. Each firm tries to maximize its utility---a function of the production cost as well as market price of the product---by deciding on the amount of production. In today's dynamic and diverse economy, many firms often compete in more than one market simultaneously, i.e., each market might be shared among a subset of these firms. In this situation, a bipartite graph models the access restriction where firms are on one side, markets are on the other side, and edges demonstrate whether a firm has access to a market or not. We call this game \\emph{Network Cournot Competition} (NCC). In this paper, we propose algorithms for finding pure Nash equilibria of NCC games in different situations. First, we carefully design a potential function for NCC, when the price functions for markets are linear functions of the production in that market. However, for nonlinear price functions, this approach is not feasible. We model the problem as a nonlinear complementarity problem in this case, and design a polynomial-time algorithm that finds an equilibrium of the game for strongly convex cost functions and strongly monotone revenue functions. We also explore the class of price functions that ensures strong monotonicity of the revenue function, and show it consists of a broad class of functions. Moreover, we discuss the uniqueness of equilibria in both of these cases which means our algorithms find the unique equilibria of the games. Last but not least, when the cost of production in one market is independent from the cost of production in other markets for all firms, the problem can be separated into several independent classical \\emph{Cournot Oligopoly} problems. We give the first combinatorial algorithm for this widely studied problem."
      }
    ]
  },
  "482": {
    "explanation": "EMG-driven teleoperation mapping for non-anthropomorphic robot hands",
    "topk": [
      {
        "node_idx": 39237,
        "score_0_10": 10,
        "title": "emg controlled non anthropomorphic hand teleoperation using a continuous teleoperation subspace",
        "abstract": "We present a method for EMG-driven teleoperation of non-anthropomorphic robot hands. EMG sensors are appealing as a wearable, inexpensive, and unobtrusive way to gather information about the teleoperator's hand pose. However, mapping from EMG signals to the pose space of a non-anthropomorphic hand presents multiple challenges. We present a method that first projects from forearm EMG into a subspace relevant to teleoperation. To increase robustness, we use a model which combines continuous and discrete predictors along different dimensions of this subspace. We then project from the teleoperation subspace into the pose space of the robot hand. Our method is effective and intuitive, as it enables novice users to teleoperate pick and place tasks faster and more robustly than state-of-the-art EMG teleoperation methods when applied to a non-anthropomorphic, multi-DOF robot hand."
      },
      {
        "node_idx": 96505,
        "score_0_10": 10,
        "title": "a continuous teleoperation subspace with empirical and algorithmic mapping algorithms for non anthropomorphic hands",
        "abstract": "Teleoperation is a valuable tool for robotic manipulators in highly unstructured environments. However, finding an intuitive mapping between a human hand and a non-anthropomorphic robot hand can be difficult, due to the hands' dissimilar kinematics. In this paper, we seek to create a mapping between the human hand and a fully actuated, non-anthropomorphic robot hand that is intuitive enough to enable effective real-time teleoperation, even for novice users. To accomplish this, we propose a low-dimensional teleoperation subspace which can be used as an intermediary for mapping between hand pose spaces. We present two different methods to define the teleoperation subspace: an empirical definition, which requires a person to define hand motions in an intuitive, hand-specific way, and an algorithmic definition, which is kinematically independent, and uses objects to define the subspace. We use each of these definitions to create a teleoperation mapping for different hands. We validate both the empirical and algorithmic mappings with teleoperation experiments controlled by novices and performed on two kinematically distinct hands. The experiments show that the proposed subspace is relevant to teleoperation, intuitive enough to enable control by novices, and can generalize to non-anthropomorphic hands with different kinematic configurations."
      },
      {
        "node_idx": 78298,
        "score_0_10": 10,
        "title": "adaptive visual tracking for robotic systems without image space velocity measurement",
        "abstract": "In this paper, we investigate the visual tracking problem for robotic systems without image-space velocity measurement, simultaneously taking into account the uncertainties of the camera model and the manipulator kinematics and dynamics. We propose a new image-space observer that exploits the image-space velocity information contained in the unknown kinematics, upon which, we design an adaptive controller without using the image-space velocity signal where the adaptations of the depth-rate-independent kinematic parameter and depth parameter are driven by both the image-space tracking errors and observation errors. The major superiority of the proposed observer-based adaptive controller lies in its simplicity and the separation of the handling of multiple uncertainties in visually servoed robotic systems, thus avoiding the overparametrization problem of the existing work. Using Lyapunov analysis, we demonstrate that the image-space tracking errors converge to zero asymptotically. The performance of the proposed adaptive control scheme is illustrated by a numerical simulation."
      },
      {
        "node_idx": 35167,
        "score_0_10": 9,
        "title": "optimization model for planning precision grasps with multi fingered hands",
        "abstract": "Precision grasps with multi-fingered hands are important for precise placement and in-hand manipulation tasks. Searching precision grasps on the object represented by point cloud, is challenging due to the complex object shape, high-dimensionality, collision and undesired properties of the sensing and positioning. This paper proposes an optimization model to search for precision grasps with multi-fingered hands. The model takes noisy point cloud of the object as input and optimizes the grasp quality by iteratively searching for the palm pose and finger joints positions. The collision between the hand and the object is approximated and penalized by a series of least-squares. The collision approximation is able to handle the point cloud representation of the objects with complex shapes. The proposed optimization model is able to locate collision-free optimal precision grasps efficiently. The average computation time is 0.50 sec/grasp. The searching is robust to the incompleteness and noise of the point cloud. The effectiveness of the algorithm is demonstrated by experiments."
      },
      {
        "node_idx": 3250,
        "score_0_10": 9,
        "title": "emg controlled hand teleoperation using a continuous teleoperation subspace",
        "abstract": "We present a method for EMG-driven teleoperation of non-anthropomorphic robot hands. EMG sensors are appealing as a wearable, inexpensive and unobtrusive way to gather information about the teleoperator's hand pose. However, mapping from EMG signals to the pose space of a non-anthropomorphic hand presents multiple challenges. We present a method that first projects from forearm EMG into a subspace relevant to teleoperation. To increase robustness, we use a model which combines continuous and discrete predictors along different dimensions of this subspace. We then project from the teleoperation subspace into the pose space of the robot hand. We show that our method is effective and intuitive, as it enables novice users to teleoperate pick and place tasks faster and more robustly than state-of-the-art EMG teleoperation methods when applied to a non-anthropomorphic, multi-DOF robot hand."
      },
      {
        "node_idx": 93768,
        "score_0_10": 9,
        "title": "trajectory advancement during human robot collaboration",
        "abstract": "As technology advances, the barriers between the co-existence of humans and robots are slowly coming down. The prominence of physical interactions for collaboration and cooperation between humans and robots will be an undeniable fact. Rather than exhibiting simple reactive behaviors to human interactions, it is desirable to endow robots with augmented capabilities of exploiting human interactions for successful task completion. Towards that goal, in this paper, we propose a trajectory advancement approach in which we mathematically derive the conditions that facilitate advancing along a reference trajectory by leveraging assistance from helpful interaction wrench present during human-robot collaboration. We validate our approach through experiments conducted with the iCub humanoid robot both in simulation and on the real robot."
      },
      {
        "node_idx": 162000,
        "score_0_10": 9,
        "title": "grasp planning for customized grippers by iterative surface fitting",
        "abstract": "Customized grippers have broad applications in industrial assembly lines. Compared with general parallel grippers, the customized grippers have specifically designed fingers to increase the contact area with the workpieces and improve the grasp robustness. However, grasp planning for customized grippers is challenging due to the object variations, surface contacts and structural constraints of the grippers. In this paper, an iterative surface fitting (ISF) algorithm is proposed to plan grasps for customized grippers. ISF simultaneously searches for optimal gripper transformation and finger displacement by minimizing the surface fitting error. A guided sampling is introduced to avoid ISF getting stuck in local optima and improve the collision avoidance performance. The proposed algorithm is able to consider the structural constraints of the gripper, such as the range of jaw width, and plan optimal grasps in real-time. The effectiveness of the algorithm is verified by both simulations and experiments."
      },
      {
        "node_idx": 34431,
        "score_0_10": 9,
        "title": "adaptive visual tracking for robotic systems without visual velocity measurement",
        "abstract": "In this paper, we investigate the visual tracking for roboti c systems without visual velocity measurement, simultaneously taking into account the uncertain depth information, the kinematic and the dynamic uncertainties. We propose a new image-space observer that exploits the image-space velocity information contained in the unknown kinematics, upon which, we design an adaptive controller without using the visual velocity signal where the estimated kinematic and depth parameters are driven by both the image-space tracking errors and observation errors. The major superiority of the proposed observerbased adaptive controller lies in its simplicity and the dec omposition of the handling of the multiple uncertainties in visually servoing robotic systems, thus a voiding the overparametrization problem of the existing results. Using Lyapunov analysis, we demonstrate that the closed-loop system is stable and that the image-space tracking errors converge to zero asymptotically. Simulation results are provided to illustrate the performance of the proposed adaptive control scheme."
      },
      {
        "node_idx": 14491,
        "score_0_10": 9,
        "title": "motion primitives for robotic flight control",
        "abstract": "We introduce a simple framework for learning aggressive maneuvers in flight control of UAVs. Having inspired from biological environment, dynamic movement primitives are analyzed and extended using nonlinear contraction theory. Accordingly, primitives of an observed movement are stably combined and concatenated. We demonstrate our results experimentally on the Quanser Helicopter, in which we first imitate aggressive maneuvers and then use them as primitives to achieve new maneuvers that can fly over an obstacle."
      },
      {
        "node_idx": 153276,
        "score_0_10": 9,
        "title": "user guidance for interactive camera calibration",
        "abstract": "For building a Augmented Reality (AR) pipeline, the most crucial step is the camera calibration as overall quality heavily depends on it. In turn camera calibration itself is influenced most by the choice of camera-to-pattern poses - yet currently there is only little research on guiding the user to a specific pose. We build upon our novel camera calibration framework that is capable to generate calibration poses in real-time and present a user study evaluating different visualization methods to guide the user to a target pose. Using the presented method even novel users are capable to perform a precise camera calibration in about 2 minutes."
      }
    ]
  },
  "484": {
    "explanation": "wireless information and power transfer optimization and tradeoffs",
    "topk": [
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 159367,
        "score_0_10": 9,
        "title": "natural deduction and the isabelle proof assistant",
        "abstract": "We describe our Natural Deduction Assistant (NaDeA) and the interfaces between the Isabelle proof assistant and NaDeA. In particular, we explain how NaDeA, using a generated prover that has been verified in Isabelle, provides feedback to the student, and also how NaDeA, for each formula proved by the student, provides a generated theorem that can be verified in Isabelle."
      },
      {
        "node_idx": 141819,
        "score_0_10": 9,
        "title": "academic torrents scalable data distribution",
        "abstract": "As competitions get more popular, transferring ever-larger data sets becomes infeasible and costly. For example, downloading the 157.3 GB 2012 ImageNet data set incurs about $4.33 in bandwidth costs per download. Downloading the full ImageNet data set takes 33 days. ImageNet has since become popular beyond the competition, and many papers and models now revolve around this data set. For sharing such an important resource to the machine learning community, the sharers of ImageNet must shoulder a large bandwidth burden. Academic Torrents reduces this burden for disseminating competition data, and also increases download speeds for end users. Academic Torrents is run by a pending nonprofit.. By augmenting an existing HTTP server with a peer-to-peer swarm, requests get re-routed to get data from downloaders. While existing systems slow down with more users, the benefits of Academic Torrents grow, with noticeable effects even when only one other person is downloading."
      },
      {
        "node_idx": 11841,
        "score_0_10": 9,
        "title": "immersive and collaborative data visualization using virtual reality platforms",
        "abstract": "Effective data visualization is a key part of the discovery process in the era of \u201cbig data\u201d. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multidimensional information poses some deep questions in the field of cognition technology and human-computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi-dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional \u201cdesktop\u201d visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data."
      },
      {
        "node_idx": 130925,
        "score_0_10": 9,
        "title": "a mobile application for self guided study of formal reasoning",
        "abstract": "In this work, we introduce AXolotl, a self-study aid designed to guide students through the basics of formal reasoning and term manipulation. Unlike most of the existing study aids for formal reasoning, AXolotl is an Android-based application with a simple touch-based interface. Part of the design goal was to minimize the possibility of user errors which distract from the learning process. Such as typos or inconsistent application of the provided rules. The system includes a zoomable proof viewer which displays the progress made so far and allows for storage of the completed proofs as a JPEG or LaTeX file. The software is available on the google play store and comes with a small library of problems. Additional problems may be opened in AXolotl using a simple input language. Currently, AXolotl supports problems that can be solved using rules which transform a single expression into a set of expressions. This covers educational scenarios found in our first-semester introduction to logic course and helps bridge the gap between propositional and first-order reasoning. Future developments will include rewrite rules which take a set of expressions and return a set of expressions, as well as a quantified first-order extension."
      },
      {
        "node_idx": 98234,
        "score_0_10": 9,
        "title": "the mythos of model interpretability",
        "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not."
      },
      {
        "node_idx": 28821,
        "score_0_10": 9,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 160175,
        "score_0_10": 8,
        "title": "altmetrics in the wild using social media to explore scholarly impact",
        "abstract": "In growing numbers, scholars are integrating social media tools like blogs, Twitter, and Mendeley into their professional communications. The online, public nature of these tools exposes and reifies scholarly processes once hidden and ephemeral. Metrics based on this activities could inform broader, faster measures of impact, complementing traditional citation metrics. This study explores the properties of these social media-based metrics or \"altmetrics\", sampling 24,331 articles published by the Public Library of Science. #R##N#We find that that different indicators vary greatly in activity. Around 5% of sampled articles are cited in Wikipedia, while close to 80% have been included in at least one Mendeley library. There is, however, an encouraging diversity; a quarter of articles have nonzero data from five or more different sources. Correlation and factor analysis suggest citation and altmetrics indicators track related but distinct impacts, with neither able to describe the complete picture of scholarly use alone. There are moderate correlations between Mendeley and Web of Science citation, but many altmetric indicators seem to measure impact mostly orthogonal to citation. Articles cluster in ways that suggest five different impact \"flavors\", capturing impacts of different types on different audiences; for instance, some articles may be heavily read and saved by scholars but seldom cited. Together, these findings encourage more research into altmetrics as complements to traditional citation measures."
      },
      {
        "node_idx": 165677,
        "score_0_10": 8,
        "title": "a digital game maturity model dgmm",
        "abstract": "Abstract   Game development is an interdisciplinary concept that embraces artistic, software engineering, management, and business disciplines. This research facilitates a better understanding of important dimensions of digital game development methodology. Game development is considered as one of the most complex tasks in software engineering. The increased popularity of digital games, the challenges faced by game development organizations in developing quality games, and high competition in the digital game industry demand a game development maturity assessment. Consequently, this study presents a Digital Game Maturity Model to evaluate the current development methodology in an organization. The framework of this model consists of assessment questionnaires, a performance scale, and a rating method. The main goal of the questionnaires is to collect information about current processes and practices. In general, this research contributes towards formulating a comprehensive and unified strategy for game development maturity evaluation. Two case studies were conducted and their assessment results reported. These demonstrate the level of maturity of current development practices in two organizations."
      },
      {
        "node_idx": 65088,
        "score_0_10": 8,
        "title": "empirical investigation of key business factors for digital game performance",
        "abstract": "A B S T R A C T Game development is an interdisciplinary concept that embraces software engineering, business, management, and artistic disciplines. This research facilitates a better understanding of the business dimension of digital games. The main objective of this research is to investigate empirically the effect of business factors on the performance of digital games in the market and to answer the research questions asked in this study. Game development organizations are facing high pressure and competition in the digital game industry. Business has become a crucial dimension, especially for game development organizations. The main contribution of this paper is to investigate empirically the influence of key business factors on the business performance of games. This is the first study in the domain of game development that demonstrates the interrelationship between key business factors and game performance in the market. The results of the study provide evidence that game development organizations must deal with multiple business key factors to remain competitive and handle the high pressure in the digital game industry. Furthermore, the results of the study support the theoretical assertion that key business factors play an important role in game business performance."
      }
    ]
  },
  "487": {
    "explanation": "wireless communication system design and optimization methodologies",
    "topk": [
      {
        "node_idx": 58129,
        "score_0_10": 10,
        "title": "the weight distributions of cyclic codes and elliptic curves",
        "abstract": "Cyclic codes with two zeros and their dual codes as a practically and theoretically interesting class of linear codes, have been studied for many years. However, the weight distributions of cyclic codes are difficult to determine. From elliptic curves, this paper determines the weight distributions of dual codes of cyclic codes with two zeros for a few more cases."
      },
      {
        "node_idx": 100857,
        "score_0_10": 10,
        "title": "wireless information and power transfer architecture design and rate energy tradeoff",
        "abstract": "Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT)."
      },
      {
        "node_idx": 16140,
        "score_0_10": 10,
        "title": "modeling and analyzing millimeter wave cellular systems",
        "abstract": "We provide a comprehensive overview of mathematical models and analytical techniques for millimeter wave (mmWave) cellular systems. The two fundamental physical differences from conventional sub-6-GHz cellular systems are: 1) vulnerability to blocking and 2) the need for significant directionality at the transmitter and/or receiver, which is achieved through the use of large antenna arrays of small individual elements. We overview and compare models for both of these factors, and present a baseline analytical approach based on stochastic geometry that allows the computation of the statistical distributions of the downlink signal-to-interference-plus-noise ratio (SINR) and also the per link data rate, which depends on the SINR as well as the average load. There are many implications of the models and analysis: 1) mmWave systems are significantly more noise-limited than at sub-6 GHz for most parameter configurations; 2) initial access is much more difficult in mmWave; 3) self-backhauling is more viable than in sub-6-GHz systems, which makes ultra-dense deployments more viable, but this leads to increasingly interference-limited behavior; and 4) in sharp contrast to sub-6-GHz systems cellular operators can mutually benefit by sharing their spectrum licenses despite the uncontrolled interference that results from doing so. We conclude by outlining several important extensions of the baseline model, many of which are promising avenues for future research."
      },
      {
        "node_idx": 21298,
        "score_0_10": 10,
        "title": "limited feedback hybrid precoding for multi user millimeter wave systems",
        "abstract": "Antenna arrays will be an important ingredient in millimeter-wave (mmWave) cellular systems. A natural application of antenna arrays is simultaneous transmission to multiple users. Unfortunately, the hardware constraints in mmWave systems make it difficult to apply conventional lower frequency multiuser MIMO precoding techniques at mmWave. This paper develops low-complexity hybrid analog/digital precoding for downlink multiuser mmWave systems. Hybrid precoding involves a combination of analog and digital processing that is inspired by the power consumption of complete radio frequency and mixed signal hardware. The proposed algorithm configures hybrid precoders at the transmitter and analog combiners at multiple receivers with a small training and feedback overhead. The performance of the proposed algorithm is analyzed in the large dimensional regime and in single-path channels. When the analog and digital precoding vectors are selected from quantized codebooks, the rate loss due to the joint quantization is characterized, and insights are given into the performance of hybrid precoding compared with analog-only beamforming solutions. Analytical and simulation results show that the proposed techniques offer higher sum rates compared with analog-only beamforming solutions, and approach the performance of the unconstrained digital beamforming with relatively small codebooks."
      },
      {
        "node_idx": 28821,
        "score_0_10": 10,
        "title": "throughput maximization in wireless powered communication networks",
        "abstract": "This paper studies the newly emerging wireless powered communication network (WPCN) in which one hybrid access point (H-AP) with constant power supply coordinates the wireless energy/information transmissions to/from distributed users that do not have energy sources. A \"harvest-then-transmit\" protocol is proposed where all users first harvest the wireless energy broadcast by the H-AP in the downlink (DL) and then send their independent information to the H-AP in the uplink (UL) by time-division-multiple-access (TDMA). First, we study the sum-throughput maximization of all users by jointly optimizing the time allocation for the DL wireless power transfer versus the users' UL information transmissions given a total time constraint based on the users' DL and UL channels as well as their average harvested energy values. By applying convex optimization techniques, we obtain the closed-form expressions for the optimal time allocations to maximize the sum-throughput. Our solution reveals \"doubly near-far\" phenomenon due to both the DL and UL distance-dependent signal attenuation, where a far user from the H-AP, which receives less wireless energy than a nearer user in the DL, has to transmit with more power in the UL for reliable information transmission. Consequently, the maximum sum-throughput is achieved by allocating substantially more time to the near users than the far users, thus resulting in unfair rate allocation among different users. To overcome this problem, we furthermore propose a new performance metric so-called common-throughput with the additional constraint that all users should be allocated with an equal rate regardless of their distances to the H-AP. We present an efficient algorithm to solve the common-throughput maximization problem. Simulation results demonstrate the effectiveness of the common-throughput approach for solving the new doubly near-far problem in WPCNs."
      },
      {
        "node_idx": 121725,
        "score_0_10": 9,
        "title": "precoder index modulation",
        "abstract": "Index modulation, where information bits are conveyed through antenna indices (spatial modulation) and subcarrier indices (subcarrier index modulation) in addition to information bits conveyed through conventional modulation symbols, is getting increased research attention. In this paper, we introduce {\\em precoder index modulation}, where information bits are conveyed through the choice of a precoder matrix at the transmitter from a set of pre-determined pseudo-random phase precoder (PRPP) matrices. Combining precoder index modulation (PIM) and spatial modulation (SM), we introduce a PIM-SM scheme which conveys information bits through both antenna index as well as precoder index. Spectral efficiency (in bits per channel use) and bit error performance of these index modulation schemes are presented."
      },
      {
        "node_idx": 47821,
        "score_0_10": 9,
        "title": "pseudo random phase precoded spatial modulation",
        "abstract": "Spatial modulation (SM) is a transmission scheme that uses multiple transmit antennas but only one transmit RF chain. At each time instant, only one among the transmit antennas will be active and the others remain silent. The index of the active transmit antenna will also convey information bits in addition to the information bits conveyed through modulation symbols (e.g.,QAM). Pseudo-random phase precoding (PRPP) is a technique that can achieve high diversity orders even in single antenna systems without the need for channel state information at the transmitter (CSIT) and transmit power control (TPC). In this paper, we exploit the advantages of both SM and PRPP simultaneously. We propose a pseudo-random phase precoded SM (PRPP-SM) scheme, where both the modulation bits and the antenna index bits are precoded by pseudo-random phases. The proposed PRPP-SM system gives significant performance gains over SM system without PRPP and PRPP system without SM. Since maximum likelihood (ML) detection becomes exponentially complex in large dimensions, we propose low complexity local search based detection (LSD) algorithm suited for PRPP-SM systems with large precoder sizes. Our simulation results show that with 4 transmit antennas, 1 receive antenna, $5\\times 20$ pseudo-random phase precoder matrix and BPSK modulation, the performance of PRPP-SM using ML detection is better than SM without PRPP with ML detection by about 9 dB at $10^{-2}$ BER. This performance advantage gets even better for large precoding sizes."
      },
      {
        "node_idx": 74729,
        "score_0_10": 9,
        "title": "a survey on non orthogonal multiple access for 5g networks research challenges and future trends",
        "abstract": "Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed."
      },
      {
        "node_idx": 78813,
        "score_0_10": 9,
        "title": "maximum width empty square and rectangular annulus",
        "abstract": "An annulus is, informally, a ring-shaped region, often described by two concentric circles. The maximum-width empty annulus problem asks to find an annulus of a certain shape with the maximum possible width that avoids a given set of $n$ points in the plane. This problem can also be interpreted as the problem of finding an optimal location of a ring-shaped obnoxious facility among the input points. In this paper, we study square and rectangular variants of the maximum-width empty anuulus problem, and present first nontrivial algorithms. Specifically, our algorithms run in $O(n^3)$ and $O(n^2 \\log n)$ time for computing a maximum-width empty axis-parallel square and rectangular annulus, respectively. Both algorithms use only $O(n)$ space."
      },
      {
        "node_idx": 77009,
        "score_0_10": 9,
        "title": "millimeter wave cellular networks a mac layer perspective",
        "abstract": "The millimeter-wave (mmWave) frequency band is seen as a key enabler of multigigabit wireless access in future cellular networks. In order to overcome the propagation challenges, mmWave systems use a large number of antenna elements both at the base station and at the user equipment, which leads to high directivity gains, fully directional communications, and possible noise-limited operations. The fundamental differences between mmWave networks and traditional ones challenge the classical design constraints, objectives, and available degrees of freedom. This paper addresses the implications that highly directional communication has on the design of an efficient medium access control (MAC) layer. The paper discusses key MAC layer issues, such as synchronization, random access, handover, channelization, interference management, scheduling, and association. This paper provides an integrated view on MAC layer issues for cellular networks, identifies new challenges and tradeoffs, and provides novel insights and solution approaches."
      }
    ]
  },
  "489": {
    "explanation": "memristor-based analog arithmetic and multiplier circuit design",
    "topk": [
      {
        "node_idx": 105413,
        "score_0_10": 10,
        "title": "memristor based circuits for performing basic arithmetic operations",
        "abstract": "In almost all of the currently working circuits, especially in analog circuits implementing signal processing applications, basic arithmetic operations such as multiplication, addition, subtraction and division are performed on values which are represented by voltages or currents. However, in this paper, we propose a new and simple method for performing analog arithmetic operations which in this scheme, signals are represented and stored through a memristance of the newly found circuit element, i.e. memristor, instead of voltage or current. Some of these operators such as divider and multiplier are much simpler and faster than their equivalent voltage-based circuits and they require less chip area. In addition, a new circuit is designed for programming the memristance of the memristor with predetermined analog value. Presented simulation results demonstrate the effectiveness and the accuracy of the proposed circuits."
      },
      {
        "node_idx": 109041,
        "score_0_10": 10,
        "title": "cmos memristive analog multiplier design",
        "abstract": "This paper proposes four quadrant analog multiplier using CMOS-memristor circuit. Currently, there are plenty of analog multipliers using resistors and CMOS transistors. They can attain perfect multiplication but have several disadvantages such as lower processing speed, higher power consumption and larger chip areas. Memristor based circuits are introduced to resolve the mentioned drawbacks. In this paper current mode four quadrant multiplier based on squaring circuits is taken as a framework, and CMOS transistors are replaced with memristors. The circuit design is simulated with SPICE, and variability analysis and performance variation with temperature is performed. The proposed circuit allows faster processing with retained data while dissipating less power retaining the multiplication characteristics."
      },
      {
        "node_idx": 37234,
        "score_0_10": 10,
        "title": "cognitive memory network",
        "abstract": "A resistive memory network that has no crossover wiring is proposed to overcome the hardware limitations to size and functional complexity that is associated with conventional analogue neural networks. The proposed memory network is based on simple network cells that are arranged in a hierarchical modular architecture. Cognitive functionality of this network is demonstrated by an example of character recognition. The network is trained by an evolutionary process to completely recognise characters deformed by random noise, rotation, scaling and shifting."
      },
      {
        "node_idx": 146266,
        "score_0_10": 10,
        "title": "holographic sensing",
        "abstract": "Holographic representations of data encode information in packets of equal importance that enable progressive recovery. The quality of recovered data improves as more and more packets become available. This progressive recovery of the information is independent of the order in which packets become available. Such representations are ideally suited for distributed storage and for the transmission of data packets over networks with unpredictable delays and or erasures.  Several methods for holographic representations of signals and images have been proposed over the years and multiple description information theory also deals with such representations. Surprisingly, however, these methods had not been considered in the classical framework of optimal least-squares estimation theory, until very recently. We develop a least-squares approach to the design of holographic representation for stochastic data vectors, relying on the framework widely used in modeling signals and images."
      },
      {
        "node_idx": 141503,
        "score_0_10": 10,
        "title": "holographic image sensing",
        "abstract": "Holographic representations of data enable distributed storage with progressive refinement when the stored packets of data are made available in any arbitrary order. In this paper, we propose and test patch-based transform coding holographic sensing of image data. Our proposal is optimized for progressive recovery under random order of retrieval of the stored data. The coding of the image patches relies on the design of distributed projections ensuring best image recovery, in terms of the $\\ell_2$ norm, at each retrieval stage. The performance depends only on the number of data packets that has been retrieved thus far. #R##N#Several possible options to enhance the quality of the recovery while changing the size and number of data packets are discussed and tested. This leads us to examine several interesting bit-allocation and rate-distortion trade offs, highlighted for a set of natural images with ensemble estimated statistical properties."
      },
      {
        "node_idx": 15976,
        "score_0_10": 9,
        "title": "remembering chandra kintala",
        "abstract": "With this contribution we would like to remember Chandra M. R. Kintala who passed away in November 2009. We will give short overviews of his CV and his contributions to the field of theoretical and applied computer science and, given the opportunity, will attempt to present the current state of limited nondeterminism and limited resources for machines. Finally, we will briefly touch on some research topics which hopefully will be addressed in the not so distant future."
      },
      {
        "node_idx": 60311,
        "score_0_10": 9,
        "title": "analog multiplier design with cmos memristor circuits",
        "abstract": "CMOS-transistors circuits have been used as a conventional approach for designing an analog multiplier in modern era of industrial electronics. However, previous studies have shown, that based on the working region of transistors, such as saturation or weak inversion regions, the circuit may face issues with output ranges and accuracy. One possible solution to that problem could be choosing CMOS-memristors as a basis for the circuit. Although memristor research is still a growing and promising field, one could argue that its implementation could bring many benefits such as increased circuit density and superior computation speeds, etc. Additionally, the era of Moore's Law of downscaling the size of transistors is to eventually come to an end. No one knows whether the end of a scaling paradigm is to happen within the next five or twenty years. Hence, the research on this particular subject is quite important. This paper proposes an analog multiplier design with CMOS and memristive components. Mainly, the aim of the paper is to compare the power consumption and overall characteristic of the multiplier such as the accuracy and the output range to that of the conventional multiplier. The designed circuit is expected to be suitable for low power applications, and it is built using 18um CMOS technology. The circuit simulations will be conducted using SPICE software. Finally, the effects of channel modulation and temperature on the multiplier performance will be discussed."
      },
      {
        "node_idx": 81726,
        "score_0_10": 9,
        "title": "algebraic soft decision decoding of reed solomon codes using bit level soft information",
        "abstract": "The performance of algebraic soft-decision decoding of Reed-Solomon codes using bit-level soft information is investigated. Optimal multiplicity assignment strategies of algebraic soft-decision decoding with infinite cost are first studied over erasure channels and the binary symmetric channel. The corresponding decoding radii are calculated in closed forms and tight bounds on the error probability are derived. The multiplicity assignment strategy and the corresponding performance analysis are then generalized to characterize the decoding region of algebraic softdecision decoding over a mixed error and bit-level erasure channel. The bit-level decoding region of the proposed multiplicity assignment strategy is shown to be significantly larger than that of conventional Berlekamp-Massey decoding. As an application, a bit-level generalized minimum distance decoding algorithm is proposed. The proposed decoding compares favorably with many other Reed-Solomon soft-decision decoding algorithms over various channels. Moreover, owing to the simplicity of the proposed bit-level generalized minimum distance decoding, its performance can be tightly bounded using order statistics."
      },
      {
        "node_idx": 109046,
        "score_0_10": 9,
        "title": "sense amplifier design using cmos memristor circuits",
        "abstract": "With the increase of the speed of computers, timing and power requirements are becoming crucial for memory devices. The main objective of the paper is to modify 180nm CMOS sense amplifier design by using memristive devices and improve the design in terms of on-chip area, power efficiency, resistance to temperatures and speed. To achieve this, NOT gates in the circuit were constructed using memristor and CMOS. The main aim of the paper is to check the effect of memristors on characteristics of sense amplifier. The design was tested on Conventional Current Sense Amplifier (CSA) circuit. Changes in power, area, sensing delay and offset are reported in the paper."
      },
      {
        "node_idx": 30682,
        "score_0_10": 9,
        "title": "universum learning for svm regression",
        "abstract": "This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach."
      }
    ]
  },
  "490": {
    "explanation": "wireless communication degrees of freedom and interference management techniques",
    "topk": [
      {
        "node_idx": 131128,
        "score_0_10": 10,
        "title": "design and characterization of a full duplex multiantenna system for wifi networks",
        "abstract": "In this paper, we present an experiment- and simulation-based study to evaluate the use of full duplex (FD) as a potential mode in practical IEEE 802.11 networks. To enable the study, we designed a 20-MHz multiantenna orthogonal frequency-division-multiplexing (OFDM) FD physical layer and an FD media access control (MAC) protocol, which is backward compatible with current 802.11. Our extensive over-the-air experiments, simulations, and analysis demonstrate the following two results. First, the use of multiple antennas at the physical layer leads to a higher ergodic throughput than its hardware-equivalent multiantenna half-duplex (HD) counterparts for SNRs above the median SNR encountered in practical WiFi deployments. Second, the proposed MAC translates the physical layer rate gain into near doubling of throughput for multinode single-AP networks. The two results allow us to conclude that there are potentially significant benefits gained from including an FD mode in future WiFi standards."
      },
      {
        "node_idx": 2659,
        "score_0_10": 10,
        "title": "ramified structural recursion and corecursion",
        "abstract": "We investigate feasible computation over a fairly general notion of data and codata. Specifically, we present a direct Bellantoni-Cook-style normal/safe typed programming formalism, RS1, that expresses feasible structural recursions and corecursions over data and codata specified by polynomial functors. (Lists, streams, finite trees, infinite trees, etc. are all directly definable.) A novel aspect of RS1 is that it embraces structure-sharing as in standard functional-programming implementations. As our data representations use sharing, our implementation of structural recursions are memoized to avoid the possibly exponentially-many repeated subcomputations a naive implementation might perform. We introduce notions of size for representations of data (accounting for sharing) and codata (using ideas from type-2 computational complexity) and establish that type-level 1 RS1-functions have polynomial-bounded runtimes and satisfy a polynomial-time completeness condition. Also, restricting RS1 terms to particular types produces characterizations of some standard complexity classes (e.g., omega-regular languages, linear-space functions) and some less-standard classes (e.g., log-space streams)."
      },
      {
        "node_idx": 157681,
        "score_0_10": 10,
        "title": "degrees of freedom of time correlated miso broadcast channel with delayed csit",
        "abstract": "We consider the time correlated multiple-input single-output (MISO) broadcast channel where the transmitter has imperfect knowledge of the current channel state, in addition to delayed channel state information. By representing the quality of the current channel state information as P-\u03b1 for the signal-to-noise ratio P and some constant \u03b1 \u2265 0, we characterize the optimal degrees of freedom region for this more general two-user MISO broadcast correlated channel. The essential ingredients of the proposed scheme lie in the quantization and multicast of the overheard interferences, while broadcasting new private messages. Our proposed scheme smoothly bridges between the scheme recently proposed by Maddah-Ali and Tse with no current state information and a simple zero-forcing beamforming with perfect current state information."
      },
      {
        "node_idx": 47313,
        "score_0_10": 9,
        "title": "optimal use of current and outdated channel state information degrees of freedom of the miso bc with mixed csit",
        "abstract": "We consider a multiple-input-single-output (MISO) broadcast channel with mixed channel state information at the transmitter (CSIT) that consists of imperfect current CSIT and perfect outdated CSIT. Recent work by Kobayashi et al. presented a scheme which exploits both imperfect current CSIT and perfect outdated CSIT and achieves higher degrees of freedom (DoF) than possible with only imperfect current CSIT or only outdated CSIT individually. In this work, we further improve the achievable DoF in this setting by incorporating additional private messages, and provide a tight information theoretic DoF outer bound, thereby identifying the DoF optimal use of mixed CSIT. The new result is stronger even in the original setting of only delayed CSIT, because it allows us to remove the restricting assumption of statistically equivalent fading for all users."
      },
      {
        "node_idx": 147905,
        "score_0_10": 9,
        "title": "interference alignment with asymmetric complex signaling settling the host madsen nosratinia conjecture",
        "abstract": "It has been conjectured by Host-Madsen and Nosratinia that complex Gaussian interference channels with constant channel coefficients have only one degree-of-freedom regardless of the number of users. While several examples are known of constant channels that achieve more than 1 degree of freedom, these special cases only span a subset of measure zero. In other words, for almost all channel coefficient values, it is not known if more than 1 degree-of-freedom is achievable. In this paper, we settle the Host-Madsen-Nosratinia conjecture in the negative. We show that at least 1.2 degrees-of-freedom are achievable for all values of complex channel coefficients except for a subset of measure zero. For the class of linear beamforming and interference alignment schemes considered in this paper, it is also shown that 1.2 is the maximum number of degrees of freedom achievable on the complex Gaussian 3 user interference channel with constant channel coefficients, for almost all values of channel coefficients. To establish the achievability of 1.2 degrees of freedom we introduce the novel idea of asymmetric complex signaling - i.e., the inputs are chosen to be complex but not circularly symmetric. It is shown that unlike Gaussian point-to-point, multiple-access and broadcast channels where circularly symmetric complex Gaussian inputs are optimal, for interference channels optimal inputs are in general asymmetric. With asymmetric complex signaling, we also show that the 2 user complex Gaussian X channel with constant channel coefficients achieves the outer bound of 4/3 degrees-of-freedom, i.e., the assumption of time-variations/frequency-selectivity used in prior work to establish the same result, is not needed."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 109425,
        "score_0_10": 9,
        "title": "ergodic interference alignment",
        "abstract": "This paper develops a new communication strategy, ergodic interference alignment, for the K-user interference channel with time-varying fading. At any particular time, each receiver will see a superposition of the transmitted signals plus noise. The standard approach to such a scenario results in each transmitter-receiver pair achieving a rate proportional to 1/K its interference-free ergodic capacity. However, given two well-chosen time indices, the channel coefficients from interfering users can be made to exactly cancel. By adding up these two observations, each receiver can obtain its desired signal without any interference. If the channel gains have independent, uniform phases, this technique allows each user to achieve at least 1/2 its interference-free ergodic capacity at any signal-to-noise ratio. Prior interference alignment techniques were only able to attain this performance as the signal-to-noise ratio tended to infinity. Extensions are given for the case where each receiver wants a message from more than one transmitter as well as the \"X channel\" case (with two receivers) where each transmitter has an independent message for each receiver. Finally, it is shown how to generalize this strategy beyond Gaussian channel models. For a class of finite field interference channels, this approach yields the ergodic capacity region."
      },
      {
        "node_idx": 124619,
        "score_0_10": 9,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 69375,
        "score_0_10": 9,
        "title": "layerbuilder layer decomposition for interactive image and video color editing",
        "abstract": "Exploring and editing colors in images is a common task in graphic design and photography. However, allowing for interactive recoloring while preserving smooth color blends in the image remains a challenging problem. We present LayerBuilder, an algorithm that decomposes an image or video into a linear combination of colored layers to facilitate color-editing applications. These layers provide an interactive and intuitive means for manipulating individual colors. Our approach reduces color layer extraction to a fast iterative linear system. Layer Builder uses locally linear embedding, which represents pixels as linear combinations of their neighbors, to reduce the number of variables in the linear solve and extract layers that can better preserve color blending effects. We demonstrate our algorithm on recoloring a variety of images and videos, and show its overall effectiveness in recoloring quality and time complexity compared to previous approaches. We also show how this representation can benefit other applications, such as automatic recoloring suggestion, texture synthesis, and color-based filtering."
      },
      {
        "node_idx": 156305,
        "score_0_10": 9,
        "title": "overview of mc cdma papr reduction techniques",
        "abstract": "High Peak to Average Power Ratio (PAPR) of the transmitted signal is a critical problem in multicarrier modulation systems (MCM) such as Orthogonal Frequency Division Multiplexing (OFDM), and Multi-Carrier Code Division Multiple Access (MC CDMA) systems, due to large number of subcarriers. High PAPR leads to reduced resolution, and battery life. It also deteriorates system performance. This paper focuses on review of different PAPR reduction techniques with attendant technical issues as well as criteria for selection of PAPR reduction technique. To reduce PAPR the constraints are low power consumption, and low Bit Error Rate (BER). Spectral bandwidth is improved by better spectral characteristics, and low complexity/cost."
      }
    ]
  },
  "491": {
    "explanation": "TCP performance optimization and network coding mechanisms",
    "topk": [
      {
        "node_idx": 48830,
        "score_0_10": 10,
        "title": "tcp selective acknowledgments and ubr drop policies to improve atm ubr performance over terrestrial and satellite networks",
        "abstract": "We study the performance of Selective Acknowledgments with TCP over the ATM-UBR service category. We examine various UBR drop policies, TCP mechanisms and network configurations to recommend optimal parameters for TCP over UBR. We discuss various TCP congestion control mechanisms compare their performance for LAN and WAN networks. We describe the effect of satellite delays on TCP performance over UBR and present simulation results for LAN, WAN and satellite networks. SACK TCP improves the performance of TCP over UBR, especially for large delay networks. Intelligent drop policies at the switches are an important factor for good performance in local area networks."
      },
      {
        "node_idx": 140375,
        "score_0_10": 10,
        "title": "multipath tcp analysis design and implementation",
        "abstract": "Multipath TCP (MP-TCP) has the potential to greatly improve application performance by using multiple paths transparently. We propose a fluid model for a large class of MP-TCP algorithms and identify design criteria that guarantee the existence, uniqueness, and stability of system equilibrium. We clarify how algorithm parameters impact TCP-friendliness, responsiveness, and window oscillation and demonstrate an inevitable tradeoff among these properties. We discuss the implications of these properties on the behavior of existing algorithms and motivate our algorithm Balia (balanced linked adaptation), which generalizes existing algorithms and strikes a good balance among TCP-friendliness, responsiveness, and window oscillation. We have implemented Balia in the Linux kernel. We use our prototype to compare the new algorithm to existing MP-TCP algorithms."
      },
      {
        "node_idx": 142942,
        "score_0_10": 10,
        "title": "on tcp based session initiation protocol sip server overload control",
        "abstract": "The Session Initiation Protocol (SIP) server overload management has attracted interest since SIP is being widely deployed in the Next Generation Networks (NGN) as a core signaling protocol. Yet all existing SIP overload control work is focused on SIP-over-UDP, despite the fact that TCP is increasingly seen as the more viable choice of SIP transport. This paper answers the following questions: is the existing TCP flow control capable of handling the SIP overload problem? If not, why and how can we make it work? We provide a comprehensive explanation of the default SIP-over-TCP overload behavior through server instrumentation. We also propose and implement novel but simple overload control algorithms without any kernel or protocol level modification. Experimental evaluation shows that with our mechanism the overload performance improves from its original zero throughput to nearly full capacity. Our work leads to the important general insight that the traditional notion of TCP flow control alone is incapable of managing overload for time-critical session-based applications, which would be applicable not only to SIP, but also to a wide range of other common applications such as database servers."
      },
      {
        "node_idx": 41975,
        "score_0_10": 10,
        "title": "distributing content simplifies isp traffic engineering",
        "abstract": "Several major Internet service providers (e.g., Level-3, AT&T, Verizon) today also offer content distribution services. The emergence of such \"Network-CDNs\" (NCDNs) are driven by market forces that place more value on content services than just carrying the bits. NCDNs are also necessitated by the need to reduce the cost of carrying ever-increasing volumes of traffic across their backbones. An NCDN has the flexibility to determine both where content is placed and how traffic is routed within the network. However NCDNs today continue to treat traffic engineering independently from content placement and request redirection decisions. In this paper, we investigate the interplay between content distribution strategies and traffic engineering and ask how an NCDN should engineer traffic in a content-aware manner. Our experimental analysis, based on traces from a large content distribution network and real ISP topologies, shows that effective content placement can significantly simplify traffic engineering and in most cases obviate the need to engineer NCDN traffic all together! Further, we show that simple demand-oblivious schemes for routing and placement such as InverseCap and LRU suffice as they achieve network costs that are close to the best possible."
      },
      {
        "node_idx": 21360,
        "score_0_10": 10,
        "title": "arq for network coding",
        "abstract": "A new coding and queue management algorithm is proposed for communication networks that employ linear network coding. The algorithm has the feature that the encoding process is truly online, as opposed to a block-by-block approach. The setup assumes a packet erasure broadcast channel with stochastic arrivals and full feedback, but the proposed scheme is potentially applicable to more general lossy networks with link-by-link feedback. The algorithm guarantees that the physical queue size at the sender tracks the backlog in degrees of freedom (also called the virtual queue size). The new notion of a node ldquoseeingrdquo a packet is introduced. In terms of this idea, our algorithm may be viewed as a natural extension of ARQ schemes to coded networks. Our approach, known as the drop-when-seen algorithm, is compared with a baseline queuing approach called drop-when-decoded. It is shown that the expected queue size for our approach is O[(1)/(1-rho)] as opposed to Omega[(1)/(1-rho)2] for the baseline approach, where rho is the load factor."
      },
      {
        "node_idx": 110825,
        "score_0_10": 10,
        "title": "stateless multicast switching in software defined networks",
        "abstract": "Multicast data delivery can significantly reduce traffic in operators' networks, but has been limited in deployment due to concerns such as the scalability of state management. This paper shows how multicast can be implemented in contemporary software defined networking (SDN) switches, with less state than existing unicast switching strategies, by utilising a Bloom Filter (BF) based switching technique. Furthermore, the proposed mechanism uses only proactive rule insertion, and thus, is not limited by congestion or delay incurred by reactive controller-aided rule insertion. We compare our solution against common switching mechanisms such as layer-2 switching and MPLS in realistic network topologies by modelling the TCAM state sizes in SDN switches. The results demonstrate that our approach has significantly smaller state size compared to existing mechanisms and thus is a multicast switching solution for next generation networks."
      },
      {
        "node_idx": 45296,
        "score_0_10": 9,
        "title": "on determining the fair bandwidth share for abr connections in atm networks",
        "abstract": "The available bit rate (ABR) service is designed to fairly allocate the bandwidth unused by higher priority services. The network indicates to the ABR sources the rates at which they should transmit to minimize their cell loss. Switches must constantly measure the demand and available capacity, and divide the capacity fairly among the contending connections. In order to compute the fair and efficient allocation for each connection, a switch needs to determine the effective number of active connections. We propose a method for determining the number of active connections and the fair bandwidth share for each. We prove the efficiency and fairness of the proposed method analytically, and simulate it for a number of configurations."
      },
      {
        "node_idx": 44820,
        "score_0_10": 9,
        "title": "an end to end probabilistic network calculus with moment generating functions",
        "abstract": "Network calculus is a min-plus system theory for performance evaluation of queuing networks. Its elegance stems from intuitive convolution formulas for concatenation of deterministic servers. Recent research dispenses with the worst-case assumptions of network calculus to develop a probabilistic equivalent that benefits from statistical multiplexing. Significant achievements have been made, owing for example to the theory of effective bandwidths, however, the outstanding scalability set up by concatenation of deterministic servers has not been shown. #R##N#This paper establishes a concise, probabilistic network calculus with moment generating functions. The presented work features closed-form, end-to-end, probabilistic performance bounds that achieve the objective of scaling linearly in the number of servers in series. The consistent application of moment generating functions put forth in this paper utilizes independence beyond the scope of current statistical multiplexing of flows. A relevant additional gain is demonstrated for tandem servers with independent cross-traffic."
      },
      {
        "node_idx": 52827,
        "score_0_10": 9,
        "title": "ubr improving performance of tcp over atm ubr service",
        "abstract": "ATM-UBR switches respond to congestion by dropping cells when their buffers become full. TCP connections running over UBR experience low throughput and high unfairness. For 100% TCP throughput each switch needs buffers equal to the sum of the window sizes of all the TCP connections. Intelligent drop policies can improve the performance of TCP over UBR with limited buffers. The UBR+ service proposes enhancements to UBR for intelligent drop. Early Packet Discard improves throughput but does not attempt to improve fairness. Selective packet drop based on per-connection buffer occupancy improves fairness. The Fair Buffer Allocation scheme further improves both throughput and fairness."
      },
      {
        "node_idx": 35935,
        "score_0_10": 9,
        "title": "effective delay control in online network coding",
        "abstract": "Motivated by streaming applications with stringent delay constraints, we consider the design of online network coding algorithms with timely delivery guarantees. Assuming that the sender is providing the same data to multiple receivers over independent packet erasure channels, we focus on the case of perfect feedback and heterogeneous erasure probabilities. Based on a general analytical framework for evaluating the decoding delay, we show that existing ARQ schemes fail to ensure that receivers with weak channels are able to recover from packet losses within reasonable time. To overcome this problem, we re-define the encoding rules in order to break the chains of linear combinations that cannot be decoded after one of the packets is lost. Our results show that sending uncoded packets at key times ensures that all the receivers are able to meet specific delay requirements with very high probability."
      }
    ]
  },
  "495": {
    "explanation": "market design and equilibrium analysis in auction and network systems",
    "topk": [
      {
        "node_idx": 87405,
        "score_0_10": 10,
        "title": "concurrent auctions across the supply chain",
        "abstract": "With the recent technological feasibility of electronic commerce over the Internet, much attention has been given to the design of electronic markets for various types of electronically-tradable goods. Such markets, however, will normally need to function in some relationship with markets for other related goods, usually those downstream or upstream in the supply chain. Thus, for example, an electronic market for rubber tires for trucks will likely need to be strongly influenced by the rubber market as well as by the truck market.#R##N##R##N#In this paper we design protocols for exchange of information between a sequence of markets along a single supply chain. These protocols allow each of these markets to function separately, while the information exchanged ensures efficient global behavior across the supply chain. Each market that forms a link in the supply chain operates as a double auction, where the bids on one side of the double auction come from bidders in the corresponding segment of the industry, and the bids on the other side are synthetically generated by the protocol to express the combined information from all other links in the chain. The double auctions in each of the markets can be of several types, and we study several variants of incentive compatible double auctions, comparing them in terms of their efficiency and of the market revenue."
      },
      {
        "node_idx": 67704,
        "score_0_10": 10,
        "title": "knightian robustness of the vickrey mechanism",
        "abstract": "We investigate the resilience of some classical mechanisms to alternative specifications of preferences and information structures. Specifically, we analyze the Vickrey mechanism for auctions of multiple identical goods when the only information a player $i$ has about the profile of true valuations, $\\theta^*$, consists of a set of distributions, from one of which $\\theta_i^*$ has been drawn. #R##N#In this setting, the players no longer have complete preferences, and the Vickrey mechanism is no longer dominant-strategy. However, we prove that its efficiency performance is excellent, and essentially optimal, in undominated strategies."
      },
      {
        "node_idx": 156289,
        "score_0_10": 10,
        "title": "offloading in heterogeneous networks modeling analysis and design insights",
        "abstract": "Pushing data traffic from cellular to WiFi is an example of inter radio access technology (RAT) offloading. While this clearly alleviates congestion on the over-loaded cellular network, the ultimate potential of such offloading and its effect on overall system performance is not well understood. To address this, we develop a general and tractable model that consists of M different RATs, each deploying up to K different tiers of access points (APs), where each tier differs in transmit power, path loss exponent, deployment density and bandwidth. Each class of APs is modeled as an independent Poisson point process (PPP), with mobile user locations modeled as another independent PPP, all channels further consisting of i.i.d. Rayleigh fading. The distribution of rate over the entire network is then derived for a weighted association strategy, where such weights can be tuned to optimize a particular objective. We show that the optimum fraction of traffic offloaded to maximize SINR coverage is not in general the same as the one that maximizes rate coverage, defined as the fraction of users achieving a given rate."
      },
      {
        "node_idx": 30559,
        "score_0_10": 10,
        "title": "period estimation in astronomical time series using slotted correntropy",
        "abstract": "In this letter, we propose a method for period estimation in light curves from periodic variable stars using correntropy. Light curves are astronomical time series of stellar brightness over time, and are characterized as being noisy and unevenly sampled. We propose to use slotted time lags in order to estimate correntropy directly from irregularly sampled time series. A new information theoretic metric is proposed for discriminating among the peaks of the correntropy spectral density. The slotted correntropy method outperformed slotted correlation, string length, VarTools (Lomb-Scargle periodogram and Analysis of Variance), and SigSpec applications on a set of light curves drawn from the MACHO survey."
      },
      {
        "node_idx": 17441,
        "score_0_10": 10,
        "title": "power control in two tier femtocell networks",
        "abstract": "In a two tier cellular network - comprised of a central macrocell underlaid with shorter range femtocell hotspots - cross-tier interference limits overall capacity with universal frequency reuse. To quantify near-far effects with universal frequency reuse, this paper derives a fundamental relation providing the largest feasible cellular Signal-to-Interference-Plus-Noise Ratio (SINR), given any set of feasible femtocell SINRs. We provide a link budget analysis which enables simple and accurate performance insights in a two-tier network. A distributed utility- based SINR adaptation at femtocells is proposed in order to alleviate cross-tier interference at the macrocell from cochannel femtocells. The Foschini-Miljanic (FM) algorithm is a special case of the adaptation. Each femtocell maximizes their individual utility consisting of a SINR based reward less an incurred cost (interference to the macrocell). Numerical results show greater than 30% improvement in mean femtocell SINRs relative to FM. In the event that cross-tier interference prevents a cellular user from obtaining its SINR target, an algorithm is proposed that reduces transmission powers of the strongest femtocell interferers. The algorithm ensures that a cellular user achieves its SINR target even with 100 femtocells/cell-site (with typical cellular parameters) and requires a worst case SINR reduction of only 16% at femtocells. These results motivate design of power control schemes requiring minimal network overhead in two-tier networks with shared spectrum."
      },
      {
        "node_idx": 40762,
        "score_0_10": 9,
        "title": "optimal pricing in networks with externalities",
        "abstract": "We study the optimal pricing strategies of a monopolist selling a divisible good (service) to consumers that are embedded in a social network. A key feature of our model is that consumers experience a (positive) local network effect. In particular, each consumer's usage level depends directly on the usage of her neighbors in the social network structure. Thus, the monopolist's optimal pricing strategy may involve offering discounts to certain agents, who have a central position in the underlying network. #R##N#First, we consider a setting where the monopolist can offer individualized prices and derive an explicit characterization of the optimal price for each consumer as a function of her network position. In particular, we show that it is optimal for the monopolist to charge each agent a price that is proportional to her Bonacich centrality in the social network. In the second part of the paper, we discuss the optimal strategy of a monopolist that can only choose a single uniform price for the good and derive an algorithm polynomial in the number of agents to compute such a price. Thirdly, we assume that the monopolist can offer the good in two prices, full and discounted, and study the problem of determining which set of consumers should be given the discount. We show that the problem is NP-hard, however we provide an explicit characterization of the set of agents that should be offered the discounted price. Next, we describe an approximation algorithm for finding the optimal set of agents. We show that if the profit is nonnegative under any feasible price allocation, the algorithm guarantees at least 88% of the optimal profit. Finally, we highlight the value of network information by comparing the profits of a monopolist that does not take into account the network effects when choosing her pricing policy to those of a monopolist that uses this information optimally."
      },
      {
        "node_idx": 119255,
        "score_0_10": 9,
        "title": "throughput analysis of proportional fair scheduling for sparse and ultra dense interference limited ofdma lte networks",
        "abstract": "Various system tasks like interference coordination, handover decisions, admission control etc. in current cellular networks require precise mid-term (spanning over a few seconds) performance models. Due to channel-dependent scheduling at the base station, these performance models are not simple to obtain. Furthermore, LTE cellular systems are interference-limited, hence, the way interference is modelled is crucial for the accuracy. In this paper we present a closed-form analytical performance model for proportional fair scheduling in OFDMA/LTE networks. The model takes into account a precise SINR distribution into account. We refine our model with respect to uniform modulation and coding, as applied in LTE networks. Furthermore, the analytical analysis is extended also for ultra-dense deployments likely to happen in the 5-th generation of cellular networks. The resulting analytical performance model is validated by means of simulations considering realistic network deployments. Compared with related work, our model demonstrates a significantly higher accuracy for long-term throughput estimation."
      },
      {
        "node_idx": 5574,
        "score_0_10": 9,
        "title": "nash equilibria for quadratic voting",
        "abstract": "Voters making a binary decision purchase votes from a centralized clearing house, paying the square of the number of votes purchased. The net payoff to an agent with utility $u$ who purchases $v$ votes is $\\Psi (S_{n+1})u-v^{2}$, where $\\Psi$ is a monotone function taking values between -1 and +1 and $S_{n+1}$ is the sum of all votes purchased by the $n+1$ voters participating in the election. The utilities of the voters are assumed to arise by random sampling from a probability distribution $F_{U}$ with compact support; each voter knows her own utility, but not those of the other voters, although she does know the sampling distribution $F_{U}$. Nash equilibria for this game are described. These results imply that the expected inefficiency of any Nash equilibrium decays like $1/n$."
      },
      {
        "node_idx": 100650,
        "score_0_10": 9,
        "title": "large system analysis of linear precoding in correlated miso broadcast channels under limited feedback",
        "abstract": "In this paper, we study the sum rate performance of zero-forcing (ZF) and regularized ZF (RZF) precoding in large MISO broadcast systems under the assumptions of imperfect channel state information at the transmitter and per-user channel transmit correlation. Our analysis assumes that the number of transmit antennas M and the number of single-antenna users K are large while their ratio remains bounded. We derive deterministic approximations of the empirical signal-to-interference plus noise ratio (SINR) at the receivers, which are tight as M, K \u2192 \u221e. In the course of this derivation, the per-user channel correlation model requires the development of a novel deterministic equivalent of the empirical Stieltjes transform of large dimensional random matrices with generalized variance profile. The deterministic SINR approximations enable us to solve various practical optimization problems. Under sum rate maximization, we derive 1) for RZF the optimal regularization parameter; 2) for ZF the optimal number of users; 3) for ZF and RZF the optimal power allocation scheme; and 4) the optimal amount of feedback in large FDD/TDD multiuser systems. Numerical simulations suggest that the deterministic approximations are accurate even for small M, K."
      },
      {
        "node_idx": 138631,
        "score_0_10": 9,
        "title": "equilibria of dynamic games with many players existence approximation and market structure",
        "abstract": "In this paper we study stochastic dynamic games with many players; these are a fundamental model for a wide range of economic applications. The standard solution concept for such games is Markov perfect equilibrium (MPE), but it is well known that MPE computation becomes intractable as the number of players increases. We instead consider the notion of stationary equilibrium (SE), where players optimize assuming the empirical distribution of others' states remains constant at its long run average. We make two main contributions. First, we provide a rigorous justification for using SE. In particular, we provide a parsimonious collection of exogenous conditions over model primitives that guarantee existence of SE, and ensure that an appropriate approximation property to MPE holds, in a general model with possibly unbounded state spaces. Second, we draw a significant connection between the validity of SE, and market structure: under the same conditions that imply SE exist and approximates MPE well, the market becomes fragmented in the limit of many firms. To illustrate this connection, we study in detail a series of dynamic oligopoly examples. These examples show that our conditions enforce a form of \"decreasing returns to larger states\"; this yields fragmented industries in the limit. By contrast, violation of these conditions suggests \"increasing returns to larger states\" and potential market concentration. In that sense, our work uses a fully dynamic framework to also contribute to a longstanding issue in industrial organization: understanding the determinants of market structure in different industries."
      }
    ]
  },
  "497": {
    "explanation": "integration of geophysical and flow data for subsurface fracture characterization",
    "topk": [
      {
        "node_idx": 157799,
        "score_0_10": 10,
        "title": "sequential geophysical and flow inversion to characterize fracture networks in subsurface systems",
        "abstract": "Subsurface applications including geothermal, geological carbon sequestration, oil and gas, etc., typically involve maximizing either the extraction of energy or the storage of fluids. Characterizing the subsurface is extremely complex due to heterogeneity and anisotropy. Due to this complexity, there are uncertainties in the subsurface parameters, which need to be estimated from multiple diverse as well as fragmented data streams. In this paper, we present a non-intrusive sequential inversion framework, for integrating data from geophysical and flow sources to constraint subsurface Discrete Fracture Networks (DFN). In this approach, we first estimate bounds on the statistics for the DFN fracture orientations using microseismic data. These bounds are estimated through a combination of a focal mechanism (physics-based approach) and clustering analysis (statistical approach) of seismic data. Then, the fracture lengths are constrained based on the flow data. The efficacy of this multi-physics based sequential inversion is demonstrated through a representative synthetic example."
      },
      {
        "node_idx": 130531,
        "score_0_10": 10,
        "title": "boosting the differences a fast bayesian classifier neural network",
        "abstract": "A Bayesian classifier that up-weights the differences in the attribute values is discussed. Using four popular datasets from the UCI repository, some interesting features of the network are illustrated. The network is suitable for classification problems."
      },
      {
        "node_idx": 2958,
        "score_0_10": 10,
        "title": "filter based regularisation for impulse response modelling",
        "abstract": "In the last years, the success of kernel-based regularisation techniques in solving impulse response modelling tasks has revived the interest on linear system identification. In this work, an alternative perspective on the same problem is introduced. Instead of relying on a Bayesian framework to include assumptions about the system in the definition of the covariance matrix of the parameters, here the prior knowledge is injected at the cost function level. The key idea is to define the regularisation matrix as a filtering operation on the parameters, which allows for a more intuitive formulation of the problem from an engineering point of view. Moreover, this results in a unified framework to model low-pass, band-pass and high-pass systems, and systems with one or more resonances. The proposed filter-based approach outperforms the existing regularisation method based on the TC and DC kernels, as illustrated by means of Monte Carlo simulations on several linear modelling examples."
      },
      {
        "node_idx": 85117,
        "score_0_10": 10,
        "title": "efficient multidimensional regularization for volterra series estimation",
        "abstract": "Abstract   This paper presents an efficient nonparametric time domain nonlinear system identification method. It is shown how truncated Volterra series models can be efficiently estimated without the need of long, transient-free measurements. The method is a novel extension of the regularization methods that have been developed for impulse response estimates of linear time invariant systems. To avoid the excessive memory needs in case of long measurements or large number of estimated parameters, a practical gradient-based estimation method is also provided, leading to the same numerical results as the proposed Volterra estimation method. Moreover, the transient effects in the simulated output are removed by a special regularization method based on the novel ideas of transient removal for Linear Time-Varying (LTV) systems. Combining the proposed methodologies, the nonparametric Volterra models of the cascaded water tanks benchmark are presented in this paper. The results for different scenarios varying from a simple Finite Impulse Response (FIR) model to a 3rd degree Volterra series with and without transient removal are compared and studied. It is clear that the obtained models capture the system dynamics when tested on a validation dataset, and their performance is comparable with the white-box (physical) models."
      },
      {
        "node_idx": 148344,
        "score_0_10": 10,
        "title": "three dimensional planar model estimation using multi constraint knowledge based on k means and ransac",
        "abstract": "Graphical abstractDisplay Omitted HighlightsA novel method for planar model reconstruction from three dimensional point cloud.It allows the accurate reconstruction of planar objects using prior knowledge.It evaluates the clusters of each face and estimates the planes by using constraints.k-means estimates the clusters and a tree search refine them by using constraints.It extends RANSAC introducing a step to evaluate if the inliers comply constraints. Plane model extraction from three-dimensional point clouds is a necessary step in many different applications such as planar object reconstruction, indoor mapping and indoor localization. Different RANdom SAmple Consensus (RANSAC)-based methods have been proposed for this purpose in recent years. In this study, we propose a novel method-based on RANSAC called Multiplane Model Estimation, which can estimate multiple plane models simultaneously from a noisy point cloud using the knowledge extracted from a scene (or an object) in order to reconstruct it accurately. This method comprises two steps: first, it clusters the data into planar faces that preserve some constraints defined by knowledge related to the object (e.g., the angles between faces); and second, the models of the planes are estimated based on these data using a novel multi-constraint RANSAC. We performed experiments in the clustering and RANSAC stages, which showed that the proposed method performed better than state-of-the-art methods."
      },
      {
        "node_idx": 62474,
        "score_0_10": 10,
        "title": "numerical weather prediction nwp and hybrid arma ann model to predict global radiation",
        "abstract": "We propose in this paper an original technique to predict global radiation using a hybrid ARMA/ANN model and data issued from a numerical weather prediction model (NWP). We particularly look at the multi-layer perceptron (MLP). After optimizing our architecture with NWP and endogenous data previously made stationary and using an innovative pre-input layer selection method, we combined it to an ARMA model from a rule based on the analysis of hourly data series. This model has been used to forecast the hourly global radiation for five places in Mediterranean area. Our technique outperforms classical models for all the places. The nRMSE for our hybrid model MLP/ARMA is 14.9% compared to 26.2% for the naive persistence predictor. Note that in the standalone ANN case the nRMSE is 18.4%. Finally, in order to discuss the reliability of the forecaster outputs, a complementary study concerning the confidence interval of each prediction is proposed."
      },
      {
        "node_idx": 162008,
        "score_0_10": 9,
        "title": "parsimonious volterra system identification",
        "abstract": "In this paper, we aim at developing algorithms for sparse Volterra system identification when the system to be identified has infinite impulse response. Assuming that the impulse response is represented as a sum of exponentials and given input-output data, the problem of interest is to find the \u201csimplest\u201d nonlinear Volterra model which is compatible with the a priori information and the collected data. By simplest, we mean the model whose impulse response has the least number of exponentials. The algorithms provided are able to handle both fragmented data and measurement noise. Academic examples at the end of paper show the efficacy of proposed approach."
      },
      {
        "node_idx": 16163,
        "score_0_10": 9,
        "title": "identification of nonlinear block oriented systems starting from linear approximations a survey",
        "abstract": "Block-oriented models are popular in nonlinear modeling because of their advantages to be quite simple to understand and easy to use. Many different identification approaches were developed over the years to estimate the parameters of a wide range of block-oriented models. One class of these approaches uses linear approximations to initialize the identification algorithm. The best linear approximation framework and the $\\epsilon$-approximation framework, or equivalent frameworks, allow the user to extract important information about the system, guide the user in selecting good candidate model structures and orders, and they prove to be a good starting point for nonlinear system identification algorithms. This paper gives an overview of the different block-oriented models that can be modeled using linear approximations, and of the identification algorithms that have been developed in the past. A non-exhaustive overview of the most important other block-oriented system identification approaches is also provided throughout this paper."
      },
      {
        "node_idx": 137258,
        "score_0_10": 9,
        "title": "normal integration a survey",
        "abstract": "The need for efficient normal integration methods is driven by several computer vision tasks such as shape-from-shading, photometric stereo, deflectometry, etc. In the first part of this survey, we select the most important properties that one may expect from a normal integration method, based on a thorough study of two pioneering works by Horn and Brooks [28] and by Frankot and Chellappa [19]. Apart from accuracy, an integration method should at least be fast and robust to a noisy normal field. In addition, it should be able to handle several types of boundary condition, including the case of a free boundary, and a reconstruction domain of any shape i.e., which is not necessarily rectangular. It is also much appreciated that a minimum number of parameters have to be tuned, or even no parameter at all. Finally, it should preserve the depth discontinuities. In the second part of this survey, we review most of the existing methods in view of this analysis, and conclude that none of them satisfies all of the required properties. This work is complemented by a companion paper entitled Variational Methods for Normal Integration, in which we focus on the problem of normal integration in the presence of depth discontinuities, a problem which occurs as soon as there are occlusions."
      },
      {
        "node_idx": 614,
        "score_0_10": 9,
        "title": "estimating failure in brittle materials using graph theory",
        "abstract": "In brittle fracture applications, failure paths, regions where the failure occurs and damage statistics, are some of the key quantities of interest (QoI). High-fidelity models for brittle failure that accurately predict these QoI exist but are highly computationally intensive, making them infeasible to incorporate in upscaling and uncertainty quantification frameworks. The goal of this paper is to provide a fast heuristic to reasonably estimate quantities such as failure path and damage in the process of brittle failure. Towards this goal, we first present a method to predict failure paths under tensile loading conditions and low-strain rates. The method uses a $k$-nearest neighbors algorithm built on fracture process zone theory, and identifies the set of all possible pre-existing cracks that are likely to join early to form a large crack. The method then identifies zone of failure and failure paths using weighted graphs algorithms. We compare these failure paths to those computed with a high-fidelity model called the Hybrid Optimization Software Simulation Suite (HOSS). A probabilistic evolution model for average damage in a system is also developed that is trained using 150 HOSS simulations and tested on 40 simulations. A non-parametric approach based on confidence intervals is used to determine the damage evolution over time along the dominant failure path. For upscaling, damage is the key QoI needed as an input by the continuum models. This needs to be informed accurately by the surrogate models for calculating effective modulii at continuum-scale. We show that for the proposed average damage evolution model, the prediction accuracy on the test data is more than 90\\%. In terms of the computational time, the proposed models are $\\approx \\mathcal{O}(10^6)$ times faster compared to high-fidelity HOSS."
      }
    ]
  },
  "499": {
    "explanation": "theoretical bounds and constructions in computational complexity and information theory",
    "topk": [
      {
        "node_idx": 115407,
        "score_0_10": 10,
        "title": "covering numbers of l_p balls of convex sets and functions",
        "abstract": "We prove bounds for the covering numbers of classes of convex functions and convex sets in Euclidean space. Previous results require the underlying convex functions or sets to be uniformly bounded. We relax this assumption and replace it with weaker integral constraints. Existing results can be recovered as special cases of our results."
      },
      {
        "node_idx": 22842,
        "score_0_10": 10,
        "title": "recursion theoretic ranking and compression",
        "abstract": "For which sets A does there exist a mapping, computed by a total or partial recursive function, such that the mapping, when its domain is restricted to A, is a 1-to-1, onto mapping to $\\Sigma^*$? And for which sets A does there exist such a mapping that respects the lexicographical ordering within A? Both cases are types of perfect, minimal hash functions. The complexity-theoretic versions of these notions are known as compression functions and ranking functions. The present paper defines and studies the recursion-theoretic versions of compression and ranking functions, and in particular studies the question of which sets have, or lack, such functions. Thus, this is a case where, in contrast to the usual direction of notion transferal, notions from complexity theory are inspiring notions, and an investigation, in computability theory. #R##N#We show that the rankable and compressible sets broadly populate the 1-truth-table degrees, and we prove that every nonempty coRE cylinder is recursively compressible."
      },
      {
        "node_idx": 36651,
        "score_0_10": 10,
        "title": "an additivity theorem for plain kolmogorov complexity",
        "abstract": "We prove the formula C(a,b) = K(a|C(a,b)) + C(b|a,C(a,b)) + O(1) that expresses the plain complexity of a pair in terms of prefix and plain conditional complexities of its components."
      },
      {
        "node_idx": 64006,
        "score_0_10": 10,
        "title": "some bounds of separating hash families",
        "abstract": "Separating hash families were first introduced by Stinson, Trung and Wei. #R##N#In this paper, we present some new bounds of SHF with small parameter. By the small parameter, we improve previously known bound of types $\\{w,w\\}$ and $\\{w_1,w_2\\}$. we also give a construction for strong separating hash family."
      },
      {
        "node_idx": 85704,
        "score_0_10": 10,
        "title": "characterization of negabent functions and construction of bent negabent functions with maximum algebraic degree",
        "abstract": "We present necessary and sufficient conditions for a Boolean function to be a negabent function for both even and odd number of variables, which demonstrate the relationship between negabent functions and bent functions. By using these necessary and sufficient conditions for Boolean functions to be negabent, we obtain that the nega spectrum of a negabent function has at most 4 values. We determine the nega spectrum distribution of negabent functions. Further, we provide a method to construct bent-negabent functions in $n$ variables ($n$ even) of algebraic degree ranging from 2 to $\\frac{n}{2}$, which implies that the maximum algebraic degree of an $n$-variable bent-negabent function is equal to $\\frac{n}{2}$. Thus, we answer two open problems proposed by Parker and Pott and by St\\v{a}nic\\v{a} \\textit{et al.} respectively."
      },
      {
        "node_idx": 93095,
        "score_0_10": 10,
        "title": "small depth multilinear formula lower bounds for iterated matrix multiplication with applications",
        "abstract": "In this paper, we study the algebraic formula complexity of multiplying $d$ many $2\\times 2$ matrices, denoted $\\mathrm{IMM}_{d}$, and show that the well-known divide-and-conquer algorithm cannot be significantly improved at any depth, as long as the formulas are multilinear. #R##N#Formally, for each depth $\\Delta \\leq \\log d$, we show that any product-depth $\\Delta$ multilinear formula for $\\mathrm{IMM}_d$ must have size $\\exp(\\Omega(\\Delta d^{1/\\Delta})).$ It also follows from this that any multilinear circuit of product-depth $\\Delta$ for the same polynomial of the above form must have a size of $\\exp(\\Omega(d^{1/\\Delta})).$ In particular, any polynomial-sized multilinear formula for $\\mathrm{IMM}_d$ must have depth $\\Omega(\\log d)$, and any polynomial-sized multilinear circuit for $\\mathrm{IMM}_d$ must have depth $\\Omega(\\log d/\\log \\log d).$ Both these bounds are tight up to constant factors. #R##N#1. Depth-reduction: A well-known result of Brent (JACM 1974) implies that any formula of size $s$ can be converted to one of size $s^{O(1)}$ and depth $O(\\log s)$; further, this reduction continues to hold for multilinear formulas. Our lower bound implies that any depth-reduction in the multilinear setting cannot reduce the depth to $o(\\log s)$ without a superpolynomial blow-up in size. #R##N#2. Separations from general formulas: Our result, along with a non-trivial upper bound for $\\mathrm{IMM}_{d}$ implied by a result of Gupta, Kamath, Kayal and Saptharishi (SICOMP 2016), shows that for any size $s$ and product-depth $\\Delta = o(\\log s),$ general formulas of size $s$ and product-depth $\\Delta$ cannot be converted to multilinear formulas of size $s^{\\omega(1)}$ and product-depth $\\Delta,$ when the underlying field has characteristic zero."
      },
      {
        "node_idx": 167942,
        "score_0_10": 10,
        "title": "fast fibonacci heaps with worst case extensions",
        "abstract": "We are concentrating on reducing overhead of heaps based on comparisons with optimal worstcase behaviour. The paper is inspired by Strict Fibonacci Heaps [1], where G. S. Brodal, G. Lagogiannis, and R. E. Tarjan implemented the heap with DecreaseKey and Meld interface in assymptotically optimal worst case times (based on key comparisons). In the paper [2], the ideas were elaborated and it was shown that the same asymptotical times could be achieved with a strategy loosing much less information from previous comparisons. There is big overhead with maintainance of violation lists in these heaps. We propose simple alternative reducing this overhead. It allows us to implement fast amortized Fibonacci heaps, where user could call some methods in variants guaranting worst case time. If he does so, the heaps are not guaranted to be Fibonacci until an amortized version of a method is called. Of course we could call worst case versions all the time, but as there is an overhead with the guarantee, calling amortized versions is prefered choice if we are not concentrated on complexity of the separate operation. #R##N#We have shown, we could implement full DecreaseKey-Meld interface, but Meld interface is not natural for these heaps, so if Meld is not needed, much simpler implementation suffices. As I don't know application requiring Meld, we would concentrate on noMeld variant, but we will show the changes could be applied on Meld including variant as well. The papers [1], [2] shown the heaps could be implemented on pointer machine model. For fast practical implementations we would rather use arrays. Our goal is to reduce number of pointer manipulations. Maintainance of ranks by pointers to rank lists would be unnecessary overhead."
      },
      {
        "node_idx": 92316,
        "score_0_10": 10,
        "title": "homogeneous formulas and symmetric polynomials",
        "abstract": "We investigate the arithmetic formula complexity of the elementary symmetric polynomials S(k,n). We show that every multilinear homogeneous formula computing S(k,n) has size at least k^(Omega(log k))n, and that product-depth d multilinear homogeneous formulas for S(k,n) have size at least 2^(Omega(k^{1/d}))n. Since S(n,2n) has a multilinear formula of size O(n^2), we obtain a superpolynomial separation between multilinear and multilinear homogeneous formulas. We also show that S(k,n) can be computed by homogeneous formulas of size k^(O(log k))n, answering a question of Nisan and Wigderson. Finally, we present a superpolynomial separation between monotone and non-monotone formulas in the noncommutative setting, answering a question of Nisan."
      },
      {
        "node_idx": 64003,
        "score_0_10": 10,
        "title": "more on compression and ranking",
        "abstract": "We study the role that honesty, selectivity, closure properties, relativization, and target spaces other than $\\Sigma^*$ have in the recursion-theoretic study of compression and ranking."
      },
      {
        "node_idx": 122446,
        "score_0_10": 10,
        "title": "relating and contrasting plain and prefix kolmogorov complexity",
        "abstract": "In [3] a short proof is given that some strings have maximal plain Kolmogorov complexity but not maximal prefix-free complexity. The proof uses Levin's symmetry of information, Levin's formula relating plain and prefix complexity and Gacs' theorem that complexity of complexity given the string can be high. We argue that the proof technique and results mentioned above are useful to simplify existing proofs and to solve open questions. #R##N#We present a short proof of Solovay's result [21] relating plain and prefix complexity: $K (x) = C (x) + CC (x) + O(CCC (x))$ and $C (x) = K (x) - KK (x) + O(KKK (x))$, (here $CC(x)$ denotes $C(C(x))$, etc.). #R##N#We show that there exist $\\omega$ such that $\\liminf C(\\omega_1\\dots \\omega_n) - C(n)$ is infinite and $\\liminf K(\\omega_1\\dots \\omega_n) - K(n)$ is finite, i.e. the infinitely often C-trivial reals are not the same as the infinitely often K-trivial reals (i.e. [1,Question 1]). #R##N#Solovay showed that for infinitely many $x$ we have $|x| - C (x) \\le O(1)$ and $|x| + K (|x|) - K (x) \\ge \\log^{(2)} |x| - O(\\log^{(3)} |x|)$, (here $|x|$ denotes the length of $x$ and $\\log^{(2)} = \\log\\log$, etc.). We show that this result holds for prefixes of some 2-random sequences. #R##N#Finally, we generalize our proof technique and show that no monotone relation exists between expectation and probability bounded randomness deficiency (i.e. [6, Question 1])."
      }
    ]
  },
  "503": {
    "explanation": "recommendation systems using artificial immune system idiotypic interactions",
    "topk": [
      {
        "node_idx": 11587,
        "score_0_10": 10,
        "title": "on the effects of idiotypic interactions for recommendation communities in artificial immune systems",
        "abstract": "It has previously been shown that a recommender based on immune system idiotypic principles can out perform one based on correlation alone. This paper reports the results of work in progress, where we undertake some investigations into the nature of this beneficial effect. The initial findings are that the immune system recommender tends to produce different neighbourhoods, and that the superior performance of this recommender is due partly to the different neighbourhoods, and partly to the way that the idiotypic effect is used to weight each neighbours recommendations."
      },
      {
        "node_idx": 5447,
        "score_0_10": 10,
        "title": "editorial for the first workshop on mining scientific papers computational linguistics and bibliometrics",
        "abstract": "The workshop \"Mining Scientific Papers: Computational Linguistics and Bibliometrics\" (CLBib 2015), co-located with the 15th International Society of Scientometrics and Informetrics Conference (ISSI 2015), brought together researchers in Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scale text analytics and sense mining of scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing (NLP). The goals of the workshop were to answer questions like: How can we enhance author network analysis and Bibliometrics using data obtained by text analytics? What insights can NLP provide on the structure of scientific writing, on citation networks, and on in-text citation analysis? This workshop is the first step to foster the reflection on the interdisciplinarity and the benefits that the two disciplines Bibliometrics and Natural Language Processing can drive from it."
      },
      {
        "node_idx": 109276,
        "score_0_10": 10,
        "title": "the author topic model for authors and documents",
        "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output."
      },
      {
        "node_idx": 162890,
        "score_0_10": 10,
        "title": "outrepasser les limites des techniques classiques de prise d empreintes grace aux reseaux de neurones",
        "abstract": "We present an application of Artificial Intelligence techniques to the field of Information Security. The problem of remote Operating System (OS) Detection, also called OS Fingerprinting, is a crucial step of the penetration testing process, since the attacker (hacker or security professional) needs to know the OS of the target host in order to choose the exploits that he will use. OS Detection is accomplished by passively sniffing network packets and actively sending test packets to the target host, to study specific variations in the host responses revealing information about its operating system. #R##N#The first fingerprinting implementations were based on the analysis of differences between TCP/IP stack implementations. The next generation focused the analysis on application layer data such as the DCE RPC endpoint information. Even though more information was analyzed, some variation of the \"best fit\" algorithm was still used to interpret this new information. Our new approach involves an analysis of the composition of the information collected during the OS identification process to identify key elements and their relations. To implement this approach, we have developed tools using Neural Networks and techniques from the field of Statistics. These tools have been successfully integrated in a commercial software (Core Impact)."
      },
      {
        "node_idx": 122822,
        "score_0_10": 10,
        "title": "guided layer wise learning for deep models using side information",
        "abstract": "Training of deep models for classification tasks is hindered by local minima problems and vanishing gradients, while unsupervised layer-wise pretraining does not exploit information from class labels. Here, we propose a new regularization technique, called diversifying regularization (DR), which applies a penalty on hidden units at any layer if they obtain similar features for different types of data. For generative models, DR is defined as divergence over the variational posteriori distributions and included in the maximum likelihood estimation as a prior. Thus, DR includes class label information for greedy pretraining of deep belief networks which result in a better weight initialization for fine-tuning methods. On the other hand, for discriminative training of deep neural networks, DR is defined as a distance over the features and included in the learning objective. With our experimental tests, we show that DR can help the backpropagation to cope with vanishing gradient problems and to provide faster convergence and smaller generalization errors."
      },
      {
        "node_idx": 155711,
        "score_0_10": 10,
        "title": "a discriminative event based model for alzheimer s disease progression modeling",
        "abstract": "The event-based model (EBM) for data-driven disease progression modeling estimates the sequence in which biomarkers for a disease become abnormal. This helps in understanding the dynamics of disease progression and facilitates early diagnosis by staging patients on a disease progression timeline. Existing EBM methods are all generative in nature. In this work we propose a novel discriminative approach to EBM, which is shown to be more accurate as well as computationally more efficient than existing state-of-the art EBM methods. The method first estimates for each subject an approximate ordering of events, by ranking the posterior probabilities of individual biomarkers being abnormal. Subsequently, the central ordering over all subjects is estimated by fitting a generalized Mallows model to these approximate subject-specific orderings based on a novel probabilistic Kendall's Tau distance. To evaluate the accuracy, we performed extensive experiments on synthetic data simulating the progression of Alzheimer's disease. Subsequently, the method was applied to the Alzheimer's Disease Neuroimaging Initiative (ADNI) data to estimate the central event ordering in the dataset. The experiments benchmark the accuracy of the new model under various conditions and compare it with existing state-of-the-art EBM methods. The results indicate that discriminative EBM could be a simple and elegant approach to disease progression modeling."
      },
      {
        "node_idx": 1779,
        "score_0_10": 9,
        "title": "conformal predictive distributions with kernels",
        "abstract": "This paper reviews the checkered history of predictive distributions in statistics and discusses two developments, one from recent literature and the other new. The first development is bringing predictive distributions into machine learning, whose early development was so deeply influenced by two remarkable groups at the Institute of Automation and Remote Control. The second development is combining predictive distributions with kernel methods, which were originated by one of those groups, including Emmanuel Braverman."
      },
      {
        "node_idx": 49174,
        "score_0_10": 9,
        "title": "evaluating topic coherence measures",
        "abstract": "Topic models extract representative word sets - called topics - from word counts in documents without requiring any semantic annotations. Topics are not guaranteed to be well interpretable, therefore, coherence measures have been proposed to distinguish between good and bad topics. Studies of topic coherence so far are limited to measures that score pairs of individual words. For the first time, we include coherence measures from scientific philosophy that score pairs of more complex word subsets and apply them to topic scoring."
      },
      {
        "node_idx": 49212,
        "score_0_10": 9,
        "title": "on affinity measures for artificial immune system movie recommenders",
        "abstract": "We combine Artificial Immune Systems 'AIS', technology with Collaborative Filtering 'CF' and use it to build a movie recommendation system. We already know that Artificial Immune Systems work well as movie recommenders from previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate the effect of different affinity measure algorithms for the AIS. Two different affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the correlation coefficients for the movie recommender. We compare the results with those published previously and show that Weighted Kappa is more suitable than others for movie problems. We also show that AIS are generally robust movie recommenders and that, as long as a suitable affinity measure is chosen, results are good."
      },
      {
        "node_idx": 85872,
        "score_0_10": 9,
        "title": "visual abstraction",
        "abstract": "In this article we revisit the concept of abstraction as it is used in visualization and put it on a solid formal footing. While the term \\emph{abstraction} is utilized in many scientific disciplines, arts, as well as everyday life, visualization inherits the notion of data abstraction or class abstraction from computer science, topological abstraction from mathematics, and visual abstraction from arts. All these notions have a lot in common, yet there is a major discrepancy in the terminology and basic understanding about visual abstraction in the context of visualization. We thus root the notion of abstraction in the philosophy of science, clarify the basic terminology, and provide crisp definitions of visual abstraction as a process. Furthermore, we clarify how it relates to similar terms often used interchangeably in the field of visualization. Visual abstraction is characterized by a conceptual space where this process exists, by the purpose it should serve, and by the perceptual and cognitive qualities of the beholder. These characteristics can be used to control the process of visual abstraction to produce effective and informative visual representations."
      }
    ]
  },
  "506": {
    "explanation": "secure key exchange using Kirchhoff-Law-Johnson-Noise (KLJN) systems",
    "topk": [
      {
        "node_idx": 47824,
        "score_0_10": 10,
        "title": "generalized kirchhoff law johnson noise kljn secure key exchange system using arbitrary resistors",
        "abstract": "The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange system has been introduced as a simple, very low cost and efficient classical physical alternative to quantum key distribution systems. The ideal system uses only a few electronic components\u2014identical resistor pairs, switches and interconnecting wires\u2014in order to guarantee perfectly protected data transmission. We show that a generalized KLJN system can provide unconditional security even if it is used with significantly less limitations. The more universal conditions ease practical realizations considerably and support more robust protection against attacks. Our theoretical results are confirmed by numerical simulations."
      },
      {
        "node_idx": 58246,
        "score_0_10": 10,
        "title": "generalized dc loop current attack against the kljn secure key exchange scheme",
        "abstract": "A new attack against the Kirchhoff Law Johnson Noise (KLJN) secure key distribution system is studied with unknown parasitic DC voltage sources at both Alices and Bobs ends. This paper is the generalization of our earlier investigation with a single end parasitic source. Under the assumption that Eve does not know the values of the parasitic sources, a new attack, utilizing the current generated by the parasitic dc voltage sources, is introduced. The attack is mathematically analyzed and demonstrated by computer simulations. Simple defense methods against the attack are shown. The earlier defense method based solely on the comparison of current/voltage data at Alice's and Bob's terminals is useless here since the wire currents and voltages are equal at both ends. However, the more expensive version of the earlier defense method, which is based on in situ system simulation and comparison with measurements, works efficiently."
      },
      {
        "node_idx": 107174,
        "score_0_10": 10,
        "title": "random resistor random temperature kirchhoff law johnson noise rrrt kljn key exchange",
        "abstract": "We introduce two new Kirchhoff-law-Johnson-noise (KLJN) secure key distribution schemes which are generalizations of the original KLJN scheme. The first of these, the Random-Resistor (RR-) KLJN sch ..."
      },
      {
        "node_idx": 58129,
        "score_0_10": 10,
        "title": "the weight distributions of cyclic codes and elliptic curves",
        "abstract": "Cyclic codes with two zeros and their dual codes as a practically and theoretically interesting class of linear codes, have been studied for many years. However, the weight distributions of cyclic codes are difficult to determine. From elliptic curves, this paper determines the weight distributions of dual codes of cyclic codes with two zeros for a few more cases."
      },
      {
        "node_idx": 128444,
        "score_0_10": 10,
        "title": "on the cracking scheme in the paper a directional coupler attack against the kish key distribution system by gunn allison and abbott",
        "abstract": "Recently, Gunn, Allison and Abbott (GAA) [http://arxiv.org/pdf/1402.2709v2.pdf] proposed a new scheme to utilize electromagnetic waves for eavesdropping on the Kirchhoff -law \u2013Johnson- noise (KLJN) secure key distribution. We proved in a former paper [ Fluct. Noise Lett. 13 (2014) 1450016 ] that GAA\u2019s mathematical model is unphysical. Here we analyze GAA\u2019s cracking scheme and show that, in the case of a loss -free cable, it provides less eavesdropping information than in the earlier (Bergou)-Scheuer-Yariv mean-square- based attack [Kish LB, Scheuer J, Phys. Lett. A 37 4:2140 \u20132142 (2010)], while it offers no information in the case of a lossy cable. We also investigate GAA\u2019s claim to be experimentally capable of distinguishing \u2014using statistics over a few correlation times only \u2014the distributions of two Gaussian noises wit h a relative variance difference of less than 10 \u20138 . Normally such distinctions would require hundreds of millions of correlations times to be observable. We identify several potential experimental artifacts as results of poor KLJN design, which can lead to GAA\u2019s assertions: deterministic currents due to spurious harmonic components caused by ground loops, DC offset, aliasing, non -Gaussian features including non-linearities and other non- idealities in generators, and the timederivative nature of GAA\u2019s schem e which tends to enhance all of these artifacts."
      },
      {
        "node_idx": 94137,
        "score_0_10": 10,
        "title": "enhanced secure key exchange systems based on the johnson noise scheme",
        "abstract": "We introduce seven new versions of the Kirchhoff -Law-Johnson-(like)-Noise (KLJN) classical physical secure key exchange scheme and a new transient protocol for practically -perfect security. While these practical improvements offer progressively enhanced security and/or speed for non-ideal conditions, the fundamental physical laws providing the security remain the same. In the \"intelligent\" KLJN (iKLJN) scheme, Alice and Bob utilize the fact that they exactly know not only their own resistor value but also the stochastic time function of their own noise, which they generate before feeding it into the loop. By using this extra information, they can reduce the duration of exchanging a single bit and in this way they achieve not only higher speed but also an enhanced security because Eve's information will significantly be reduced due to smaller statistics. In the \"multiple\" KLJN (MKLJN) system, Alice and Bob have publicly known identical sets of different resistors with a proper, publicly known truth table about the bit-interpretation of their combination. In this new situation, for Eve to succeed, it is not enough to find out which end has the higher resistor. Eve must exactly identify the actual resistor values at both sides. In the \"keyed\" KLJN (KKLJN) system, by using secure communication with a formerly shared key, Alice and Bob share a proper time -dependent truth table for the bit-interpretation of the resistor situation for each secure bit exchange step during generating the next key. In this new situation, for Eve to succeed, it is not enough to find out the resistor values at the two ends. Eve must also know the former key. The remaining four KLJN schemes are the combinations of the above protocols to synergically enhance the security properties. The se are: the \"intelligent-multiple\" (iMKLJN), the \"intelligent-keyed\" (iKKLJN), the \"keyed -multiple\" (KMKLJN) and the \"intelligent-keyed-multiple\" (iKMKLJN) KLJN key exchange systems. Finally, we introduce a new transient -protocol offering practically-perfe ct security without privacy amplification, which is not needed in practical applications but it is shown for the sake of ongoing discussions."
      },
      {
        "node_idx": 167365,
        "score_0_10": 10,
        "title": "gradient descent provably optimizes over parameterized neural networks",
        "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. #R##N#Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods."
      },
      {
        "node_idx": 6608,
        "score_0_10": 9,
        "title": "resource requirements and speed versus geometry of unconditionally secure physical key exchanges",
        "abstract": "The imperative need for unconditional secure key exchange is expounded by the increasing connectivity of networks and by the increasing number and level of sophistication of cyberattacks. Two concepts that are theoretically information-secure are quantum key distribution (QKD) and Kirchoff-Law-Johnson-Noise (KLJN). However, these concepts require a dedicated connection between hosts in peer-to-peer (P2P) networks which can be impractical and or cost prohibitive. A practical and cost effective method is to have each host share their respective cable(s) with other hosts such that two remote hosts can realize a secure key exchange without the need of an additional cable or key exchanger. In this article we analyze the cost complexities of cable, key exchangers, and time required in the star network. We mentioned the reliability of the star network and compare it with other network geometries. We also conceived a protocol and equation for the number of secure bit exchange periods needed in a star network. We then outline other network geometries and trade-off possibilities that seem interesting to explore."
      },
      {
        "node_idx": 39105,
        "score_0_10": 9,
        "title": "gradient descent finds global minima of deep neural networks",
        "abstract": "Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result."
      },
      {
        "node_idx": 123236,
        "score_0_10": 9,
        "title": "noncontinous additive entropies of partitions",
        "abstract": "In a previous paper: A. Paszkiewicz, T. Sobieszek, Additive Entropies of Partitions, we have given a description of additive partition entropies that is real functions $I$ on the set of finite partitions that are additive on stochastically independent partitions in a given probability space. We now present an analogical result, this time without assuming continuity. #R##N#As a by-product of our efforts we solve a 2-cocycle functional equation for certain subsets of convex cones."
      }
    ]
  },
  "507": {
    "explanation": "visual abstraction and information visualization techniques",
    "topk": [
      {
        "node_idx": 85872,
        "score_0_10": 10,
        "title": "visual abstraction",
        "abstract": "In this article we revisit the concept of abstraction as it is used in visualization and put it on a solid formal footing. While the term \\emph{abstraction} is utilized in many scientific disciplines, arts, as well as everyday life, visualization inherits the notion of data abstraction or class abstraction from computer science, topological abstraction from mathematics, and visual abstraction from arts. All these notions have a lot in common, yet there is a major discrepancy in the terminology and basic understanding about visual abstraction in the context of visualization. We thus root the notion of abstraction in the philosophy of science, clarify the basic terminology, and provide crisp definitions of visual abstraction as a process. Furthermore, we clarify how it relates to similar terms often used interchangeably in the field of visualization. Visual abstraction is characterized by a conceptual space where this process exists, by the purpose it should serve, and by the perceptual and cognitive qualities of the beholder. These characteristics can be used to control the process of visual abstraction to produce effective and informative visual representations."
      },
      {
        "node_idx": 95469,
        "score_0_10": 9,
        "title": "a dynamic id based remote user authentication scheme",
        "abstract": "Password-based authentication schemes are the most widely used techniques for remote user authentication. Many static ID-based remote user authentication schemes both with and without smart cards have been proposed. Most of the schemes do not allow the users to choose and change their passwords, and maintain a verifier table to verify the validity of the user login. In this paper we present a dynamic ID-based remote user authentication scheme using smart cards. Our scheme allows the users to choose and change their passwords freely, and do not maintain any verifier table. The scheme is secure against ID-theft, and can resist the reply attacks, forgery attacks, guessing attacks, insider attacks and stolen verifier attacks."
      },
      {
        "node_idx": 70823,
        "score_0_10": 9,
        "title": "an information theoretic approach to the cost benefit analysis of visualization in virtual environments",
        "abstract": "Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs."
      },
      {
        "node_idx": 36666,
        "score_0_10": 9,
        "title": "the role of social networks in information diffusion",
        "abstract": "Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these technologies on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed."
      },
      {
        "node_idx": 131864,
        "score_0_10": 8,
        "title": "empirical analysis of predictive algorithms for collaborative filtering",
        "abstract": "Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metrics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time."
      },
      {
        "node_idx": 27244,
        "score_0_10": 8,
        "title": "virtualidentity privacy preserving user profiling",
        "abstract": "User profiling from user generated content (UGC) is a common practice that supports the business models of many social media companies. Existing systems require that the UGC is fully exposed to the module that constructs the user profiles. In this paper we show that it is possible to build user profiles without ever accessing the user's original data, and without exposing the trained machine learning models for user profiling -- which are the intellectual property of the company -- to the users of the social media site. We present VirtualIdentity, an application that uses secure multi-party cryptographic protocols to detect the age, gender and personality traits of users by classifying their user-generated text and personal pictures with trained support vector machine models in a privacy-preserving manner."
      },
      {
        "node_idx": 69651,
        "score_0_10": 8,
        "title": "temporal proximity induces attributes similarity",
        "abstract": "Users consume their favorite content in temporal proximity of consumption bundles according to their preferences and tastes. Thus, the underlying attributes of items implicitly match user preferences, however, current recommender systems largely ignore this fundamental driver in identifying matching items. In this work, we introduce a novel temporal proximity filtering method to enable items-matching. First, we demonstrate that proximity preferences exist. Second, we present an induced similarity metric in temporal proximity driven by user tastes and third, we show that this induced similarity can be used to learn items pairwise similarity in attribute space. The proposed model does not rely on any knowledge outside users' consumption bundles and provide a novel way to devise user preferences and tastes driven novel items recommender."
      },
      {
        "node_idx": 11141,
        "score_0_10": 8,
        "title": "secure communication over fading channels",
        "abstract": "The fading broadcast channel with confidential messages (BCC) is investigated, where a source node has common information for two receivers (receivers 1 and 2), and has confidential information intended only for receiver 1. The confidential information needs to be kept as secret as possible from receiver 2. The broadcast channel from the source node to receivers 1 and 2 is corrupted by multiplicative fading gain coefficients in addition to additive Gaussian noise terms. The channel state information (CSI) is assumed to be known at both the transmitter and the receivers. The parallel BCC with independent subchannels is first studied, which serves as an information-theoretic model for the fading BCC. The secrecy capacity region of the parallel BCC is established. This result is then specialized to give the secrecy capacity region of the parallel BCC with degraded subchannels. The secrecy capacity region is then established for the parallel Gaussian BCC, and the optimal source power allocations that achieve the boundary of the secrecy capacity region are derived. In particular, the secrecy capacity region is established for the basic Gaussian BCC. The secrecy capacity results are then applied to study the fading BCC. Both the ergodic and outage performances are studied."
      },
      {
        "node_idx": 84843,
        "score_0_10": 8,
        "title": "geo indistinguishability differential privacy for location based systems",
        "abstract": "The growing popularity of location-based systems, allowing unknown/untrusted servers to easily collect huge amounts of information regarding users' location, has recently started raising serious privacy concerns. In this paper we introduce geoind, a formal notion of privacy for location-based systems that protects the user's exact location, while allowing approximate information -- typically needed to obtain a certain desired service -- to be released.   This privacy definition formalizes the intuitive notion of protecting the user's location within a radius $r$ with a level of privacy that depends on r, and corresponds to a generalized version of the well-known concept of differential privacy. Furthermore, we present a mechanism for achieving geoind by adding controlled random noise to the user's location.   We describe how to use our mechanism to enhance LBS applications with geo-indistinguishability guarantees without compromising the quality of the application results. Finally, we compare state-of-the-art mechanisms from the literature with ours. It turns out that, among all mechanisms independent of the prior, our mechanism offers the best privacy guarantees."
      },
      {
        "node_idx": 84579,
        "score_0_10": 8,
        "title": "cameras viewing cameras geometry",
        "abstract": "A basic problem in computer vision is to understand the structure of a real-world scene given several images of it. Here we study several theoretical aspects of the intra multi-view geometry of calibrated cameras when all that they can reliably recognize is each other. With the proliferation of wearable cameras, autonomous vehicles and drones, the geometry of these multiple cameras is a timely and relevant problem to study."
      }
    ]
  },
  "509": {
    "explanation": "video action recognition and spatio-temporal modeling",
    "topk": [
      {
        "node_idx": 26460,
        "score_0_10": 10,
        "title": "ucf101 a dataset of 101 human actions classes from videos in the wild",
        "abstract": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
      },
      {
        "node_idx": 21564,
        "score_0_10": 10,
        "title": "quo vadis action recognition a new model and the kinetics dataset",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. #R##N#We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."
      },
      {
        "node_idx": 121343,
        "score_0_10": 9,
        "title": "pyramid scene parsing network",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
      },
      {
        "node_idx": 21714,
        "score_0_10": 9,
        "title": "mobilenetv2 inverted residuals and linear bottlenecks",
        "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. #R##N#The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters"
      },
      {
        "node_idx": 52018,
        "score_0_10": 8,
        "title": "inception v4 inception resnet and the impact of residual connections on learning",
        "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge"
      },
      {
        "node_idx": 108093,
        "score_0_10": 8,
        "title": "temporal segment networks towards good practices for deep action recognition",
        "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices."
      },
      {
        "node_idx": 69942,
        "score_0_10": 8,
        "title": "long term recurrent convolutional networks for visual recognition and description",
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \"temporally deep\", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \"doubly deep\"' in that they can be compositional in spatial and temporal \"layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
      },
      {
        "node_idx": 35496,
        "score_0_10": 8,
        "title": "aggregated residual transformations for deep neural networks",
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
      },
      {
        "node_idx": 151734,
        "score_0_10": 8,
        "title": "learning deep features for discriminative localization",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them"
      },
      {
        "node_idx": 49351,
        "score_0_10": 8,
        "title": "deep inside convolutional networks visualising image classification models and saliency maps",
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
      }
    ]
  },
  "511": {
    "explanation": "full-duplex transceiver self-interference modeling and cancellation techniques",
    "topk": [
      {
        "node_idx": 124619,
        "score_0_10": 10,
        "title": "bag of tricks for efficient text classification",
        "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
      },
      {
        "node_idx": 129779,
        "score_0_10": 10,
        "title": "widely linear digital self interference cancellation in direct conversion full duplex transceiver",
        "abstract": "This paper addresses the modeling and cancellation of self-interference in full-duplex direct-conversion radio transceivers, operating under practical imperfect radio frequency (RF) components. First, detailed self-interference signal modeling is carried out, taking into account the most important RF imperfections, namely, transmitter power amplifier nonlinear distortion as well as transmitter and receiver IQ mixer amplitude and phase imbalances. The analysis shows that after realistic antenna isolation and RF cancellation, the dominant self-interference waveform at the receiver digital baseband can be modeled through a widely linear transformation of the original transmit data, opposed to classical purely linear models. Such widely linear self-interference waveform is physically stemming from the transmitter and receiver IQ imaging and cannot be efficiently suppressed by classical linear digital cancellation. Motivated by this, novel widely linear digital self-interference cancellation processing is then proposed and formulated, combined with efficient parameter estimation methods. Extensive simulation results demonstrate that the proposed widely linear cancellation processing clearly outperforms the existing linear solutions, hence enabling the use of practical low-cost RF front ends utilizing IQ mixing in full-duplex transceivers."
      },
      {
        "node_idx": 100731,
        "score_0_10": 10,
        "title": "analysis of oscillator phase noise effects on self interference cancellation in full duplex ofdm radio transceivers",
        "abstract": "This paper addresses the analysis of oscillator phase-noise effects on the self-interference cancellation capability of full-duplex direct-conversion radio transceivers. Closed-form solutions are derived for the power of the residual self-interference stemming from phase noise in two alternative cases of having either independent oscillators or the same oscillator at the transmitter and receiver chains of the full-duplex transceiver. The results show that phase noise has a severe effect on self-interference cancellation in both of the considered cases, and that by using the common oscillator in upconversion and downconversion results in clearly lower residual self-interference levels. The results also show that it is in general vital to use high quality oscillators in full-duplex transceivers, or have some means for phase noise estimation and mitigation in order to suppress its effects. One of the main findings is that in practical scenarios the subcarrier-wise phase-noise spread of the multipath components of the self-interference channel causes most of the residual phase-noise effect when high amounts of self-interference cancellation is desired."
      },
      {
        "node_idx": 160265,
        "score_0_10": 10,
        "title": "full duplex transceiver system calculations analysis of adc and linearity challenges",
        "abstract": "Despite the intensive recent research on wireless single-channel full-duplex communications, relatively little is known about the transceiver chain nonidealities of full-duplex devices. In this paper, the effect of nonlinear distortion occurring in the transmitter power amplifier (PA) and the receiver chain is analyzed, beside the dynamic range requirements of analog-to-digital converters (ADCs). This is done with detailed system calculations, which combine the properties of the individual electronics components to jointly model the complete transceiver chain, including self-interference cancellation. They also quantify the decrease in the dynamic range for the signal of interest caused by self-interference at the analog-to-digital interface. Using these system calculations, we provide comprehensive numerical results for typical transceiver parameters. The analytical results are also confirmed with full waveform simulations. We observe that the nonlinear distortion produced by the transmitter PA is a significant issue in a full-duplex transceiver and, when using cheaper and less linear components, also the receiver chain nonlinearities become considerable. It is also shown that, with digitally intensive self-interference cancellation, the quantization noise of the ADCs is another significant problem."
      },
      {
        "node_idx": 164653,
        "score_0_10": 9,
        "title": "blind receive beamforming for autonomous grant free high overloading multiple access",
        "abstract": "Massive number of internet of things (IoT) devices are expected to simultaneously connect to the mMTC and beyond future generations of wireless network, posing severe challenge to aspects such as RACH procedure, user equipment detection and channel estimation. Although spatial combining has provided significant gains in conventional grant-based transmission, this technique is stuck in dilemma when it comes to autonomous grant-free transmission tailored for IoT use cases. To address this, blind spatial combining and its incorporation in the data-only MUD are elaborated in this paper answering to both the academic and industry's concern in the overloading potential of autonomous grant-free (AGF) transmission. Blind spatial combining could be interpreted as blind receive beamforming heuristically. Simulation results show that the blind spatial combining enhanced data-only MUD performance for AGF transmission is rather impressive."
      },
      {
        "node_idx": 157681,
        "score_0_10": 9,
        "title": "degrees of freedom of time correlated miso broadcast channel with delayed csit",
        "abstract": "We consider the time correlated multiple-input single-output (MISO) broadcast channel where the transmitter has imperfect knowledge of the current channel state, in addition to delayed channel state information. By representing the quality of the current channel state information as P-\u03b1 for the signal-to-noise ratio P and some constant \u03b1 \u2265 0, we characterize the optimal degrees of freedom region for this more general two-user MISO broadcast correlated channel. The essential ingredients of the proposed scheme lie in the quantization and multicast of the overheard interferences, while broadcasting new private messages. Our proposed scheme smoothly bridges between the scheme recently proposed by Maddah-Ali and Tse with no current state information and a simple zero-forcing beamforming with perfect current state information."
      },
      {
        "node_idx": 131128,
        "score_0_10": 9,
        "title": "design and characterization of a full duplex multiantenna system for wifi networks",
        "abstract": "In this paper, we present an experiment- and simulation-based study to evaluate the use of full duplex (FD) as a potential mode in practical IEEE 802.11 networks. To enable the study, we designed a 20-MHz multiantenna orthogonal frequency-division-multiplexing (OFDM) FD physical layer and an FD media access control (MAC) protocol, which is backward compatible with current 802.11. Our extensive over-the-air experiments, simulations, and analysis demonstrate the following two results. First, the use of multiple antennas at the physical layer leads to a higher ergodic throughput than its hardware-equivalent multiantenna half-duplex (HD) counterparts for SNRs above the median SNR encountered in practical WiFi deployments. Second, the proposed MAC translates the physical layer rate gain into near doubling of throughput for multinode single-AP networks. The two results allow us to conclude that there are potentially significant benefits gained from including an FD mode in future WiFi standards."
      },
      {
        "node_idx": 41227,
        "score_0_10": 9,
        "title": "all digital self interference cancellation technique for full duplex systems",
        "abstract": "Full-duplex systems are expected to double the spectral efficiency compared to conventional half-duplex systems if the self-interference signal can be significantly mitigated. Digital cancellation is one of the lowest complexity self-interference cancellation techniques in full-duplex systems. However, its mitigation capability is very limited, mainly due to transmitter and receiver circuit's impairments (e.g., phase noise, nonlinear distortion, and quantization noise). In this paper, we propose a novel digital self-interference cancellation technique for full-duplex systems. The proposed technique is shown to significantly mitigate the self-interference signal as well as the associated transmitter and receiver impairments, more specifically, transceiver nonlinearities and phase noise. In the proposed technique, an auxiliary receiver chain is used to obtain a digital-domain copy of the transmitted Radio Frequency (RF) self-interference signal. The self-interference copy is then used in the digital-domain to cancel out both the self-interference signal and the associated transmitter impairments. Furthermore, to alleviate the receiver phase noise effect, a common oscillator is shared between the auxiliary and ordinary receiver chains. A thorough analytical and numerical analysis for the effect of the transmitter and receiver impairments on the cancellation capability of the proposed technique is presented. Finally, the overall performance is numerically investigated showing that using the proposed technique, the self-interference signal could be mitigated to    $\\sim$  3 dB higher than the receiver noise floor, which results in up to 76% rate improvement compared to conventional half-duplex systems at 20 dBm transmit power values."
      },
      {
        "node_idx": 47313,
        "score_0_10": 9,
        "title": "optimal use of current and outdated channel state information degrees of freedom of the miso bc with mixed csit",
        "abstract": "We consider a multiple-input-single-output (MISO) broadcast channel with mixed channel state information at the transmitter (CSIT) that consists of imperfect current CSIT and perfect outdated CSIT. Recent work by Kobayashi et al. presented a scheme which exploits both imperfect current CSIT and perfect outdated CSIT and achieves higher degrees of freedom (DoF) than possible with only imperfect current CSIT or only outdated CSIT individually. In this work, we further improve the achievable DoF in this setting by incorporating additional private messages, and provide a tight information theoretic DoF outer bound, thereby identifying the DoF optimal use of mixed CSIT. The new result is stronger even in the original setting of only delayed CSIT, because it allows us to remove the restricting assumption of statistically equivalent fading for all users."
      },
      {
        "node_idx": 51426,
        "score_0_10": 9,
        "title": "learning transferable features with deep adaptation networks",
        "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks."
      }
    ]
  }
}